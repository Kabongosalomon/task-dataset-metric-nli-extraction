<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PARE: Part Attention Regressor for 3D Human Body Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Hao</forename><forename type="middle">P</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
							<email>black@tue.mpg.deotmar.hilliges@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PARE: Part Attention Regressor for 3D Human Body Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mean 3D joint error (mm) <ref type="figure" target="#fig_10">Figure 1</ref>: Occlusion Sensitivity Analysis. Given an input image (a), a small occluding patch (shown in gray) causes SPIN <ref type="bibr" target="#b28">[29]</ref> to fail (b,c), whereas our method (PARE) (e,f) is robust to the occluder. Sub-figures on the right show the sensitivity of SPIN (d) and PARE (g) to an occluding patch (the size of the white squares) centered at every point in the image. Warmer colors mean higher average joint error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Despite significant progress, we show that state of the art 3D human pose and shape estimation methods remain sensitive to partial occlusion and can produce dramatically wrong predictions although much of the body is observable. To address this, we introduce a soft attention mechanism, called the Part Attention REgressor (PARE), that learns to predict body-part-guided attention masks. We observe that state-of-the-art methods rely on global feature representations, making them sensitive to even small occlusions. In contrast, PARE's part-guided attention mechanism overcomes these issues by exploiting information about the visibility of individual body parts while leveraging information from neighboring body-parts to predict occluded parts. We show qualitatively that PARE learns sensible attention masks, and quantitative evaluation confirms that PARE achieves more accurate and robust reconstruction results than existing approaches on both occlusion-specific and standard benchmarks. The code and data are available for research purposes at https://pare.is.tue.mpg.de/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Regressing 3D human pose and shape (HPS) directly from RGB images has many applications in robotics, computer graphics, AR/VR and beyond. The task is to take a single image <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41]</ref> or video sequence <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref> as input and to regress the parameters of a human body model such as SMPL <ref type="bibr" target="#b32">[33]</ref> as output. Powered by deep CNNs, this task has seen rapid progress <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41]</ref>. However, in fully in-the-wild settings, people often appear under occlusion either due to self-overlapping body-parts, due to closerange interaction with other people or due to occluding objects such as furniture or other scene content. While pose estimation under occlusion has been treated in the literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr">61]</ref>, we highlight that this issue is particularly important in the context of direct regression methods. Such methods use all the pixels in the input to predict a single set of pose and shape parameters. Thus their pose estimates are particularly sensitive to even small perturbations in the observations of the body and its parts.</p><p>In this paper, we apply a visualization technique <ref type="bibr">[60]</ref> for occlusion sensitivity analysis that yields insights into when and why such methods fail. This indicates that, for state-ofthe-art (SOTA) methods, relatively small occlusions, even of only a single joint, can lead to entirely implausible pose predictions. This is illustrated in <ref type="figure" target="#fig_10">Fig. 1</ref>, where we slide an occluder over the image, regress body pose, and compute the average 3D joint error with respect to ground truth. The heatmaps in <ref type="figure" target="#fig_10">Fig. 1 (d,g)</ref> illustrate a method's sensitivity to a square occluder centered at each pixel location (shown in white). The visualization reveals that methods like SPIN <ref type="bibr" target="#b40">[41]</ref> are highly sensitive to localized part occlusion. To address this issue, we propose a method, based on a novel part-guided attention mechanism, making direct regression approaches more robust to occlusion.</p><p>The proposed method is called Part Attention REgressor (PARE). It has two tasks: the primary one is learning to regress 3D body parameters in an end-to-end fashion, and the auxiliary task is learning attention weights per body part. Each task has its own pixel-aligned feature extraction branch. We guide the attention branch with part segmentation labels in the early stages of training and continue without them for the later stages, thus we call it body-partdriven attention. Our key insight is that, to be robust to occlusions, the network should leverage pixel-aligned image features of visible parts to reason about occluded parts.</p><p>Given the success of attention-based methods on other tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr">57]</ref>, we exploit insights gained from the occlusion sensitivity analysis to focus attention on body parts. Therefore, we supervise the attention mask with part segmentations, but then train end-to-end with pose supervision only, allowing the attention mechanism to leverage all useful information from the body and the surrounding pixels. This gives the network freedom to attend to regions it finds informative in an unsupervised way. As a result, PARE learns to rely on visible parts of the body to improve robustness to occluded parts and overall performance on 3D pose estimation ( <ref type="figure" target="#fig_10">Fig. 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e-f).</head><p>To quantitatively evaluate the performance of PARE, we perform experiments on the 3DPW <ref type="bibr" target="#b53">[54]</ref>, 3DOH [61], and 3DPW-OCC <ref type="bibr" target="#b53">[54]</ref> datasets. The results show that PARE yields consistently lower error than the state-of-the-art for both occlusion and non-occlusion cases.</p><p>In summary, our key contributions are: (1) We apply a visualization technique [60] to study how local part occlusion can influence global pose; we call this occlusion sensitivity analysis. (2) This analysis motivates a novel bodypart-driven attention framework for 3D HPS regression that leverages pixel-aligned localized features to regress body pose and shape. (3) The network uses part visibility cues to reason about occluded joints by aggregating features from the attended regions, and by doing so, achieves robustness to occlusions. (4) We achieve SOTA results on a 3D pose estimation benchmark featuring occluded bodies, as well as a standard benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We focus on 3D human shape and pose estimation from RGB images and discuss how previous approaches handle occlusions in various scenarios, e.g. self occlusion, camera frame occlusion, and scene object occlusion.</p><p>3D pose and shape from a single image. In estimating human shape and pose, many methods output the parameters of 3D human body models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref>. Initial work predicts the 3D body using keypoints and silhouettes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b47">48]</ref>. These approaches are fragile, need manual input, use additional data, e.g. multi-view images, or do not generalize well to in-the-wild images. SMPLify <ref type="bibr" target="#b6">[7]</ref> was the first automated method to fit the SMPL model to the output of a 2D keypoint detector <ref type="bibr" target="#b41">[42]</ref>. Lassner et al. <ref type="bibr" target="#b30">[31]</ref> employ silhouettes together with keypoints during fitting. In contrast, deep neural networks regress SMPL parameters directly from pixels <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>. In order to deal with the lack of in-the-wild 3D ground-truth, methods use a 2D keypoint re-projection loss as weak supervision <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>, use intermediate 2D representations, e.g. body/part segmentation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr">59]</ref>, 2D sparse keypoints <ref type="bibr" target="#b46">[47,</ref><ref type="bibr">59]</ref>, or leverage a human in the loop <ref type="bibr" target="#b30">[31]</ref>. Note that the use of part segmentation in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr">59</ref>] is very different from our approach, in which part segmentations are used to facilitate soft attention. Kolotouros et al. <ref type="bibr" target="#b28">[29]</ref> combine HMR <ref type="bibr" target="#b23">[24]</ref> and SMPLify <ref type="bibr" target="#b6">[7]</ref> in a training loop. At each step, HMR initializes SMPLify, which fits the body model to 2D joints, resulting in better supervision for the network. The above methods are typically sensitive to occlusion.</p><p>Implicit occlusion handling (data augmentation). Ideally, the regressed 3D body should be the same with or without occlusion. Current SOTA pose and shape estimation methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref> directly encode the entire input region as one CNN feature after global average pooling, followed by body model parameter regression. The lack of pixelaligned structure makes it hard for networks to explicitly reason about the locations and visibility of body parts. A common way to achieve robustness to occlusion in these frameworks is through data augmentation. For example, frame occlusion is often simulated by cropping <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b43">44]</ref>, whereas object occlusion is approximated by overlaying object patches on the image <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b44">45]</ref>. Instead of applying augmentation to input images, Cheng et al. <ref type="bibr" target="#b7">[8]</ref> apply augmentations to heatmaps that contain richer semantic information and hence occlusions can be simulated in a more intelligent way. While helpful, these synthetic occlusions do not fully capture the complexity of occlusions in realistic images, nor do they provide insight into how to improve the network architecture to be inherently more robust to occlusion.</p><p>Explicit occlusion handling. To reason more explicitly about occlusions, previous work exploits visibility information. For example, Cheng et al. <ref type="bibr" target="#b8">[9]</ref>   bility information is obtained by approximating the human body as a set of cylinders, which is not realistic and only handles self occlusion. Wang et al. <ref type="bibr" target="#b55">[56]</ref> learn to predict occlusion labels to zero out occluded keypoints before applying temporal convolution over a sequence of 2D keypoints.</p><p>Person-person occlusion is particularly common and challenging. For multi-person regression, Jiang et al. <ref type="bibr" target="#b20">[21]</ref> use an interpenetration loss to avoid collision and an ordinal loss to resolve depth ambiguity. <ref type="bibr">Sun</ref>  Zhang et al.</p><p>[61] leverage saliency masks as visibility information to gain robustness to scene/object occlusions. Human meshes are parameterized by UV maps where each pixel stores the 3D location of a vertex, and occlusions are cast as an image-inpainting problem. The requirement of accurate saliency maps limits the performance on in-thewild images. Furthermore, UV-coordinates can result in mesh artifacts, as shown in Sup. Mat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Occlusion Sensitivity Analysis</head><p>To extract features from the input image region I, current direct regression approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29]</ref> use a ResNet-50 <ref type="bibr" target="#b16">[17]</ref> backbone and take the features after global average pooling (GAP), followed by an MLP that regresses and refines the parameters iteratively. In this section, we investigate the impact of occlusions on this type of architecture. Our analysis is inspired by Zeiler et al.</p><p>[60] who systematically cover different portions of the image with a gray square to analyze how feature maps and classifier output changes. In contrast, we slide a gray occlusion patch over the image and regress body poses using SPIN <ref type="bibr" target="#b28">[29]</ref>. Instead of computing a classification score as in [60], we measure the per-  <ref type="figure">Figure 3</ref>: Occlusion sensitivity meshes for SPIN <ref type="bibr" target="#b28">[29]</ref>.</p><p>joint Euclidean distance between ground truth and predicted joints. We create an error heatmap, in which each pixel indicates how much error the model creates for joint j when the occluder is centered on this pixel. In addition to per-joint heatmaps, we compute an aggregate occlusion sensitivity map, that shows how the average joint error is influenced by an occlusion; this is visualized in <ref type="figure" target="#fig_10">Fig. 1(d)</ref> and in greater detail in the Sup. Mat. The per-joint error heatmaps for SPIN are visualized in <ref type="figure" target="#fig_0">Fig. 2</ref> for a sample image from the 3DPW dataset <ref type="bibr" target="#b53">[54]</ref>. Each sub-image corresponds to a particular joint and hot regions are locations where occlusion causes high error in this joint. This visualization allows us to make several observations. (1) Errors are low in the background and high on the body. This shows that SPIN has learned to attend to meaningful regions. (2) Joints visible in the original image have high errors when they are occluded by the square, as expected. <ref type="formula">(3)</ref> For joints that are naturally occluded, the network relies on other regions to reason about the occluded poses. For example, in the top row of <ref type="figure" target="#fig_0">Fig. 2</ref>, we observe high errors for the left/right ankles (which are occluded) when we occlude the thigh region. Since the network has no image features for the occluded parts, it must look elsewhere in the image for evidence. (4) Such dependencies happen not only between neighboring parts; occlusion can have long-range effects (e.g. occluding the pelvis causes errors in the head).</p><p>We further overlay the estimated body on the heatmap </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>Given the observations above, PARE is designed with the following insights. First, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, SOTA networks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref> learn to attend to meaningful regions implicitly, despite limited spatial information after global average pooling. To better understand whether body parts are visible or not, and to know if their locations are occluded, PARE exploits a pixel-aligned structure, where each pixel corresponds to a region in the image and stores a pixel-level representation, namely, a feature volume. Second, since estimating attention weights and learning end-to-end trainable features for 3D poses are two different tasks, PARE is equipped with two feature volumes: one from the 2D part branch that estimates attention weights and one from the 3D body branch that performs SMPL parameter regression. Finally, to model the body part dependencies observed above, PARE exploits part segmentations as soft attention masks to adjust the contribution of each feature in the 3D body branch differently for each joint.</p><p>Preliminaries: Body Model. SMPL <ref type="bibr" target="#b32">[33]</ref> represents the body pose and shape by ?, which consists of the pose ? ? R 72 and shape ? ? R 10 parameters. Here we use the gender-neutral shape model as in previous work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29]</ref>. Given these parameters, the SMPL model is a differentiable function that outputs a posed 3D mesh M(?, ?) ? R 6890?3 . The 3D joint locations J 3D = W M ? R J?3 , J = 24, are computed with a pretrained linear regressor W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model Architecture and Losses</head><p>The overall framework of PARE is depicted in <ref type="figure" target="#fig_3">Fig. 4</ref>. Our architecture works as follows: given an image I, we first run a CNN backbone to extract volumetric features, e.g. before the global average pooling layer for ResNet-50, followed by two separate feature extraction branches to obtain two volumetric image features. We denote the 2D part branch as P ? R H?W ?(J+1) , modelling J part attention and 1 background masks, where H and W are the height and width of the feature volume and each pixel (h, w) stores the likelihood of belonging to a body part j. The other branch, denoted by F ? R H?W ?C , is used for 3D body parameter estimation. It has the same spatial dimensions H ? W as P but a different number of channels, C.</p><p>Let P j ? R H?W and F c ? R H?W denote the j-th and c-th channel of P and F , respectively, and let F ? R J?C represent the final feature tensor. Each element in F c contributes proportionally to F according to the corresponding elements in P j after spatial softmax normalization ?. Formally, the element at location (j, c) in F is computed as:</p><formula xml:id="formula_0">F j,c = h,w ?(P j ) F c ,<label>(1)</label></formula><p>where is the Hadamard product. In other words, we use ?(P j ) as a soft attention mask to aggregate features in F c . This operation can be efficiently implemented as a dot product similar to existing attention implementations: F = ?(P ) F , whereP ? R HW ?J andF ? R HW ?C denote the reshaped P (omitting the background mask) and F respectively. This attention operation suggests that if a particular pixel has a higher attention weight, its corresponding feature contributes more to the final representation F . We supervise the 2D part branch P with ground-truth segmentation labels, which helps the attention maps of visible parts converge to the corresponding regions. For occluded parts, however, this encourages 0 attention weights for all pixels in P j because they do not exist in the ground-truth segmentation labels. An attention map with all 0 weights is undesirable and, in practice, also impossible since the spatial softmax ensures that all elements sum to 1. Therefore, we adopt a hybrid approach that supervises the 2D part branch only for the initial stage and continues training without any supervision. This allows the network to attend to other regions to estimate the poses of an occluded joint.</p><p>We take the full feature tensor F to regress body shape ? and a weak-perspective camera model with scale and translation parameters [s, t], t ? R 2 , while each row, F j , is also sent to different MLPs to predict the rotation of each part, ? j , parameterized as a 6D vector following <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref> </p><formula xml:id="formula_1">1 .</formula><p>Overall, our total loss is:</p><formula xml:id="formula_2">L = ? 3D L 3D + ? 2D L 2D + ? SM P L L SM P L + ? P L P ,<label>(2)</label></formula><p>where each term is calculated as:</p><formula xml:id="formula_3">L 3D = J 3D ?? 3D 2 F , L 2D = J 2D ?? 2D 2 F , L SMPL = ? ?? 2 2 , L P = 1 HW h,w CrossEntropy ?(P h,w ),P h,w ,</formula><p>wherex represents the ground truth for the corresponding variable x. To compute the 2D keypoint loss, we need the SMPL 3D joint locations J 3D (?, ?) = W M(?, ?), which are computed from the body vertices with a pretrained linear regressor W . With the inferred weak-perspective camera, we compute the 2D projection of the 3D joints J 3D ,</p><formula xml:id="formula_4">as J 2D ? R J?2 = s?(RJ 3D ) + t, where R ? SO(3)</formula><p>is the camera rotation matrix and ? is the orthographic projection. ? is a scalar coefficient to balance the loss terms. Let P h,w ? R 1?1?(J+1) denote the fiber of P at the location (h, w), andP h,w ? {0, 1} (J+1) denotes the ground-truth part label at the same location, expressed as a one-hot vector. The part segmentation loss L P is the cross-entropy loss between P h,w after softmax andP h,w , averaged over H?W elements. Note that this softmax normalizes along the fiber P h,w while the one in Eq. 1 normalizes over the slice P j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>As mentioned above, the body-part label supervision via L P is applied on the attention tensor P only in the initial stages of training. It is later removed by setting ? P to zero, turning the attention mechanism into an unsupervised pure soft-attention. The absence of body-parts due to occlusion is the main motivation for this training scheme. Setting ? P  <ref type="figure">Figure 6</ref>: Per joint occlusion sensitivity analysis of three different methods: SPIN <ref type="bibr" target="#b28">[29]</ref>, HMR-EFT <ref type="bibr" target="#b22">[23]</ref> (trained with occlusion augmentation), and PARE. PARE is consistently more robust to occlusion.</p><p>to zero allows the attention mechanism to also consider pixels beyond the body itself. Hence, the final attention maps do not necessarily (and often do not) resemble body part segmentations, as shown later in <ref type="figure">Fig. 7</ref> and Sup. Mat. If a body part is visible, it focuses on that part directly; if it is occluded, the attention is free to leverage other informative regions in the image. In Sec. 5, we analyze how the accuracy of part segmentation impacts body reconstruction.</p><p>We evaluate both ResNet-50 <ref type="bibr" target="#b16">[17]</ref> and HRNet-W32 <ref type="bibr" target="#b49">[50]</ref> networks as the backbone. Since ResNet-50 is widely used in other SOTA methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref>, we choose it as the default backbone for most of the experiments unless stated otherwise. We extract the 7 ? 7 ? 2048 feature volumes before global average pooling. For the 2D and 3D branches, we use three 2? upsampling followed by 3?3 convolutional layers applied with batch-norm and ReLU. The number of conv kernels is 256. For HRNet-W32, since it already provides volumetric features with a higher resolution, we only use two 3 ? 3 convolutional layers applied with batch-norm and ReLU as the 2D and 3D branches.</p><p>To obtain part attention maps, we apply J + 1 1 ? 1 convolutional kernels to 2D part features to reduce the channel dimension. After obtaining the J ? C final feature F , we use separate linear layers to predict each SMPL joint rotation ? j . We regress shape and camera parameters from the flattened F vector. We use a fixed image size of 224 ? 224 for all experiments. The Adam optimizer with a learning rate of 5 ? 10 ?5 and batch size 64 is used to optimize our model. PARE is end-to-end trainable in a single stage, unlike recent multi-stage methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr">59]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Training. We train PARE on COCO <ref type="bibr" target="#b31">[32]</ref>, MPII <ref type="bibr" target="#b1">[2]</ref>, LSPET <ref type="bibr" target="#b21">[22]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b36">[37]</ref>, and Human3.6M <ref type="bibr" target="#b19">[20]</ref> datasets. More details about these datasets are provided in Sup. Mat. Pseudo-ground-truth SMPL annotations for inthe-wild datasets are provided by EFT <ref type="bibr" target="#b22">[23]</ref>. The part segmentation labels are obtained through rendering segmented SMPL meshes, as visualized in <ref type="figure" target="#fig_3">Fig. 4</ref>. We use 24 parts corresponding to 24 SMPL joints. See Sup. Mat. for samples of part segmentation labels. We used the PyTorch reimplementation <ref type="bibr" target="#b27">[28]</ref> of Neural Mesh Renderer <ref type="bibr" target="#b25">[26]</ref> to render the parts. For samples without a part segmentation label, we do not supervise the 2D branch. For the ablation experiments, we train PARE and our baselines on COCO for 175K steps and evaluate on 3DPW and 3DPW-OCC datasets. We then incorporate all the training data to compare PARE to previous SOTA methods. This pretraining strategy accelerates convergence and reduces the overall training time. It takes about 72 hours to train PARE until convergence on an Nvidia RTX2080Ti GPU.</p><p>To increase robustness to occlusion, we use common occlusion augmentation techniques; i.e. synthetic occlusion (SynthOcc) <ref type="bibr" target="#b44">[45]</ref> and random crop (RandCrop) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b43">44]</ref>. All PARE and baseline HMR-EFT models are trained with Syn-thOcc augmentation unless stated otherwise, e.g.  methods. We report PARE results with two different backbones: ResNet-50 and HRNet-W32. PARE improves the PA-MPJPE performance by 10% compared to HMR-EFT <ref type="bibr" target="#b22">[23]</ref>, one of the best-performing recent methods. <ref type="table" target="#tab_5">Table 2</ref> demonstrates the performance of PARE on occlusion-specific datasets. Here Zhang et al.</p><p>[61], HMR-EFT <ref type="bibr" target="#b22">[23]</ref>, and PARE are trained with COCO, Human3.6M, and 3DOH for a fair comparison. We report the SPIN results for reference. HMR-EFT is the fair alternative to SPIN, since SPIN uses HMR as the architecture. PARE consistently improves the performance on these occlusion datasets. Although HMR-EFT is trained with exactly the same augmentation and data as PARE, it performs worse.</p><p>We also quantify our occlusion sensitivity analysis.  <ref type="figure">Figure 6</ref> shows the per-joint breakdown of the mean 3D error from the occlusion sensitivity analysis for three different methods, SPIN, HMR-EFT, and PARE. Here, we retrain HMR-EFT using SynthOcc for a fair comparison. Again, PARE improves the occlusion robustness of all joints. Qualitative comparison. We qualitatively compare SPIN, HMR-EFT, and PARE in <ref type="figure" target="#fig_7">Fig. 8</ref>. Even though occlusion augmentation improves robustness to occlusion as seen in the HMR-EFT results, it is not sufficient on its own. PARE, with its attention mechanism, performs well even in challenging occlusion scenarios. More qualitative samples, including failure cases, are provided in Sup. Mat. Does part attention help? <ref type="table" target="#tab_7">Table 3</ref> summarizes our ablation experiments that explore the concept of part attention. First, we compare our results with Neural Body Fitting <ref type="bibr" target="#b38">[39]</ref> trained with identical settings to ours. NBF <ref type="bibr" target="#b38">[39]</ref> can be seen as a straightforward combination of part segmentation and human body regression. <ref type="table" target="#tab_7">Table 3</ref> shows that NBF's two-stage approach is outperformed even by the HMR-EFT baseline. Subsequently, we compare different types of supervision for the 2D part branch P and sampling methods to obtain final features F from F . "Unsup" means P is not supervised. Inspired by HoloPose <ref type="bibr" target="#b15">[16]</ref>, we first supervise the 2D branch with keypoints and pool the 3D features via bilin-  <ref type="table" target="#tab_7">Table 3</ref>: Exploring part attention. The "P Supervision" column shows the type of supervision for the 2D part branch P . "F Sampling" shows the type of feature sampling method for F . All methods are trained on COCO-EFT with a ResNet-50 backbone.</p><p>ear sampling <ref type="table" target="#tab_7">(Table 3</ref>-a). Even though this gives lower error than HMR, the improvement is not significant. Intuitively, sparse keypoints do not cover enough spatial area to be able to reason about body parts. Because the 2D branch predicts Gaussian heatmaps, which cover a larger spatial area than discrete keypoints, we explore soft attention instead of pooling to have a larger effective receptive field ( <ref type="table" target="#tab_7">Table 3-</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b).</head><p>In doing so, however, we do not leverage the full potential of soft attention, which can learn which regions to attend to implicitly from the data. So, we remove supervision for the 2D branch to see if soft attention alone can work as well as explicit supervision <ref type="table" target="#tab_7">(Table 3</ref>-c). Upon visualizing the resulting attention maps, we find that they are not focused on the body parts. To induce more structure, we supervise the 2D branch with part segmentation labels ( <ref type="table" target="#tab_7">Table 3-d)</ref>. This approach works significantly better than the above attempts. There is a remaining caveat, however: by supervising with a segmentation loss, we constrain the attention map to the parts only, whereas a pure soft attention has the potential to attend to any region it finds informative. Consequently, we train with mixed supervision, applying the part segmentation loss for around 125K steps, then continuing to train without supervision (   the "best of both worlds" and the lowest error. We also experiment with part segmentation and pooling to explore the effect of soft-attention <ref type="table" target="#tab_7">(Table 3-f)</ref>. Finally, to demonstrate the statistical significance, we performed a two-sided t-test for all experiments in <ref type="table" target="#tab_7">Table 3</ref>; specifically p&lt;0.01 for rows (c) vs. In addition to joint errors, we measure the mean part segmentation IoU (intersection over union) to better understand how part segmentation and the final pose and shape estimation interact when we do not use part supervision. Mean IoU on the 3DPW test set is 1%, 85%, 74% for (c) unsup, (d) parts, and (e) parts/unsup methods respectively. Lower segmentation accuracy does not hurt the body reconstruction. We provide further body-part segmentation results during different stages of the training in Sup. Mat. <ref type="figure">Figure 7</ref> visualizes these attention maps on sample images. Part attention learns to attend to body parts or image regions as needed to estimate body shape and pose. Occlusion Augmentation. We report the effect of occlu-  <ref type="table" target="#tab_4">Table 4</ref>. SynthOcc improves the performance on both 3DPW and 3DPW-OCC over vanilla training. Applying RandCrop right at the beginning of the training hurts the performance. Therefore, we start applying crop augmentation after 175K training steps. Between 30%-50% of a bounding box is cropped with the probability of 0.3. Even though crop augmentation does not improve performance on 3DPW and 3DPW-OCC, we find it useful for true in-the-wild images, which often contain significant frame occlusion. See Sup. Mat. for more examples. Effect of CNN backbones. As shown in <ref type="table" target="#tab_9">Table 5</ref>, HRNet-W32, which produces effective high-resolution representations, performs better than ResNet-50. PARE provides consistent improvements over HMR-EFT with both backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We present a novel Part Attention Regressor, PARE, which regresses 3D human pose and shape by exploiting information about the visibility of individual body parts, and thus gaining robustness to occlusion. PARE is based on the insights gleaned from our occlusion sensitivity analysis. In particular, we observe dependencies between body parts and argue that the network should rely on visible parts to improve predictions for occluded parts and, hence, the overall performance of 3D pose estimation. Our novel bodypart-driven attention mechanism captures such dependencies, using soft attention guided by regressed body part segmentation masks. The network learns to use part segmentations as visibility cues to reason about occluded joints and aggregating features from the attended regions. This improves robustness to occlusions of different types: scene, self, and frame occlusion. Detailed ablation studies show how each choice contributes to our state-of-the-art performance on benchmark datasets.  <ref type="figure">Figure 9</ref>: Body part segmentation labels used for the 2D part branch. For each joint in the SMPL kinematic tree, we have a body part label. Correspondences between joints (right) and body part labels (left) are shown in this figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>The Supplementary Material consists of this document and a video. They include acknowledgement, disclosure, additional information and visualizations of our method and results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Methods</head><p>Implementation Details. In all our experiments, we use the weights pretrained on MPII [2] for a 2D pose estimation task to initialize both ResNet-50 and HRNet-W32, because we observe slower convergence with ImageNet pretrained weights. For <ref type="table" target="#tab_7">Table 3</ref>-5 in the main paper, we train PARE and our baselines on COCO for 175K steps and evaluate on 3DPW and 3DPW-OCC datasets. We then include all the training data for the SOTA experiment in <ref type="table" target="#tab_3">Table 1</ref> of the main paper. For <ref type="table" target="#tab_5">Table 2</ref>, we use the training data of [61] to align the experiment settings.</p><p>Loss. We use different weight coefficients ? for each term in the loss function. They are ? 3D = 300, ? 2D = 300, ? SM P L = 60, ? P = 60.</p><p>Body Part Segmentation labels. Since we have SMPL annotations for most of the samples in our datasets, we do not need additional body part segmentation annotations. We directly use the SMPL annotations to obtain supervision. In <ref type="figure">Fig 9,</ref> we visualize this body part labels. For each joint in the SMPL kinematic tree, we have a corresponding body part label.</p><p>Occlusion augmentation. In <ref type="figure" target="#fig_9">Fig. 10</ref>, we demonstrate the results of synthetic occlusion and random crop augmentations on two sample images.</p><p>Runtime PARE is only 1 ms/image slower than HMR, with runtime of 14.8ms on a GTX2080Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Training Datasets</head><p>Our training datasets closely follow previous work, namely EFT <ref type="bibr" target="#b22">[23]</ref>, SPIN <ref type="bibr" target="#b28">[29]</ref>, and HMR <ref type="bibr" target="#b23">[24]</ref>. Here we provide the details for completeness. <ref type="bibr" target="#b36">[37]</ref> is a multi-view indoor 3D human pose estimation dataset. 3D annotations are captured via a commercial markerless mocap software, therefore it is less accurate than some of the 3D datasets e.g. Human3.6M <ref type="bibr" target="#b19">[20]</ref>. We use all of the training subjects S1 to S8 which makes 90K images in total. Human3.6M <ref type="bibr" target="#b19">[20]</ref> is an indoor, multi-view 3D human pose estimation dataset. Following previous methods, for training, we use 5 subjects (S1, S5, S6, S7, S8) which means 292K images. In-the-wild 2D datasets COCO <ref type="bibr" target="#b31">[32]</ref>, MPII <ref type="bibr" target="#b1">[2]</ref> and LSPET <ref type="bibr" target="#b21">[22]</ref> are in-the-wild 2D keypoint datasets. MPII has 14K, COCO has 75K, LSPET has 7K instances labeled with 2D keypoints. In addition to 2D keypoint annotations, we utilize the pseudo SMPL annotations provided by the EFT <ref type="bibr" target="#b22">[23]</ref>       Thus, we bring them to the same camera coordinate frame as PARE through Procrustes Alignment and overlay them on the input as shown in <ref type="figure" target="#fig_10">Fig. 13(c)</ref>. One clearly sees mesh artifacts (red ovals), which is common for non-parametric models. The requirement of accurate saliency maps certainly limits the performance of [61] on in-the-wild images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPI-INF-3DHP</head><p>Comparing to state-of-the-art Temporal Models. In <ref type="table" target="#tab_14">Table 6</ref>, we compare PARE to recent state-of-the-art video based models. To do so, we run a SOTA multi-object tracker and then run PARE independently on each frame of the tracklets, with no temporal smoothing. Even the SOTA video methods have access to extra temporal information, PARE outperforms them. We show some qualitative results of VIBE and PARE on some challenging images in <ref type="figure" target="#fig_3">Fig 14.</ref> Please see the supplemental video for a better visualization of the video results (starts at 05:21).</p><p>2D keypoint projection accuracy We evaluate the 2D keypoint accuracy of our method by projecting the 3D keypoints to the image space using the estimated camera parameters on 3DPW test set. Percentage of correct keypoints (PCK) is used as the evaluation metric. The results are reported in <ref type="table" target="#tab_15">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More on visualizing attention of networks</head><p>Two new visualizations are proposed in this work: (1) an occlusion sensitivity map/mesh and (2) a part attention map. We provide more examples and discussions for both visualizations. Please see the video for an animation of the sensitivity analysis, which more clearly illustrates the approach.</p><p>(a) <ref type="figure" target="#fig_10">Figure 11</ref>: Challenging scenarios where PARE fails to produce fairly good reconstructions.</p><formula xml:id="formula_5">(f) (c) (e) (g) (h) (g) (b) (d)</formula><p>Occlusion Sensitivity. There are many visualization techniques <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr">60,</ref><ref type="bibr">62]</ref> available to inspect what CNNs learn. We are, however, more interested in studying how perturbations in the input image affect the output rather than visualizing the internal filters learned by CNNs. We therefore follow the framework of [60] and replace the classification score with an error measure for body poses, as described in the main paper. We choose MPJPE as the error measure without Procrustes Alignment, because PA-MPJPE leads to artificially low error by aligning global orientations, which are a major source of error. This analysis is not limited to a particular network architecture so we also apply it on PARE and visualize the error maps together with those from SPIN <ref type="bibr" target="#b28">[29]</ref> in <ref type="figure" target="#fig_4">Fig. 15</ref>. Warmer colors correspond to higher MPJPEs w.r.t. ground truth when those pixels are occluded, suggesting that methods rely on the regions to estimate body poses. One clearly sees that PARE is more robust to localized part occlusion. Please see the video for animation (starts at 00:53).</p><p>Additionally, we also map the per-pixel error to the overlaying 3D vertex, and aggregate over the whole 3DPW dataset <ref type="bibr" target="#b53">[54]</ref>. In this way, we visualize the per-joint error on the SMPL template mesh, which we term the occlusion sensitivity mesh. <ref type="figure" target="#fig_10">Fig. 16</ref> shows the occlusion sensitivity mesh for four different joints and averaged over all joints from both SPIN and PARE. We again observe that SPIN is very sensitive to localized part occlusion. For example, occlusions of right arm or face regions result in high error for right wrist. On the other hand, occlusion sensitivity meshes of PARE have more consistent cold colors over the body, again confirming that it is more robust to partial occlusion.</p><p>Part Attention. We also visualize the estimated part attention P before softmax in <ref type="figure" target="#fig_10">Fig. 17</ref> for four sample images from 3DPW <ref type="bibr" target="#b53">[54]</ref>. When body parts are visible, the shapes of warm regions resemble part segmentation labels, which means the network focuses on body part regions (e.g. Left/Right Knee and Ankle in the third row). For naturally occluded body parts, the attended regions get wider, covering other parts and the scene. This suggests that PARE implicitly learns to reason about the visibility of body parts and leverages available information to solve the task. In particular, <ref type="figure" target="#fig_0">Fig. 12</ref> illustrates the progression of attention maps during training for two occluded parts Left/Right Ankles. We see that deactivating the part supervision helps attention maps to focus on more meaningful and explainable regions.</p><p>In addition to part attention maps, we also visualize the results as segmentation maps in <ref type="figure" target="#fig_7">Fig. 18</ref>. We visualize the results of two different models; (a) a model trained with  full part segmentation supervision, (b) a model trained with part segmentation initially and unsupervised for the final stages. Note that part segmentation IoU decreases significantly when we do not use part segmentation, however we see an increase in body reconstruction accuracy especially in the case of occlusion.</p><p>(a) SPIN (b) HMR-EFT (c) Zhang et al. <ref type="bibr" target="#b16">[17]</ref> (d) PARE Input <ref type="figure" target="#fig_10">Figure 13</ref>: Qualitative comparison. Here, we compare PARE with recent state-of-the-art methods i.e. SPIN <ref type="bibr" target="#b28">[29]</ref>, HMR-EFT <ref type="bibr" target="#b22">[23]</ref>, and Zhang et al. <ref type="bibr">[61]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Occlusion sensitivity analysis. Heatmaps illustrate the error of SPIN [29] in individual joints caused by an occluder placed at each image location. Image size: 224 ? 224; occluding patch: 40 ? 40. The title of each heatmap names the joint and notes the range of the 3D error in mm visualized in the heatmap. See Section 3 for analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>et al. [58] estimate all people in an image simultaneously, enabling their method to learn about person-person occlusion. While [58] learns features that are robust to person-person occlusion, PARE learns to focus attention on individual body parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>PARE model architecture. Given an input image, PARE extracts two pixel-level features P and F , which are fused by part attention (green box) leading to the final feature F for camera and SMPL body regression.to transfer the per-pixel error to visible vertices. We run this analysis over the complete 3DPW dataset, pool the pervertex error across the dataset and visualize the result on a SMPL body model, giving one occlusion sensitivity mesh per joint. For example,Fig. 3(a)shows that the left elbow is sensitive to occlusion of the face, the left shoulder and the left upper arm region. See Sup. Mat. for more examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Occlusion sensitivity mesh. Meshes visualize the (a) SPIN and (b) PARE average joint errors. Ankle Knee Hip Wrist Elbow Shoulder Head</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig- ure 5</head><label>5</label><figDesc>shows the average joint error of SPIN and PARE methods on the 3DPW test split. SPIN is quite sensitive to upper body occlusions, especially around the head and back. PARE is more robust to occlusions and yields lower error overall. See Sup. Mat. for the per-joint version of Fig. 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(d), (d) vs. (e), and (b) vs. (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative results on COCO (rows 1-4) and 3DPW (rows 5-6) datasets. From left to right: Input image, (a) SPIN [29] results, (b) HMR-EFT [23] results, (c) PARE results. sion augmentation techniques in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Body part segmentation (d) Synthetic occlusion aug. (e) Random crop aug. (f) SMPL after rand. crop aug.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Training samples after synthetic occlusion and random crop augmentations are applied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 1 :</head><label>1</label><figDesc>Attention map progression during training. Training continues with body-part supervision until step 125K (a-b) and without supervision until 200K (c-d). Final attention maps of occluded parts focus on parents which allow to sample more meaningful features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Attention map progression during training. Training uses body-part supervision only until step 125K (a-b). Note that the final attention maps for occluded parts (at 200K (c-d)) focus on visible parents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>1</head><label></label><figDesc>Max Planck Institute for Intelligent Systems, T?bingen, Germany 2 ETH Zurich {mkocabas,paul.huang,black}@tue.mpg.de otmar.hilliges@inf.ethz.ch</figDesc><table><row><cell>(a) Input Image</cell><cell>(b) SPIN [1]</cell><cell>(c) SPIN [2]</cell><cell>(d) SPIN Occlusion Sensitivity Heatmap</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>250</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>200</cell></row><row><cell></cell><cell>(e) PARE [1]</cell><cell>(f) PARE [2]</cell><cell>(g) PARE Occlusion Sensitivity Heatmap</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>150</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>3DPWMethodMPJPE ? PA-MPJPE ? PVE ?</figDesc><table><row><cell></cell><cell>HMMR [24]</cell><cell>116.5</cell><cell>72.6</cell><cell>-</cell></row><row><cell>temporal</cell><cell>Doersch et al. [12] Sun et al. [51] VIBE [27]</cell><cell>--93.5</cell><cell>74.7 69.5 56.5</cell><cell>--113.4</cell></row><row><cell></cell><cell>MEVA [35]</cell><cell>86.9</cell><cell>54.7</cell><cell>-</cell></row><row><cell>multi stage</cell><cell>Pose2Mesh [10] Zanfir et al. [59] I2L-MeshNet [38] LearnedGD [49]</cell><cell>89.2 90.0 93.2 -</cell><cell>58.9 57.1 58.6 56.4</cell><cell>----</cell></row><row><cell></cell><cell>HMR [24]</cell><cell>130.0</cell><cell>76.7</cell><cell></cell></row><row><cell>single stage</cell><cell>CMR [30] SPIN [29] HMR-EFT [23] PARE (R50)</cell><cell>-96.9 -82.9</cell><cell>70.2 59.2 54.2 52.3</cell><cell>-135.1 -99.7</cell></row><row><cell></cell><cell>PARE (HRNet-W32)</cell><cell>82.0</cell><cell>50.9</cell><cell>97.9</cell></row><row><cell></cell><cell>PARE (HRNet-W32) w. 3DPW</cell><cell>74.5</cell><cell>46.5</cell><cell>88.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Evaluation on the 3DPW dataset. The units for mean joint and vertex errors are in mm. PARE models out- perform temporal, multi-stage, and single-stage state-of- the-art methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Evaluation. The 3DPW [54] test split, 3DPW-OCC [54, 61], and 3DOH [61] datasets are used for evaluation. We report Procrustes-aligned mean per joint position error (PA-MPJPE) and mean per joint position error (MPJPE) in mm. For 3DPW we also report per vertex error (PVE) in mm. Comparison to the state-of-the-art. Table 1 compares PARE with previous single-RGB-image HPS estimation</figDesc><table><row><cell></cell><cell></cell><cell>3DPW-OCC</cell><cell></cell><cell></cell><cell>3DOH</cell></row><row><cell>Method</cell><cell>MPJPE ?</cell><cell>PA-MPJPE ?</cell><cell>PVE ?</cell><cell>MPJPE ?</cell><cell>PA-MPJPE ?</cell></row><row><cell>Zhang et al. [61]</cell><cell>-</cell><cell>72.2</cell><cell>-</cell><cell>-</cell><cell>58.5</cell></row><row><cell>SPIN [29]</cell><cell>95.6</cell><cell cols="2">60.8 121.6</cell><cell>104.3</cell><cell>68.3</cell></row><row><cell>HMR-EFT [23]</cell><cell>94.4</cell><cell cols="2">60.9 111.3</cell><cell>75.2</cell><cell>53.1</cell></row><row><cell>PARE (R50)</cell><cell>90.5</cell><cell cols="2">56.6 107.9</cell><cell>63.3</cell><cell>44.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Evaluation on occlusion datasets 3DPW-OCC, 3DOH. Here all methods except SPIN are trained with the same datasets, i.e. COCO, Human3.6M and 3DOH.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>PARE attention visualization. Attention maps predicted by the 2D part branch for different joints in image (a). For occluded joints like row 2 right hand, PARE learns to attend to larger, more distant, regions to glean information.</figDesc><table><row><cell>(a) Input Image</cell><cell>Hips</cell><cell></cell><cell>Left Knee</cell><cell></cell><cell>Right Knee</cell><cell>Left Ankle</cell><cell>Right Ankle</cell><cell>Neck</cell><cell>Left Shoulder</cell></row><row><cell>(b) PARE result</cell><cell cols="2">Right Shoulder</cell><cell>Left Arm</cell><cell></cell><cell>Right Arm</cell><cell>Left Elbow</cell><cell>Right Elbow</cell><cell>Left Hand</cell><cell>Right Hand</cell></row><row><cell cols="4">Figure 7: 3DPW</cell><cell cols="2">3DPW-OCC</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell>MPJPE ?</cell><cell>PA-MPJPE ?</cell><cell>MPJPE ?</cell><cell>PA-MPJPE ?</cell><cell></cell><cell></cell></row><row><cell></cell><cell>NBF [39]</cell><cell>100.4</cell><cell>63.2</cell><cell>103.5</cell><cell>70.4</cell><cell></cell><cell></cell></row><row><cell></cell><cell>HMR-EFT</cell><cell>99.0</cell><cell>59.9</cell><cell>97.9</cell><cell>64.7</cell><cell></cell><cell></cell></row><row><cell cols="2">P Supervision F Sampling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(a) Joints</cell><cell>Pooling</cell><cell>95.2</cell><cell>58.9</cell><cell>95.4</cell><cell>63.1</cell><cell></cell><cell></cell></row><row><cell>(b) Joints</cell><cell>Attention</cell><cell>95.3</cell><cell>58.8</cell><cell>98.9</cell><cell>63.9</cell><cell></cell><cell></cell></row><row><cell>(c) Unsup</cell><cell>Attention</cell><cell>94.8</cell><cell>57.9</cell><cell>95.9</cell><cell>62.7</cell><cell></cell><cell></cell></row><row><cell>(d) Parts</cell><cell>Attention</cell><cell>94.5</cell><cell>57.3</cell><cell>94.7</cell><cell>61.2</cell><cell></cell><cell></cell></row><row><cell cols="2">(e) Parts/Unsup Attention</cell><cell>93.4</cell><cell>57.1</cell><cell>93.9</cell><cell>61.6</cell><cell></cell><cell></cell></row><row><cell>(f) Parts</cell><cell>Pooling</cell><cell>97.9</cell><cell>59.1</cell><cell>99.8</cell><cell>64.8</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>3DPW</cell><cell>3DPW-OCC</cell></row></table><note>-e). This final version produces</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Ablation of different occlusion augmentation strategies. We demonstrate the effect of synthetic occlusion (SynthOcc) and random crop (RandCrop) augmentation on the final performance. All methods are trained on COCO-EFT with ResNet-50 as the backbone.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>3DPW</cell><cell cols="2">3DPW-OCC</cell></row><row><cell>Method</cell><cell></cell><cell>MPJPE ?</cell><cell>PA-MPJPE ?</cell><cell>MPJPE ?</cell><cell>PA-MPJPE ?</cell></row><row><cell cols="2">HMR-EFT ResNet-50</cell><cell>99.0</cell><cell>59.9</cell><cell>97.9</cell><cell>64.7</cell></row><row><cell>PARE</cell><cell>ResNet-50</cell><cell>93.4</cell><cell>57.1</cell><cell>93.9</cell><cell>61.6</cell></row><row><cell cols="2">HMR-EFT HRNet-W32</cell><cell>92.6</cell><cell>55.9</cell><cell>90.2</cell><cell>57.8</cell></row><row><cell>PARE</cell><cell>HRNet-W32</cell><cell>89.0</cell><cell>54.3</cell><cell>87.1</cell><cell>57.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Ablation of backbone architectures. All methods are trained on COCO-EFT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>[ 57 ]</head><label>57</label><figDesc>Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, pages 7794-7803, 2018. 2 [58] Sun Yu, Bao Qian, Liu Wu, Fu Yili, and Mei Tao. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European Conference on Computer Vision, 2014. 1, 2, 3 [61] Tianshu Zhang, Buzhen Huang, and Yangang Wang. Objectoccluded human shape and pose estimation from a single color image. In IEEE Conference on Computer Vision and</figDesc><table><row><cell>24</cell><cell cols="2">Cen-terHMR: a bottom-up single-shot method for multi-person 3d mesh recovery from a single image. 2020. 3 4 7 16 13 10 14 17 19 21 23 18 20 22 15</cell><cell>[60] Matthew D Pattern Recognition, pages 7374-7383, 2020. 1, 2, 3, 6, 5</cell></row><row><cell cols="3">[59] Andrei Zanfir, Eduard Gabriel Bazavan, Hongyi Xu, Bill 1</cell><cell>[62] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,</cell></row><row><cell></cell><cell cols="2">Freeman, Rahul Sukthankar, and Cristian Sminchisescu.</cell><cell>and Antonio Torralba. Learning deep features for discrimi-</cell></row><row><cell></cell><cell cols="2">Weakly supervised 3d human pose and shape reconstruction 2 3</cell><cell>native localization. In IEEE Conference on Computer Vision</cell></row><row><cell></cell><cell cols="2">with normalizing flows. In European Conference on Com-</cell><cell>and Pattern Recognition, pages 2921-2929, 2016. 3</cell></row><row><cell></cell><cell cols="2">puter Vision, pages 465-481, 2020. 2, 5, 6</cell></row><row><cell></cell><cell>6</cell><cell>5</cell></row><row><cell></cell><cell>9</cell><cell>8</cell></row><row><cell></cell><cell>12</cell><cell>11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>method. Training Dataset Ratios. To obtain the final best performing model, we follow EFT [23] and SPIN [29] which use fixed data sampling ratios for each batch. After training 100% with COCO-EFT for 175K steps, we incorporate 50% Human3.6M, 30% In-the-wild (i.e. [COCO, MPII, LSPET ]-EFT), and 20% MPI-INF-3DHP datasets into training. We also observe that using [50% Human3.6M, 30% COCO-EFT, 20% MPI-INF-3DHP ] or [20% Human3.6M, 30% COCO-EFT, 50% MPI-INF-3DHP ] gives equivalent performance on the 3DPW dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 :</head><label>6</label><figDesc>Evaluation on the 3DPW dataset. The numbers are average joint errors in mm. PARE models outperform video-based methods which leverage temporal information.</figDesc><table><row><cell></cell><cell cols="3">SPIN [29] HMR-EFT PARE</cell></row><row><cell>PCK ?</cell><cell>81.5</cell><cell>83.4</cell><cell>85.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>Evaluation of 2D keypoint project accuracy on 3DPW dataset. Failure Cases. In Fig. 11, we show a few examples where PARE fails to reconstruct reasonable human body poses. Zhang et al.[61] parameterize human meshes as UV maps where each pixel stores the 3D location of a vertex. They leverage saliency masks as visibility information and cast occlusions as an image-inpainting problem. However, we find that the raw predicted vertex locations from [61] yield arbitrary global scale, rotation, translation, and there are no camera parameters associated with the output. What they show in the main paper are the post-processed results after fitting a SMPL model, which is not described in their released implementation; how to visualize the unprocessed, raw predicted meshes is unclear.</figDesc><table><row><cell>The scenarios range from (a-b) too many people in the crop,</cell></row><row><cell>(b-d) rarely-seen extreme poses, (e-f) children whose body</cell></row><row><cell>shapes cannot be fully explained by the SMPL model, and</cell></row><row><cell>(g-h) extreme occlusion.</cell></row><row><cell>Comparing to [61].</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">With slight abuse of notations, ? is in axis-angle form when passed to the SMPL model but in 6D-vector form during the regression and loss computation.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b26">[27]</ref> <p>with our method, PARE. Note that VIBE is a video-based method, while PARE is run on each video frame independently.</p><p>(a) Input Image (b) SPIN <ref type="bibr" target="#b0">[1]</ref> (c) SPIN <ref type="bibr">[</ref>      </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recovering 3D human pose from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="58" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scape: Shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The naked truth: Estimating body shape under clothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detailed human shape and pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Alexandru O Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">E</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><forename type="middle">W</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haussecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d multi-bodies: Fitting sets of plausible 3d human models to ambiguous image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Biggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Ehrhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">3d human pose estimation using spatio-temporal networks with explicit occlusion training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11822</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wending</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="723" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pose2Mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Monocular expressive body regression through body-driven attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="20" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sim2real transfer learning for 3D pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical kinematic human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parsing occluded people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inferring 3D structure with a statistical image-based shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">HoloPose: Holistic 3D human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Epipolar transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoou-I</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7779" to="7788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating human pose from occluded images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="48" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transaction on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Coherent reconstruction of multiple humans from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Exemplar fine-tuning for 3D human pose fitting towards in-thewild 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03686</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning 3D human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5614" to="5623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">VIBE: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5252" to="5262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pytorch implementation of the neural mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4501" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unite the People: Closing the loop between 3D and 2D human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4704" to="4713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Trans. Graphics (Proc. SIG-GRAPH Asia)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">See more, know more: Unsupervised video object segmentation with co-attention siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3D human motion estimation via motion compression and refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><forename type="middle">S</forename><surname>Zhengyi Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Golestaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="324" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5188" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation in the wild using improved CNN supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3DVision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">I2L-MeshNet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model-based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3DVision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3D hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10975" to="10985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">DeepCut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A semantic occlusion model for human pose estimation from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umer</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Full-body awareness from partial observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Rockwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="522" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">How robust is 3d human pose estimation to occlusion? IROS workshops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Istv?n</forename><surname>S?r?ndi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timm</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Synthetic training for accurate 3d human pose and shape estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Combined discriminative and generative articulated pose and non-rigid shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Human body model fitting by learned gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Indirect deep structured learning for 3D human shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun Kai Vince</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wei</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5236" to="5246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep 3d human pose estimation under partial body presence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">A</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="569" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangrui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kidzinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13517</idno>
		<title level="m">3D pose detection in videos: Focusing on occlusion</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
