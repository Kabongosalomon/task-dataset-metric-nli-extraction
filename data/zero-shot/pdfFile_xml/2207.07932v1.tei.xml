<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Keypoint Detector and Descriptor for Retinal Image Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiazhen</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MoE Key Lab of DEKE</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">AIMC Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>2?</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MoE Key Lab of DEKE</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">AIMC Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">AIMC Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Vistel AI Lab</orgName>
								<orgName type="institution">Visionary Intelligence Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Xu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Institute of Ophthalmology</orgName>
								<orgName type="institution">Tongren Hospital</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayong</forename><surname>Ding</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Vistel AI Lab</orgName>
								<orgName type="institution">Visionary Intelligence Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Keypoint Detector and Descriptor for Retinal Image Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Retinal image matching</term>
					<term>trainable detector and descriptor</term>
					<term>progressive keypoint expansion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For retinal image matching (RIM), we propose SuperRetina, the first end-to-end method with jointly trainable keypoint detector and descriptor. SuperRetina is trained in a novel semi-supervised manner. A small set of (nearly 100) images are incompletely labeled and used to supervise the network to detect keypoints on the vascular tree. To attack the incompleteness of manual labeling, we propose Progressive Keypoint Expansion to enrich the keypoint labels at each training epoch. By utilizing a keypoint-based improved triplet loss as its description loss, Su-perRetina produces highly discriminative descriptors at full input image size. Extensive experiments on multiple real-world datasets justify the viability of SuperRetina. Even with manual labeling replaced by auto labeling and thus making the training process fully manual-annotation free, SuperRetina compares favorably against a number of strong baselines for two RIM tasks, i.e. image registration and identity verification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper is targeted at retinal image matching (RIM), which is to match color fundus photographs based on their visual content. Matching criteria are task dependent. As the retinal vasculature is known be unique, stable across ages and naturally anti-counterfeiting <ref type="bibr" target="#b27">[28]</ref>, retinal images are used for high-security identity verification <ref type="bibr" target="#b18">[19]</ref>. In this context, two retinal images are considered matched if they were taken from the same eye. RIM is also crucial for retinal image registration, which is to geometrically align two or more images taken from different regions of the same retina (at different periods). Aligned images can be used for wide-field imaging <ref type="bibr" target="#b3">[4]</ref>, precise cross-session assessment of retinal condition progress <ref type="bibr" target="#b7">[8]</ref>, and accurate laser treatment on the retina <ref type="bibr" target="#b30">[31]</ref>. RIM is thus a valuable topic in computer vision.</p><p>Developing a generic method for RIM is nontrivial. Due to varied factors in fundus photography such as illumination condition, abnormal retinal changes and natural motions of the fixating eye, retinal images of the same eye may vary significantly in terms of their visual appearance. Common lesions in diabetic retinopathy such as microaneurysm and intraretinal hemorrhage appear as dark dots, while cotton-wool spots look like white blobs <ref type="bibr" target="#b33">[34]</ref>. The classical SIFT detector <ref type="bibr" target="#b16">[17]</ref>, which finds corners and blobs in a scale-invariant manner, tends to respond around the lesions and the boundary between the circular foreground and the dark background, see <ref type="figure" target="#fig_0">Fig. 1</ref>. SIFT keypoints detected at these areas lack both repeatability and reliability.</p><p>Recently, GLAMpoints <ref type="bibr" target="#b30">[31]</ref> is proposed as a trainable detector for RIM. GLAMpoints learns to detect keypoints in a self-supervised manner, exploiting known spatial correspondence between a specific image and its geometric transformation produced by a controlled homography <ref type="bibr" target="#b4">5</ref> . Such full self-supervision has a downside of having many detections on non-vascular areas that are adverse to high-resolution image registration, see <ref type="figure" target="#fig_0">Fig. 1</ref>. The non-vascular areas are also unreliable for identity verification. As GLAMPoints is a detector, an external descriptor, e.g. rootSIFT <ref type="bibr" target="#b2">[3]</ref>, is needed. To the best of our knowledge, RIM with jointly trainable keypoint detector and descriptor is non-existing.</p><p>We depart from SuperPoint <ref type="bibr" target="#b6">[7]</ref>, a pivotal work on natural image matching with end-to-end keypoint detection and description. SuperPoint is a deep network with one encoder followed by two independent decoders. Given a h ? w gray-image input, SuperPoint first uses the encoder to generate a down-sized feature map of h 8 ? w 8 ? 128. With the feature map as a common input, one decoder produces a full-sized keypoint detection map, while the other decoder produces 256-dimensional descriptor per pixel on a h 8 ? w 8 image. Despite its encouraging performance on natural image matching, directly applying SuperPoint for RIM is problematic due to the following issues. First, in order to optimize its descriptor, SuperPoint has to compute hinge losses between all pixels, resulting in a complexity of O((w ? h) 2 ) for both computation and memory footprint. Such a high complexity significantly limits the input image size, in particular for training, making SuperPoint suboptimal for high-resolution retinal image registration. Second, the description loss is computed without taking the detected keypoints into account, making the learned descriptors less discriminative for disentangling genuine pairs from impostors for identity verification. Lastly, while the loss is computed on the h 8 ? w 8 ? 256 descriptor tensor, the tensor has be upsampled to h ? w ? 256 to provide descriptors for keypoints detected at the original size. Such an inherent discrepancy between descriptors used in the training and the inference stages affects the performance, see our ablation study. More recent advances such as R2D2 <ref type="bibr" target="#b20">[21]</ref> and NCNet <ref type="bibr" target="#b22">[23]</ref> have similar or other issues, as we will discuss in Section 2, motivating us to develop a novel method for RIM.</p><p>We propose SuperRetina, a semi-Supervised deep learning method for joint detection and description of keypoints for Retinal image matching. In contrast to <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref> which limit themselves to fully self-supervised (without using any manual annotation), we opt to initialize the training procedure with a relatively small set of (nearly 100) images, sparsely labeled to make the labelling cost well affordable. Such small-scale, incomplete yet precise supervision lets SuperRetina quickly focus on specific vascular points such as crossover and bifurcation that are more stable and repeatable. To overcome the incompleteness of manual labeling, we propose Progressive Keypoint Expansion (PKE) to enrich the labeled set at each training epoch. This allows SuperRetina to detect keypoints at previously untouched areas of the vascular tree. Moreover, we modify the network architecture of SuperPoint to directly produce a full-sized descriptor tensor of h ? w ? 256, see <ref type="figure">Fig. 2b</ref>. Consequently, our description loss is a keypoint-based improved triplet loss, which not only leads to highly discriminative descriptors but also has a quadratic complexity w.r.t. the number of detected keypoints. As this number is much smaller than h ? w, SuperRetina allows a larger input for training. Hence, SuperRetina detects keypoints that are spread over the image plane and at the same time on the vascular tree, making it versatile for multiple RIM tasks. In sum, our contributions are as follows:</p><p>? We propose SuperRetina, the first end-to-end method for RIM with jointly trainable keypoint detector and descriptor.</p><p>? We propose PKE to address the incompleteness of manual labeling in semisupervised learning. To enlarge the input size for both training and inference and for highly discriminative descriptors, we re-purpose and adapt a triplet loss as our keypoint-based description loss.</p><p>? Extensive experiments on two RIM tasks, i.e. retinal image registration and retina-based identity verification, show the superior performance of SuperRetina against the previous methods including three dedicated to RIM, i.e. PBO <ref type="bibr" target="#b18">[19]</ref>, REMEP <ref type="bibr" target="#b7">[8]</ref> and GLAMpoints <ref type="bibr" target="#b30">[31]</ref>, and four generic, i.e. SuperPoint <ref type="bibr" target="#b6">[7]</ref>, R2D2 <ref type="bibr" target="#b20">[21]</ref>, SuperGlue <ref type="bibr" target="#b24">[25]</ref> and NCNet <ref type="bibr" target="#b22">[23]</ref>. Code is available at GitHub 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Progress on Retinal Image Matching. Previous works on RIM are tailored to a specific task, let it be single-modal <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref> or multi-modal <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33]</ref> image registration, or identity verification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref>. For retinal image registration, LoSPA <ref type="bibr" target="#b0">[1]</ref> and DeepSPA as its deep learning variant <ref type="bibr" target="#b14">[15]</ref> focus on describing image patches by step pattern analysis (SPA), with keypoints found by detecting intersection points. Designed for feature matching between multi-modal retinal images of the same eye, the SPA descriptor lacks discrimination in revealing eye identity. GLAMpoints <ref type="bibr" target="#b30">[31]</ref> is trained in a labeling-free manner by exploiting spatial correspondences between a given image and its geometric transformations. However, such full self-supervision tends to detect many keypoints on non-vascular areas. REMPE <ref type="bibr" target="#b7">[8]</ref> first finds many candidate points by vessel bifurcation detection and the SIFT detector <ref type="bibr" target="#b16">[17]</ref>, and then performs point pattern matching (PPM) based on eye modelling and camera pose estimation to identify geometrically valid matches. The PPM algorithm involves expensive online optimization, requiring over three minutes to complete a registration, and thus putting its practical use into question.</p><p>For identity verification, existing works focus on detecting a few landmarks on the vascular tree, mainly crossover and bifurcation points known to be unique and stable across persons and ages <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref>. With the detected landmarks as input, PPM is then performed. PBO <ref type="bibr" target="#b18">[19]</ref> improves PPM by considering principal bifurcation orientations. BGM <ref type="bibr" target="#b13">[14]</ref> formulates the retinal vasculature as a spatial graph and consequently implements PPM by graph matching. Aleem et al . <ref type="bibr" target="#b1">[2]</ref> enhance point patterns of a given image based on spatial relationships between the landmarks, and then vectorize the patterns to a matching template. The number of keypoints required for identity verification is much less than that for image registration. Probably due to this reason, we see no attempt to re-purpose an identity verification method for image registration. In short, while there are few separated efforts on trainable detector (GLAMpoints) and descriptor (DeepSPA) for RIM, a joint effort remains missing.</p><p>Progress on Natural Image Matching. In contrast to RIM, a number of end-to-end methods exist for natural image matching, including SuperPoint <ref type="bibr" target="#b6">[7]</ref>, R2D2 <ref type="bibr" target="#b20">[21]</ref>, SuperGlue <ref type="bibr" target="#b24">[25]</ref>, NCNet <ref type="bibr" target="#b22">[23]</ref>, LoFTR <ref type="bibr" target="#b28">[29]</ref>, COTR <ref type="bibr" target="#b9">[10]</ref>, PDC-Net <ref type="bibr" target="#b31">[32]</ref>, etc. As the newly developed methods focus on natural scenes where detecting repeatable keypoints is difficult due to the lack of repetitive texture patterns, we notice a new trend of keypoint-free image matching. R2D2 softens the notion of keypoint detection by producing two probalistic maps to measure the reliability and the repeatability per pixel. In NCNet, all pairwise feature matches are computed, resulting in a quadratic complexity w.r.t. the number of pixels. As a consequence, the feature map used for matching has to be substantially downsized to make the computation affordable. LoFTR improves over SuperGlue with transformers to exploit self-/inter-correlations among the dense-positioned local features. These dense features are powerful for finding correspondences in lowtexture areas, desirable for scene image matching. However, this will produce many unwanted matches in non-vasuclar areas when matching retinal images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>SuperRetina is a deep neural network that takes as input a (gray-scale) h ? w retinal image I, detects and describes keypoints in the given image with high repeatability and reliability in a single forward pass. We describe the network architecture in Section 3.1, followed by the proposed training algorithms in Section 3.2. The use of SuperRetina for RIM is given in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>We adapt the SuperPoint network. Conceptually, our network consists of an encoder to extract down-sized feature maps F from the given image I. The feature map is then fed in parallel into two decoders, one for keypoint detection and the other for keypoint description, which we term Det-Decoder and Des-Decoder, respectively. The Det-Decoder generates a full-sized probabilistic map P , where P i,j indicates the probability of a specific pixel being a keypoint, i = 1, . . . , h and j = 1, . . . , w. The Des-Decoder produces a h ? w ? d tensor D, where D i,j denotes a d-dimensional descriptor. Note that in the inference stage, Non-Maximum Suppression (NMS) is applied on P to obtain a binary mask P as the final detection result. We formalize the above process as follows:</p><formula xml:id="formula_0">? ? ? ? ? ? ? F ? Encoder(I), P ? Det-Decoder(F ), D ? Des-Decoder(F ), P ? NMS(P ).<label>(1)</label></formula><p>As illustrated in <ref type="figure">Fig. 2b</ref>, we modify both Det-Decoder and Des-Decoder for RIM.</p><p>U-Net as Det-Decoder. Effectively capturing low-level patterns such as crossover and bifurcation on the vascular tree is crucial for detecting retinal keypoints in a reliable and repeatable manner. We therefore opt to use U-Net <ref type="bibr" target="#b23">[24]</ref>, originally developed for biomedical image segmentation with its novel design of re-using varied levels of features from the encoder in the decoder by skip connections. In order to support high-resolution input, our encoder is relatively shallow, with a conv layer to generate low-level full-sized feature maps, followed by three conv blocks, each consisting of two conv layers, 2 ? 2 max pooling and ReLU. Consequently, the high-level feature maps F have a size of h 8 ? w 8 ? 128. In order to recover full-sized feature maps, our Det-Decoder uses three conv blocks, each having two conv layers, followed by bilinear upsampling 7 , ReLU and concatenation to merge the corresponding feature maps from the encoder. Lastly, a conv. block consisting of three conv. layers and one sigmoid activation is applied on the full-sized feature maps to produce the detection map P .</p><p>Full-sized Des-Decoder. Different from SuperPoint which computes its description loss on a down-sized tensor of h 8 ? w 8 ? d, we target optimizing the descriptors on the full size of h ? w, where each pixel is associated with a ddimensional descriptor. Naturally, such dense results are obtained by interpolation, meaning gradient correlation between each keypoint and its neighborhood during backpropagation. Enlarging the neighborhood enhances the correlation, and is thus helpful for training with a larger receptive field <ref type="bibr" target="#b4">[5]</ref>. In that regard, our Des-Decoder first downsizes F to more compact feature maps of h 16 ? w 16 ? d, and then uses an upsampling block (using transposed conv) to generate the full-sized descriptor tensor D of h ? w ? d. All the descriptors are l 2 -normalized.</p><p>Our network adaption may seem to be conceptually trivial. Note that producing a full-sized descriptor tensor is computationally prohibitive for a pixel-based description loss as used in SuperPoint and NCNet. A keypoint-based description loss is needed. Nonetheless, keypoint-based training is nontrivial, as inadequate annotations will make the network quickly converge to a local, suboptimal solution. However, having many training images adequately labeled is known to be expensive. To tackle the practical challenge, we develop a semi-supervised training algorithm that works with a small amount of incompletely labeled images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv. &amp; Upsampling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv. &amp; Max pooling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Det-Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Des-Decoder</head><p>Encoder <ref type="bibr">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Algorithm</head><p>Semi-Supervised Training of Det-Decoder. We formulate keypoint detection as a pixel-level binary classification task <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref>. Due to the sparseness and incompleteness of manually labeled keypoints, training Det-Decoder using a common binary cross-entropy (CE) loss is difficult. To attack the sparseness (and the resultant class imbalance) issue, we leverage two tactics. The first tactic, borrowed from Pose Estimation <ref type="bibr" target="#b34">[35]</ref>, is to convert the binary labels Y to soft labels? by 2D Gaussian blur, where each keypoint is a peak surrounded by neighbors with their values decaying exponentially. The second tactic is to use the Dice loss <ref type="bibr" target="#b17">[18]</ref>, found to be more effective than the weighted CE loss and the Focal loss to handle extreme class imbalance <ref type="bibr" target="#b33">[34]</ref>. The Dice-based classification loss ? clf per image is computed as</p><formula xml:id="formula_1">? clf (I; Y ) = 1 ? 2 ? i,j (P ?? ) i,j i,j (P ? P ) i,j + i,j (? ?? ) i,j ,<label>(2)</label></formula><p>where ? denotes element-wise multiplication.</p><p>To attack the incompleteness issue, we propose Progressive Keypoint Expansion (PKE). The basic idea is to progressively expand the labeled keypoint set Y by adding novel and reliable keypoints found by Det-Detector, which itself is continuously improving after each epoch. To distinguish from such a dynamic Y , for each training image we now use Y 0 to indicate its initial keypoints, and S t to denote keypoints detected at the t-th epoch, t = 1, 2, . . .. We obtain the expanded keypoint set Y t as Y 0 ? S t , which is used for training at the t-th epoch.</p><p>As S t is auto-constructed, improper keypoints are inevitable, in particular at the early stage when the Det-Decoder is relatively weak. Given that a good detector shall detect the same keypoint under different viewpoints and scales, GLAMpoints performs a geometric matching to identify keypoints that can be repeatedly detected from a given image and its projective transformations. We improve over GLAMpoints by adding a content-based matching, making it a double-matching strategy. As <ref type="figure">Fig. 2c</ref> shows, suppose a keypoint detected in a non-vascular area in I (orange circle) has a geometrically matched keypoint (orange square) in I ? = H(I), with H as a specific homography. Non-vascular areas lack specificity in visual appearance, meaning descriptors extracted such areas are relatively close. Hence, even if the square is the best match to the circle in the descriptor space, it is not sufficiently different from the second-best match to pass Lowe's ratio test <ref type="bibr" target="#b16">[17]</ref>. Double matching is thus crucial.</p><p>As illustrated in <ref type="figure">Fig. 3</ref>, the PKE module works as follows: 1) Construct I ? , a geometric mapping of I, using I ? = H(I).</p><p>2) Feed I ? to SuperRetina to obtain its probabilistic detection map P ? . The inverse projection of the map w.r.t. I is obtained as P ? * = H ?1 (P ? ). 3) Geometric matching: For each point (i, j) in P , add it to S t if (P ? * ) i,j &gt; 0.5. 4) Content-based matching: For each point (i, j) in S t , we obtain its descriptor by directly sampling the output of the Des-Decoder, resulting in a descriptor set <ref type="figure">Fig. 3</ref>: Key dataflow within the PKE module.</p><formula xml:id="formula_2">D t . Similarly, we extract D ? t from I ? based on H(S t ). Each descriptor in D t is ? NMS Inverse Projection * " S # Double Matching # $ % $ ? #</formula><p>used as a query to perform the nearest neighbor search on D ? t . A point (i, j) will be preserved in S t , only if its spatial correspondence (i ? , j ? ) passes the ratio test.</p><p>The above procedure allows us to progressively find new and reliable keypoints, see <ref type="figure">Fig. 2d</ref>. Moreover, in order to improve the holistic consistency between the detection maps of I and its geometric transformation I ? , we additionally compute the Dice loss between P and P ? * , termed as ? geo (I, H). Our detection loss ? det conditioned on Y t and H is computed as</p><formula xml:id="formula_3">? det (I; Y t , H) = ? clf (I; Y t ) + ? geo (I, H).<label>(3)</label></formula><p>Self-Supervised Training of Des-Decoder. Ideally, the output of the Des-Decoder shall be invariant to homography. That is, for each keypoint (i, j) detected in I, its descriptor shall be identical to the descriptor extracted at the corresponding location (i ? , j ? ) in I ? . To avoid a trivial solution of yielding a constant descriptor, we choose to optimize a triplet loss <ref type="bibr" target="#b26">[27]</ref> such that the distance between paired keypoints shall be smaller than the distance between unpaired keypoints. Recall that keypoints are automatically provided by the Det-Decoder, our Des-Decoder is trained in a fully self-supervised manner. Such a property lets the Des-Decoder learn from unlabeled data with ease.</p><p>Feeding I and I ? separately into SuperRetina allows us to access their fullsized descriptor tensors D and D ? . For each element (i, j) in the non-maximum suppressed keypoint set P , let D i,j be its descriptor. As (i, j) and (i ? , j ? ) shall be paired, the distance of their descriptors, denoted as ? i,j , has to be reduced. With (i ? , j ? ) excluded, we use ? rand i,j to indicate the descriptor distance between (i, j) and a point chosen randomly from H( P ). Let ? hard i,j be the minimal distance. We argue that using ? rand i,j or ? hard i,j alone as the negative term in the triplet loss is problematic. As the requirement of ? i,j &lt; ? rand i,j is relatively easy to fulfill, using ? rand i,j alone is inadequate to obtain descriptors of good discrimination. Meanwhile, as the network at its early training stage lacks ability to produce good descriptors, using ? hard i,j exclusively will make the network hard to train. To resolve the issue, we propose a simple trick by using the mean of ? rand i,j and ? hard i,j as the negative term. Our description loss ? des is thus defined as</p><formula xml:id="formula_4">? des (I; H) = (i,j)? P max(0, m + ? i,j ? 1 2 (? rand i,j + ? hard i,j )),<label>(4)</label></formula><p>where m &gt; 0 is a hyper-parameter controlling the margin. Note that ? des has a quadratic time complexity w.r.t. the size of P , which is much smaller than h ? w. Hence, our description loss is much more efficient than its counterpart in SuperPoint, which is quadratic w.r.t. h ? w. As such, given the same amount of GPU resources, SuperRetina can be trained on higher-resolution images. While we describe the training algorithms of Det-Decoder and Des-Decoder separately, they are jointly trained by minimizing the following combined loss:</p><formula xml:id="formula_5">?(I; Y t , H) = ? det (I; Y t , H) + ? des (I; H),<label>(5)</label></formula><p>where the homography H varies per mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Keypoint-based Retinal Image Matching</head><p>Once trained, the use of SuperRetina for RIM is simple. Given a query image I q and a reference image I r , we feed them separately into SuperRetina to obtain their keypoint probabilistic maps P q and P r and associated descriptor tensors D q and D r . NMS is performed on P q and P r to obtain keypoints as Kp q and Kp r .</p><p>Recall that D q and D r are full-sized, so the corresponding descriptors desc q and desc r are fetched directly from the two tensors. Initial matches between Kp q and Kp r are obtained by an OpenCV brute-force matcher. The homography matrix H are then computed using the matched pairs to register q w.r.t. r. As for identity verification, H is reused to remove outliers. The two images are accepted as genuine, i.e. from the same eye, if the number of matched points exceeds a predetermined threshold, and impostor otherwise. The above process can be written in just a few lines of Python-style code, see the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>To evaluate SuperRetina in a real scenario, we train it on fixed data. The model is then applied directly (w/o re-training) for different RIM tasks on multiple testsets independent of the training data <ref type="table" target="#tab_1">(Table 1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Common Setup</head><p>Training data. We built a small labeled set as follows. We invited 10 members (staffs and students) from our lab. With ages ranging from 22 to 42, the subjects are with normal retinal condition. Multiple color fundus images of the posterior pole (FoV of 45 ? ) were taken per eye, using a SYSEYE Reticam 3100 fundus camera. We collected 97 images in total. The number of keypoints manually labeled 8 per image is between 46 and 147 with a mean value of 93.3. We term the labeled dataset Lab. In addition, to support training of our Des-Decoder, we collected an auxiliary dataset of 844 retinal images from 120 subjects having varied retinal diseases. Recall that Des-Decoder is trained in a fully self-supervised manner, so the auxiliary dataset requires no extra annotation. Implementation. We implement SuperRetina using PyTorch. Subject to our GPU resource (an NVIDIA GeForce RTX 2080 Ti), we choose a training input size of 768 ? 768. The network is trained end-to-end by SGD with minibatch size of 1. The optimizer is Adam <ref type="bibr" target="#b11">[12]</ref>, with ? = (0.9, 0.999) and an initial learning rate of 0.001. Standard data augmentation methods are used: gaussian blur, changes of contrast, and illumination. The number of maximum training epochs is 150. The descriptor length d is 256. For inference, the NMS size is 10?10 pixels. For homography fitting, we use cv2.findHomography with LMEDS. Performance metrics. Following <ref type="bibr" target="#b30">[31]</ref>, we report three sorts of rates, i.e. failed, inaccurate and acceptable. Given a query image I q and its reference I r , a registration is considered failed if the number of matches is less than 4, the minimum required to estimate a homography H. Otherwise, for each query point p q in I q , we compute the l 2 distance between H(p q ) and its reference p r in I r . Per query image, the median distance is defined as the median error (MEE), with the maximum distance as the maximum error (MAE). A registration is considered acceptable if MEE &lt; 20 and MAE &lt; 50, and inaccurate otherwise.</p><p>Besides, we report Area Under Curve (AUC) proposed by <ref type="bibr" target="#b8">[9]</ref>, which estimates the expectation of the acceptance rates w.r.t. the decision threshold, and thus reflects the overall performance of a specific method. Following <ref type="bibr" target="#b8">[9]</ref>, we compute AUC per category, i.e. Easy, Mod and Hard, and take their mean (mAUC) as an overall measure. Higher acceptance rate / AUC and lower inaccurate / failed rates are better. All the metrics are computed on the original size of 2912?2912.</p><p>Baselines. For a reproducible comparison, we choose competitor methods that have either source code or pre-trained models released by paper authors. Accordingly, we have eight baselines as follows:</p><p>? SIFT detector <ref type="bibr" target="#b16">[17]</ref> plus RootSIFT descriptor <ref type="bibr" target="#b2">[3]</ref>, using OpenCV APIs.</p><p>? PBO <ref type="bibr" target="#b18">[19]</ref>, a traditional keypoint extraction and matching method with authorprovided Matlab implementation.</p><p>? REMPE <ref type="bibr" target="#b7">[8]</ref>, performing retinal image registration through eye modelling and pose estimation 9 . ? SuperPoint 10 [7] trained on MS-COCO <ref type="bibr" target="#b15">[16]</ref>.</p><p>? GLAMpoints 11 <ref type="bibr" target="#b30">[31]</ref> (+ RootSIFT descriptor) trained on private fundus images. ? R2D2 12 <ref type="bibr" target="#b20">[21]</ref>, trained on the Aachen dataset <ref type="bibr" target="#b25">[26]</ref>.</p><p>? SuperGlue 13 <ref type="bibr" target="#b24">[25]</ref>, trained on ScanNet <ref type="bibr" target="#b5">[6]</ref>.</p><p>? NCNet 14 <ref type="bibr" target="#b22">[23]</ref>, pretrained on the Indoor Venues Dataset <ref type="bibr" target="#b21">[22]</ref>.</p><p>Due to the natural domain gap between retinal images and natural images, the baseline models pretrained on natural images might not be in their optimal condition for RIM. We take this into account by finetuning SuperPoint, GLAMpoints, R2D2 and NCNet on our training data.</p><p>Comparison with the Existing Methods. As shown in <ref type="table">Table 2</ref>, Super-Retina, with zero failure, an inaccurate rate of 1.49% and an acceptance rate of 98.51% is the best. Interestingly, we find that REMPE, which relies on traditional image processing enhanced by geometric modeling of the retina, performs better than the deep learning based alternatives including GLAMpoints, R2D2, SuperPoint, SuperGlue and NCNet. SuperRetina beats this strong baseline.</p><p>Similar results are observed in terms of AUC scores. The only exception is on the Easy group, where REMPE obtains a higher AUC (0.958 versus 0.940). Recall that images in this group have large overlap and no anatomic change, so the heavy modeling of the retinal structure in REMPE is advantageous. The benefit of end-to-end learning becomes more evident when dealing with the Moderate and Hard groups. SuperRetina scores a substantially higher AUC-Mod than REMPE (0.783 versus 0.660). Moreover, while REMPE takes 198 seconds to perform one registration, SuperRetina is far more efficient, requiring 1 second, most of which is spent on data IO and preprocessing. As only the query image has to be computed on the fly, while images in the database can be precom-9 https://projects.ics.forth.gr/cvrl/rempe/ 10 https://github.com/rpautrat/SuperPoint 11 https://github.com/PruneTruong/GLAMpoints pytorch 12 https://github.com/naver/r2d2 13 https://github.com/magicleap/SuperGluePretrainedNetwork 14 https://github.com/ignacio-rocco/ncnet <ref type="table">Table 2</ref>: Performance of the state-of-the-art for two RIM tasks, i.e. retinal image registration and retina based identity verification. Methods postfixed with finetune have been finetuned on our training data. The proposed SuperRetina compares favorably against the existing methods, even with the initial keypoint set Y 0 automatically detected by the PBO method. puted, the entire image matching process can be much accelerated. In short, the advantage of SuperRetina over REMPE is three-fold: (i) The end-to-end learned detector is more reliable than REMPE's vessel bifurcation detector for handling images with large anatomical changes, (ii) SuperRetina works for both image registration and identity verification, and (iii) SuperRetina is nearly 200x faster. Manual Labeling versus Auto-Labeling for Y 0 . The last three rows of <ref type="table">Table 2</ref> are SuperRetina with distinct choices of the initial keypoint set Y 0 . Pretraining means we tried to first train SuperRetina on the synthetic corner dataset as used by SuperPoint, and then use this pre-trained SuperRetina to produce Y 0 . The second-last row means using PBO-detected keypoints as Y 0 . Their results show that even with the auto-produced Y 0 , SuperRetina compares favorably against the current methods. In particular, using PBO-based Y 0 obtains mAUC of 0.750. The number, although lower than using the manual Y 0 (mAUC 0.755), clearly outperforms the best baseline, i.e. REMPE (mAUC 0.720). At the cost of merely 0.66% relative loss in performance, SuperRetina can indeed be trained in a manual-annotation free manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Registration</head><p>Evaluating the Influence of PKE. As <ref type="table" target="#tab_3">Table 3</ref> shows, SuperRetina w/o PKE suffers from a clear performance drop. Without PKE, the average number of keypoints detected by SuperRetina is substantially reduced, from 530 to 109 per image. We also tried PKE without content-based matching, making it effectively the keypoint selection strategy used by GLAMpoints. Its lower performance (row#3 in <ref type="table" target="#tab_3">Table 3</ref>) verifies the necessity of the proposed double-matching strategy. The above results justify the effectiveness of PKE for expanding the annotation data for semi-supervised learning.</p><p>For the description loss, we simultaneously leverage the hard negative instance and a random negative for computing the negative term in Eq. (4). We tried an alternative strategy of semi-hard negative sampling, where the negative ranked at the middle among all candidate negatives in a given mini-batch is chosen for computing the negative term. This alternative strategy (row#4 in <ref type="table" target="#tab_3">Table 3</ref>) is ineffective.</p><p>In addition, we re-run the same training pipeline, but w/o descriptor upsampling, w/o 2D Gaussain blur and using the (weighted) CE loss instead of Dice, respectively. Their consistent lower performance supports the necessity of the proposed changes regarding the network and its training strategy. Performance metric. We report Equal Error Rate (EER). As a common metric for evaluating a biometric system, EER is the value when the system's False Accept Rate and False Reject Rate are equal. Lower is better.</p><p>Baselines. We re-use the baselines from Section 4.2 except for REMPE <ref type="bibr" target="#b7">[8]</ref>, which is inapplicable for identity verification.</p><p>Comparison with State-of-the-Art. As <ref type="table">Table 2</ref> shows, SuperRetina, with EER of 0% on VARIA, 0.83% on CLINICAL and 1.18% on BES, compares favorably against the baselines. All the deep learning based methods perform well on VARIA, which has a small FoV with clearly visible vessels. However, their performance decreases noticeably on CLINICAL and BES, especially for GLAMpoints and R2D2, both using self-supervised training. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, GLAMpoints and R2D2 tend to detect keypoints on non-vascular areas. By contrast, SuperRetina keypoints are mostly distributed along the vascular tree, thus more suited for identity verification.</p><p>Ablation Study. <ref type="table" target="#tab_3">Table 3</ref> shows that PKE also matters for identity verification. As for the choice of Y 0 , using the PBO-produced labels achieves comparable results for two out of the three test sets, i.e. VARIA and BES. Note that its higher EER of 1.02% on CLINICAL remains better than the best baseline, i.e. SuperPoint with EER of 1.06%. We compare the SuperRetina detector with three existing detectors, i.e. SIFT, SuperPoint and GLAMpoints, all using the SuperRetina descriptor. We also compare the SuperRetina descriptor with two existing descriptors, i.e. RootSIFT previously used by GLAMPoints for RIM and SOSNet, a widely used deep descriptor <ref type="bibr" target="#b29">[30]</ref>. <ref type="table" target="#tab_3">Table 3</ref> shows that our detector and descriptor remain competitive even used separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Real-world experiments allow us to conclude as follows. The proposed PKE strategy is effective for resolving the incompleteness of manual labeling for semisupervised training, improving mAUC from 0.685 to 0.755 for retinal image registration on the FIRE dataset and reducing EER from 5.14% to 0.83% for retinabased identity verification on the most challenging CLINICAL dataset. Super-Retina beats the best baselines, i.e. REMPE for image registration (mAUC: 0.755 versus 0.720), and SuperPoint for identity verification (EER: 0.83% versus 1.06% on CLINICAL, 1.18% versus 2.00% on BES). Even with the manually labeled training data fully replaced by auto-labeling, and thus making the training process fully manual annotation free, SuperRetina preserves mostly its performance and compares favorably against the previous methods for RIM. <ref type="figure">Fig. 4</ref>: Detected keypoints in retinal images from distinct data sources, i.e. VARIA <ref type="bibr" target="#b19">[20]</ref>, FIRE <ref type="bibr" target="#b8">[9]</ref> and CLINICAL, a private dataset collected in clinical scenarios by this paper. Numbers below each image are the amount of keypoints found by a specific method. The proposed SuperRetina detects keypoints that spread over the field-of-view and in the meantime fall on the vascular tree.</p><p>Performance curves of different methods for retinal image registration. <ref type="figure" target="#fig_2">Fig. 5</ref> shows performance curves of the registration successful rate w.r.t. the error threshold at varied levels in three distinct modes, i.e. Easy, Moderate and Hard. We plot the curves using the source code kindly provided by the developers of the FIRE dataset <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> via personal correspondence.</p><p>DET graphs of different methods for retina-based identity verification. <ref type="figure" target="#fig_4">Fig. 6</ref> shows the Detection Error Tradeoff (DET) graphs for the identity verification task.    Other features (lesion / optic disc) for RIM? We do not consider lesions and optic disc. While lesions are stable in a specific examination session, they can be unstable across sessions, e.g., reduced after proper treatment or growing due to disease progress. Lesions are not eye-specific, and thus unsuited for identity verification. Moreover, unlike vascular keypoints, labeling lesions requires retinal expertise, making them nontrivial to obtain <ref type="bibr" target="#b33">[34]</ref>. The optic disc is not eye-specific also. Moreover, for retinal images of the posterior pole (FoV of 45 ? ), the optic disc possesses a relatively small percentage of the FoV area (1/36), making it less significant for matching. That said, our method is generic</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Retinal image matching by different methods. Keypoints corresponding to geometrically valid/invalid matches are shown in green/red dots. The first three rows are positive pairs, i.e. retinal images taken from the eye. More green dots and fewer red dots on the positive pairs indicate better matching. For the negative pair, fewer green is better. Best viewed on screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Performance curves of the image registration successful rate w.r.t. the error threshold. A curve closer to the top left corner is better. The overall performance is measured by the Area Under the Curve (AUC) scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Detection Error Tradeoff (DET) graphs on three test sets (a) VARIA, (b) CLINICAL and (c) BES for identity verification. The equal error rate (EER) point per DET graph is shown in solid dots. Lower EER [%] is better. Pseudo-code of using a trained SuperRetina model for retinal image matching. The use of SuperRetina for RIM mentioned in Section 3.3 can be written in just a few lines of Python-style code, see Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Proposed SuperRetina. Green/orange markers in (c) indicate genuine/fake keypoints. Blue/red dots in (d) indicate the initial keypoints (autodetected by PBO [19]) / iteratively detected keypoints for training.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>( h 4 , w 4 , 256)</cell><cell>( h 2 , w 2 , 192) (h, w, 128)</cell><cell>(h, w, 1)</cell></row><row><cell></cell><cell></cell><cell>h, w, 1)</cell><cell>(h, w, 64) ( h 2 , w 2 , 64)</cell><cell>( h 4 , w 4 , 128) ( h 8 , w 8 , 128)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Upsampling &amp; L2-Norm</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">( h 16 , w 16 , 256)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(h, w, 256)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">(b) Network architecture</cell></row><row><cell>?</cell><cell>!</cell><cell></cell><cell>= 0</cell><cell>= 1</cell><cell cols="2">= 10</cell><cell>= 100</cell></row><row><cell></cell><cell></cell><cell></cell><cell>! = 0</cell><cell>! = 9</cell><cell cols="2">! = 52</cell><cell>! = 81</cell></row><row><cell>geometric matching failed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>content-based matching failed</cell><cell></cell><cell></cell><cell>! = 0</cell><cell>! = 8</cell><cell cols="2">! = 34</cell><cell>! = 80</cell></row><row><cell cols="2">(c) Double-matching strategy</cell><cell cols="5">(d) Newly added keypoints St (red dots)</cell></row><row><cell>Fig. 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Our experimental data. Large cross-dataset divergence w.r.t. subjects, retinal conditions, imaging FoV etc. allows us to evaluate the effectiveness and generalization ability of SuperRetina. All test images are resized to 768?768, except for images from VAIRA which use 512 ? 512 due to their smaller FoV. Nidek AFC-210 fundus camera (FOV of 45 ? ) and 134 registered image pairs. The pairs have been divided into three groups according to their registration difficulty: Easy (71 pairs with high overlap and no anatomical change), Moderate (14 pairs with high overlap and large anatomical changes), and Hard (49 pairs with small overlap and no anatomical changes).</figDesc><table><row><cell>Dataset</cell><cell cols="3">Subjects Eyes Images</cell><cell cols="2">Image pairs</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Total Genuine Impostor</cell></row><row><cell>Training sets:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lab (labeled)</cell><cell>10</cell><cell>20</cell><cell>97</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Auxiliary (unlabeled)</cell><cell cols="2">120 215</cell><cell>844</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Test set for retinal image registration</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FIRE [9]</cell><cell>-</cell><cell>-</cell><cell>129</cell><cell>134</cell><cell>134</cell><cell>-</cell></row><row><cell cols="4">Test sets for retina based identity verification</cell><cell></cell><cell></cell><cell></cell></row><row><cell>VARIA [20]</cell><cell cols="2">-139</cell><cell cols="2">233 27,028</cell><cell>155</cell><cell>26,873</cell></row><row><cell>CLINICAL</cell><cell cols="2">100 180</cell><cell cols="2">691 16,203</cell><cell>1,473</cell><cell>14,730</cell></row><row><cell>BES [11, 36]</cell><cell cols="5">2,066 4,132 24,880 99,846 49,923</cell><cell>49,923</cell></row><row><cell cols="4">4.2 Task 1. Retinal Image Registration</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Test set. We adopt FIRE [9], a benchmark set consisting of 129 images of size</cell></row><row><cell cols="2">2, 912 ? 2, 912 acquired with a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study. Larger mAUC on FIRE and lower EER on VARIA, CLINICAL and BES are better. Topcon NW-100 camera. The images are optic disc centered, with a small FoV of around 20 ? . BES, acquired for a population-based study conducted in Beijing between 2001 and 2011, has 24,880 color fundus photos taken from 4,132 eyes at different periods. As images taken at earlier periods were digital scans of printed photos, the image quality of BES varies. Our private set, termed CLINICAL, consists of 691 images from 100 patients, acquired with a Topcon Trc-Nw6 fundus camera at an outpatient clinic of ophthalmology with due ethics approval. CLINICAL exhibits more diverse abnormal conditions such as old macula lesion, retinitis pigmentosa and macular edema. The joint use of the testsets leads to a systematic evaluation covering retinas in normal (VARIA)/abnormal (CLINICAL) conditions and across ages (BES).</figDesc><table><row><cell>Setup</cell><cell cols="4">FIRE(?) VARIA(?) CLINICAL(?) BES(?)</cell></row><row><cell>Full-setup</cell><cell>0.755</cell><cell>0</cell><cell>0.83</cell><cell>1.18</cell></row><row><cell>w/o PKE</cell><cell>0.685</cell><cell>0.01</cell><cell>5.14</cell><cell>3.11</cell></row><row><cell>PKE w/o content-based mathcing</cell><cell>0.670</cell><cell>0</cell><cell>1.48</cell><cell>1.19</cell></row><row><cell>Semi-hard negative sampling</cell><cell>0.407</cell><cell>2.75</cell><cell>10.18</cell><cell>7.83</cell></row><row><cell>w/o upsampling</cell><cell>0.697</cell><cell>0.03</cell><cell>3.46</cell><cell>4.15</cell></row><row><cell>w/o Gaussian blur</cell><cell>0.574</cell><cell>8.38</cell><cell>7.44</cell><cell>10.82</cell></row><row><cell>Dice ? CE</cell><cell>0.653</cell><cell>0.65</cell><cell>4.20</cell><cell>2.48</cell></row><row><cell>Dice ? weighted CE</cell><cell>0.704</cell><cell>0.02</cell><cell>1.79</cell><cell>1.32</cell></row><row><cell>Compare with other detectors:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Det: SIFT, Des: SuperRetina</cell><cell>0.585</cell><cell>0</cell><cell>4.40</cell><cell>4.23</cell></row><row><cell>Det: GLAMpoints, Des: SuperRetina</cell><cell>0.605</cell><cell>0</cell><cell>2.84</cell><cell>1.51</cell></row><row><cell>Det: SuperPoint, Des: SuperRetina</cell><cell>0.673</cell><cell>0</cell><cell>1.60</cell><cell>1.68</cell></row><row><cell>Compare with other descriptors:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Det: SuperRetina, Des: RootSIFT</cell><cell>0.705</cell><cell>0</cell><cell>2.81</cell><cell>2.10</cell></row><row><cell>Det: SuperRetina, Des: SOSNet</cell><cell>0.712</cell><cell>0</cell><cell>0.88</cell><cell>1.78</cell></row><row><cell cols="3">4.3 Task 2. Retina-based Identity Verification</cell><cell></cell><cell></cell></row><row><cell cols="5">Test Sets. We use three test sets: VARIA [20], Beijing Eye Study (BES) [11,36],</cell></row><row><cell cols="5">and a private set. VARIA has 233 gray-scale retinal images from 139 eyes, ac-</cell></row><row><cell>quired with a</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">As fundus images depict small area of retina, it is justified to apply the planar assumption in generating homographies<ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/ruc-aimc-lab/SuperRetina</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We use bilinear upsampling, as transposed convolutions originally used by U-Net are computationally more expensive, and introduce unwanted checkerboard artifact<ref type="bibr" target="#b12">[13]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Keypoint labeling requires little medical knowledge. The first author performed the labeling task in 4 working hours, which we believe was affordable.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this supplementary material, we provide more details of our evaluation, which are not included in the paper due to space limit.</p><p>Qualitative comparison between detector-based methods. <ref type="figure">Fig. 4</ref> shows that GLAMpoints and R2D2 tend to detect keypoints in non-vascular areas which are not discriminative for retinal image matching. By contrast, the keypoints found by SuperRetina are mostly distributed along the vascular tree, thus more suited for retinal image matching. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A low-dimensional step pattern analysis algorithm with application to multimodal retinal image registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Addison Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hai Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wing Kee Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Laude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Han Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast and accurate retinal identification system: Using retinal blood vasculature landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aleem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Retina mosaicing using local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sz?kely</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MICCAI</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Scan-Net: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SuperPoint: Self-supervised interest point detection and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">REMPE: Registration of retinal images through eye modelling and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernandez-Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zabulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Argyros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FIRE: Fundus image registration dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernandez-Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zabulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Triantafyllou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anyfanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Douma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Argyros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Modeling and Artificial Intelligence in Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">COTR: Correspondence transformer for matching across images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Beijing eye study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Jonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Ophthalmologica</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">M2U-Net: Effective and efficient retinal vessel segmentation for real-world applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laibacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jalali</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Retina verification system based on biometric graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lajevardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arakala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Horadam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A deep step pattern representation for multimodal retinal image registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft COCO: Common objects in context</title>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">V-Net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identity verification based on vessel matching from fundus images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Oinonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Forsvik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ruusuvuori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yli-Harja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Voipio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huttunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP (2010) 1, 3, 4</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Retinal verification using a feature points-based biometric pattern</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rouco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Carreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">R2D2: Repeatable and reliable detector and descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Humenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neighbourhood consensus networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">NC-Net: Neighbourhood consensus networks for estimating image correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">SuperGlue: Learning feature matching with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPR (2020) 3, 4</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Benchmarking 6DOF outdoor visual localization in changing conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Safari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A new scientific method of identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New York state journal of medicine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">LoFTR: Detector-free local feature matching with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">SOSNet: Second order similarity regularization for local descriptor learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Heijnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GLAMpoints: Greedily learned accurate match points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Apostolopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mosinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stucky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ciller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Zanet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning accurate dense correspondences and when to trust them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A segmentation based robust deep learning framework for multimodal retinal image registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cavichini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jhingan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Amador-Patarroyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">U G</forename><surname>Bartsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICASSP</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learn to segment retinal lesions and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<title level="m">Convolutional pose machines</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Subfoveal choroidal thickness: The Beijing eye study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Jonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
