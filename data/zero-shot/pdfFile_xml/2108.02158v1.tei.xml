<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Physics-based Noise Modeling for Extreme Low-light Photography</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixuan</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Ying</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Yinqiang</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Physics-based Noise Modeling for Extreme Low-light Photography</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Enhancing the visibility in extreme low-light environments is a challenging task. Under nearly lightless condition, existing image denoising methods could easily break down due to significantly low SNR. In this paper, we systematically study the noise statistics in the imaging pipeline of CMOS photosensors, and formulate a comprehensive noise model that can accurately characterize the real noise structures. Our novel model considers the noise sources caused by digital camera electronics which are largely overlooked by existing methods yet have significant influence on raw measurement in the dark. It provides a way to decouple the intricate noise structure into different statistical distributions with physical interpretations. Moreover, our noise model can be used to synthesize realistic training data for learning-based low-light denoising algorithms. In this regard, although promising results have been shown recently with deep convolutional neural networks, the success heavily depends on abundant noisy-clean image pairs for training, which are tremendously difficult to obtain in practice. Generalizing their trained models to images from new devices is also problematic. Extensive experiments on multiple low-light denoising datasets -including a newly collected one in this work covering various devices -show that a deep neural network trained with our proposed noise formation model can reach surprisingly-high accuracy. The results are on par with or sometimes even outperform training with paired real data, opening a new door to real-world extreme low-light photography.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>L IGHT is of paramount importance to photography.</p><p>Night and low light place very demanding constraints on photography due to very limited photon count and inescapable noise. One natural reaction is to gather more light by, e.g., enlarging aperture setting, lengthening exposure time and opening flashlight. However, each method brings a trade-off -large aperture incurs small depth of field, and is usually unavailable in smartphone cameras; long exposure can induce blur due to scene variations or camera motions; flash can cause color aberrations and is useful only for nearby objects.</p><p>Instead of attempting to gather more light during capturing, another way for low-light photography is to make use of a small number of photon detection to recover the desired scene information in the dark. This can be achieved either by hardware-based approach using advanced lowlight imagers <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> or computation-based approach using modern denoising algorithms <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Our interest in this paper lies in computation-based denoising, which could be applied on commodity cameras without need for dedicated imaging hardware. Along this line of research, significant progress has been made in recent years. Representative works include the burst processing scheme <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>  aligns and fuses a burst of photos to increase the signal-tonoise ratio (SNR), as well as the deep learning approach <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref> that automatically learns the mapping from a low-light noisy image to its long-exposure clean counterpart. Nevertheless, the existing methods still suffer from several limitations and the problem is still far from beings solved. For example, burst processing may generate unwanted ghosting effect <ref type="bibr" target="#b24">[25]</ref> when capturing dynamic scenes in the presence of vehicles, humans, etc.; the deep learning approach, on the other hand, requires a large volume of densely-labeled real training data which is tremendously difficult to acquire. This paper approaches computational low-light imaging from a fundamental perspective -directly modeling the noise in the imaging procedure of photosensors. Our first observation is that the successful design of denoising algorithms is highly contingent upon the accuracy of the adopted noise model <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b30">[31]</ref>. A precise noise model can not only motivate the design of suitable regularizers for optimization-based methods, but also benefit learning-based algorithms by synthesizing rich realistic training data. Our second observation is the noise pattern are complex and cannot be accurately modeled by existing methods. Even the state-of-the-art heteroscedastic Gaussian noise model <ref type="bibr" target="#b28">[29]</ref> cannot delineate the full picture of sensor noise under severely low illuminance. An illustrative example is shown in <ref type="figure">Figure 1</ref>, where the objectionable banding pattern artifacts, an unmodeled noise component that is exacerbated in dim environments, become clearly noticeable by human eyes.</p><p>In this work, we mainly focus on the noise formation model for raw images to avoid the impact on noise model from the image processing pipeline (ISP) when converting raw data to sRGB (e.g., gamma correction, gamut mapping, and tone mapping). We propose a physics-based noise arXiv:2108.02158v1 [eess.IV] 4 Aug 2021 SID <ref type="bibr" target="#b21">[22]</ref> DRV <ref type="bibr" target="#b27">[28]</ref> ELD (a) Short-exposure (b) Amplified input (c) BM3D <ref type="bibr" target="#b10">[11]</ref> (d) G+P <ref type="bibr" target="#b28">[29]</ref> (e) Paired data <ref type="bibr" target="#b21">[22]</ref> (f) Ours (g) Long-exposure <ref type="figure">Fig. 1</ref>: Raw denoising results of images from three real-world datasets, i.e., the See-in-the-Dark (SID) dataset <ref type="bibr" target="#b21">[22]</ref>, the Dark Raw Video (DRV) dataset <ref type="bibr" target="#b27">[28]</ref> and our Extreme Low-light Denoising (ELD) dataset, where we present (a) the shortexposure low-light image; (b) amplified noisy input image from (a); (c) the denoised output by the well-known denoising method BM3D <ref type="bibr" target="#b10">[11]</ref>; (g) the long-exposure reference image; (d-f) the outputs of UNets <ref type="bibr" target="#b29">[30]</ref> trained with (d) synthetic data generated by the signal-dependent heteroscedastic Gaussian noise model (G+P) <ref type="bibr" target="#b28">[29]</ref>, (e) paired real data of <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>, and (f) synthetic data generated by our proposed noise model respectively. All images were converted from raw Bayer space to sRGB for visualization; similarly hereinafter. (Best viewed with zoom)</p><p>formation model for extreme low-light imaging, which explicitly leverages the characteristics of CMOS photosensors to better match the physics of noise formation. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, our proposed synthetic pipeline derives from the inherent process of electronic imaging by considering how photons go through several stages, and model sensor noise in a fine-grained manner that includes many noise sources, e.g., photon shot noise, dark current noise, and pixel circuit noise. This yields a comprehensive statistical noise model that can accurately represent various elusive noise-driven phenomena (e.g., the banding pattern artifacts and the color bias issue) in very low light. It also provides a way to decouple the complicated real noise structure into different statistical distributions with clear physical interpretations, which facilitates the understanding of the real noise occurred in extreme low-light conditions. Furthermore, we devise a method to calibrate the noise parameters from available digital cameras, by first estimating the noise parameters at various ISO settings, then modeling the joint distributions of noise parameters. With this calibration technique, our approach can be easily adapted into any new camera devices at little cost on recording calibration data, thus bypassing the time-consuming paired real training data collection. In order to systematically investigate the generality of our noise model, we additionally introduce an Extreme Low-light Denoising (ELD) dataset taken by various camera devices.</p><p>We evaluate our approach on a wide spectrum of lowlight imaging applications, including extreme low-light raw denoising, extreme low-light image processing, extreme low-light video denoising as well as several downstream vision applications (i.e., depth estimation, optical flow, object detection/recognition) in the dark. For these tasks, we use the proposed noise model to synthesize training data for deep neural networks. The experiments collectively show that the network trained only with the synthetic data from our noise model can generate highly-accurate denoising results, which are on par with or sometimes even outperform training with real labeled data.</p><p>Our main contributions are summarized as follows:</p><p>? We formulate a comprehensive noise model that can accurately characterize the real noise structure in the dark. This not only enables us to synthesize realistic noisy images that can match the quality of real data, but also facilitates the understanding of the complicated real noise structure. We demonstrate the usefulness of our noise model on a wide range of low-light imaging applications and show that training deep neural network with it achieves stateof-the-art denoising performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We propose a noise parameter calibration method that can fit our noise model into any given camera devices at various ISO settings. It allows the fast adaptation and deployment of our noise model into any new devices at minimal extra cost, thereby circumventing the labor-intensive paired real data acquisition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We collect an extreme low-light denoising dataset ELD which contains images captured by various camera devices under different scenes to verify the effectiveness and generality of the proposed noise model and compare different methods.</p><p>The remainder of this paper is organized as follows. In Section 2, we review related low-light imaging methods including both hardware-based approach and computationbased approach. Section 3 introduces our physics-based noise formation model and the noise parameter calibration method for extreme low-light imaging. Both quantitative and qualitative experimental analysis of our approach are provided in Section 4, followed by discussions of applicability scope in Section 5. Conclusions and discussions of open problems are drawn in Section 6. A preliminary version of this work was presented as a conference paper <ref type="bibr" target="#b31">[32]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The challenge of imaging in low light is well-known in the computational photography/imaging community. In this section, we provide an overview of low-light imaging and denoising techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hardware-based Approach</head><p>Conventional photon detectors i.e., CMOS and CCD photosensors, generally require collecting a large number of photons (? 10 3 photons per pixel) to make a photograph <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, which is largely infeasible in many scientific applications, e.g., non-stellar observation in astronomy <ref type="bibr" target="#b34">[35]</ref>, remote sensing of human activity from space <ref type="bibr" target="#b35">[36]</ref> as well as imaging of delicate biological samples in microscopy <ref type="bibr" target="#b36">[37]</ref>. Therefore, many advanced imagers with superior light sensitivity, e.g., EMCCD, ICCD, and sCMOS imagers, have been designed and fabricated to address these imaging challenges <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. All these imagers could be operated at very lowphoton-flux regimes, where moonlight (? 10 photons per pixel) could provide sufficient illumination. To further push the limit of low-light imaging, recent years have witnessed significant advancements on ultralow-photon imaging techniques powered by single-photon avalanche diode (SPAD) imagers <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, making imaging under even starlight (? 1 photon per pixel) possible.</p><p>However, the complex physical mechanism of these hardware-based techniques renders the whole imaging system sophisticated and expensive. Moreover, their applications are often restricted to the laboratory environment, thus are not suitable for consumer-level applications in the wild, e.g., night vision for autonomous vehicles. In this work, we mainly focus on computational low-light imaging using a conventional camera. We demonstrate our method can considerably reduce the number of photons needed by the conventional camera by one to two orders of the magnitude, which even approaches the low-light sensitivity of advanced low-light imagers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Computation-based Approach</head><p>In contrast to the hardware-based approach, computationbased approaches typically rely on image priors and noise model to reconstruct a high-SNR image from its low-SNR observation acquired by camera. Crafting an analytical regularizer associated with image priors (e.g., smoothness, sparsity, self-similarity, low rank), therefore, plays a critical role in the design pipeline of traditional denoising algorithms <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>. In the modern era, most image denoising algorithms are entirely datadriven, which rely on deep neural networks that implicitly learn the statistical regularities to infer clean images from their noisy counterparts <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Although simple and powerful, these learning-based approaches are often trained on synthetic image data due to practical constraints. The most widely-used additive, white, Gaussian noise model deviates strongly from realistic evaluation scenarios, resulting in significant performance declines on photographs with real noise <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>.</p><p>To step aside the domain gap between synthetic images and real photographs, some works have resorted to collecting paired real data not just for evaluation but for training <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. Notwithstanding the promising results, collecting sufficient real data with ground-truth labels to prevent overfitting is exceedingly expensive and time-consuming. Recent works exploit the use of paired (Noise2Noise <ref type="bibr" target="#b50">[51]</ref>) or single (Noise2Void <ref type="bibr" target="#b51">[52]</ref>) noisy images as training data instead of paired noisy and noise-free images. Still, they can not substantially ease the burden of labor requirements for capturing a massive amount of realworld training data.</p><p>Another line of research has focused on improving the realism of synthetic training data to circumvent the difficulties in acquiring abundant real data. By considering both photon arrival statistics ("shot" noise) and sensor readout effects ("read" noise), the works of <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b30">[31]</ref> employed a signal-dependent heteroscedastic Gaussian model <ref type="bibr" target="#b28">[29]</ref> to characterize the noise properties in raw sensor data. Most recently, <ref type="bibr" target="#b52">[53]</ref> proposes a noise model which considers the dynamic streak noise, color channel heterogeneous and clipping effect, to simulate the high-sensitivity noise on real low-light color images. Concurrently, a flow-based generative model <ref type="bibr" target="#b53">[54]</ref> is proposed to formulate the distribution of real noise using latent variables with tractable density, and a GAN-based model is presented to learn camera-aware noise model <ref type="bibr" target="#b54">[55]</ref>. However, these approaches oversimplify the modern sensor imaging pipeline, especially the noise sources caused by camera electronics, which have been extensively studied in the electronic imaging community <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>.</p><p>In this work, we propose a physics-based comprehensive noise formation model stemming from the essential process of electronic imaging to synthesize the noisy-image dataset. We show sizeable improvements of denoising performance on real data, particularly under extremely low illuminance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PHYSICS-BASED NOISE FORMATION MODEL</head><p>The creation of a digital sensor raw image D can be generally formulated by a linear model</p><formula xml:id="formula_0">D = KI + N,<label>(1)</label></formula><p>where I is the number of photoelectrons that is proportional to the scene irradiation, K represents the overall system gain composed by analog and digital gains, and N denotes the summation of all noise sources physically caused by light or camera. Under extreme low light, the characteristics of N are formated in terms of the sensor physical process, which is beyond the existing noise models. In the following, we first describe the detailed procedures of the physical formation of a sensor raw image as well as the noise sources introduced during the whole process. An overview of this process is shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sensor Raw Image Formation</head><p>Our photosensor model is primarily based upon the CMOS sensor, which is the dominating imaging sensor nowadays <ref type="bibr" target="#b65">[66]</ref>. We consider the electronic imaging pipeline of how incident light is converted from photons to electrons, from electrons to voltage, and finally from voltage to digital numbers, to model noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">From Photon to Electrons</head><p>During exposure, incident lights in the form of photons hit the photosensor pixel area, which liberates photongenerated electrons (photoelectrons) proportional to the light intensity given the photoelectric effect.</p><p>Photon shot noise. Due to the quantum nature of light, there exists an inevitable uncertainty in the number of electrons collected. Such uncertainty imposes a Poisson distribution over this number of electrons, which follows</p><formula xml:id="formula_1">(I + N p ) ? P (I) ,<label>(2)</label></formula><p>where N p is termed as the photon shot noise and P denotes the Poisson distribution. This type of noise depends on the signal (i.e., light intensity). Shot noise is a fundamental limitation and cannot be avoided even for a perfect sensor. There are some other noise sources introduced during the photon-to-electron stage, such as photo response nonuniformity and dark current noise, as reported in the previous literature <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b63">[64]</ref>. Over the last decade, technical advancements in CMOS sensor design and fabrication, e.g., on-sensor dark current suppression, have led to a new generation of digital single lens reflex (DSLR) cameras with lower dark current and better photo response uniformity <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>. Therefore, we assume a constant photo response and absorb the effect of dark current noise N d into read noise N read , which will be presented next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">From Electrons to Voltage</head><p>After electrons are collected at each site, they are typically integrated, amplified and read out as measurable charge or voltage at the end of exposure time. Noise present during the electrons-to-voltage stage depends on the circuit design and processing technology used, and thus is referred to as pixel circuit noise <ref type="bibr" target="#b57">[58]</ref>. It includes thermal noise, reset noise <ref type="bibr" target="#b55">[56]</ref>, source follower noise <ref type="bibr" target="#b68">[69]</ref> and banding pattern noise <ref type="bibr" target="#b57">[58]</ref>. The physical origin of these noise components can be found in the electronic imaging literature <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b68">[69]</ref>. For instance, source follower noise is attributed to the action of traps in silicon lattice which randomly capture and emit carriers; banding pattern noise, consisting of row and column artifacts, is associated with the CMOS circuit readout pattern and the amplifier gain mismatch. By leveraging this knowledge, we consider the thermal noise N t , source follower noise N s and banding pattern noise N b in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Read noise.</head><p>To simplify analysis, we absorb multiple noise sources including dark current noise N d , thermal noise N t and source follower noise N s into a unified term, i.e., read noise:</p><formula xml:id="formula_2">N read = N d + N t + N s .<label>(3)</label></formula><p>Banding pattern noise N b will be considered later in this section. Read noise is generally assumed to follow a Gaussian distribution, but the analysis of noise data (in Section 3.2) tells a long-tailed nature of its shape. This can be attributed by the flicker and random telegraph signal components of source follower noise <ref type="bibr" target="#b57">[58]</ref>, or the dark spikes raised by dark current <ref type="bibr" target="#b55">[56]</ref>. Therefore, we propose using a statistical distribution that can better characterize the long-tail shape. Specifically, we model the read noise by a Tukey lambda distribution (T L) <ref type="bibr" target="#b69">[70]</ref>, which is a distributional family that can approximate a number of common distributions (e.g., a heavy-tailed Cauchy distribution):</p><formula xml:id="formula_3">N read ? T L (?; 0, ? T L ) ,<label>(4)</label></formula><p>where ? and ? T L indicate the shape and scale parameters respectively, while the location parameter is set to be zero with zero-mean noise assumption.</p><p>Color-biased read noise. Although zero-mean noise assumption is generally applicable in most situations, we find it breaks down under extreme low-light settings, because of the non-negligible direct current (DC) (i.e., zerofrequency) noise component (see Section 3.2). This component originates from the dark current noise, i.e., the averaged number of thermally generated electrons, which renders the noise distribution no longer zero-centered. Though this component has been largely eliminated by the modern onsensor dark current suppression techniques, e.g., pinned photodiode architecture <ref type="bibr" target="#b70">[71]</ref>, the residual component is still impactful due to the large system gain K applied to amplify the signal as well as the noise. Even worse, the evaluation (in Section 3.2) reveals this DC noise component varies across color channels, potentially owing to the stack photodiodes design of modern CMOS sensors <ref type="bibr" target="#b71">[72]</ref>. Such a color heterogeneous DC component is the culprit that leads to the color bias phenomenon frequently observed under extreme low illuminance (c.f. <ref type="figure">Figure 1</ref>).</p><p>To make our noise model simple yet compact, we model the DC noise component (i.e., color bias) as a mean value of the read noise model. As a result, we modify the Equation <ref type="bibr" target="#b3">(4)</ref> to</p><formula xml:id="formula_4">N read ? T L (?; ? c , ? T L ) ,<label>(5)</label></formula><p>where ? c denotes the color-wise DC noise component. The separated analysis of DC component and long-tailed T L distribution will be performed in Section 4.2.</p><p>Row noise. We introduce row noise N r to account for banding pattern noise N b . Though N b may appear in images as horizontal or vertical lines, we only consider the row-wise component (horizontal stripes) in our model, as the columnwise counterpart is generally negligible when measuring the noise data (Section 3.2). We simulate the N r by sampling a value from zero-mean Gaussian distribution N (0, ? r ) with scale parameter ? r for each row, i.e.,</p><formula xml:id="formula_5">N r ? N (0, ? r ) ,<label>(6)</label></formula><p>then adding it as an offset to all the pixels within that row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">From Voltage to Digital Numbers</head><p>To generate an image that can be stored in a digital storage medium, the analog voltage signal read out during last stage  is quantized into discrete codes using an analog-to-digital converter (ADC), which introduces quantization noise.</p><p>Quantization noise. Quantization noise N q is a rounding error between the analog input voltage to the ADC and the output digitized value, which can be assumed to follow a uniform distribution, i.e.,</p><formula xml:id="formula_6">N q ? U (?1/2q, 1/2q) ,<label>(7)</label></formula><p>where U (?, ?) denotes the uniform distribution over the range [?1/2q, 1/2q] and q is the quantization step.</p><p>To summarize, our noise formation model consists of four major noise components:</p><formula xml:id="formula_7">N = KN p + N read + N r + N q ,<label>(8)</label></formula><p>where K, N p , N read , N r and N q denotes the overall system gain, photon shot noise, read noise, row noise and quantization noise, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sensor Noise Evaluation</head><p>In this section, we present a noise parameter calibration method attached to our proposed noise formation model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantization noise Nq</head><p>Uniform distribution None Nq ? U (?1/2q, 1/2q) <ref type="table" target="#tab_1">Table 1</ref> summarizes the necessary parameters in our noise model, which include overall system gain K for photon shot noise N p , shape and scale parameters (? and ? T L ) for read noise N read , ? c for DC noise component (color bias), and scale parameter ? r for row noise N r . Given a camera device, our noise calibration method consists of two main procedures: 1) estimating noise parameters at various ISO settings 1 , and 2) modeling joint distributions of noise parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Estimating Noise Parameters</head><p>Our calibration method makes use of two types of raw images captured at specialized settings, i.e., flat-field frames and bias frames to estimate noise parameters. Flat-field frames are the images captured when sensor is uniformly illuminated. We take them of a white paper on a uniformly-lit wall. The camera is mounted on a tripod close to the paper, and the lens is focused on infinity to diminish non-uniformity. Bias frames are the images captured under a lightless environment with the shortest exposure time. We take them at a dark room and the camera lens is capped-on. Flat-field frames characterize the light-dependent photon shot noise, thus can be used to estimate the related parameter K, while bias frames delineate the dark noise picture independent of light, which can be used to derive other noise parameters ? c , ? r , ?, ? T L sequentially.</p><p>Estimate K for photon shot noise. According to Equation (1) (2) (8), a noisy sensor raw data D can be expressed by</p><formula xml:id="formula_8">D = K(I + N p ) + N o<label>(9)</label></formula><p>where N o = N read + N r + N q accounts for other noise sources independent of light. Following <ref type="formula" target="#formula_8">(9)</ref>, the variance of noisy raw data is given by</p><formula xml:id="formula_9">V ar(D) = K 2 V ar(I + N p ) + V ar(N o )<label>(10)</label></formula><p>where V ar(?) denotes the variance operator that calculates the variance of a given random variable. Given that (I +N p ) follows a Possion distribution, whose variance equals to its mean, we have</p><formula xml:id="formula_10">V ar(D) = K 2 I + V ar(N o ) = K(KI) + V ar(N o )<label>(11)</label></formula><p>where KI is the underlying true signal and represented by digital numbers. Equation <ref type="formula" target="#formula_0">(11)</ref>   Signal variance is easy to calculate, but the true signal value is generally unavailable. By utilizing the flat-field frame, the true signal value KI can be approximated by the median statistics owing to the uniformity. Now we can plot the estimated signal intensity against the variance of the noisy image, and calculate a linear least square regression for these two sets of measurements. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, these data points almost perfectly lie in a straight line, whose slope characterizes the overall system gain K at the measured ISO setting of the camera. Given estimated K, we can simulate realistic photon shot noise by firstly convert a raw digital signal D into the number of photoelectrons I, then impose a Poisson distribution on it, and finally revert it to D.</p><p>Estimate ? c for color bias. Given a bias frame, the DC noise component can be examined by averaging all pixel values within each color channel of the bias frame. If the noise distribution is zero-centered without DC component, the resulting color-wise values should almost equal to the black level 2 (per channel) recorded in the metadata of raw images. However, we find these values are severely biased from the recorded black level, which cannot be explained as random fluctuations caused by other zero-mean noisethis discloses the existence of DC noise component. <ref type="figure" target="#fig_2">Figure 4</ref> shows the density histogram of these biases ? c for each color channel calculated from a number of bias frames. Obvious differences on histogram can be observed among color channels, which demonstrate the varied statistics of DC noise component over color channels.</p><p>Note this observation and the new model are very important to extreme low-light denoising, since small bias could lead to severe color shift in extreme low light due to the large digital gain (e.g., ?100) applied to signal as well as noise. It challenges the commonly used zero-mean 2. Black level denotes the ideal noise-free readout value when no light hits the pixel array. noise assumption, and significantly improves the low-light denoising performance (see Section 4.2).</p><p>To exclude the influence of DC noise in the following evaluation of other noise components, the mean values of each color channel are subtracted from the bias frames.</p><p>Estimate ? r for row noise. The banding pattern noise can be tested via performing discrete Fourier transform on the bias frame. In <ref type="figure">Figure 5</ref>, the highlighted vertical pattern in the centralized Fourier spectrum reveals the existence of row noise component. To analyze the distribution of row noise, we extract the mean values of each row from raw data. These values, therefore, serve as good estimates to the underlying row noise intensities, given the zero-mean nature of other remaining noise sources. A normal probability plot <ref type="bibr" target="#b72">[73]</ref> is drawn in <ref type="figure">Figure 6</ref>, to compare the empirical distribution of row noise with a normal distribution. The normality of the row noise data is also tested by a Shapiro-Wilk test <ref type="bibr" target="#b73">[74]</ref>: the resulting p-value is higher than 0.05, suggesting the null hypothesis that the data are normally distributed cannot be rejected. The related scale parameter ? r can be easily estimated by maximizing the log-likelihood. The estimated row noise are then removed from the bias frames for the following calibration process.</p><p>Estimate ? and ? T L for read noise. Statistical models can be used to fit the empirical distribution of the residual read noise. A preliminary diagnosis <ref type="figure" target="#fig_5">(Figure 7</ref> Left) shows the main body of the data may follow a Gaussian distribution, but it also unveils the long-tail nature of the underlying distribution. In contrast to regarding extreme values as outliers, we observe an appropriate long-tail statistical distribution can characterize the noise data better.</p><p>We generate a probability plot correlation coefficient (PPCC) plot <ref type="bibr" target="#b74">[75]</ref> to identify a statistical model from a Tukey lambda distributional family <ref type="bibr" target="#b69">[70]</ref> that best describes the data. The Tukey lambda distribution is a family of distributions that can approximate many distributions by varying its shape parameter ?. It can approximate a Gaussian distribution if ? = 0.14, or derive a heavy-tailed distribution  The goodness-of-fit can be evaluated by R 2 -the coefficient of determination w.r.t. the resulting probability plot <ref type="bibr" target="#b75">[76]</ref>. The R 2 of the fitted Tukey Lambda distribution is much higher than the Gaussian distribution (e.g., 0.972 vs. 0.886), indicating a much better fit to the empirical data. It should be noted the Poisson mixture model was used in <ref type="bibr" target="#b76">[77]</ref> for heavy tail modeling, which utilized two Poisson components to capture the long-tailed behavior of sensor noise. It implicitly imposes a signal/light-dependent nature on noise due to the Poisson model. However, as we found in <ref type="figure" target="#fig_5">Figure 7</ref>, the long-tailed characteristics is virtually a feature of read noise, which is the noise present at bias frames captured under lightless conditions. It violates the underlying assumption of Poisson mixture model. Consequently, we still adopt the Tukey lambda distribution for long-tailed read noise modeling.</p><p>Although we use a unified noise model for different cameras, the noise parameters estimated from different cameras are highly diverse. <ref type="figure" target="#fig_5">Figure 7</ref> shows the selected optimal shape parameter ? differs camera by camera, implying distributions with varying degree of heavy tails across cameras. The visual comparisons of real and simulated bias frames are presented in <ref type="figure" target="#fig_6">Figure 8</ref>. Our model is capable of synthesizing realistic noise across various cameras, which outperforms the Gaussian noise model both in terms of the goodness-of-fit measure (i.e., R 2 ) and the visual similarity to real noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Modeling Joint Parameter Distributions</head><p>To choose noise parameters for our noise formation model at various ISO settings, we need to model joint parameter distributions such that the noise parameters can be sampled in a coupled way. Note the system overall gain K is closely related to the ISO setting, so it's rational to model the joint distributions of K and other noise parameters. As shown in <ref type="figure" target="#fig_7">Figure 9</ref>, we model the joint distributions of (K, ? T L ) and (K, ? r ) using the linear least squares method 3 to find the line of best fit for two sets of log-scaled measurements. Our noise parameter sampling procedure is</p><formula xml:id="formula_11">log (K) ? U log(K min ), log(K max ) , log (? T L ) | log (K) ? N (a T L log(K) + b T L ,? T L ) , (12) log (? r ) | log (K) ? N (a r log(K) + b r ,? r ) ,</formula><p>where U (?, ?) denotes a uniform distribution and N (?, ?) denotes a Gaussian distribution with mean ? and standard deviation ?.K min andK max are the estimated overall system gains at the minimum and maximum ISO of a camera respectively. a and b indicate the fitted line's slope and intercept respectively.? is an unbiased estimator of standard deviation of the linear regression under the Gaussian error assumption. For shape parameter ? and color bias ? c , we simply sample them from the empirical distribution 4 of the estimated parameter samples as we do not observe any clear statistical relationship to K (as well as ISO).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Noisy Image Synthesis</head><p>To synthesize noisy images, clean images are chosen and divided by low light factors sampled uniformly from [100, 300] to simulate low photon count in the dark. Noise is then generated and added to the scaled clean samples, according to Equation (8) <ref type="bibr" target="#b11">(12)</ref>. The created noisy images are finally normalized by multiplying the same low light factors to expose bright but excessively noisy contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Extreme Low-light Denoising Dataset (ELD)</head><p>To systematically study the generality of the proposed noise formation model, we collect an extreme low-light dataset that covers 10 indoor scenes and 4 camera devices from multiple brands (i.e., SonyA7S2, NikonD850, CanonEOS70D, CanonEOS700D) for benchmarking. We also record bias and flat-field frames for each camera to calibrate our noise <ref type="bibr" target="#b2">3</ref>. Other non-linear models (e.g., exponential) could yield better fits but not clearly improve the denoising performance. For simplicity, we use the linear model for joint distribution.</p><p>4. We do not find any analytical distributions that can characterize the distribution of color bias (see <ref type="figure" target="#fig_2">Figure 4</ref>). for Each Scene do <ref type="bibr">3:</ref> Meter the scene to find a exposure time t that well exposes the image at the base ISO g b ; <ref type="bibr">4:</ref> for Each ISO g h for noisy images do <ref type="bibr">5:</ref> Take a reference image at exposure setting (g b , t);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>for Each low light factor f do <ref type="bibr">7:</ref> Capture a noisy image at (g h , t * /f ); <ref type="bibr">8:</ref> end for <ref type="bibr">9:</ref> Take another reference image at (g b , t);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>end for <ref type="bibr">11:</ref> end for 12: end for model. The data capture setup is shown in <ref type="figure">Figure 10</ref>. The camera is mounted on a sturdy optical table and controlled by a remote software to avoid misalignments caused by camera motion, and the scene is illuminated by natural or direct current light sources to avoid flickering effect of alternating current lights <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b77">[78]</ref>. For each scene of a given camera, a reference image at the base ISO was firstly taken, followed by noisy images whose exposure time was deliberately decreased by low light factors f to simulate extreme low light conditions. Another reference image then was taken akin to the first one, to ensure no accidental errors (e.g., drastic illumination change or accidental camera/scene motion) occurred. The detailed procedures are presented in Algorithm 1. We choose three ISO levels (800, 1600, and 3200) <ref type="bibr" target="#b4">5</ref> and two low light factors (100, 200) for noisy images to capture our dataset, resulting in 240 (3?2?10?4) raw image pairs in total. The hardest example in our dataset resembles the image captured at a "pseudo" ISO up to 640000 (3200?200).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we first present the experimental setting including both implementation details and competing methods. Then we conduct comprehensive ablation studies for an in-depth analysis of the noise models and compare our method against prior art. Both quantitative and visual results on various datasets including the new ELD dataset are presented. Finally, we test our approach on low-light videography and several downstream vision tasks (depth estimation, optical flow estimation, object detection/recognition) in the dark. <ref type="bibr" target="#b4">5</ref>. Most modern digital cameras are ISO-invariant when ISO is set higher than 3200 <ref type="bibr" target="#b78">[79]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Implementation Details</head><p>To substantiate the effectiveness of our noise formation model, a deep neural network scheme is constructed to perform low-light raw denoising. We utilize the same U-Net architecture <ref type="bibr" target="#b29">[30]</ref> as <ref type="bibr" target="#b21">[22]</ref>. Raw Bayer images from SID Sony training set are used to create training data. We pack the raw Bayer images into four channels, and crop non-overlapped 512 ? 512 regions augmented by random flipping and rotation. Our approach only uses clean raw images, as the corresponding noisy images are generated by our proposed noise model on-the-fly. Besides, we also train networks based upon other training schemes as references, including training with paired real data (short exposure and long exposure counterpart) and training with paired real noisy images (i.e., Noise2Noise <ref type="bibr" target="#b50">[51]</ref>). The whole pipeline of our raw image denoising is depicted in <ref type="figure" target="#fig_9">Figure 11</ref> (note the RGB2RGB and raw2RGB experiments in Section 4.5 will use slightly different pipelines).</p><p>Our implementation is based on PyTorch. We train the models with 200 epoch using a simple L 1 loss and the Adam optimizer <ref type="bibr" target="#b79">[80]</ref> with batch size 1. The base learning rate is set to 10 ?4 and halved at epoch 100, then reduced to 10 ?5 at epoch 180.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Competing Methods</head><p>To understand how accurate our proposed noise model would be, we compare our method with 1) the representative non-deep methods, i.e., BM3D <ref type="bibr" target="#b10">[11]</ref> and Anscombe-BM3D (A-BM3D) <ref type="bibr" target="#b80">[81]</ref>; 2) the approaches that use real noisy data for training, i.e., Noise2Noise <ref type="bibr" target="#b50">[51]</ref> and "paired real data" <ref type="bibr" target="#b21">[22]</ref>  <ref type="bibr" target="#b5">6</ref> ; 3) the state-of-the-art noise models, i.e., Noiseflow <ref type="bibr" target="#b53">[54]</ref> and heteroscedastic Gaussian noise models (G+P) <ref type="bibr" target="#b28">[29]</ref>.</p><p>For non-deep methods, the noise level parameters required are provided by the off-the-shelf image noise level estimators <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b81">[82]</ref>. We note though there are other non-deep methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> that might achieve better results than BM3D, their computation cost is generally unaffordable for large-size images. As for Noiseflow, it requires paired real data to obtain noise data (by subtracting the ground truth images from the noisy ones) for training. We train the Noiseflow model on the SID Sony training dataset, then use the trained model to synthesize noisy dataset for training denoising networks. <ref type="bibr" target="#b5">6</ref>. The work of <ref type="bibr" target="#b21">[22]</ref> used paired real data to train raw-to-sRGB image processing (i.e., noisy raw images with clean sRGB counterparts as training labels). Here we adapt its setting to raw-to-raw denoising.   </p><formula xml:id="formula_12">(g) G * +P * (h) G * +P * +B (i) G * +P * +B+R (j) G * +P * +B+R+U (k) Paired</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on SID Sony Dataset</head><p>Single image raw denoising experiment is firstly conducted on SID Sony dataset. The numerical results are reported on whole SID Sony validation and test sets, instead of only indoor scenes in the preliminary version <ref type="bibr" target="#b31">[32]</ref>. To account for the imprecisions of shutter speed and analog gain <ref type="bibr" target="#b47">[48]</ref>, a single scalar is calculated and multiplied into the reconstructed image to minimize the mean square error evaluated by the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Ablation Study on Noise Models</head><p>To verify the efficacy of the proposed noise model, we compare the performance of networks trained with different noise models developed in Section 3.1. All noise parameters are calibrated using our ELD dataset, and sampled using Equation <ref type="bibr" target="#b11">(12)</ref>. The results of the other methods described in Section 4.1.2 are also presented as references.</p><p>As shown in <ref type="table" target="#tab_6">Table 2</ref>, the domain gap is significant between the heteroscedastic Gaussian model (G+P ) and the de facto noise model (characterized by the model trained with paired real data). This can be attributed to 1) the Gaussian approximation of Possion distribution is not justified under extreme low illuminance; 2) the long-tail nature of read noise is overlooked; 3) the color-wise DC noise component is not modeled; 4) the horizontal banding noise are not considered in the model. By taking all these factors into account, our final model, i.e., G * +P * +B+R+U gives rise to a striking result: the result is comparable to or sometimes even better than the model trained with paired real data. A visual comparison of our final model and other baseline methods is shown in <ref type="figure" target="#fig_0">Figure 12</ref>, suggesting the effectiveness of our proposed noise formation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Method Comparison</head><p>The qualitative results of all competing methods on both indoor and outdoor scenes from SID Sony set are presented in <ref type="figure" target="#fig_1">Figure 13</ref>. It can be seen that the random noise can be suppressed by the model learned with heteroscedastic Gaussian noise (G+P) <ref type="bibr" target="#b28">[29]</ref>, but the resulting colors are distorted, the banding artifacts become conspicuous, and the image details are barely discernible. Training only with real lowlight noisy data is not effective enough, due to the color-bias  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on our ELD Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Method Comparisons</head><p>To see whether our noise model can be applicable to other camera devices as well, we assess model performance on our ELD dataset. <ref type="table" target="#tab_7">Table 3</ref> and <ref type="figure" target="#fig_2">Figure 14</ref> summarize the results of all competing methods on ELD dataset. It can be seen that the non-deep denoising methods, i.e., BM3D and A-BM3D, fail to address the banding residuals, the color bias and the extreme values presented in the noisy input, whereas our model recovers vivid image details, which can be hardly detected from the noisy image by human observers. Moreover, our model trained with synthetic data even often outperforms the model trained with paired real data. We note the finding here conforms with the evaluation of sensor noise presented in Section 3.2, especially in <ref type="figure" target="#fig_5">Figure 7</ref> and 8, where we show the underlying noise distribution varies camera by camera. Consequently, training with paired real data from SID Sony camera inevitably overfits to the noise pattern merely existed on the Sony camera, leading to suboptimal results on other types of cameras. In contrast, our model relies on a very flexible noise model and a noise calibration process, making it adapts to noise characteristics of other (calibrated) camera models as well. Additional evidence can be found in <ref type="figure" target="#fig_11">Figure 15</ref>, where we apply these two models to an image captured by a smartphone camera. Our reconstructed image is clearer and cleaner than what is restored by the model trained with paired real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Training with More Synthesized Data</head><p>A useful merit of our approach compared to training with paired real data, is that our model can be easily incorporated with more real clean samples to train. <ref type="figure">Figure 16(a)</ref> shows the relative improvements of our model when training with the  dataset synthesized by additional clean raw images from MIT5K dataset <ref type="bibr" target="#b82">[83]</ref>. We find the major improvements are owing to the more accurate color and brightness restoration, as shown in <ref type="figure" target="#fig_5">Figure 17</ref>. By training with more raw image samples from diverse cameras, the network learns to infer the appearances of pictures more naturally and precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Sensitivity to Noise Calibration</head><p>Another benefit of our approach is we only need clean samples and a noise calibration process to adapt to a new camera, in contrast to capturing real noisy images accompanied with densely-labeled ground truth. Besides, the noise calibration process can be simplified once we already have a collection of parameter samples from various cameras. <ref type="figure">Figure 16(b)</ref> shows models can reach comparable performance on target cameras without noise calibration, by simply sampling parameters from other three calibrated cameras instead. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Applicability to Non-Bayer Raw Data</head><p>In the previous sections, we have evaluated our noise formation model on sensor raw images with Bayer color filter array (CFA). Here, we show our noise formation model is also applicable to photosensors with different CFA, e.g., X-Trans used in Fuji cameras. We follow <ref type="bibr" target="#b21">[22]</ref> to preprocess the X-Trans raw data by packing it into 9 channels, and create multiple training datasets for different training schemes, based upon the SID Fuji training dataset. <ref type="table" target="#tab_8">Table 4</ref> summarizes the quantitative results of different methods on the SID Fuji testing dataset. <ref type="bibr" target="#b6">7</ref> As we do not have bias/flat-field frames recorded by the Fuji camera, we simply sample the noise parameters required by noise models using the cameras in our ELD dataset. Even without accurate noise calibration, our final model still reaches a performance comparable to the model trained with paired real data (e.g., 38.33 v.s. 38.56 in ?250). <ref type="figure" target="#fig_6">Figure 18</ref> visually compares the results of different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Applicability to sRGB Denoising</head><p>Given a known image processing pipeline (ISP), our noise formation model allows us to generate downstream noise on sRGB space as well, i.e., noisy raw images are firstly generated according to our noise formation model, then are post-processed by a pre-defined ISP to synthesize the final noisy color images. As a proof of concept, here, we assume 7. Noiseflow cannot be trained on 9-channel raw images (its normalizing flow implementation requires the number of input channels to be even) thus is not compared here  the color images are finished by a simple ISP with typical functions, including white balance, color correction and a camera response function (CRF). <ref type="bibr" target="#b7">8</ref> Note that the major goal here is to corroborate the applicability of our noise model to color image denoising. In practice, it can be easily extended into real-world cases when more realistic ISP models for targeted camera devices are accessible (e.g., for camera vendors and/or ISP designers), but this is beyond the scope of this paper.</p><p>The necessary parameters to specify some of these operations including the per-channel gain for white balance and the camera-to-sRGB conversion matrix, are directly read from the camera raw files <ref type="bibr" target="#b8">9</ref> . The CRF is a non-linear mapping to compress a wide range of irradiance values within a fixed range of measurable image intensity values. It often serves as a key feature to distinguish different ISPs <ref type="bibr" target="#b83">[84]</ref>, <ref type="bibr" target="#b84">[85]</ref>. Rather than assuming a standard gamma curve as CRF, we adopt a realistic CRF calibrated on a SonyA7S2 camera (the one used in SID dataset) using a PCA-based radiometric calibration method <ref type="bibr" target="#b84">[85]</ref>.</p><p>We retrain the networks for color image (i.e., RGB2RGB) <ref type="bibr" target="#b7">8</ref>. We do not perform demosaicking; instead, the Bayer data is split into separate RGB channels to form raw RGB, where the green channel is obtained by averaging the two green pixels in two-by-two block.</p><p>9. These parameters are only used in training stage, they can be implicitly inferred by a trained model at inference. denoising based on the same training policy described in Section 4.1.1, except the used raw data have been preprocessed by the pre-defined ISP to produce their color counterparts. As shown in <ref type="table" target="#tab_10">Table 5</ref>, the network trained with our proposed noise model still reaches a highly competitive performance compared to the network learned with paired real data, indicating the applicability of our model to color image denoising as well.</p><p>Moreover, by preparing datasets of both raw and RGB formats, we can train networks to play different roles in the whole low-light imaging pipeline, including raw space denoising as a kind of preprocessing (Raw2Raw), color space denoising as a kind of post-processing (RGB2RGB), and joint performing multiple tasks as a kind of ISP (Raw2RGB) -the networks are learned to do denoising, white balance, color correction and camera response (tone mapping, gamma correction) simultaneously. We compare these three approaches on the SID Sony dataset, using either paired real data or synthetic data generated by our noise model. The quantitative results are shown in <ref type="table" target="#tab_12">Table 6</ref>. Interestingly, the approaches of Raw2Raw+ISP and Raw2RGB achieve comparable performance, and outperform the RGB2RGB approach. We note although the Raw2Raw setup is flexible to plug in different ISPs to render different styles of sRGB images, it requires non-blind white balance coefficients, which are often difficult to be deduced in very low light. By contrast, Raw2RGB can automatically infer color constancy from the noisy raw input itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Applicability to Extreme Low-light Videography</head><p>Our approach can be naturally extended into the extreme low-light videography. In this section, we conduct raw video denoising experiment on the Dark Raw Video (DRV) dataset <ref type="bibr" target="#b27">[28]</ref>. The DRV training set only contains static videos with corresponding long-exposure ground truth, since collecting paired noisy and noise-free dynamic videos simultaneously is usually impossible. The learning-based video processing  pipeline built on <ref type="bibr" target="#b27">[28]</ref> uses a sophisticated preprocessing (e.g., 2 ? 2 binning, VBM4D <ref type="bibr" target="#b85">[86]</ref> denoising) followed by a U-Net to postprocess videos frame-by-frame. The network is trained on both the recovery loss to encourage the output to be close to the ground truth, as well as the self-consistency loss <ref type="bibr" target="#b27">[28]</ref> to enforce the two output frames of the same scene to be similar.</p><p>To keep our setting simple, we discard the VBM4D preprocessing, and only use the U-Net with the same loss function as that in <ref type="bibr" target="#b27">[28]</ref>  <ref type="bibr" target="#b9">10</ref> to perform raw-to-raw video denoising in a frame-wise manner. The full experimental setting resembles the one used in raw-to-raw image denoising (See Section 4.1) except the extra self-consistency loss for temporal stability. We calibrate our noise model using a Sony RX100 VI camera, the same camera model capturing the DRV dataset. During training, our approach <ref type="bibr" target="#b9">10</ref>. In <ref type="bibr" target="#b27">[28]</ref>, the loss function is defined on the VGG <ref type="bibr" target="#b86">[87]</ref> feature space. As we focus on raw-to-raw denoising, we use the (raw) image space instead.   still only need clean raw frames, as we can synthesize arbitrary number of noisy frames to construct paired data. We also implement other training schemes (i.e., Noise2Noise <ref type="bibr" target="#b50">[51]</ref>, Noiseflow <ref type="bibr" target="#b53">[54]</ref>, paired real data <ref type="bibr" target="#b27">[28]</ref>  <ref type="bibr" target="#b10">11</ref> , heteroscedastic Gaussian noise model G+P <ref type="bibr" target="#b28">[29]</ref>) as baselines based on DRV training dataset, and adopt the same U-Net architecture and loss function for learning. For each static video, we assess the denoising performance by the averaged PSNR/SSIM over frames (5 continuous frames per video) as well as the Spatio-Temporal Reduced Reference Entropic Differences (ST-RRED) <ref type="bibr" target="#b87">[88]</ref>. The ST-RRED is a high performing video quality assessment metrics, which takes both the spatial and temporal distortions into account. Lower ST-RRED score indicates better restoration accuracy. <ref type="table" target="#tab_13">Table 7</ref> and <ref type="figure" target="#fig_7">Figure 19</ref> display <ref type="bibr" target="#b10">11</ref>. We adapt the video processing pipeline of <ref type="bibr" target="#b27">[28]</ref> to raw-to-raw video denoising without sophisticated preprocessing. raw video denoising results on DRV testing dataset. It can be seen that our method still arrives at the performance comparable to the "paired real data" model, and produces temporal stable results with less chroma artifacts.</p><p>It is worthwhile to highlight that our synthetic pipeline allows us to create a paired dynamic video dataset via adding synthetic noise to clean dynamic video frames <ref type="bibr" target="#b11">12</ref> . This circumvents the difficulty of paired real dynamic video data collection, and makes learning multi-frame denoising networks feasible. To support this argument, we collect 100 clean raw dynamic video clips (5 to 10 frames per clip) using the Sony RX100 VI camera and randomly divide them into 95 clips for training and 5 clips for testing. Both single-frame (U-Net) and multi-frame approaches (FastDVDnet <ref type="bibr" target="#b88">[89]</ref>) are compared on dynamic videos. These networks are trained either on paired real data or our synthetic data. Besides, either static video or dynamic video datasets are provided for training FastDVDnet. <ref type="table" target="#tab_14">Table 8</ref> presents the numerical results on synthetic dynamic video clips, and the visual results on both synthetic and real low-light video frames can be found in <ref type="figure" target="#fig_0">Figure 20</ref>. It can be seen that the multi-frame network fastDVDnet trained on the static video data cannot generalize to dynamic video frames, since it is impossible to learn motion correspondence based upon only static videos. As a result, it only averages and merges frames, which leads to highly blurry results on dynamic videos. In contrast, the fastDVDnet trained on our synthetic dynamic video dataset performs quite well on dynamic videos, which surpasses the single-frame approaches owing to its additional capability to exploit the temporal correlation over frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Application to Downstream Vision Tasks</head><p>We have demonstrated the advantages of our noise model on both low-light photography and videography. Here, we show our approach could also be useful for downstream computer vision applications, e.g., depth estimation in the dark, optical flow in the dark as well as object detection/recognition in the dark. For all these tasks, we follow the same pipeline ( <ref type="figure" target="#fig_9">Figure 11</ref>) to process images, i.e., 1) acquiring an image on RAW format; 2) performing denoising on it; 3) converting it into sRGB domain; 4) executing offthe-shelf vision algorithms on the processed sRGB image. We note in practice, this whole process can be seamlessly integrated into a hardware ISP system executed silently on a chip, such that users do not need to directly interact with RAW formatted images.</p><p>State-of-the-art/representative methods are adopted to carry out vision tasks under very limited luminance, including MiDaS <ref type="bibr" target="#b89">[90]</ref> for depth estimation, RAFT <ref type="bibr" target="#b90">[91]</ref> for optical flow estimation as well as YOLOv5 <ref type="bibr" target="#b91">[92]</ref> for object detection. We also test a commercial black-box object recognition solution provided by Google Vision API, which represents a wild uncontrolled task setting. Both visual and numerical results are shown in <ref type="figure" target="#fig_0">Figure 21</ref>-24 and <ref type="table">Table 9</ref> respectively, which collectively demonstrate that our low-light denoising <ref type="bibr" target="#b11">12</ref>. To enforce each synthesized frame at a video clip simulates a real one captured at the same low-light setting, we sample noise parameters once, and use the same parameters to synthesize noise instances for each frame. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION OF APPLICABILITY SCOPE</head><p>Our principal approach can be applied on a variety of realworld application scenarios, ranging from extreme low-light image/video processing to downstream computer vision applications at night. However, it should be clarified the applicability scope here is largely restricted to cases where the RAW images are available. As a result, our method is unfortunately unable to handle some practical situations, e.g., video surveillance from closed-circuit television data, or processing images from internet. Nevertheless, we note RAW image is accessible in many practical and important scenarios from both scientific and industrial domains, where our method is applicable and more desirable. For example, in many scientific imaging applications (e.g., microscopy <ref type="bibr" target="#b36">[37]</ref>, astronomy <ref type="bibr" target="#b34">[35]</ref>, remote sensing <ref type="bibr" target="#b35">[36]</ref>), what we actually care is to faithfully record the physical world (i.e., the scene irradiance) to analyze and induce physical properties. In such cases, the ISP functionalities in those scientific imagers are either simplified (typically with demosaicking only) or fully disabled, resulting in RAW images that are linearly response to irradiance. Besides, from industrial perspective, our method is also useful for camera and ISP designers with 9: Quantitative results of downstream vision applications on SID Sony set (depth/detection) and our collected moved scenes (flow). We report the root-mean-square error (RMSE), endpoint error (EPE) and the average number of objects recognized per image (# of Objects) for these three tasks respectively, using the results of long-exposure references as ground truth. full accessibility to camera RAW and ISP. In this regard, our method could not only facilitate developing scientific imaging solutions, but also help camera/smartphone vendors to enhance their ISP system by inserting our sensor-specific denoising algorithm. It has the potential to unlock the powerful low-light imaging capability on commodity/scientific imagers, which in turn has significant impacts on downstream consumers. Moreover, recent years have witnessed significant advancements on smartphone cameras. Nowadays, almost every mainstream smartphone camera (e.g., iPhone, Huawei, Samsung, etc.) has provided user interface of RAW image acquisition for non-professional users. We envision acquiring RAW images would be more and more convenient in future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>We have presented a physics-based noise formation model together with a noise parameter calibration method to help resolve the difficulty of extreme low-light imaging. We revisit the electronic imaging pipeline and investigate the influential noise sources overlooked by existing noise models. This enables us to synthesize realistic noisy raw data that match the underlying physical process of noise formation better. We systematically study the efficacy of our noise formation model, by introducing a new dataset that covers four representative camera devices. By training only with our synthetic data, we demonstrate a convolutional neural network can compete with or sometimes even outperform the network trained with paired real data on real-world benchmarks.</p><p>Our approach opens a new door to real-world computational low-light imaging, which significantly alleviates the burden of capturing paired real training data from diverse camera devices. It's highly flexible and enables rapid adaptation and deployment on a variety of consumer-level cameras. Nevertheless, to build a whole computational lowlight imaging system, several challenges remain unresolved. For example, both auto-focus and auto-exposure modules would fail under very low light. Their failure will render user experience unfriendly, as taking desired photos with sharp appearance would be excessively difficult. Furthermore, preserving high dynamic range is very appealing for low-light photography. Finally, in many commodity cameras, e.g., smartphone cameras, the computation resource is often very limited. Thus it's worth investigating lightweight deep architectures specially tailored for low-light image processing as well. We foresee future works to further tackle these challenges and push the boundaries of computational low-light photography.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of electronic imaging pipeline and visualization of noise source and resulting image at each stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Estimation of the overall system gain K for the three cameras. The noisy signal variance (y-axis) and the underlying true signal value (x-axis) satisfy a linear function whose slope implies K at the measured ISO (1600) setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Normalized density histogram (with bin size set to 0.5) of the color-wise DC noise component ? c from different cameras (14-bit depth).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>reveals a linear relationship between the signal variance V ar(D) and the underlying clean digital 1. Noise parameters are generally stationary at a fixed ISO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Centralized Fourier spectrum of the bias frames captured by three cameras. Normal probability plots of row noise data for three cameras. The resulting image looks close to a straight line if the data are approximately normally distributed. signal KI. Determining K, therefore, simply requires fitting a line, if a set of observations (V ar(D), KI) are accessible to us.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Distribution fitting of read noise for three cameras. Left: probability plot against the Gaussian distribution. Middle: Tukey lambda PPCC plot that determines the optimal ? (shown in red line). Right: probability plot against the Tukey Lambda distribution. A higher R 2 indicates a better fit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Simulated and real bias frames of three cameras; A higher R 2 indicates a better fit. (Best viewed with zoom) if ? &lt; 0.14. The PPCC plot (Figure 7 Middle) is used to find a good value of ?. The probability plot [73] (Figure 7 Right) is then employed to estimate the scale parameter ? T L .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Linear least-square fitting from estimated noise parameter samples (blue dots) from three cameras. Upper and lower figures show the joint distributions of (K, ? T L ) and (K, ? r ) respectively, where we sample the noise parameters from the blue shadow regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :Algorithm 1</head><label>101</label><figDesc>Image capture setup and some sample images from our ELD dataset. Image capture protocol Require: g b = 100; g h t * = g b t;1: for Each Camera do 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :</head><label>11</label><figDesc>Our raw image denoising pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 15 :</head><label>15</label><figDesc>The denoising results of a low-light image captured by a Huawei Honor 10 camera.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 16 :Fig. 17 :</head><label>1617</label><figDesc>(a) Performance boost when training with more synthesized data. (b) Noise parameter sensitivity test. The recovery results of a low-light image captured by Nikon D850 camera.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 20 :</head><label>20</label><figDesc>Raw video denoising results on synthetic and real dynamic frames from our dynamic video dataset and DRV dynamic video dataset respectively. "S" and "D" indicate the multi-frame networks are trained on static and dynamic video datasets respectively.(Best viewed on screen with zoom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 21 :Fig. 22 :Fig. 23 : 24 :</head><label>21222324</label><figDesc>Monocular depth estimation in the dark. (Best viewed on screen with zoom) Optical flow in the dark. (Best viewed on screen with zoom) Object detection in the dark. (Best viewed on screen with zoom) Low-light object recognition results on Google Vision API. (Best viewed on screen with zoom) algorithm significantly improves the performance of downstream vision applications under extreme low-light settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Summary of our noise model Np) ? P (I) Read noise N read Tukey lambda distribution N read ? T L (?; ?c, ? T L ) Shape ? Color bias ?c Scale ? T L</figDesc><table><row><cell>Noise type</cell><cell>Formulation</cell><cell>Parameters</cell></row><row><cell cols="2">Photon shot noise Np (I + Row noise Nr Poisson distribution Gaussian distribution Nr ? N (0, ?r)</cell><cell>System gain K Scale ?r</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Best viewed on screen with zoom)</head><label></label><figDesc>Raw image denoising results on both indoor and outdoor scenes from SID Sony dataset. The PSNR value of each resulting image is reported below; similarly hereinafter. (</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>real data</cell><cell cols="2">(l) Reference</cell></row><row><cell>(28.41)</cell><cell></cell><cell>(32.03)</cell><cell>(32.20)</cell><cell>(32.25)</cell><cell></cell><cell>(31.81)</cell><cell></cell><cell>(PSNR)</cell></row><row><cell cols="9">Fig. 12: Visual result comparison of different training schemes. Our full noise model (G  *  +P  *  +B+R+U ) suppresses the</cell></row><row><cell cols="8">color bias, residual bandings and chroma artifacts compared to other baselines. (Best viewed with zoom)</cell></row><row><cell>Input</cell><cell>BM3D</cell><cell>A-BM3D</cell><cell>Noise2Noise</cell><cell>Noiseflow</cell><cell>G+P</cell><cell cols="2">Paired real data</cell><cell>Ours</cell></row><row><cell>23.08</cell><cell>31.83</cell><cell>31.90</cell><cell>34.30</cell><cell>33.23</cell><cell>34.18</cell><cell>34.48</cell><cell></cell><cell>34.40</cell></row><row><cell>26.82</cell><cell>30.66</cell><cell>35.58</cell><cell>33.57</cell><cell>34.54</cell><cell>33.67</cell><cell>37.98</cell><cell></cell><cell>38.67</cell></row><row><cell>29.25</cell><cell>35.82</cell><cell>35.81</cell><cell>35.54</cell><cell>35.26</cell><cell>36.10</cell><cell>45.97</cell><cell></cell><cell>47.14</cell></row><row><cell>23.87</cell><cell>29.39</cell><cell>28.79</cell><cell>30.12</cell><cell>31.60</cell><cell>29.88</cell><cell>35.31</cell><cell></cell><cell>35.43</cell></row><row><cell>Fig. 13:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Raw image denoising results on our ELD dataset, where we show the visual results on four evaluation scenes.</figDesc><table><row><cell>Input</cell><cell>BM3D</cell><cell>A-BM3D</cell><cell>Noise2Noise</cell><cell>Noiseflow</cell><cell>G+P</cell><cell>Paired real data</cell><cell>Ours</cell></row><row><cell>27.95</cell><cell>35.45</cell><cell>35.46</cell><cell>38.81</cell><cell>36.12</cell><cell>40.25</cell><cell>44.53</cell><cell>45.58</cell></row><row><cell>28.48</cell><cell>35.73</cell><cell>35.02</cell><cell>41.71</cell><cell>37.40</cell><cell>44.49</cell><cell>46.57</cell><cell>49.51</cell></row><row><cell>20.71</cell><cell>29.07</cell><cell>29.02</cell><cell>33.44</cell><cell>33.66</cell><cell>32.93</cell><cell>35.82</cell><cell>37.07</cell></row><row><cell>26.57</cell><cell>33.55</cell><cell>36.31</cell><cell>38.31</cell><cell>40.13</cell><cell>38.14</cell><cell>41.89</cell><cell>42.65</cell></row><row><cell>Fig. 14:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(Best viewed on screen with zoom)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 2 :</head><label>2</label><figDesc>Quantitative Results on Sony set of the SID dataset. G: the Gaussian model for read noise N read ; G * : the tukey lambda model for N read ; P : the Gaussian approximation for photon shot noise N p ; P * : the true Poisson model for N p ; B: the color-wise DC model for color bias; R: the Gaussian model for row noise N r ; U : the uniform distribution model for quantization noise N q . The best and second-best results are indicated by red and blue, respectively.</figDesc><table><row><cell></cell><cell>?100</cell><cell>?250</cell><cell>?300</cell></row><row><cell>Model</cell><cell cols="3">PSNR / SSIM PSNR / SSIM PSNR / SSIM</cell></row><row><cell>BM3D</cell><cell>38.49 / 0.875</cell><cell>34.48 / 0.821</cell><cell>30.98 / 0.766</cell></row><row><cell>A-BM3D</cell><cell>39.24 / 0.869</cell><cell>33.74 / 0.742</cell><cell>30.29 / 0.729</cell></row><row><cell>Noise2Noise</cell><cell>40.47 / 0.890</cell><cell>35.74 / 0.757</cell><cell>32.34 / 0.707</cell></row><row><cell>Noiseflow</cell><cell>41.11 / 0.926</cell><cell>36.97 / 0.838</cell><cell>32.38 / 0.759</cell></row><row><cell>Paired real data</cell><cell>42.76 / 0.948</cell><cell>40.59 / 0.935</cell><cell>36.48 / 0.919</cell></row><row><cell>G</cell><cell>39.47 / 0.866</cell><cell>34.83 / 0.726</cell><cell>31.80 / 0.685</cell></row><row><cell>G+P</cell><cell>40.44 / 0.894</cell><cell>35.67 / 0.773</cell><cell>32.26 / 0.716</cell></row><row><cell>G+P  *</cell><cell>41.26 / 0.925</cell><cell>37.27 / 0.849</cell><cell>33.68 / 0.800</cell></row><row><cell>G  *  +P  *</cell><cell>41.64 / 0.936</cell><cell>38.20 / 0.891</cell><cell>34.56 / 0.844</cell></row><row><cell>G  *  +P  *  +B</cell><cell>42.55 / 0.948</cell><cell>40.44 / 0.930</cell><cell>36.29 / 0.912</cell></row><row><cell>G  *  +P  *  +B+R</cell><cell>42.62 / 0.948</cell><cell>40.55 / 0.930</cell><cell>36.33 / 0.913</cell></row><row><cell>G  *  +P  *  +B+R+U</cell><cell>42.75 / 0.949</cell><cell>40.60 / 0.932</cell><cell>36.46 / 0.916</cell></row><row><cell cols="4">effect (that violates the zero-mean noise assumption) and the</cell></row><row><cell cols="4">large variance of corruptions (that leads to a large variance</cell></row><row><cell cols="4">of the Noise2Noise solution) [51]. Besides, the learning-</cell></row><row><cell cols="4">based noise model -Noiseflow [54] cannot fully capture</cell></row><row><cell cols="4">the low-light real noise structure, leading to color-shifted</cell></row><row><cell cols="4">and over-smoothed results. By contrast, our model produces</cell></row><row><cell cols="4">visually appealing results as if it had been trained with</cell></row><row><cell>paired real data.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 3 :</head><label>3</label><figDesc>Quantitative results of different methods on our ELD dataset which contains four representative cameras.</figDesc><table><row><cell></cell><cell cols="3">CAMERA</cell><cell></cell><cell></cell><cell>f</cell><cell>INDEX</cell><cell></cell><cell cols="6">NON-DEEP BM3D [11] A-BM3D [81] Noise2Noise [51] Paired data [22] Noiseflow [54] TRAINING WITH REAL DATA TRAINING WITH SYNTHETIC DATA G+P [29] Ours</cell></row><row><cell cols="4">SonyA7S2</cell><cell></cell><cell></cell><cell>?100 ?200</cell><cell>PSNR SSIM PSNR SSIM</cell><cell></cell><cell></cell><cell cols="2">37.69 0.803 34.06 0.696</cell><cell></cell><cell></cell><cell>37.74 0.776 35.26 0.721</cell><cell>41.63 0.856 37.98 0.775</cell><cell>44.50 0.971 42.45 0.945</cell><cell>40.10 0.831 37.19 0.757</cell><cell>42.46 0.889 38.88 0.812</cell><cell>45.44 0.975 43.42 0.954</cell></row><row><cell cols="5">NikonD850</cell><cell></cell><cell>?100 ?200</cell><cell>PSNR SSIM PSNR SSIM</cell><cell></cell><cell></cell><cell cols="2">33.97 0.725 31.36 0.618</cell><cell></cell><cell></cell><cell>36.60 0.779 32.59 0.723</cell><cell>40.47 0.848 37.98 0.820</cell><cell>41.28 0.938 39.44 0.910</cell><cell>40.62 0.870 37.61 0.805</cell><cell>40.29 0.845 37.26 0.786</cell><cell>42.27 0.936 40.36 0.908</cell></row><row><cell cols="5">CanonEOS70D</cell><cell></cell><cell>?100 ?200</cell><cell>PSNR SSIM PSNR SSIM</cell><cell></cell><cell></cell><cell cols="2">30.79 0.589 28.06 0.540</cell><cell></cell><cell></cell><cell>31.88 0.692 28.66 0.597</cell><cell>38.21 0.826 34.33 0.704</cell><cell>40.10 0.931 37.32 0.867</cell><cell>36.69 0.787 34.88 0.772</cell><cell>40.94 0.934 37.64 0.873</cell><cell>41.20 0.949 38.78 0.908</cell></row><row><cell cols="6">CanonEOS700D</cell><cell>?100 ?200</cell><cell>PSNR SSIM PSNR SSIM</cell><cell></cell><cell></cell><cell cols="2">29.70 0.556 27.52 0.537</cell><cell></cell><cell></cell><cell>30.13 0.640 27.68 0.579</cell><cell>38.29 0.859 34.94 0.766</cell><cell>39.05 0.906 36.50 0.850</cell><cell>37.79 0.861 35.36 0.785</cell><cell>40.08 0.897 37.86 0.879</cell><cell>40.49 0.938 38.15 0.899</cell></row><row><cell></cell><cell>47</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>47</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>45 46</cell><cell>+0.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SID only SID+MIT5K</cell><cell></cell><cell>45 46</cell><cell>+0.36</cell><cell></cell><cell cols="3">w/o. target camera noise parameters w. target camera noise parameters</cell></row><row><cell>PSNR values (dB)</cell><cell>40 41 42 43 44</cell><cell></cell><cell>+0.11</cell><cell>+1.15</cell><cell>+1.00</cell><cell>+0.50</cell><cell>+0.66</cell><cell>PSNR values (dB)</cell><cell>40 41 42 43 44</cell><cell></cell><cell>+0.31</cell><cell>+0.24</cell><cell>+0.20</cell><cell>+0.23</cell><cell>+0.08</cell></row><row><cell></cell><cell>39</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+0.12</cell><cell></cell><cell></cell><cell>39</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+0.05</cell></row><row><cell></cell><cell>38</cell><cell>x 100</cell><cell>x 200</cell><cell></cell><cell></cell><cell></cell><cell>+0.26</cell><cell></cell><cell>38</cell><cell>x 100</cell><cell>x 200</cell><cell></cell><cell></cell><cell>+0.01</cell></row><row><cell></cell><cell>37</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>37</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">SonyA7S2</cell><cell cols="2">NikonD850</cell><cell cols="2">CanonEOS70D CanonEOS70D</cell><cell></cell><cell></cell><cell cols="2">SonyA7S2</cell><cell cols="2">NikonD850</cell><cell>CanonEOS70D CanonEOS70D</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Cameras</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Cameras</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 4 :</head><label>4</label><figDesc>Quantitative results on Fuji set of SID dataset. Note our results are obtained without noise calibration on the target Fuji camera.</figDesc><table><row><cell></cell><cell>?100</cell><cell>?250</cell><cell>?300</cell></row><row><cell>Model</cell><cell cols="3">PSNR / SSIM PSNR / SSIM PSNR / SSIM</cell></row><row><cell>BM3D</cell><cell>37.26 / 0.889</cell><cell>33.16 / 0.804</cell><cell>30.59 / 0.739</cell></row><row><cell>A-BM3D</cell><cell>37.19 / 0.879</cell><cell>30.34 / 0.746</cell><cell>28.92 / 0.678</cell></row><row><cell>Noise2Noise</cell><cell>39.03 / 0.893</cell><cell>35.18 / 0.769</cell><cell>33.54 / 0.740</cell></row><row><cell>Paired real data</cell><cell>40.13 / 0.958</cell><cell>38.56 / 0.941</cell><cell>37.44 / 0.928</cell></row><row><cell>G+P</cell><cell>38.84 / 0.923</cell><cell>34.86 / 0.815</cell><cell>32.83 / 0.753</cell></row><row><cell>Ours</cell><cell>39.88 / 0.959</cell><cell>38.33 / 0.937</cell><cell>37.23 / 0.924</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>viewed on screen with zoom)</head><label></label><figDesc></figDesc><table><row><cell>Input</cell><cell>BM3D</cell><cell>A-BM3D</cell><cell>Noise2Noise</cell><cell>G+P</cell><cell>Paired real data</cell><cell>Ours</cell><cell>Reference</cell></row><row><cell>29.54</cell><cell>35.54</cell><cell>30.06</cell><cell>35.06</cell><cell>34.53</cell><cell>36.68</cell><cell>36.46</cell><cell>?</cell></row><row><cell>13.50</cell><cell>25.85</cell><cell>27.83</cell><cell>35.70</cell><cell>29.52</cell><cell>36.79</cell><cell>37.65</cell><cell>?</cell></row><row><cell>24.62</cell><cell>31.95</cell><cell>32.87</cell><cell>33.44</cell><cell>32.42</cell><cell>38.50</cell><cell>38.74</cell><cell>?</cell></row><row><cell>25.05</cell><cell>32.27</cell><cell>33.99</cell><cell>32.73</cell><cell>32.24</cell><cell>37.50</cell><cell>37.10</cell><cell>?</cell></row><row><cell cols="8">Fig. 18: Raw image denoising results on images from SID Fuji dataset. All images are converted from raw X-Trans space to</cell></row><row><cell cols="2">sRGB for visualization. (Best</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 5 :</head><label>5</label><figDesc>Color image (RGB2RGB) denoising results on SID Sony dataset. All the results are evaluated in sRGB space.</figDesc><table><row><cell></cell><cell>?100</cell><cell>?250</cell><cell>?300</cell></row><row><cell>Model</cell><cell cols="3">PSNR / SSIM PSNR / SSIM PSNR / SSIM</cell></row><row><cell>Noise2Noise</cell><cell>19.67 / 0.479</cell><cell>15.11 / 0.332</cell><cell>14.48 / 0.321</cell></row><row><cell>Noiseflow</cell><cell>20.77 / 0.512</cell><cell>16.66 / 0.380</cell><cell>15.57 / 0.352</cell></row><row><cell>Paired real data</cell><cell>24.63 / 0.598</cell><cell>23.15 / 0.560</cell><cell>22.23 / 0.531</cell></row><row><cell>G+P</cell><cell>21.08 / 0.529</cell><cell>16.64 / 0.390</cell><cell>15.38 / 0.354</cell></row><row><cell>Ours</cell><cell>24.45 / 0.599</cell><cell>22.95 / 0.553</cell><cell>21.64 / 0.523</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 6 :</head><label>6</label><figDesc>Performance comparison of learning models applied to different stages of the image processing pipeline. We test the same set of noisy images from SID Sony dataset to generate input data of both raw and RGB formats, in which the input RGB images are generated from their corresponding raw counterparts via a pre-defined ISP. The outputs of Raw2Raw are post-processed by the ISP with oracle white balance. All the results are evaluated on sRGB space.</figDesc><table><row><cell></cell><cell>?100</cell><cell>?250</cell><cell>?300</cell></row><row><cell>Model</cell><cell cols="3">PSNR / SSIM PSNR / SSIM PSNR / SSIM</cell></row><row><cell>PAIRED DATA</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Raw2Raw+ISP</cell><cell>25.37 / 0.600</cell><cell>23.49 / 0.554</cell><cell>22.13 / 0.523</cell></row><row><cell>RGB2RGB</cell><cell>24.63 / 0.598</cell><cell>23.15 / 0.560</cell><cell>22.23 / 0.531</cell></row><row><cell>Raw2RGB</cell><cell>24.92 / 0.612</cell><cell>23.33 / 0.571</cell><cell>22.69 / 0.547</cell></row><row><cell>OURS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Raw2Raw+ISP</cell><cell>24.95 / 0.608</cell><cell>23.60 / 0.560</cell><cell>22.13 / 0.533</cell></row><row><cell>RGB2RGB</cell><cell>24.45 / 0.599</cell><cell>22.95 / 0.553</cell><cell>21.64 / 0.523</cell></row><row><cell>Raw2RGB</cell><cell>24.65 / 0.605</cell><cell>23.50 / 0.561</cell><cell>22.53 / 0.542</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 7 :</head><label>7</label><figDesc>Quantitative results on static video clips from DRV dataset.</figDesc><table><row><cell>Model</cell><cell cols="3">PSNR? SSIM? ST-RRED?</cell></row><row><cell>VBM4D [86]</cell><cell>33.29</cell><cell>0.804</cell><cell>1.568</cell></row><row><cell>Noise2Noise [51]</cell><cell>32.73</cell><cell>0.712</cell><cell>0.773</cell></row><row><cell>Noiseflow [54]</cell><cell>35.14</cell><cell>0.856</cell><cell>0.441</cell></row><row><cell>Paired real data [28]</cell><cell>37.24</cell><cell>0.903</cell><cell>0.369</cell></row><row><cell>G+P [29]</cell><cell>34.38</cell><cell>0.816</cell><cell>0.467</cell></row><row><cell>Ours</cell><cell>36.61</cell><cell>0.896</cell><cell>0.407</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 8 :</head><label>8</label><figDesc>Quantitative results on synthetic dynamic video clips from our collected dynamic video dataset. "S" and "D" indicate the multi-frame networks are trained on static and dynamic video clips respectively.</figDesc><table><row><cell>Model</cell><cell cols="3">PSNR? SSIM? ST-RRED?</cell></row><row><cell>UNET (SINGLE-FRAME)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Paired real data</cell><cell>33.66</cell><cell>0.925</cell><cell>0.482</cell></row><row><cell>Ours</cell><cell>36.40</cell><cell>0.930</cell><cell>0.230</cell></row><row><cell>FASTDVD (MULTI-FRAME)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Paired real data (S)</cell><cell>28.66</cell><cell>0.870</cell><cell>5.746</cell></row><row><cell>Ours (S)</cell><cell>28.47</cell><cell>0.860</cell><cell>5.373</cell></row><row><cell>Ours (D)</cell><cell>37.48</cell><cell>0.940</cell><cell>0.224</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Noise performance comparison of iccd with ccd and emccd cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dussault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hoess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE -The International Society for Optical Engineering</title>
		<meeting>SPIE -The International Society for Optical Engineering</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5563</biblScope>
			<biblScope unit="page" from="195" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Intensified-ccd focal plane detector for space applications: a second generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Spielmaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied optics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="2768" to="2777" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast single-photon imager acquires 1024 pixels at 100 kframe/s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guerrieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE -The International Society for Optical Engineering</title>
		<meeting>SPIE -The International Society for Optical Engineering</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">7249</biblScope>
			<biblScope unit="page">72490</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Single-photon avalanche diode imagers in biophotonics: review and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bruschini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Homulle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Antolovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Burri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Charbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Light: Science &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">First-photon imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Venkatraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cola?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">343</biblScope>
			<biblScope unit="issue">6166</biblScope>
			<biblScope unit="page" from="58" to="61" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imaging with a small number of photons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Aspden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Padgett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Singlephoton sensitive light-in-fight imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gariepy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krstaji?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Buller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heshmat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Faccio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Photon-efficient imaging with a single-photon camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Venkatraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lussana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Villa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Subpicosecond photon-efficient 3d imaging using single-photon sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reconstructing transient images from singlephoton sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>O&amp;apos;toole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2289" to="2297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-guided network for fast image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2511" to="2520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-channel weighted nuclear norm minimization for real color image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1105" to="1113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A trilateral weighted sparse coding scheme for real-world image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2802" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4549" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep boosting for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Toward convolutional blind denoising of real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1712" to="1722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real image denoising with feature attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3155" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to see in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3291" to="3300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast burst images denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Burst denoising with kernel prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2502" to="2510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Burst photography for high dynamic range and low-light imaging on mobile cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">192</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Handheld mobile photography in very low light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Liba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Karnad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Basis prediction networks for effective burst denoising with large kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">850</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Seeing motion in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3184" to="3193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Practical poissonian-gaussian noise modeling and fitting for single-image raw-data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trimeche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1737" to="1754" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unprocessing images for learned raw denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A physics-based noise formation model for extreme low-light raw denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2755" to="2764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Ccd arrays, cameras, and displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Holst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Coded aperture compressive temporal imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Llull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kittle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics express</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="10" to="526" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Electronic imaging in astronomy: detectors and instrumentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Mclean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science and Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Remote sensing of night lights: A review and an outlook for the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Kyba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>De Miguel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Rom?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Portnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Molthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jechow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page">111443</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Helium ion microscopy (him) for the imaging of biological samples at sub-nanometer resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Joens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Kasuboski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferranti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zeitvogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Obst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chalasani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">3514</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An iterative regularization method for total variation-based image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Modeling and Simulation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="460" to="489" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sparsity-based image denoising via dictionary learning and structural clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sparse representation for color image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2774" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On learning optimized reaction diffusion processes for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5261" to="5269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep joint demosaicking and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<idno>191:1-191:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Benchmarking denoising algorithms with real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2750" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A high-quality denoising dataset for smartphone cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1692" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deepisp: Learning end-to-end image processing pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to see moving objects in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7323" to="7332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Noise2noise: Learning image restoration without clean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munkberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hasselgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2971" to="2980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Noise2void -learning denoising from single noisy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-O</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2124" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Enhancing low light videos by exploring high sensitivity camera noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4110" to="4118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Noise flow: Noise modeling with conditional normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3165" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning camera-aware noise models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="343" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">High-level numerical simulations of noise in ccd and cmos photosensors: review and tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Konnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Welsh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4031</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Radiometric ccd camera calibration and noise estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondepudy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A comprehensive tool for modeling cmos imagesensor-noise performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Gow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Findlater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mcleod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Nicol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Electron Devices</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1321" to="1329" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">A model for dark current characterization and simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Baer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6068</biblScope>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Cmos image sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">El</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eltoukhy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Circuits and Devices Magazine</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="6" to="20" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sensor calibration and simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okincha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Parmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE -The International Society for Optical Engineering</title>
		<meeting>SPIE -The International Society for Optical Engineering</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">6817</biblScope>
			<biblScope unit="page" from="249" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A technique for evaluation of ccd video-camera noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Mckinnon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Unsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Woodhead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="280" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">An analysis of camera noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Boie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="671" to="674" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Noise modeling for design and simulation of computational imaging systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R D</forename><genName>Jr</genName></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5438</biblScope>
			<biblScope unit="page" from="159" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Virtual sensor design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Costantini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5301</biblScope>
			<biblScope unit="page" from="408" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Image sensors market analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Research</surname></persName>
		</author>
		<ptr target="http://www.grandviewresearch.com/industry-analysis/imagesensors-market" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>online</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A review of the pinned photodiode for ccd and cmos image sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Hondongwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of the Electron Devices Society</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="33" to="43" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">High performance cmos light detector with dark current suppression in variable-temperature systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Trap competition inducing r.t.s noise in saturation range in nmosfets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leyris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valenza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Vildeuil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE -The International Society for Optical Engineering</title>
		<meeting>SPIE -The International Society for Optical Engineering</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5844</biblScope>
			<biblScope unit="page" from="41" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Some properties of the range in samples from tukey&apos;s symmetric lambda distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Joiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Publications of the American Statistical Association</title>
		<imprint>
			<date type="published" when="1971" />
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="394" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">On-sensor dark current suppression technology dark frames are no longer necessary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Clark</surname></persName>
		</author>
		<ptr target="https://clarkvision.com/articles/dark-current-suppression-technology/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Real-time color imaging with a cmos sensor having stacked photodiodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Gilblom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ventura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ultrahigh-and High-Speed Photography, Photonics, and Videography</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5210</biblScope>
			<biblScope unit="page" from="105" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Probability plotting methods for the analysis of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Wilk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gnanadesikan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">An approximate analysis of variance test for normality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Francia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">337</biblScope>
			<biblScope unit="page" from="215" to="216" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The probability plot correlation coefficient test for normality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Filliben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="111" to="117" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Probability distributions for offshore wind speeds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lackner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Baise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy Conversion and Management</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Improved denoising via poisson mixture modeling of image sensor noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hirakawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1565" to="1578" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Computational imaging on the electric grid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sheinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Kutulakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2363" to="2372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Exposure and digital cameras, part 1: What is iso on a digital camera? when is a camera isoless? iso myths and digital cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Clark</surname></persName>
		</author>
		<ptr target="http://www.clarkvision.com/articles/iso/" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Optimal inversion of the anscombe transformation in low-count poisson image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Makitalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="109" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">An efficient statistical method for image noise level estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="477" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Learning photographic global tonal adjustment with a database of input / output image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bychkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A new in-camera imaging model for color computer vision and its application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>S?sstrunk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2289" to="2302" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Modeling the space of camera response functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Grossberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1272" to="1282" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Video denoising, deblocking, and enhancement through separable 4-d nonlocal spatiotemporal transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3952" to="3966" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Video quality assessment by reduced reference spatio-temporal entropic differencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="684" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Fastdvdnet: Towards realtime deep video denoising without flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tassano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Veit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1351" to="1360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">YOLOv5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stoken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Borovec</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4418161</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4418161" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
