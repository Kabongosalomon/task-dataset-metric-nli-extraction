<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Empirical Study of End-to-End Temporal Action Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">ByteDance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Empirical Study of End-to-End Temporal Action Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action detection (TAD) is an important yet challenging task in video understanding. It aims to simultaneously predict the semantic label and the temporal interval of every action instance in an untrimmed video. Rather than end-to-end learning, most existing methods adopt a head-only learning paradigm, where the video encoder is pre-trained for action classification, and only the detection head upon the encoder is optimized for TAD. The effect of end-to-end learning is not systematically evaluated. Besides, there lacks an in-depth study on the efficiencyaccuracy trade-off in end-to-end TAD. In this paper, we present an empirical study of end-to-end temporal action detection. We validate the advantage of end-to-end learning over head-only learning and observe up to 11% performance improvement. Besides, we study the effects of multiple design choices that affect the TAD performance and speed, including detection head, video encoder, and resolution of input videos. Based on the findings, we build a mid-resolution baseline detector, which achieves the stateof-the-art performance of end-to-end methods while running more than 4? faster. We hope that this paper can serve as a guide for end-to-end learning and inspire future research in this field. Code and models are available at https://github.com/xlliu7/E2E-TAD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the development of information technology, the numbers of videos generated and accessed are rapidly increasing, underscoring the need for automatic video understanding, such as human action recognition and temporal action detection (TAD) <ref type="bibr" target="#b0">1</ref> . Action recognition aims to predict the action label ( e.g., basketball dunk) of a short, trimmed video. Differently, TAD aims to determine the label, as well as the temporal interval of every action instance in a long untrimmed video. It is more challenging and also practical * Corresponding author <ref type="bibr" target="#b0">1</ref> Also known as temporal action localization (TAL).   in real-world actions, such as security surveillance, sports analysis, and smart video editing.</p><p>Owing to the strong discriminative power of neural networks, deep learning methods have dominated the field of temporal action detection <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57]</ref>. As depicted in <ref type="figure" target="#fig_1">Fig. 1</ref>, a temporal action detector typically consists of a video encoder and a detection head, similar to the backbone-head structure in object detection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43]</ref>. Different from modern object detectors that are trained endto-end 2 , most TAD methods adopt a head-only learning paradigm. They first pre-train the video encoder on a large action recognition dataset (e.g., Kinetics <ref type="bibr" target="#b6">[7]</ref>) then freeze it for offline feature extraction. After that, only the detection head upon the features is trained for the TAD task on the target datasets. This leaves the video features sub-optimal and restricts the performance.</p><p>Although a few works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b48">49]</ref> have adopted end-toend learning, there lacks an in-depth analysis of it. The actual benefit of end-to-end learning is still unclear. Besides, the effects of many factors in end-to-end TAD, such as the video encoder, the detection head, the image and temporal resolution of input videos, are not systematically studied. In a way, lack of such a study blocks the research of endto-end TAD. Moreover, existing works more or less neglect the efficiency, which is an important factor in real-world applications. For example, in large-scale systems, such as online video platforms, running time determines computational expenses. Unfortunately, most methods do not discuss the computation cost. A few works discuss the running time of certain parts of the full model, e.g., the detection head <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b57">58]</ref> or report inference speed (FPS) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b48">49]</ref>. But they do not explore the efficiency-accuracy trade-off. This paper aims to address the above issues.</p><p>We conduct an empirical study of end-to-end temporal action detection. Four video encoders and three detection heads with different high-level designs are evaluated on two standard TAD datasets, i.e., THUMOS14 and Activi-tyNet. Firstly, we uncover the benefit of end-to-end learning. It is shown that end-to-end trained video encoders with a medium image resolution (96 2 ) can match or surpass pretrained ones with standard image resolution (224 2 ) in terms of TAD performance. Secondly, we evaluate the effect of a series of design choices on performance and efficiency, including detection head, video encoder, image resolution and temporal resolution. It may serve as a guide for seeking the efficiency-accuracy trade-off. Lastly, we build a baseline detector based on our study. It achieves state-of-the-art performance of end-to-end TAD while running more than 4? faster (see Tab. 1). Specifically, it can process a 4-minute video in only 0.6 seconds. We hope that our work can facilitate future research in temporal action detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Temporal Action Detection Methods. Current temporal action detection methods can be roughly categorized into three groups. Anchor-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b62">63]</ref> first generate a dense set of anchors, i.e., temporal segments that may contain an action, then leverage a classifier to classify them into background or one action class. In these methods, anchors are generated by uniform sampling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b48">49]</ref>, grouping potential action boundaries <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61]</ref>, or a combination of the them <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref>. Anchor-free methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b55">56]</ref> directly predict the action class for each frame in the video. Then they group frames with the same class into temporal segments. Some methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b53">54]</ref> additionally regress the distance to action boundaries. Query-based methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b41">42]</ref> draw inspiration from the DETR object detection framework <ref type="bibr" target="#b5">[6]</ref>. They take as input a small set of learnable embeddings called action queries and video features, and map each query to an action prediction. This is achieved via Transformer attention <ref type="bibr" target="#b45">[46]</ref> that models the relations between query embeddings and video features. Owing to a one-to-one matching mechanism between ground truth actions and queries, they generate spare and unique action predictions. Different from previous methods that mostly focus on the design of network architecture or framework, we focus on the learning paradigm and efficiency-accuracy trade-off.</p><p>Video Encoders. The video encoders in TAD are adapted from action recognition networks by dropping the classification heads. In previous methods, two-stream networks (e.g., TSN <ref type="bibr" target="#b46">[47]</ref>) and 3D Convolutional Neural Networks (e.g., C3D <ref type="bibr" target="#b43">[44]</ref>, I3D <ref type="bibr" target="#b6">[7]</ref>) are commonly used video encoders. Two-stream networks, firstly proposed in <ref type="bibr" target="#b39">[40]</ref>, consist of two 2D Convolutional Neural Network (CNN) streams that operate on RGB frames and optical flow frames separately and their outputs are fused. In two-stream methods, optical flow is crucial for high performance as they explicitly capture motion cues. However, the calculation of optical flow is very expensive. Differently, 3D networks can capture motion information from a sequence of frames, at the cost of more parameters and computation than 2D networks. I3D <ref type="bibr" target="#b6">[7]</ref>, a representative of this kind, is widely used in previous TAD methods. To mitigate the above issues of 3D networks, recent methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48]</ref> use different ways to approximate 3D convolution. For example, decomposing 3D convolution into 1D and 2D convolution, or combining a temporal shift operation <ref type="bibr" target="#b21">[22]</ref> with 2D convolution. In this paper, we evaluate various video encoders to examine their performance and efficiency in temporal action detection. Their effects have not been systematically studied before.</p><p>Learning Paradigms of TAD. Most TAD methods first extract features with video encoders pre-trained on action recognition (classification) datasets (e.g. Kinetics-400 <ref type="bibr" target="#b6">[7]</ref>, similar to the role of ImageNet in image recognition). Then they train and evaluate the detection head with the extracted features. In this way, the experimental period can be greatly shortened. Therefore, it is adopted by most existing works. However, there are two issues in this learning paradigm, I3D TSM/TSN SlowFast C T <ref type="figure">Figure 2</ref>. A high-level diagram of the video encoders studied in this work. For simplicity, we do not show the spatial dimension.</p><p>task inconsistency and data inconsistency between the pretraining stage and the downstream TAD stage. To deal with the task inconsistency issue, <ref type="bibr" target="#b49">[50]</ref> designs a pre-training task that classifies synthesized video clips with different kinds of boundaries. To cope with the data inconsistency issue, some works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> pre-train the video encoder for action recognition on the target TAD dataset. This paper explores an alternative way of end-to-end training on the TAD datasets. The goal of this paper is not to compare end-toend training with other pre-training techniques. Instead, we aim to explore the effects of a series of factors on speed and accuracy and seek a trade-off between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Setup</head><p>In this section, we review the video encoders and temporal action detection heads that we study in this paper. The datasets for performance evaluation and the implementation details are also described here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Video Encoders</head><p>We mainly study four kinds of video encoders, TSN <ref type="bibr" target="#b46">[47]</ref>, TSM <ref type="bibr" target="#b21">[22]</ref>, I3D <ref type="bibr" target="#b6">[7]</ref> and SlowFast <ref type="bibr" target="#b10">[11]</ref>. <ref type="figure">Fig. 2</ref> illustrates the network structures of these video encoders.</p><p>TSN is a pure 2D CNN encoder. It processes each frame independently.</p><p>TSM combines a temporal shift operation with 2D convolution as a basic building block of video encoders. The shift operation moves a small fraction of channels of the input feature map forward and another fraction backward in the temporal axis. It is equivalent to temporal 1D convolution with constant parameters but introduces no computation cost. Spatiotemporal features from multiple frames are then captured with 2D convolution on the shifted features.</p><p>I3D follows the design of the Inception network <ref type="bibr" target="#b16">[17]</ref> for image recognition but inflates all convolutional and pooling layers into 3D counterparts. As temporal pooling is involved, it outputs feature maps with different resolution in different stages of the network.</p><p>SlowFast (SF) consists of a slow pathway and fast pathway that operate on sparsely and densely sampled video frames respectively. The fast pathway has fewer channels than the slow pathway. Therefore it can efficiently capture motion information, which is fused to the slow pathway stage by stage. It follows recent works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b44">45]</ref> to apply 1D and 2D convolution iteratively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal Action Detection Heads</head><p>We study three kinds of temporal action detection heads (methods), anchor-based, anchor-free, and query-based. G-TAD <ref type="bibr" target="#b51">[52]</ref>, AFSD <ref type="bibr" target="#b20">[21]</ref>, and TadTR <ref type="bibr" target="#b29">[30]</ref> are selected as the representative of each kind for their state-of-the-art performances. Here we briefly describe their frameworks.</p><p>G-TAD views a video as a graph and all snippets in the video as its nodes. With such a formulation, the context information in the video can be captured by graph convolution on these nodes. These nodes are sampled as potential action boundaries and paired nodes become anchors. Similar to RoIAlign <ref type="bibr" target="#b15">[16]</ref>, an SGAlign operation is designed to extract aligned features within the temporal region of each anchor. These anchors are then classified by several fully connected layers upon the aligned features.</p><p>AFSD is an anchor-free detector. Inspired by the anchorfree methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b42">43]</ref> in object detection, it detects actions by predicting the action class and the distances to action boundaries for each frame. Using this formulation, it first generates coarse action predictions with pyramid features from the video encoder. To enhance the detection performance, a saliency-based refinement module is designed. It extracts the salient features around the boundaries of each predicted action via a boundary pooling operation. These features are utilized to generate refined predictions.</p><p>TadTR views TAD as a direct set prediction problem. Based on Transformer <ref type="bibr" target="#b45">[46]</ref>, it maps a small set of learned action query embeddings to corresponding action predictions with a Transformer encoder-decoder architecture. The Transformer encoder takes as input the features from the video encoder. It models the long-range dependency in the temporal dimension with a sparse attention mechanism and captures the global context. The decoder looks up global context related to each query via cross-attention and predicts the boundaries and the action class thereon. In pursuit of more accurate boundaries and confidence scores, it utilizes a segment refinement mechanism that iteratively refines the boundaries in each decoder layer and an actions regression head that re-computes a confidence score according to the final predicted boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">End-to-end Learning</head><p>We drop the classifier in the original network of each video encoder and modify the last global pooling layer to only perform spatial pooling. Then the detection head is attached to the last layer of the encoder, resulting in a unified network. The network directly takes video frames as input and is trained with the loss functions defined by each detector. During training, gradients flow backward to both the head and the video encoder. In this way, they can be optimized simultaneously towards stronger temporal action detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Datasets</head><p>We conduct evaluations on two datasets, THU-MOS14 <ref type="bibr" target="#b17">[18]</ref> and ActivityNet <ref type="bibr" target="#b4">[5]</ref> (v1.3). THUMOS14 collects sports videos from 20 classes. It contains 200 and 212 untrimmed videos for training and testing. The actions are densely distributed and very short. The average length of videos and actions is 4.4 minutes and 5 seconds respectively. ActivityNet consists of 19994 videos in 200 action classes of daily activities. It contains 10024, 4926, and 5044 videos in the training, validation, and testing sets. Following previous work, we use the validation set for evaluation, as the annotations on the testing set are reserved by the organizers. The average length of videos and actions is 2 minutes and 48 seconds respectively.</p><p>Evaluation Metrics. For both datasets, we use mean Average Precision (mAP) at different temporal IoU thresholds as the evaluation metric. On THUMOS14, the IoU thresholds are {0.3, 0.4, 0.5, 0.6, 0.7}. On ActivityNet, we choose 10 values ranging from 0.5 to 0.95 with a step of 0.05. We also report the average of the mAP at all thresholds, which is the primary metric for performance comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head><p>Video Encoders. The SlowFast encoder has several variants. We choose the "SlowFast 4x16, R50" variant for its efficiency. Given an input clip of N frames, the fast and the slow pathway sample N and N/8 frames respectively. We resize the output features of the two pathways to the same length and concatenate them into one. The length is set to N/4. In other words, the temporal output stride is 4. I3D extract features of multiple temporal resolutions. A feature fusion strategy is applied to better utilize these features. We temporally up-sample the features from the fifth stage by 2? and fuse it with the features from the fourth stage. In this way, the temporal output stride is also 4. As a reference, the temporal output stride of TSM and TSN is 1.</p><p>Clip Sampling. We use video clips for training and evaluation. On THUMOS14, we uniformly sample clips of 25.6 seconds, which is longer than 99.6% of all action instances. The sampling stride between adjacent clips is set to 25% and 75% of the clip length during training and evaluation, respectively. Unless specially noted, TSM and TSN sample video frames at 3.75 FPS on THUMOS14. SlowFast and I3D sample frames at 10 FPS. On ActivityNet, as the ratio of action length to video length is much larger, we follow <ref type="bibr" target="#b20">[21]</ref> to treat each full video as a single clip and sample a fixed number of frames as the input to video encoders. According to <ref type="bibr" target="#b20">[21]</ref>, this strategy is better than sampling with a fixed frame rate. This number is set to 384 for SlowFast and I3D and 96 for TSM and TSN. In this way, the output features of these encoders have the same length of 96 (an average of 0.8 FPS). By default, we set the image size of the input video to 96?96, which has 5.4? fewer pixels than the commonly used 224?224 resolution.</p><p>Training. The models are trained with Adam <ref type="bibr" target="#b18">[19]</ref> optimizer, setting weight decay to 10 ?4 . The base learning rate is set to 10 ?4 and 5?10 ?5 on THUMOS14 and ActivityNet empirically. The learning rate of the video encoder is multiplied by a factor of 0.1, which helps to stabilize training. We divide the learning rate by 10 after ? 1 epochs and the total number of epochs is ? 2 . We set ? 1 = 10 and ? 2 = 12 on THUMOS14. On ActivityNet, they are set to 8 and 10, respectively. We set the batch size to 4 for all models and freeze the batch normalization layers in the video encoders. With this configuration, most models can be trained using a single GPU with 12 GB of memory. We analyze the effect of batch size in the supplementary and observe that varying batch size from 4 to 16 gives similar performance. We use cropping, horizontal flipping, rotation and photometric distortion for image augmentation. The angle range of random rotation is (-45, 45) degree. The settings of photometric distortion follow <ref type="bibr" target="#b26">[27]</ref>. The probability of the latter three transformations is 0.5.</p><p>Inference. We follow the details of each detection head in their original implementation. On ActivityNet, we follow previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b57">58]</ref> to perform classagnostic localization and use the video-level classification labels from <ref type="bibr" target="#b61">[62]</ref>. Latency is measured on a single TITAN Xp GPU, with the batch size set to 1. We take the average time of 100 runs after 10 warm-up runs. Unless specially noted, the computation costs on THUMOS14 are measured for video clips of 25.6 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Analyses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The Effect of End-to-end Learning</head><p>Head-only vs. E2E. In Tab. 2, we compare the performance of traditional head-only learning and end-to-end learning using the TadTR detector. When studying the performance gain of end-to-end learning, we keep the same mid-resolution (96 ? 96) setting. We also list the performance of head-only learning with 224? 224 resolution. We see that: (I) End-to-end learning consistently improves performance on multiple datasets and backbones. On THUMOS14, endto-end learning improves the average mAP by 9.41% and 11.21% with TSM ResNet-18 and TSM ResNet-50 encoders respectively. On ActivityNet, it achieves an im-provement of 1.30% and 1.38% average mAP with the two encoders respectively. We show that this also generalizes to other video encoders (I3D and SlowFast) and detection heads (AFSD and G-TAD) in the supplementary. (II) The performance of mid-resolution (96 2 ) end-to-end models can match or surpass that of standard-resolution (224 2 ) models trained in the head-only paradigm. On THUMOS14, the former outperforms the latter by 7.52% (45.25% vs. 37.77%) on the TSM ResNet-50 encoder. A similar observation is drawn on TSM ResNet-18. On Activ-ityNet, the performance of the above two settings is close. It indicates that end-to-end training is an effective way of enhancing efficient mid-resolution models. (III) The performance gains of end-to-end learning on Ac-tivityNet are smaller than those on THUMOS14. There are two reasons. 1) The performance gain on ActivityNet only reflects the effect of end-to-end learning on the localization sub-task, as the detectors only perform classagnostic localization on this dataset. To verify this, we evaluate the effect of end-to-end training on class-aware detection on ActivityNet. Compared with head-only learning, end-to-end learning enjoys a gain of 5.70% mAP (19.38% to 25.08%, with TSM ResNet50), which is larger than the gain on the localization sub-task. It means the classification sub-task also benefits from E2E learning. 2) ActivityNet and THUMOS14 have different characteristics. THU-MOS14 poses a great challenge to temporal localization, as the actions are shorter and each video has a large amount of background (71%) on average. Differently, on ActivityNet, actions are much longer and each video has only 36% background on average. To verify the effect of different characteristics, we conduct a comparison of E2E and head-only learning on HACS Segments <ref type="bibr" target="#b58">[59]</ref>, which shares the same classes and has a similar distribution as ActivityNet. We observe that E2E learning results in an improvement of 6.28% mAP (19.28% to 25.70%, with TSM ResNet-50), similar to the observation on class-aware detection on ActivityNet.</p><p>Image Augmentations. One particular benefit of end-toend learning is the feasibility of image augmentations. Except for the commonly used random cropping and random horizontal flipping augmentations, we also study stronger augmentations, including random rotation and random photometric distortion. The effect of these augmentations is depicted in Tab. 3. On both datasets, they result in large performance gains. On THUMOS14, random cropping brings a 3.32% improvement. Random flipping further improves the performance by 1.09%. Using stronger augmentations, the average mAP is boosted by 1.35%. In total, the improvement is 5.76%. This is reasonable as THUMOS14 is a relatively smaller dataset. On ActivityNet, the average mAP improves from 31.98% to 33.42% (+1.44%). We find that stronger data augmentations do not provide a clear performance gain, as ActivityNet is already a large-scale dataset.  It is worth noting that end-to-end learning without image augmentation performs worse than head-only learning, possibly due to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation of Design Choices</head><p>Detection Heads. Tab. 4 and Tab. 5 compare different heads on ActivityNet and THUMOS14 respectively. Note that we use the labels from the external video-level action classifiers for G-TAD following the original paper <ref type="bibr" target="#b51">[52]</ref>, as this head is designed to generate class-agnostic proposals.</p><p>Although the detection head only contributes to a small fraction of the computation cost of a detector, there are still differences between detectors in performance, computation cost, and model size. To be specific:  performance on THUMOS14, as the external action classifier restricts the classification accuracy. Making class-aware predictions like the other two heads is likely to boost its performance.</p><p>(II) Computation cost: G-TAD has much higher FLOPs than the other two heads, as it generates dense anchors. It acounts for around 1/3 of the full network's latency. TadTR has the lowest latency as it outputs very sparse detections. Therefore, reducing the number of detections is a promising direction for building efficient detectors. (III) Model size: AFSD has the smallest model size, only 66.7% that of TadTR. Therefore it is a better choice when a small model size is desired.</p><p>Video Encoders. Tab. 6 compares different encoders on THUMOS14 and ActivityNet. We observe that: (I) While using a smaller backbone reduces the computation cost, it may severely downgrade the detection performance. For example, TSM ResNet-18 achieves 7% lower average mAP than TSM ResNet-50 on THUMOS14.</p><p>(II) Motion information is important for temporal action detection. The commonly used TSN encoder falls far behind the others, for lack of motion information modeling. It is even weaker than TSM ResNet-18, which models motion information but has a smaller backbone.</p><p>(III) TSM performs on par with I3D, another typical video encoder in TAD. Meanwhile, its latency is around half of I3D. We observe that the advantage of I3D lies in mAP at high IoU thresholds, as it uses a higher sampling frame rate. Therefore TSM is a desirable replacement for I3D when there is no strict demand on localization accuracy. (IV) SlowFast achieves the best performance on both datasets. This is reasonable, as SlowFast is a state-of-theart action recognition model. Its advantage is particularly large on THUMOS14, as the fast pathway can effectively model fast-changing motion, which helps to localize short actions on this dataset. Meanwhile, it is also efficient. It has lower FLOPs than TSM R50, TSN R50, and I3D. The incosistency between FLOPs and latency might be due to the low GPU utilization at low the video resolution.</p><p>Temporal Resolution. <ref type="figure" target="#fig_2">Fig. 3</ref> compares the performance of TadTR using different input frame rates. We use temporal linear interpolation to ensure the output feature sequence has the same length. It is observed that increasing the input frame rate from 2.5 to 5 steadily improves the detection performance of TSM <ref type="bibr" target="#b21">[22]</ref> on THUMOS14, where most actions instances are very short. Therefore, we switch the encoder to SlowFast <ref type="bibr" target="#b10">[11]</ref>, which performs as well as TSM at 5 FPS but runs much faster, owing to the efficiency of its fast pathway. The performance improves by a sizable margin as the frame rate increases to 10 FPS. We show in <ref type="figure">Fig. 4</ref> that the increase is mainly from short actions. It indicates that a high frame rate is important for detecting short actions. Further increasing the frame rate does not bring a clear performance gain.</p><p>Image Resolution. <ref type="figure" target="#fig_3">Fig. 5</ref>   <ref type="figure">Figure 4</ref>. Increasing the input frame rate (from 5 FPS to 10 FPS) helps to detect short actions. Actions are divided into five groups according to their length (in seconds): XS (0, 3], S <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6]</ref>, M <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b11">12]</ref>, L <ref type="bibr" target="#b11">(12,</ref><ref type="bibr" target="#b17">18]</ref>, XL <ref type="bibr">(18, inf)</ref>. Detector: TadTR. different input image resolution on THUMOS14. The slop of each line segment roughly represents the average performance gain per pixel. We observe that: (I) Increasing image resolution boosts TAD performance, at the expense of efficiency. The improvement is especially large when the resolution increases from small (64 2 ) to medium (96 2 ). It indicates that a sufficient image resolution is critical for good performance. After that, the average performance gain per pixel gradually decreases. Therefore we choose the 64 2 resolution for a balance between performance and efficiency. (II) Increasing image resolution is less important than switching to a more suitable video encoder. We find that SlowFast ResNet-50 encoder with 96 2 resolution outperforms TSM ResNet-50 encoder with 160 2 resolution.</p><p>Due to space limit, we put the analyses of the the effect of video resolution on ActivityNet in the supplementary. We also analyze the effects of the other two design choices, feature fusion and the frame sampling manner in it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-art Methods</head><p>In the above study, we identify that SlowFast well balances between performance and accuracy and that TadTR is a strong and efficient action detection head. combine them as a baseline detector for comparison with state-of-the-art methods. The default resolution is used. Detection Performance. Tab. 7 compares the detection performance of different methods on THUMOS14 and ActivityNet. We divide them into two groups according to whether end-to-end training is used. Alghough S-CNN <ref type="bibr" target="#b38">[39]</ref>, CDC <ref type="bibr" target="#b37">[38]</ref>, and SSN <ref type="bibr" target="#b60">[61]</ref> are multi-stage methods, we still regard them as end-to-end methods as the encoder and the head are jointly optimized in each stage. We observe that: (I) On both datasets, the baseline detector achieves the best performance among end-to-end methods. This is a result of the better video encoder and the stronger detection head.</p><p>(II) Without optical flow, this detector surpasses those twostream methods that are based on pre-trained features, such as MUSES <ref type="bibr" target="#b28">[29]</ref> and VSGN <ref type="bibr" target="#b57">[58]</ref>. Similarly, AFSD-RGB also outperforms many two-stream methods. It means that optical flow is not necessary for TAD, as the video encoders learn to capture cues of action boundaries from RGB frames via end-to-end training.</p><p>Computation Cost. In Tab. 1, we already compare the computation cost with of the state-of-the-art methods. The baseline detector has a lower computation cost than previous end-to-end detector, as a result of the more efficient  <ref type="table">Table 7</ref>. State-of-the-art comparison in terms mAP at different thresholds. Only the methods in the second group are end-to-end trained.</p><p>video encoder and detection head. Compared with the stateof-the-art method <ref type="bibr" target="#b28">[29]</ref> that is based on pre-trained features, the baseline runs 126? faster. We analyze the reason for the huge difference between their computation costs in the supplementary.</p><p>Besides, we compare the inference speed in terms of inference FPS in Tab. 8. Note that this metric has a bias. It is more favorable for methods that use a high input frame rate (e.g., 25 in R-C3D <ref type="bibr" target="#b48">[49]</ref>). Therefore we also report the speedup ratio, i.e. the ratio of inference FPS to the input frame rate. Our detector runs at 5076 FPS and has a speedup ratio of 508, which is much faster than the other end-to-end methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We conduct an empirical study of end-to-end temporal action detection. We show that end-to-end training gives rise to much better performance than the traditional head-only learning paradigm, where the video encoder is only optimized for action recognition. We also study multiple factors that affect the performance and accuracy of end-to-end temporal action detection to seek a efficiencyaccuracy trade-off. Based on our findings, we build a mid-resolution detector that outperforms previous end-toend methods while running more than 4? faster. It is also encouraging that the detector surpasses the previous two- stream models without optical flow. The results show that end-to-end learning is a promising direction for building strong and efficient TAD models. Hopefully, this work can serve as a useful reference guide for end-to-end training and inspire future research. Limitation. End-to-end learning may still restrict the use of stronger video encoders, higher video resolution due to the constraint of GPU memory. In the future, we plan to explore the complementarity of end-to-end learned features with pre-trained features to address this limitation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Initialization of Video Encoders. All the video encoders are initialized with the pretrianed models on Kinetics-400 <ref type="bibr" target="#b6">[7]</ref>. This is similar to the common practice of Ima-geNet <ref type="bibr" target="#b8">[9]</ref> pre-training in image understanding.</p><p>Combination with External Video Labels on Activi-tyNet. As mentioned in the main body of the paper, most videos on ActivityNet only contain one action class. Therefore, most previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b62">63]</ref>, including some end-to-end methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref>, decompose temporal action detection (TAD) into class-agnostic temporal localization and video-level action classification. Following these works, we use the video-level action classification results of <ref type="bibr" target="#b61">[62]</ref>, a winning solution in ActivityNet Challenge 2017. To be concrete, we assign the top two video-level classes predicted by <ref type="bibr" target="#b61">[62]</ref> to all class-agnostic detections, forming class-aware detections. The confidence score of the original class-agnostic detection and the classification score by <ref type="bibr" target="#b61">[62]</ref> are fused by multiplication.</p><p>Implementation Details of AFSD. When implementing AFSD <ref type="bibr" target="#b20">[21]</ref>, we follow the details in their official code. They use a smaller batch size (1 vs. 4), a smaller learning rate (10 ?5 vs. 10 ?4 ), and a longer training schedule (16 epochs vs. 12 epochs). We tried the setting defined in this paper but observed a performance drop of around 1.5% average mAP. We also observe that using stronger augmentations does not improve performance on AFSD. Therefore, we stick to the original settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Results</head><p>The Effect of Batch Size. Tab. A1 studies the effect of batch size for training. We change the learning rate following the linear scaling rule <ref type="bibr" target="#b14">[15]</ref> when changing the batch size. We observe that increasing batch size from 4 (the default setting) to 16 results in similar performance. Therefore, we may use a larger batch size to improve GPU utilization and speed up training. For example, we can finish training of TadTR <ref type="bibr" target="#b29">[30]</ref> with TSM ResNet-18 <ref type="bibr" target="#b21">[22]</ref> encoder on ActivityNet using two NVIDIA RTX 3090 GPUs in 41 minutes. Even so, we stick to a relatively small batch size so that our experiments can be reproduced more easily.  Generality of the Effectiveness of End-to-End Training.</p><p>In the main body of this paper, we validate the effectiveness of end-to-end training on TadTR with TSM <ref type="bibr" target="#b21">[22]</ref> encoders. Here we also conduct the validation on more video encoders (SlowFast <ref type="bibr" target="#b10">[11]</ref> and I3D <ref type="bibr" target="#b6">[7]</ref>) and detection heads (AFSD <ref type="bibr" target="#b20">[21]</ref> and G-TAD <ref type="bibr" target="#b51">[52]</ref>). The results are summarized in Tab. A2. All results are obtained with the default setting. We see that end-to-end learning consistently improves detection performance. The performance gain ranges between 9.79% and 10.54% on THUMOS14 <ref type="bibr" target="#b17">[18]</ref>. On Activ-ityNet, the performance gain is at least 1.77%. The results show that the effectiveness of end-to-end learning is general. Note that the results of G-TAD with TSM ResNet-18 are lower than those in BSP <ref type="bibr" target="#b49">[50]</ref> and LowFi <ref type="bibr" target="#b50">[51]</ref>. The reason might be the low resolution of videos. It is interesting that the performance difference between TadTR and G-TAD increases when we switch from headonly learning to end-to-end learning. It indicates that traditional head-only learning might not be appropriate for benchmarking different approaches, as head-only learning restricts the performance of an approach.</p><p>Besides THUMOS14 and ActivityNet, we also evaluate the effect of end-to-end learning on HACS Segments <ref type="bibr" target="#b58">[59]</ref>. As can be observed in Tab. A3, end-to-end training improves the performance by 4.71% and 6.42% in terms of average mAP with TSM ResNet-18 and TSM ResNet-50, respectively. This again demonstrates the benefit of E2E learning and its generality.</p><p>The Effects of Image and Temporal Resolution on Activ-ityNet. <ref type="figure" target="#fig_1">Fig. A1 and Fig. A2</ref>   mance and also increases computation cost. As the image resolution reaches 96 2 , the performance gain of increasing image resolution decreases. Compared to image resolution, the detection performance is less sensitive to temporal resolution. The reason might be that the action instances on Ac-tivityNet are relatively longer than those on THUMOS14. Besides, the action classes on ActivityNet are more related to scenes than motion. This observation also supports our choice of using sparse frame sampling on ActivityNet. The Effect of Feature Fusion. In the I3D encoder, we fuse the features from the fourth stage and fifth stage. We verify the effectiveness of this strategy in Tab. A4. It is observed that this strategy improves the performance by 3%. It helps to compensate for the temporal information loss due to a decrease of the frame rate from 20 FPS to 10 FPS.</p><p>The Effect of the Frame Sampling Manner. By default, the encoder takes all frames from an input clip and extract feature in a temporally fully convolutional manner in our study. An alternative way is to sample fixed-length snippets one by one in a sliding-window manner and extract features for each snippet. The length of snippets is defined when the video encoder is pre-trained for action recognition (e.g., 8 for TSM). It is adopted by most works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b51">52]</ref> based on offline features. However, it actually increases the total computation cost as adjacent snippets overlap with each other. In end-to-end training, we can still use this manner, at the expense of efficiency. Due to a high memory usage, we are only able to conduct the experiment with TSM ResNet-18 on 4 GPUs. As can be observed in Tab. A5, the snippet-wise manner actually gives lower performance than the fully convolutional manner, probability due to a limited temporal receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Computation Cost Analyses</head><p>Reasons for the Lower Computation Cost Than <ref type="bibr" target="#b28">[29]</ref>. In Tab. 1 of the main document, we show that the detector built in this work is 126? faster (587ms vs. 74.1s) than the previous state-of-the-art non-end-to-end method <ref type="bibr" target="#b28">[29]</ref>. The speed-up comes from three aspects. Firstly, we use a smaller image size and a lower frame rate. The setting of image size and frame rate in <ref type="bibr" target="#b28">[29]</ref> is 224 2 and 30 FPS, while the setting in our case is 96 2 and 10 FPS. Secondly, we extract features in a fully convolutional manner instead of the conventional snippet-wise manner used in <ref type="bibr" target="#b28">[29]</ref>. To be concrete, they use a sliding window strategy to sample snip-Encoder N I N O FLOPs Latency TSM R18 <ref type="bibr" target="#b21">[22]</ref> 64 64 21.5G 21.2ms TSM R50 <ref type="bibr" target="#b21">[22]</ref> 64 64 48.8G 37.0ms I3D* <ref type="bibr" target="#b6">[7]</ref> 256 64 83.2G 62.1ms SlowFast R50* <ref type="bibr" target="#b10">[11]</ref> 256 64 41.4G 51.2ms C3D (VGG-11) <ref type="bibr" target="#b43">[44]</ref> 512 64 906G 277ms R(2+1)D-18 <ref type="bibr" target="#b44">[45]</ref> 512 64 960G 382ms R(2+1)D-34 <ref type="bibr" target="#b44">[45]</ref> 512 64 1803G 638ms pets of 64 frames, which is the default input length of the I3D encoder, for feature extraction. The stride between two adjacent windows is 8 frames, 1/8 of the window length. Therefore, redundant computation is introduced. Finally, the encoder (SlowFast) and the head (TadTR) are more efficient than those in <ref type="bibr" target="#b28">[29]</ref>. We note that these issues are not unique to <ref type="bibr" target="#b28">[29]</ref>. They are prevalent in previous methods based on offline features and impede the application of TAD in real-world scenarios. We believe that end-to-end TAD can help eliminate these obstacles.</p><p>Computation Cost of Various Video Encoders. Except for the video encoders studied in the main paper, we analyze the computation cost of several video encoders used in previous end-to-end methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b48">49]</ref> and the pre-training method <ref type="bibr" target="#b0">[1]</ref> in Tab. A6. As they have different settings in temporal pooling, we adjust the number of sampled frames to ensure that they output features of the same length. We observe that C3D <ref type="bibr" target="#b43">[44]</ref> and R(2+1)D-18/34 <ref type="bibr" target="#b44">[45]</ref> are much heavier than SlowFast, although the former two have shallower backbones. For example, C3D, the fastest among them, is around 5? slower than SlowFast. They are less appropriate for temporal action detection. Therefore we do not use them in our experiments.</p><p>Training Time. Using the settings described in the implementation details, training SlowFast with TadTR head takes around 4 GPU hours on THUMOS14. On ActivityNet, the training time is around 11 and 1.5 GPU hours using Slow-Fast and TSM ResNet-18, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>T ? H ? W ? 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Head-only learning (a) vs. end-to-end learning (b) for temporal action detection. Solid arrows and dashed arrows represent forward pass and the gradient flow of back propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The effect of input frame rate on TAD performance (left Y-axis, solid lines) and latency (right Y-aixs, dashed lines) on THUMOS14. Red lines and blue lines are with TSM ResNet-50 encoder and SlowFast ResNet-50 encoder respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The effect of image resolution on THUMOS14. The input frame rate is set to 3.75 and 10 for TSM and SlowFast respectively. Detector: TadTR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure A1 .Figure A2 .</head><label>A1A2</label><figDesc>The effect of spatial resolution on ActivityNet. Encoder: TSM ResNet-18. Head: TadTR. The effect of temporal resolution (number of input frames) on ActivityNet. Encoder: TSM ResNet-18. Head: TadTR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison between the baseline detector built in this work (ours) with state-of-the-art methods. The latency and FLOPs are measured at the video level. The time of optical flow extraction is not included in latency. *The time cost of I3D [7] feature extraction. E2E: end-to-end.</figDesc><table><row><cell>Method</cell><cell>E2E Flow FLOPs</cell><cell>Latency</cell><cell>mAP</cell></row><row><cell></cell><cell>THUMOS14</cell><cell></cell><cell></cell></row><row><cell>MUSES [29]</cell><cell>17.5T</cell><cell>72s*+2.1s</cell><cell>53.4</cell></row><row><cell>AFSD [21]</cell><cell>2780G</cell><cell>2472ms</cell><cell>52.0</cell></row><row><cell>Ours</cell><cell>475G</cell><cell>587ms</cell><cell>54.2</cell></row><row><cell></cell><cell>ActivityNet</cell><cell></cell><cell></cell></row><row><cell>AFSD [21]</cell><cell>499G</cell><cell>291ms</cell><cell>34.39</cell></row><row><cell>Ours</cell><cell>62G</cell><cell>63ms</cell><cell>35.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>The effect of image augmentations. Head: TadTR. Video encoder: TSM ResNet-50 on THUMOS14 and TSM ResNet-18 on ActivityNet.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>45M 49.56 35.24 9.93 34.35    Comparison of end-to-end trained detectors with different heads on ActivityNet. Encoder: I3D. All methods use 384 frames inputs (except * uses 768 frames). The values before and after each slash are measured for the full network and the head respectively.</figDesc><table><row><cell></cell><cell></cell><cell>Head</cell><cell></cell><cell cols="3">FLOPs/G Latency/ms Params</cell><cell>0.5</cell><cell>0.75 0.95 Avg.</cell></row><row><cell></cell><cell></cell><cell cols="3">AFSD* 249.4/3.3</cell><cell>145.5/26.9</cell><cell>30M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>32.90</cell></row><row><cell></cell><cell></cell><cell cols="3">G-TAD 169.2/44.6</cell><cell>99.5/31.0</cell><cell>38M</cell><cell>49.22 34.55 4.74 33.17</cell></row><row><cell></cell><cell></cell><cell>TadTR</cell><cell></cell><cell>125.6/0.9</cell><cell>78.4/9.7</cell><cell></cell></row><row><cell>Head</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7 Avg.</cell><cell></cell></row><row><cell></cell><cell cols="5">I3D with a frame rate of 10 FPS</cell><cell></cell></row><row><cell cols="6">AFSD* 57.7 52.8 45.4 34.9 22.0 43.6</cell><cell></cell></row><row><cell cols="6">G-TAD 52.5 45.9 37.6 28.5 19.1 36.7</cell><cell></cell></row><row><cell>TadTR</cell><cell cols="5">59.6 54.5 47.0 37.8 26.5 45.1</cell><cell></cell></row><row><cell cols="6">TSM ResNet-50 with a frame rate of 2.5 FPS</cell><cell></cell></row><row><cell>AFSD</cell><cell cols="5">56.0 50.0 42.2 32.8 20.5 40.3</cell><cell></cell></row><row><cell cols="6">G-TAD 51.5 43.4 33.8 23.5 13.6 33.2</cell><cell></cell></row><row><cell>TadTR</cell><cell cols="5">58.1 52.9 44.6 36.2 24.1 43.2</cell><cell></cell></row></table><note>I) Performance: On both datasets, the query-based detec- tor TadTR achieves the best performance. Its advantage is large in mAP at high IoU thresholds. Specifically, it outper- forms G-TAD by 5.19% at the strict IoU threshold 0.95 on ActivityNet. On THUMOS14, it outperforms AFSD [21] by 4.5% in terms of mAP@0.7 on THUMOS14 using the I3D encoder. We observe that G-TAD achieves much lower</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table /><note>Comparison of end-to-end trained detectors with different heads on THUMOS14. * Results from [21].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>47.9 39.8 30.7 20.3 38.3 49.12 34.00 9.74 33.42 TSM R50 73.2G 41.4ms 36M 60.5 55.5 47.5 37.6 25.3 45.3 49.59 34.74 9.72 34.14 TSN R50 73.2G 41.4ms 36M 44.2 39.6 31.9 22.9 13.7 30.5 48.97 33.26 7.84 32.Comparison of end-to-end trained detectors with different video encoders. FLOPs and latency are measured on ActivityNet.</figDesc><table><row><cell>compares the performance with</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>56.0 46.4 34.9 54.2 50.47 35.99 10.83 35.10</figDesc><table><row><cell>Method</cell><cell>Encoder Flow</cell><cell>0.3</cell><cell>0.4</cell><cell cols="2">THUMOS14 0.5 0.6</cell><cell cols="2">0.7 Avg.</cell><cell>0.5</cell><cell cols="2">ActivityNet 0.75 0.95</cell><cell>Avg.</cell></row><row><cell cols="2">Yeung et al. [55] VGG16</cell><cell cols="3">36.0 26.4 17.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TAL-Net [8]</cell><cell>I3D</cell><cell cols="10">53.2 48.5 42.8 33.8 20.8 39.8 38.23 18.30 1.30 20.22</cell></row><row><cell>BSN [25]</cell><cell>TSN</cell><cell>53.5</cell><cell>45</cell><cell cols="2">36.9 28.4</cell><cell>20</cell><cell>-</cell><cell cols="4">46.45 29.96 8.02 30.03</cell></row><row><cell>BMN [23]</cell><cell>TSN</cell><cell cols="8">56.0 47.4 38.8 29.7 20.5 38.5 50.07 34.7</cell><cell cols="2">8.29 33.85</cell></row><row><cell>G-TAD [52]</cell><cell>TSN</cell><cell cols="10">54.5 47.6 40.2 30.8 23.4 39.3 50.36 34.60 9.02 34.09</cell></row><row><cell>BC-GNN [2]</cell><cell>TSN</cell><cell cols="10">57.1 49.1 40.4 31.2 23.1 40.2 50.56 34.75 9.37 34.26</cell></row><row><cell>A2Net [54]</cell><cell>I3D</cell><cell cols="10">58.6 54.1 45.5 32.5 17.2 41.6 43.55 28.69 3.70 27.75</cell></row><row><cell>P-GCN [57]</cell><cell>I3D</cell><cell cols="3">63.6 57.8 49.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">48.26 33.16 3.27 31.11</cell></row><row><cell>MUSES [29]</cell><cell>I3D</cell><cell cols="10">68.9 64.0 56.9 46.3 31.0 53.4 50.02 34.97 6.57 33.99</cell></row><row><cell>VSGN [58]</cell><cell>TSN</cell><cell cols="10">66.7 60.4 52.4 41.0 30.4 50.2 52.38 36.01 8.37 35.07</cell></row><row><cell>S-CNN [39]</cell><cell>C3D</cell><cell cols="2">36.3 28.7</cell><cell>19</cell><cell cols="2">10.3 5.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>R-C3D [49]</cell><cell>C3D</cell><cell cols="3">44.8 35.6 28.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>26.80</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SS-TAD [3]</cell><cell>C3D</cell><cell>45.7</cell><cell>-</cell><cell>29.2</cell><cell>-</cell><cell>9.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CDC [38]</cell><cell>C3D</cell><cell cols="5">40.1 29.4 23.3 13.1 7.9</cell><cell>22.8</cell><cell>45.3</cell><cell>26.0</cell><cell>0.2</cell><cell>23.8</cell></row><row><cell>SSN [62]</cell><cell>TSN</cell><cell cols="3">51.9 41.0 29.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GTAN [32]</cell><cell>P3D</cell><cell cols="3">57.8 47.2 38.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">52.61 34.14 8.91 34.31</cell></row><row><cell>PBRNet [26]</cell><cell>I3D</cell><cell cols="10">58.5 54.6 51.3 41.8 29.5 47.1 53.96 34.97 8.98 35.01</cell></row><row><cell>AFSD [21]</cell><cell>I3D</cell><cell cols="10">67.3 62.4 55.5 43.7 31.1 52.0 52.38 35.27 6.47 34.39</cell></row><row><cell>AFSD-RGB [21]</cell><cell>I3D</cell><cell cols="6">57.7 52.8 45.4 34.9 22.0 43.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>32.90</cell></row><row><cell>Ours</cell><cell>SF R50</cell><cell cols="2">69.4 64.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Comparison of the inference speed, measure by the number of processed frames per second (FPS) and the speedup ratio.</figDesc><table><row><cell>Model</cell><cell>GPU</cell><cell cols="2">Infer. FPS Speedup</cell></row><row><cell>S-CNN [39]</cell><cell>-</cell><cell>60</cell><cell>-</cell></row><row><cell>CDC [38]</cell><cell>TITAN Xm</cell><cell>500</cell><cell>-</cell></row><row><cell cols="2">SS-TAD [3] TITAN Xm</cell><cell>701</cell><cell>23</cell></row><row><cell cols="2">R-C3D [49] TITAN Xm</cell><cell>569</cell><cell>23</cell></row><row><cell cols="2">R-C3D [49] TITAN Xp</cell><cell>1030</cell><cell>45</cell></row><row><cell>AFSD [21]</cell><cell>TITAN Xp</cell><cell>3403*</cell><cell>340*</cell></row><row><cell>Ours</cell><cell>TITAN Xp</cell><cell>5076</cell><cell>508</cell></row></table><note>*Only measure the RGB network.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A1 .</head><label>A1</label><figDesc>The effect of batch size, measured by average mAP on ActivityNet. Encoder: TSM ResNet-18. Detector: TadTR. Only cropping and horizontal flipping augmentations are used. All models are trained using a single GPU (except * uses two GPUs).</figDesc><table><row><cell>Batch Size</cell><cell>4</cell><cell>8</cell><cell>16</cell></row><row><cell>mAP</cell><cell>33.40</cell><cell>33.43</cell><cell>33.25</cell></row><row><cell cols="4">Training Time 96min 85min 41min*</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A2</head><label>A2</label><figDesc></figDesc><table><row><cell>. End-to-end learning is effective for different video en-</cell></row><row><cell>coders and detection heads. SF: SlowFast. R18/50: ResNet-18/50.</cell></row><row><cell>Performance measured by average mAP.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Head-only 26.29 16.14 4.59 16.56 E2E 33.02 20.66 6.50 21.27 (+4.71)</figDesc><table><row><cell>Paradigm</cell><cell>0.5</cell><cell>0.75 0.95</cell><cell>Avg. (Gain)</cell></row><row><cell></cell><cell cols="2">TSM ResNet-18</cell><cell></cell></row><row><cell></cell><cell cols="2">TSM ResNet-50</cell><cell></cell></row><row><cell cols="3">Head-only 30.69 18.94 5.26</cell><cell>19.28</cell></row><row><cell>E2E</cell><cell cols="3">40.32 24.97 7.71 25.70 (+6.42)</cell></row><row><cell cols="4">Table A3. The effect of end-to-end learning on HACS Segments.</cell></row><row><cell cols="3">Encoder: TSM ResNet-18/50. Head: TadTR.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>illustrate the effects of image</cell></row><row><cell></cell><cell></cell><cell></cell><cell>resolution and temporal resolution (number of input frames)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>on ActivityNet, respectively. Similar to THUMOS14, in-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>creasing image resolution steadily boosts detection perfor-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table A4 .Table A5 .</head><label>A4A5</label><figDesc>The effect of multi-scale feature fusion. Encoder: I3D. The effect of frame sampling manner. Encoder: TSM ResNet-18. Detector: TadTR. Dataset: THUMOS14. FLOPs are measured on clips of 25.6 seconds.</figDesc><table><row><cell>Frame Rate</cell><cell>20 FPS</cell><cell>10 FPS</cell></row><row><cell>Feature Fusion</cell><cell>-</cell><cell>-</cell></row><row><cell>mAP</cell><cell>47.5</cell><cell>42.1 45.1</cell></row><row><cell>Detector: TadTR.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">mAP FLOPs</cell></row><row><cell>snippet-wise</cell><cell cols="2">34.25 171.8G</cell></row><row><cell cols="3">fully-convolutional 36.12 21.5G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table A6 .</head><label>A6</label><figDesc>Comparison of the computation costs of various video encoders, measured on video clips of 25.6 seconds. NI and NO are the number of input frames and the length of output features, respectively. Image resolution: 96 2 . *Modified by us to make a temporal output stride of 4.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">"End-to-end" has diverse meanings in literature. Here we mean joint learning of the video encoder and the detection head in a detector.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tsp: Temporally-sensitive pretraining of video encoders for localization tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops, 2021. 3</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Boundary content graph neural network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueran</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end, single-stream temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6373" to="6382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ctap: Complementary temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3648" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Thumos challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Three birds with one stone: Multitask temporal action detection via recycling temporal annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4751" to="4760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning salient boundary feature for anchor-free temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Progressive boundary refinement network for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-shot temporal event localization: a benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12596" to="12606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-shot temporal event localization: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">End-to-end temporal action detection with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10271</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3604" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchen</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Temporal gaussian mixture layer for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Changxin Gao, and Nong Sang. Temporal context aggregation network for temporal action proposal refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="485" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Borderdet: Border feature for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="549" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Class semantics-based attention for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niamul</forename><surname>Quader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikanth</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juwei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Relaxed transformer decoders for direct action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="13526" to="13535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">R-c3d: region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Boundary-sensitive pre-training for temporal localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>P?rez-R?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Low-fidelity video encoder optimization for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>Perez-Rua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. G-Tad</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Background-click supervision for temporal action localization. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Revisiting anchor mechanisms for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="8535" to="8548" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Temporal action localization by structured maximal sums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">C</forename><surname>Ze-Huan Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Video selfstitching graph network for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">HACS: human action clips and segments dataset for recognition and temporal localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Bottom-up temporal action localization with mutual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peisen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08011</idno>
		<title level="m">CUHK &amp; ETHZ &amp; SIAT submission to ActivityNet challenge 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Enriching local and global contexts for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>Nanning Zheng, and Gang Hua</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
