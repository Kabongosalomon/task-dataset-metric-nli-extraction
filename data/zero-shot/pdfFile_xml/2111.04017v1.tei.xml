<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Out-of-Domain Human Mesh Reconstruction via Dynamic Bilevel Online Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20211">MARCH 2021 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanyan</forename><surname>Guan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><forename type="middle">Z</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">Out-of-Domain Human Mesh Reconstruction via Dynamic Bilevel Online Adaptation</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</title>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
							<date type="published" when="20211">MARCH 2021 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider a new problem of adapting a human mesh reconstruction model to out-of-domain streaming videos, where performance of existing SMPL-based models are significantly affected by the distribution shift represented by different camera parameters, bone lengths, backgrounds, and occlusions. We tackle this problem through online adaptation, gradually correcting the model bias during testing. There are two main challenges: First, the lack of 3D annotations increases the training difficulty and results in 3D ambiguities. Second, non-stationary data distribution makes it difficult to strike a balance between fitting regular frames and hard samples with severe occlusions or dramatic changes. To this end, we propose the Dynamic Bilevel Online Adaptation algorithm (DynaBOA). It first introduces the temporal constraints to compensate for the unavailable 3D annotations, and leverages a bilevel optimization procedure to address the conflicts between multi-objectives. DynaBOA provides additional 3D guidance by co-training with similar source examples retrieved efficiently despite the distribution shift. Furthermore, it can adaptively adjust the number of optimization steps on individual frames to fully fit hard samples and avoid overfitting regular frames. DynaBOA achieves state-of-the-art results on three out-of-domain human mesh reconstruction benchmarks. ! 1. It is a common practice to use the ground-truth 2D keypoints for crossdomain human mesh reconstruction and pose estimation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Human mesh reconstruction from streaming videos shows remarkable significance in real-world applications, e.g., sports performance analysis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, VR/AR <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> and human-computer interaction <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Most existing models assume that training and testing videos are identically distributed <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. However, this assumption does not always hold in practice, as it is difficult to cover the richness of in-the-wild data during the training phase, so that there can be large distribution shifts between training and testing scenarios. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, typical distribution shifts are reflected in camera parameters, lengths of body bones, backgrounds, and occlusions. Reconstructing human mesh from out-of-domain videos is under-explored and challenging, in which most of the advanced models, including SPIN <ref type="bibr" target="#b10">[11]</ref> and VIBE <ref type="bibr" target="#b15">[16]</ref>, underperform due to inadequate generalization ability.</p><p>In this paper, we study the problem of out-of-domain human mesh reconstruction from a new perspective, i.e., adapting the model online at test time before making the prediction. The key insight is that the test samples, although being unlabeled, contain visual information that can be properly used to progressively correct the bias of the model against the distribution shift and thus improve its performance on subsequent test frames. There are two major challenges in the online adaptation framework. First, the lack of 3D annotations for test data increases the training difficulty at inference time. Second, we observe that the online adaptation can be easily influenced by the hard samples in the rapidly changing environment of the streaming test domain.</p><p>For the first challenge, despite the lack of 3D annotations for test data, some optimization-based approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> provide alternative solutions that iteratively fit on each test frame ? S. <ref type="bibr">Guan</ref>  Other domain gaps are reflected in the statistics of body skeleton and camera parameters. Bone length refers to the sum of the lengths between human joints, whose topology is shared across datasets.</p><p>through frame-wise losses, e.g., the pose re-projection loss of 2D keypoints <ref type="bibr" target="#b0">1</ref> . Notably, these objective functions are imperfect due to inevitable mismatches with 3D evaluation metrics, and therefore do not always lead to effective online adaptation. For example, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the optimization of 2D re-projection loss may lead to ambiguity in the learning process related to depth information, thus affecting the quality of mesh reconstruction. To reduce the ambiguity, we first propose to use temporal constraints to regularize the training process of 2D losses. However, we find that simply combining multiple objectives leads to undesirable results due to the competition and incompatibility between them, in the sense that the gradient of the loss terms may interfere with each other. To solve this problem, we propose a new training approach named Bilevel Online Adaptation (BOA) that greatly benefits joint learning of frame-wise losses and temporal constraints. Specifically, when a new frame is observed, the lower-level optimization step of BOA probes rational model parameters under frame-wise losses. On this basis, the upper-level optimization step is to find feasible responses to the overall objectives with temporal constraints and then update the model with approximated second-order derivatives <ref type="bibr" target="#b21">[22]</ref>. To further reduce the mismatch between the online learning objective and the 3D evaluation metrics, in BOA, we leverage 3D annotations that are available and effective in the training set, and co-train the model with data samples from both source and target domains. As an extension of its predecessor at CVPR'2021 <ref type="bibr" target="#b22">[23]</ref>, we efficiently perform feature retrieval on different clusters of the offline training set, and use the obtained source video frames with 2D postures similar to the target sample as training guidance.</p><p>For the second challenge, since the streaming video frames arrive asynchronously, the distribution of test data observed is usually non-stationary. Therefore, it is difficult to strike a balance between avoiding over-fitting regular frames and avoiding underfitting hard samples that are referred to as "key-frames" and typically correspond to severe occlusion, sudden movement, and scene changes. In this paper, we improve our previous work with a dynamic learning strategy termed as DynaBOA, which adaptively refines the upper-level optimization phase for key-frames through multiple training steps, so that the model can fully adapt to these hard samples. The key idea is that when adapting the model on key-frames, the parameters would change dramatically responding to non-stationary data distribution, because the priors learned from regular frames cannot be fully reused. Based on this fact, DynaBOA measures the distance between features before and after a round of bilevel optimization, and dynamically adjusts the number of training iterations on each frame until feature convergence.</p><p>This paper also makes a substantial extension to the experiments of its preliminary work. We use Human 3.6M <ref type="bibr" target="#b23">[24]</ref> as the source domain and perform DynaBOA on three challenging human mesh reconstruction datasets with streaming video frames, namely 3DPW <ref type="bibr" target="#b24">[25]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b25">[26]</ref>, and SURREAL <ref type="bibr" target="#b26">[27]</ref>. These datasets yield remarkable domain gaps from the source training set. Our approach significantly outperforms baselines in quantitative evaluation and qualitative results. Further, we present analyses to validate the effectiveness of each component of DynaBOA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">3D Human Mesh Reconstruction</head><p>SMPL <ref type="bibr" target="#b27">[28]</ref> is a widely used parametric model for 3D human mesh reconstruction, which is also used in this work. Early methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> gradually fit a standard T-Pose SMPL model to an input image based on the silhouettes <ref type="bibr" target="#b30">[31]</ref> or 2D keypoints <ref type="bibr" target="#b17">[18]</ref>. These optimization-based methods are timeconsuming, struggling at the inference time on a single input. Recently, many approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> use deep neural networks to regress the parameters of the SMPL model, which are efficient and can produce more accurate reconstruction results if large-scale 3D data is available. However, most existing datasets with accurate 3D annotations are captured in constrained environments, such as a green screen studio. It's challenging for models trained on these datasets to generalize well to in-the-wild images. To tackle this problem, Kanazawa et al. <ref type="bibr" target="#b8">[9]</ref> proposed an adversarial training framework to utilize the unpaired 3D annotations to facilitate the reconstruction of int-the-wild data. Other approaches <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> improves the in-the-wild performance by designing more effective temporal features <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> or employing more informative input such as RGB-D <ref type="bibr" target="#b35">[36]</ref> and segmentation images <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>. Unlike the above methods, in this work, we further tackle this problem in out-of-domain streaming scenarios by using an online adaptation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unsupervised Online Adaptation</head><p>In this paper, we present a pilot study of unsupervised online adaptation in the context of human mesh reconstruction, which refers to sequentially adapting a pre-trained model to streaming test data without 3D labels of the target domain. It is an emerging technique to prevent model crashing when the test data has a significant distribution shift from the training data. Previous methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref> use the online adaptation framework for tasks other than mesh reconstruction, such as video segmentation <ref type="bibr" target="#b43">[44]</ref>, tracking <ref type="bibr" target="#b42">[43]</ref>, and stereo matching <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>. Beyond unsupervised online adaptation, many previous approaches effectively learn generalizable features through meta-learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>, learning domain-invariant representations <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>, or learning with adversarial examples <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref>. However, none of these approaches have been focused on how to online adaptation on streaming data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Setup</head><p>The SMPL-based solution to human mesh reconstruction can be usually specified as a tuple of (X, ?, ? C , L), where X denotes the observation space; ? = {?, ?} denotes the parameters of the SMPL model <ref type="bibr" target="#b27">[28]</ref>, where ? and ? correspond to the shape and the posture of human body respectively. For each input frame x t ? X, a first-stage model is trained to estimate ? t . Then the SMPL model generates the corresponding mesh and recovers 3D keypoints denoted by J t using a mesh-to-3D-skeleton mapping pre-defined in SMPL. The third element in the tuple is a weakperspective projection model for projecting J t to 2D space, i.e., j t = ? Ct ( J t ), where C t denotes the parameters of the weakperspective projection estimated from x t . The last one in the tuple defines a loss function L(?) on ( ? t , ? t , C t , J t , j t ) to learn the first-stage model M ? , usually in terms of neural networks.</p><p>In this work, we make two special modifications to the above task. First, we focus on out-of-domain scenarios, in the sense that large discrepancies may exist between the data distributions of the source training domain D src and the target test domain D trg . Second, we specifically focus on dealing with streaming video frames {x t } T t=1 that arrive in a sequential order at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Online Adaptation Framework</head><p>This paper provides an early study of applying an online adaptation framework to test data to solve the out-of-domain 3D mesh reconstruction problem. In this section, we introduce the general framework of test-time adaptation and provide a further discussion about its two main training difficulties. We denote the pre-trained model from the source domain as M ? 0 . Note that we do not have special requirements for the pretraining method. Typically, M ? 0 has an encoder to extract image features, and a regressor to estimate the parameters of SMPL and the camera configurations from the image features.</p><p>Given sequentially arrived target video frames {x t } T t=1 ? D trg , a straightforward solution to quickly absorbing the domain-specific knowledge is to fine-tune M consecutively on each individual x t , following the online adaptation paradigm proposed by Tonioni et al. <ref type="bibr" target="#b44">[45]</ref>. We take it as a baseline algorithm that computes the loss function L with pose constraints 2 on each x t , and performs a single optimization step as follows before the inference step:</p><formula xml:id="formula_0">? t ? ? t?1 ? ?? ? L(x t ; M ? t?1 ),<label>(1)</label></formula><p>where ? is the learning rate of gradient descent. There are two potential drawbacks in the baseline algorithm to be improved:</p><p>? Although fine-tuning a learned model on unlabeled target data may help to handle continuously changing test environments, due to the lack of 3D supervisions, an imperfect frame-wise loss may lead to wrong directions of gradient descent and thus increase the 3D ambiguities of the final results. ? Because of the non-stationarity of the streaming data, it is difficult to strike a balance between avoiding overfitting of regular frames, which will increase the training difficulty on subsequent videos, and avoiding under-fitting of key-frames, which correspond to severe occlusions, sudden movements, or scene changes. We observe that a temporally uniform optimization strategy, i.e., Eq. (1), cannot effectively adapt the model to the hard cases in the key-frames, which may also have negative and long-term effects on the results of subsequent regular frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPROACH</head><p>In this section, we propose a new training approach named dynamic bilevel online adaptation (DynaBOA) as a solution to out-of-domain human mesh reconstruction from streaming videos. <ref type="figure">Fig. 3</ref> gives an overview of the entire DynaBOA framework. DynaBOA has the following three technical contributions to solve the aforementioned two challenges in online adaptation. For the first challenge of lacking 3D supervisions in the new domain:</p><p>2. We discuss more about the specific forms of L in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Bilevel Online Adaptation</head><p>Input: Sequential frames {xt} T t=1 from the test set, a base model M ? 0 parameterized by ? 0 , the learning rates ? and ?, the moving average rate ? of the teacher model. Output: SMPL and camera parameters ?t, Ct.</p><p>1: # Initialize the teacher model 2: Initialize ?0 ? ? 0 3: for t = 1, . . . , T do 4:</p><p># Lower-level weight probe 5:</p><formula xml:id="formula_1">? t ? ? t?1 ? ?? ? (LF (xt; ? t?1 ))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p># Upper-level weight update 7:</p><formula xml:id="formula_2">? t ? ? t?1 ? ?? ? (LT (xt, xt?? ; ?t?1, ? t ) + LF (xt; ? t ))</formula><p>8:</p><p># Update the teacher model 9:</p><formula xml:id="formula_3">?t ? ??t?1 + (1 ? ?)? t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p># Infer SMPL and camera parameters 11:</p><p>?t, Ct ? M ? t (xt) 12: end for ? Bilevel online adaptation (Section 4.1): we introduce additional temporal constraints and alleviate the conflicts between the spacetime multi-objectives through a bilevel online adaptation (BOA) procedure. ? 3D exemplar guidance (Section 4.2): It regularizes the training process on the streaming data by retrieving similar source exemplar, and including the well-defined 3D annotations in bilevel optimization.</p><p>For the second challenge of under-fitting key-frames due to the non-stationarity of streaming data:</p><p>? Dynamic update strategy (Section 4.3): We improve BOA with the dynamic update strategy, which fully adapts the model to hard streaming samples (termed as key-frames) while adaptively avoiding overfitting regular frames.</p><p>Notably, in both Section 4.2 and Section 4.3, DynaBOA makes substantial improvements over its predecessor at CVPR'21 <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Bilevel Optimization with Space-Time Constraints</head><p>Bilevel Online Adaptation is the main body of the DynaBOA, which is illustrated in Alg. 1. In BOA, we introduce additional temporal constraints to compensate for the unavailable 3D annotations, and alleviate the conflicts between multiple training objectives through a bilevel optimization procedure.</p><p>Drawbacks of single-level optimization. In our problem setup, there are naturally strong temporal dependencies between the streaming frames, which can be leveraged to improve the quality of out-of-domain online adaptation. Suppose that we have two objectives respectively for frame-wise shape and pose constraints (L F ) and temporal consistency across frames (L T ), whose specific forms are discussed later, straightforward methods to combine L F and L T include jointly optimizing them by adding them together or iteratively optimizing L F and L T in two stages. However, as shown in <ref type="figure">Fig. 4</ref>, the above methods usually lead to sub-optimal results compared to the one only using L F . The possible reason is that there exists the competition and incompatibility between L F and L T , in the sense that the gradient of the single-frame constraint may interfere with the training of the temporal one. It motivates us to design the bilevel optimization method to prevent the overfitting of a single objective function and to make both the single-frame constraint and the time constraint work.  <ref type="figure">Fig. 3</ref>: An overview of the proposed approach of Dynamic Bilevel Online Adaptation (DynaBOA), where the lower-level training step serves as a weight probe to find a feasible response to the frame-wise pose constraints, and the upper-level training step minimizes the overall multi-objectives in space-time and updates the model with approximated second-order derivatives. The model uses an adaptive number of optimization steps to adjust dynamically to address the non-stationary distribution shift over time. It also retrieves source exemplars as 3D guidance to compensate for the lack of target 3D supervision. Finally, the model updated by DynaBOA infers the parameters ? t of SMPL and the camera configurations C t of the current frame.</p><formula xml:id="formula_4">? ! " ! ? # + ? $ + ? % ? $ + ? % &amp;'( ? ? ! !"# (? $ +? % ) ? , &amp; &amp;'( ? ? ! " ! (? # + ? $ +? % ) ? &amp; &amp; ) ! Retrieval &amp;'* &amp;</formula><formula xml:id="formula_5">? ! !"# Encoder Regressor Encoder Regressor &amp; + 2 &amp;'* Unlabeled video &amp; ? ? ? DynaBOA on ? ! ! ) !</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Lower-Level Weight Probe</head><p>We formulate the problem of identifying effective model weights under space-time multi-objectives as a bilevel optimization problem. In this setup, as shown in <ref type="figure">Fig. 3</ref>, the lower-level optimization step serves as a weight probe to rational models under single-frame pose constraints, while the upper-level optimization step finds a feasible response to temporal constraints. Specifically, for the t-th test sample, the model from the last online adaptation step, denoted by M ? t?1 , is firstly optimized with the single-frame constraints, L F , to obtain a set of temporary weights denoted by ? t . We name this procedure as the lower-level probe (Line 5 in Alg. 1), in the sense that first, ? t can be feasible responses to the easy component of multi-objectives with respect to pose priors, which best facilitates the rest of the learning procedure for temporal consistency; Second, ? t is not directly used to update M ? t?1 . At this level, we focus on the spatial constraints on individual frames:</p><formula xml:id="formula_6">L F = ? 1 ||j t ? j t || 2 2 + ? 2 ?( ? t , ? t ),<label>(2)</label></formula><p>where {? 1 , ? 2 } are the loss weights. The first loss term is the supervision of the re-projection error of 2D keypoints. The second term is the prior constraint on the shape and pose parameters ? = {?, ?}, which is a common practice in 3D human mesh reconstruction. Here, ?(?) calculates the distance of the estimated ? t , ? t to their statistic priors <ref type="bibr" target="#b2">3</ref> . The lower-level optimization step produces a hypothetic model M ?t for subsequent upper-level optimization step but does not update M ? t?1 . Note that, due to the lack of 3D supervisions in the target domain, the above L F is insufficient to recover the 3D body (see <ref type="figure" target="#fig_1">Fig. 2</ref>) where multiple 3D vertices may be projected to the same 2D position. Therefore, it is a potentially effective method to reduce 3D ambiguity by using the multi-frame information in the streaming data and optimizing the time consistency of the reconstruction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Upper-Level Weight Update</head><p>At the upper-level BOA step, we calculate the overall space-time multi-objectives using M ?t obtained from lower-level optimization, and then perform backpropagation to update the original model <ref type="bibr" target="#b2">3</ref>. These priors are obtained from a commonly-used third-party database.</p><formula xml:id="formula_7">? ! ? " ? ! + ? " ? ! 1 + ? " ( 2)</formula><p>Reconstruction Error (mm) Frame Index <ref type="figure">Fig. 4</ref>: The reconstruction errors of various single-level optimization schemes, indicating the incompatibility between the space-time multi-objectives. We use MPJPE as the evaluation metric.</p><p>? t?1 (Line 7 in Alg. 1). As for the specific form of the temporal constraints, given two images (x t?? , x t ) at a time interval ? , we define a motion loss on their 2D keypoints (j t?? , j t ) and the estimation ( j t?? , j t ) by M ?t :</p><formula xml:id="formula_8">L motion = j t ? j t?? ? j t ? j t?? 2 2 .<label>(3)</label></formula><p>Another part of the temporal constraints is to overcome the effect of catastrophic forgetting of target distribution and to avoid overfitting the current frames. We thus maintain a teacher model T ? parametrized by the exponential moving average of historical ? t (Line 9 in Alg. 1). Inspired by the work of Mean Teacher <ref type="bibr" target="#b59">[60]</ref>, we regularize the output of M ?t to be consistent with T ?t?1 :</p><formula xml:id="formula_9">L teacher = T ?t?1 (x t ) ? M ?t (x t ) 2 2 .<label>(4)</label></formula><p>L teacher is then combined with the motion loss to obtain the overall temporal constraints:</p><formula xml:id="formula_10">L T = ? 3 L motion + ? 4 L teacher ,<label>(5)</label></formula><p>where ? 3 and ? 4 indicate the weights of the temporal loss terms. These two losses are complementary: the regularization from the teacher model maintains long-term temporal information, while the motion loss focuses more on short-term consistency. As shown in Alg. 1), we finally update the model parameter based on ? t?1 with both the frame-wise losses in Eq. (2) and the temporal losses in Eq. <ref type="bibr" target="#b4">(5)</ref>. By these means, BOA avoids overfitting the temporal constraints by retaining the pose prior loss for the upper optimization level. On the other hand, it avoids overfitting the pose priors by updating the model weights based on ? t?1 instead of ? t . As a result, BOA effectively combines single-frame and temporal constraints, achieving considerable improvement over the straightforward solutions mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">3D Exemplar Guidance</head><p>In addition to using temporal constraints to effectively reduce depth ambiguity, in this part, we consider another solution from a new perspective to compensate for the lack of 3D annotations in the target domain. The basic idea is to introduce complete 3D supervisions into the BOA training process by performing cross-domain feature retrieval for source exemplars, and then train the model jointly on streaming test data and the retrieved source data. Despite the notable distribution shift between the source and target domains, we argue that some source samples, which contain similar shapes and postures to the streaming data, can be used to ease the online adaptation process. They have well-defined 3D annotations that can provide accurate 2D-to-3D exemplar guidance. By leveraging them in BOA, we can greatly reduce the impact of inconsistencies between the objective functions of model training and the measurements of model evaluation.</p><p>In the conference paper <ref type="bibr" target="#b22">[23]</ref>, we randomly select the source data from the whole source set. Although the use of random selection gains improvement, it inevitably brings in negative samples that are very different from the current target sample in terms of posture, camera viewpoint, and etc., which may partly reduce the ability of BOA in mitigating the distribution shift. Therefore, we improve the random selection with a retrieval method that is based on the similarity between samples across domains. <ref type="figure" target="#fig_3">Fig. 5</ref> shows the overview of the retrieval procedure. We conduct retrieval in the latent space of M ? to avoid interference from the noises in the observation space, e.g., lighting and backgrounds. In <ref type="figure" target="#fig_4">Fig. 6</ref>, we visualize the retrieved results to analyze the effect of exemplar retrieval. By comparing the query images in the first row and retrieved images in the second row, we can observe that performing cross-domain retrieval in the latent space of M ? obtains results with similar postures, even when the query image contains a severely occluded person or a noisy background.</p><p>The entire retrieval method includes two processing stages that are respectively conducted offline and online. In the offline processing stage, we first use M E ?0 , which is the encoder of the source model M ? 0 , to extract the source features</p><formula xml:id="formula_11">F S = {f sj } N S j=1 from the source dataset D src . N S is the number of source data. f sj ? R d f is the feature of the j-th image s j ? D src .</formula><p>As shown in the middle blue box in <ref type="figure" target="#fig_3">Fig. 5</ref>, a na?ve method is to directly search in the entire set of offline source features, termed as complete retrieval. However, since the source set is generally large (e.g., the Human3.6M <ref type="bibr" target="#b23">[24]</ref> dataset used in our work contains about 3.5 million samples), the complete retrieval strategy requires massive memory resources and time costs that are not feasible in online adaptation. Therefore, in the offline processing stage, we further group the source features F S into N C clusters by spherical k-means (see the upper orange box in <ref type="figure" target="#fig_3">Fig. 5</ref>). The cluster centers are denoted as {f cn } N C n=1 . We then randomly sample a ? Offline processing source dataset </p><formula xml:id="formula_12">? Online retrieval $ { - # * } -?/ { - # * , - # * } -?/ { - # * } -?/ source dataset !"# source dataset !"#</formula><formula xml:id="formula_13">S c * = arg max n f xt ? f cn f xt f cn ,<label>(6)</label></formula><p>where ? is 2 norm. In the following sections, we use Sim(?) to represent the cosine similarity. Next, we randomly sample K images from S c * as the auxiliary training data, which are denoted as {s c * k } k?? . Here, ? denotes the collection of indexes corresponding to the K selected source images. The efficient retrieval scheme has two advantages over the complete retrieval strategy that searches the entire source dataset (see <ref type="table" target="#tab_2">Table 1</ref>): First, it largely reduces time costs since we only calculate the similarity N C ( N S ) times between the streaming data and the cluster centers; Second, the proposed retrieval scheme is more memoryefficient, since we only store smaller subsets instead of the entire source training set.</p><p>Given the retrieved source images {s c * k } k?? and the corresponding 3D SMPL annotations {? c * k , ? c * k } k?? , we train the model M ? using the following loss function:</p><formula xml:id="formula_14">LS = ?1 ? ? ? 2 2 + ?2 ? ? ? 2 2 + ?3 J ? J 2 2 + ?4 j ? j 2 2 ,<label>(7)</label></formula><p>where we omit the superscripts and subscripts for simplicity. J denotes the 3D keypoints generated from {?, ?} by a mapping approach pre-defined in SMPL, and j indicates the ground-truth 2D keypoints. In DynaBOA, we co-train the model using Eq. <ref type="formula" target="#formula_14">(7)</ref> upon the retrieved source data and using the proposed space-time constraints upon the streaming frames. With Eq. <ref type="formula" target="#formula_14">(7)</ref>, the model can  obtain better 3D guidance to avoid depth ambiguity, and make the overall objective function better match 3D evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dynamic Update Strategy</head><p>To cope with the key-frames challenge mentioned in Section 3.2, we further improve the adaptation ability of BOA with a dynamic update strategy. Note that the key-frames correspond to the hard cases for human mesh reconstruction in non-stationary streaming data, e.g., sudden movements, severe occlusions, and scene changes. From the orange curve in <ref type="figure" target="#fig_5">Fig. 7</ref>, we can see that the reconstruction error of the original BOA <ref type="bibr" target="#b22">[23]</ref> drastically increases at key-frames, indicating a severe under-fitting problem. Intuitively, since the distribution of the observed streaming data is highly non-stationary, in the sense that the hard cases in the keyframes are beyond what has been previously learned, the model may need more than one BOA step to explore and handle the significant distribution shift at key-frames. A straightforward solution is to provide BOA with uniformly more training iterations at every frame, which, however, would greatly affect the computational efficiency online or might make the model unnecessarily overfit the regular frames with clear backgrounds, postures, and movements. Therefore, we improve BOA with a dynamic update strategy, which dynamically and adaptively adjusts the number of optimization steps according to the difficulty of each streaming sample.</p><p>Alg. 2 illustrates the procedure of the dynamic update strategy. The core of the dynamic update strategy is to measure the feature distance before and after a round of bilevel adaptation to determine whether the model M ? t needs to be refined on the current image (Lines 2-6 in Alg. 2). Specifically, given frame x t , we first extract its feature f xt ? t?1 before bilevel adaptation by M E ? t?1 , which is the encoder of the model at time step t ? 1. After bilevel optimization, we can use the encoder M E ? t of the adapted model to extract f xt ? t . Then, we calculate the cosine distance between f xt</p><formula xml:id="formula_15">? t?1 and f xt ? t as d t = 1 ? Sim(f xt ? t , f xt ? t?1 )</formula><p>. If d t is larger than a predefined threshold ?, M ? t will be refined on x t with the same objectives used at the upper-level update step. We repeat the refinement step Algorithm 2 Dynamic update strategy Input: The t-th target frame xt, the models M ? t?1 and M ? t before and after BOA (see Alg. 1), the teacher model T? t with a moving average rate ?, the feature distance threshold ?, the maximum steps of dynamic update L, the learning rate ?. Output: M ? t 1: # Extract features before and after BOA 2:</p><formula xml:id="formula_16">f x t ? t?1 , f x t ? t ? M E ? t?1 (xt), M E ? t (xt) 3: # Calculate the feature distance 4: dt ? 1 ? Sim(f x t ? t , f x t ? t?1 ) 5: l ? 0 6: while dt &gt; ? and l &lt; L do 7:</formula><p># For 3D exemplar guidance, refer to Section 4.2 <ref type="bibr" target="#b7">8</ref>:</p><formula xml:id="formula_17">{s c * k } k?? ? Retrieval(xt, {s c * k } k?? ; ? t )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p># Model refinement with upper-level objectives <ref type="bibr" target="#b9">10</ref>:</p><formula xml:id="formula_18">? t ? ? t ? ?? ? (LT (xt, xt?? ; ?t?1, ? t ) + LF (xt; ? t ) + LS({s c * k } k?? ; ? t )))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p># Re-calculate the feature distance <ref type="bibr" target="#b11">12</ref>:</p><formula xml:id="formula_19">f x t ? t ? M E ? t (xt), dt ? 1 ? Sim(f x t ? t , f x t ? t ) 13</formula><p>:</p><formula xml:id="formula_20">f x t ? t ? f x t ? t , ?t ? ? t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p># Update the teacher model <ref type="bibr" target="#b14">15</ref>:</p><formula xml:id="formula_21">?t ? ??t + (1 ? ?)? t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>l ? l + 1 17: end while on x t (Lines 8-10 in Alg. 2) and calculate the feature distance (Line 12 in Alg. 2) until it is smaller than ?. The main idea is that the model should change greatly if the input is a key-frame since it violates the previously learned test distribution, but for a regular frame, the model should change only slightly from the last time step. Moreover, to avoid excessive time costs, we set the maximum number of model refinement on each frame to L. As shown in <ref type="figure" target="#fig_5">Fig. 7</ref>, for key-frames, DynaBOA can produce more stable and accurate reconstruction results than its predecessor. Meanwhile, DynaBOA avoids overfitting the regular frames, which may cause training difficulties on subsequent streaming data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>We provide the source code and video results at https://sites. google.com/view/dynaboa, and include descriptions of network architectures and training details in the Supplementary Materials.</p><p>Datasets. We train the source model M ?0 using the Human3.6M dataset <ref type="bibr" target="#b23">[24]</ref> and respectively adapt the model to three test sets: 3DPW <ref type="bibr" target="#b24">[25]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b25">[26]</ref>, and SURREAL <ref type="bibr" target="#b26">[27]</ref>. <ref type="table" target="#tab_3">Table 2</ref> presents the statistics of typical domain gaps among these datasets.</p><p>? Human3.6M <ref type="bibr" target="#b23">[24]</ref> is captured in a red-screen studio with 4 digital video cameras. It has 11 subjects in total. Like existing approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>, we train the base model on 5 subjects (S1, S5-S8) and down-sample all videos from 50fps to 10fps. ? 3DPW <ref type="bibr" target="#b24">[25]</ref> is a large-scale in-the-wild dataset captured by a moving phone camera and IMU sensors. Different from Hu-man3.6M, 3DPW contains dynamic scenes with serve occlusions by objects or humans. Also, it includes novel actions beyond the scope of Human3.6M, such as climbing and fencing. We use its test set as the streaming target domain. ? MPI-INF-3DHP (3DHP) <ref type="bibr" target="#b25">[26]</ref> is captured with wide field-ofview cameras, leading to strong camera distortion. Like above, we use its test set as the streaming target domain, which has 2,929 images. It consists of 3 dynamic scenes, i.e., in the studio with or without a green screen, and outdoor, each with  2 subjects. All of the following factors increase the data nonstationarity of this benchmark and expand its distribution shift from Human3.6M, including various camera configurations, more complex environments, and novel actions. ? SURREAL <ref type="bibr" target="#b26">[27]</ref> is a large-scale synthetic dataset with groundtruth SMPL annotations, where most video clips have 100 frames. In addition to the appearance gap between the synthetic images and the real images in Human3.6M, SURREAL is more diverse in body shapes and postures, backgrounds, and camera views. Unlike the previous literature <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66]</ref> that typically uses the middle frames of the validation sequences to construct the test set, we take the full 100-frame sequences as the streaming test set that has 19,500 frames in total. All compared models are evaluated on the same test split.</p><p>Evaluation details. We use multiple evaluation metrics on the target datasets, including MPJPE, PA-MPJPE, PVE, PCK, and AUC. More details can be found in the Supplementary Materials. Particularly on the 3DHP dataset, we follow the existing approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref> to evaluate the reconstruction results at the 17 body joints. On the 3DPW dataset, we adopt the data preprocessing protocols (i.e., #PH, #PV, and #PS) from HMMR <ref type="bibr" target="#b66">[67]</ref>, VIBE <ref type="bibr" target="#b15">[16]</ref>, and SPIN <ref type="bibr" target="#b10">[11]</ref>. The protocols differ in two aspects (see <ref type="table" target="#tab_4">Table 3</ref>). First, unlike #PS and #PV that adopt the original SMPL labels, #PH uses the fitted neutral meshes as the ground truth, which are generated by minimizing the Euclidean vertex error between the fitted meshes and the original meshes. Second, the protocols have different criteria to select valid frames, resulting in different numbers of test frames. We find that the use of different preprocessing protocols has a significant impact on the final results (see <ref type="table" target="#tab_5">Table 4</ref>). Therefore, we present results under all three protocols and compare them with those of the state-of-the-art methods under the corresponding protocols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">3DPW</head><p>We compare DynaBOA with end-to-end approaches with framebased losses <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b75">76]</ref> and temporal losses <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b68">69]</ref>. We also include iterative optimization-based methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b77">78]</ref> into comparison. The evaluation results are presented in <ref type="table" target="#tab_5">Table 4</ref>, where the second column is the protocol used in the original literature of each compared method.</p><p>Compared with the end-to-end methods in <ref type="table" target="#tab_5">Table 4</ref> (the first block), DynaBOA achieves better performance under all evaluation protocols and particularly outperforms the methods with a carefully designed training scheme to improve the generalization ability of the model <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b71">72]</ref>. Notably, DynaBOA also outperforms the methods trained directly on the training set of 3DPW <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b75">76]</ref>. Furthermore, we provide stronger competitors, i.e., * SPIN, by fine-tuning SPIN 4 on the test set of 3DPW. We fine-tune the officially released models for 5 epochs using their source code and hyper-parameter setting. For a fair comparison, only 2D keypoints are available during fine-tuning. DynaBOA significantly outperforms * SPIN, which indicates that our test-time adaptation approach can better mitigate the distribution shift by properly exploiting the streaming data from the test domain. Moreover, DynaBOA shows its superiority in PA-MPJPE to the optimization-based methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b77">78]</ref> in <ref type="table" target="#tab_5">Table 4</ref> (the second block), which iteratively adapt the model on the test set of 3DPW in an offline manner. That is to say, the neural networks or the SMPL parameters are optimized independently on each target image. Compared with these methods, DynaBOA continuously refines the model on the new domain, where the dynamic bilevel optimization scheme, the motion losses, as well as the mean-teacher constraints, all benefit the learning process on the non-stationary distribution of the streaming data.</p><p>To better understand the improvement of DynaBOA, we also report the results of the base model M ?0 in <ref type="table" target="#tab_5">Table 4</ref> (the third block), which is trained on the source dataset only. Note that DynaBOA remarkably reduces the PVE metric by 71.1% on protocol #PH (from 307.8mm to 89.0mm), 67.2% on protocol #PV and 67.6% on protocol #PS, which further validates the superiority of dynamic bilevel online adaptation in mitigating domain gaps.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">3DHP</head><p>We then quantitatively compare DynaBOA with various state-ofthe-art methods on 3DHP, including the methods trained on the 3DHP training split <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b79">80]</ref>, trained with multi-view supervision <ref type="bibr" target="#b79">[80]</ref>, and trained with weak supervision signal <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b81">82]</ref>. Besides, we report the performance of our base model, which is trained on Human3.6M. The results are reported in <ref type="table" target="#tab_6">Table 5</ref>. We can see that the dynamic online adaptation significantly improves the performance in all metrics. After all, as mentioned in Section 5.1, the test set of 3DHP only has 2,929 valid images, which indicates that although the target streaming video is very short, DynaBOA can quickly correct the bias of the base model and improve the performance on the unseen target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">SURREAL</head><p>As a large-scale synthetic dataset, SURREAL provides a more significant distribution shift from Human3.6M (see <ref type="table" target="#tab_3">Table 2</ref>). On this dataset, we first compare DynaBOA with SPIN, a representative SMPL-based method. We also include in the comparison two typical SMPL-free methods: GraphCMR, which treats vertices as graph nodes, and DecoMR, which predicts UV position images. The compared models are trained under three different setups, with the results shown in different blocks of <ref type="table" target="#tab_7">Table 6</ref> (from top to bottom). In Lines 1-3, models are trained on Human3.6M and TABLE 7: Results of using different online optimization schemes (#PS on 3DPW). "Single" indicates the single-level multi-objective optimization, in which B4-B6 optimize multi-objectives in two stages. "?" indicates utilizing the losses on the left first and then on the right in different optimization steps. other data sources excluding the target dataset. We thus directly evaluate the officially released models of the compared methods on the test set of SURREAL. In Lines 4-6, we fine-tune the released models of GraphCMR and SPIN on the training split of SURREAL for 5 epochs under supervisions of 2D keypoints. For DecoMR, we directly use its released model that is trained on the SURREAL training data with 3D annotations. In Lines 7-9, similar to the idea of DynaBOA, we fine-tune the above models on the test split of SURREAL in an offline manner, also under the supervision of 2D keypoints. Obviously, DynaBOA deals with a more challenging problem because it is more difficult to estimate the target distribution from a single frame in the streaming data than from batches of target frames. From <ref type="table" target="#tab_7">Table 6</ref>, we have two observations: First, models trained on the SURREAL training set (Lines 4-6) obtain remarkable improvements over those trained without SURREAL data (Lines 1-3), which indicates that the distribution shift between source and target domains has significant impacts on the performance. Second, DynaBOA achieves the best PA-MPJPE result, and the second-best results in the other two metrics (slightly worse than ? DecoMR that is trained with 3D supervisions in SURREAL). Notably, ? DecoMR outperforms * DecoMR by a large margin, which validates the impact of using target domain 3D supervisions. Despite the absence of these 3D annotations, DynaBOA effectively reduces the distribution shift through space-time constraints, source exemplar guidance, as well as the bilevel optimization scheme for a compatible use of multi-objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative Results</head><p>In <ref type="figure">Fig. 8 (Top)</ref>, we present a typical showcase of mesh reconstruction on the challenging 3DPW dataset. We here select a sequence of playing guitar as the example, which is extremely different from the actions in Human3.6M. The compared methods, including VIBE and ROMP, are also specifically designed for videos and take the training data of 3DPW (along with the 3D annotations) as a part of the training set. However, in this example, the compared models cannot correctly estimate the positions of the arms and legs. In contrast, our model can better align the human meshes to the ground-truth body structures. This verifies that DynaBOA can effectively adapt the model to novel actions in streaming data.</p><p>To demonstrate that DynaBOA contributes to the adaptation to difficult key-frames, in <ref type="figure">Fig. 8 (Bottom)</ref>, we show the mesh reconstruction results of target frames under severe occlusions. We can see that the subject of interest (the man behind) is heavily </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analyses of the Bilevel Optimization Scheme</head><p>Alternative online optimization schemes. Previously in Section 4.1, we discuss the straightforward single-level optimization schemes of online adaptation. <ref type="table" target="#tab_10">Table 7</ref> gives the corresponding results obtained under the #PS protocol on 3DPW. We have the following observations. First, the space-time multi-objectives are beneficial with carefully designed optimization schemes, as B6 and BOA outperform B5 and B8 accordingly. However, as B3 performs worse than B2, we may conclude that a simple combination of the multi-objectives easily leads to sub-optimal results. Second, BOA achieves consistent improvements over B6 in all three evaluation metrics, which indicates that the bilevel optimization scheme can effectively combine the best of both frame-wise and temporal constraints. The major difference between BOA and B6 is that whether the second training step under L F + L T is to update the parameters in M ? t (in BOA) or M ?t?1 (in B6).</p><p>Different forms of temporal constraints. <ref type="table" target="#tab_9">Table 8</ref> shows the ablation studies for the two components of the proposed temporal constraints, i.e., L motion and L teacher , that are used in the upper-level optimization step of BOA. We observe that, first, the individual use of L motion (B9) and L teacher (B10) respectively improves the final results of the baseline model only trained with frame-wise losses (B8). Second, the advantages of these two loss components are complementary, as BOA consistently outperforms the two baseline models (B9 and B10). A possible reason is that L motion captures short-term temporal consistency of shapes and postures (in a short video snippet with a length of ? ), while L teacher focuses more on long-term information.</p><p>BOA vs. overfitting. As we have discussed in Section 4.1, the bilevel optimization can facilitate the compatibility of multiobjectives and prevent the model from overfitting to either L F or L T . To verify this, we increase the number of optimization steps on each test image, i.e., repeating the operations in Lines 4-9 in Alg. 1 for N times, and compare the results with a single-level optimization baseline (B3 in MPJPE 2D re-projection error 2D re-projection error 2D re-projection error B2 (in <ref type="table" target="#tab_10">Table 7</ref>) B3 (in <ref type="table" target="#tab_10">Table 7</ref>) BOA <ref type="figure" target="#fig_0">Fig. 10</ref>: Correlations between the 2D re-projection error and the evaluation metric. Each point represents a target sample. We use the color intensity to indicate the point density. The points inside the dashed ellipses are typical samples suffering from 3D ambiguity.</p><p>with an objective function of L F + L T . As shown in <ref type="figure">Fig. 9</ref>, with the growth of the optimization steps, the error of the single-level optimization scheme increases quickly in all metrics including PA-MPJPE, MPJPE, and PVE. It indicates that a direct combination of the space-time constraints is prone to result in overfitting to the current video frame, and thus makes it difficult for the model to quickly adapt to the next frame. In contrast, as N grows, BOA achieves better performance, indicating that the proposed bilevel optimization scheme can alleviate overfitting.</p><p>Can BOA reduce 3D ambiguity? In <ref type="figure" target="#fig_0">Fig. 10</ref>, the X-axis refers to the 2D keypoint re-projection error the Y-axis is the 3D mesh reconstruction error in MPJPE. Each point corresponds to a target sample, and we estimate the probability density of target samples using Gaussian kernels. The hot spot indicates areas with higher point density. The three sub-figures respectively show the results of the vanilla baseline only trained with frame-wise loss functions (B2 in <ref type="table" target="#tab_10">Table 7</ref>), the results of the baseline model trained with multi-objectives (B3), and those of the model trained with BOA. As shown by the point density inside the dashed areas (with low 2D error but high 3D error), both B2 and B3 suffer from the problem of 3D ambiguity, while BOA effectively alleviates this issue due to effective optimization of the space-time constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Analyses of New Contributions in DynaBOA</head><p>We provide in-depth ablation studies on the new components in DynaBOA based on its predecessor <ref type="bibr" target="#b22">[23]</ref>, i.e., 3D exemplar  Ablation studies. We make two extensions on the BOA: the 3D exemplar guidance and the dynamic update strategy. <ref type="table" target="#tab_11">Table 9</ref> gives the ablation results of each component, from which we observe that individually performing cross-domain retrieval or dynamic update gains significant improvements over the original BOA model. For example, B11 reduces PA-MPJPE by 2.5mm and B12 reduces PA-MPJPE by 4.1mm. Furthermore, by jointly using these two techniques in DynaBOA, we obtain more performance gains, reducing PA-MPJPE by 7.7mm. The above results suggest that the advantages of the new components are complementary, which is in line with our expectations, as the exemplar guidance is designed to compensate for the lack of direct 3D supervisions, while the dynamic update strategy is designed to enhance online adaptation on difficult streaming samples.</p><p>Analyses on 3D exemplar guidance. Compared to the previous conference version, we replace the random selection of source exemplars with efficient retrieval. To examine this modification, we take two random selection schemes as baselines: One is random selection from the whole source dataset ("Rand-All" in <ref type="figure" target="#fig_0">Fig. 11</ref>) and the other is random selection from the source clusters {S cn } N C n=1 (see <ref type="figure" target="#fig_3">Fig. 5</ref>) that are preprocessed offline ("Rand-Cluster"). In addition, one may consider what if the proposed retrieval process is only performed on the key-frames that have more than one model refinement step in the process of dynamic update ("EG-Keyframe"). From <ref type="figure" target="#fig_0">Fig. 11</ref>, we observe that random selection schemes perform much worse than the approaches based on similarity-driven retrieval. The reason is that random selection would inevitably bring in source samples with a large distribution shift from the current target sample, thus playing a negative role in online adaptation. By comparing "EG-Keyframe" with "EG-DynaBOA", which is the final scheme in this work, we find the necessity of retrieving source exemplars for every target frame, even including the regular target frames.</p><p>Analyses on the dynamic update strategy. We illustrate the advantage of the dynamic update by counting the steps of model G4 G5 G7 G6 G3 G2 G1 G0 G0 (45.72%) G1 (23.73%) G2 (18.93%) G3 (6.69%) G4 (2.71%) G5 (1.20%) G6 (0.49%) G7 (0.53%) <ref type="table" target="#tab_2">G4  G5  G7  G6   G3  G2  G1  G0</ref> G0 (45.72%) G1 (23.73%) G2 (18.93%) G3 (6.69%) G4 (2.71%) G5 (1.20%) G6 (0.49%) G7 (0.53%)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistics of Dynamic Update Times</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistics of Dynamic Update Times</head><p>Dynamic update steps after BOA Ratio of samples with improved results by dynamic update (%) G4 G5 G7 G6 G3 G2 G1 G0 G0 (45.72%) G1 (23.73%) G2 (18.93%) G3 (6.69%) G4 (2.71%) G5 (1.20%) G6 (0.49%) G7 (0.53%)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistics of Dynamic Update Times</head><p>Statistics of dynamic update steps <ref type="figure" target="#fig_0">Fig. 12</ref>: Left: The ratio of test frames with different steps of model refinement in the process of dynamic update and corresponding showcases from 3DPW. Right: The ratio of samples in the groups of G1-G7 with reconstruction results after l steps of model refinement. <ref type="figure" target="#fig_0">Fig. 13</ref>: The standard deviations of the reconstruction errors of 3DPW test samples. The superscript ? indicates adapting the models to non-stationary streaming data with highly mixed 3DPW and 3DHP videos, which is different from the standard setting <ref type="table" target="#tab_11">(Table 9</ref>). Smaller standard deviations of ? DynaBOA indicate being capable of coping with difficult key-frames. refinement after BOA at each target frame. For example, G0 represents the cases where the model is refined 0 steps after the optimization process of BOA. From the pie chart, we observe that 45.72% images (G0) have been sufficiently processed by BOA and need no further training steps, while only 4.93% images require more than 3 additional training steps. We also show typical target frames corresponding to different numbers of dynamic updates, indicating that the dynamic update strategy allows the model to be trained with an adaptive number of iterations according to the difficulty of each target frame or the degree of variations between consecutive frames. In <ref type="figure" target="#fig_0">Fig. 12 (Right)</ref>, we show the ratio of samples (%) in the groups of G1-G7 with better reconstruction results after l steps of model refinement, compared with l = 0. The upward trend of each curve indicates that the dynamic update strategy is effective for target images in all groups.</p><formula xml:id="formula_22">BOA ? B11 ? B12 ? DynaBOA ? BOA ? B11 ? B12 ? DynaBOA ? BOA ? B11 ? B12 ? DynaBOA ?</formula><p>Adapting to non-stationary streaming data with highly mixed 3DPW and 3DHP videos. To provide the test streaming data with more non-stationary changes, we mix the videos of 3DPW and 3DHP to form a new target dataset. In this case, the scene changes frequently, and the corresponding frames can be regarded as the difficult key-frame. <ref type="figure" target="#fig_0">Fig. 13</ref> gives the standard deviations of the reconstruction errors of 3DPW samples. The smaller standard deviation of ? DynaBOA implies a more stable adaptation process and that our approach can ease the training difficulty at unlabeled key-frames with dramatic scene changes. It finally achieves 41.1mm / 66.8mm / 70.4m in PA-MPJPE / MPJPE / PVE, which are comparable with the results of DynaBOA in <ref type="table" target="#tab_11">Table 9</ref>. Note that these two models are trained on different streaming sequences, but evaluated on the same set of 3DPW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we presented a new research problem of reconstructing human meshes from out-of-domain streaming videos. To tackle the distribution shift, we proposed a new test-time adaptation algorithm named DynaBOA. The basic idea is to effectively leverage temporal consistency through bilevel online adaptation (BOA), which has been shown to avoid overfitting the ambiguous 2D supervisions. Besides, we proposed to retrieve source images efficiently as exemplars guidance to further compensate for the lack of 3D annotations in the process of test-time training. Furthermore, we introduced the dynamic update strategy, a natural extension of BOA, to enhance the adaptation ability on key-frames while preventing the model from overfitting regular frames. We conducted experiments on three benchmarks with streaming data that has a remarkable distribution shift from the source domain. On all benchmarks, DynaBOA consistently outperforms the state-of-theart mesh reconstruction methods in the new out-of-domain setup, showing the ability to alleviate the training difficulty when adapting the model to highly non-stationary streaming videos.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Top: Problem setup of learning to reconstruct human meshes from out-of-domain streaming videos. In terms of visual inputs, typical domain gaps include plain/crowded backgrounds, slight/heavy occlusions, simple/complex motions, etc. Bottom:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Adapting the model with 2D re-projection losses leads to 3D ambiguity, where multiple 3D vertices are projected to the same 2D position. It may result in wrong estimation of the posture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Illustration of the cross-domain retrieval for 3D example guidance. Top: The source data is processed offline. Bottom: A comparison of the efficiency between the baseline method of complete retrieval and the proposed retrieval method. "indexing" refers to finding the data with the same index in the source domain.small number of features from each cluster and use corresponding source images to form N C subsets {S cn } N C n=1 . In the online retrieval stage, as shown in the bottom orange box inFig. 5, the feature extracted by M E ? from a query image x t ? D trg is denoted as f xt . Here, M E ? is the encoder of the entire online adaptation model parameterized by either M ?t?1 or M ?t , since the retrieval process is performed in both optimization steps of lower-level weight probe and upper-level weight update in DynaBOA. Next, we search the nearest cluster S c * by calculating the cosine similarity between f xt and the features at all cluster centers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Showcases of the query images from the target domain and the corresponding retrieved images from the source domain. The retrieved results have similar postures to the query images, even though the persons in the query images are severely occluded (highlighted in the red boxes in the two columns on the right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Performance of BOA (Alg. 1) and DynaBOA (Alg. 2) on non-stationary streaming data, where the key-frames correspond to the hard cases that can drastically increase the error of human mesh reconstruction, e.g., sudden movements, severe occlusions, and scene changes. Thanks to the dynamic update strategy, DynaBOA fully adapts to the hard samples while avoiding overfitting regular frames adaptively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 :</head><label>11</label><figDesc>Comparison of reconstruction errors (#PS protocol on 3DPW) of different retrieval schemes for 3D exemplar guidance. See text for detailed descriptions of each scheme. guidance and dynamic update. We include more experiments about hyperparameter sensitivity in Supplementary Materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, J. Xu, Y. Wang, B. Ni, and X. Yang are with MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China. ? M. Z. He is with Purdue University, USA. Work done in part at Shanghai Jiao Tong University.</figDesc><table><row><cell></cell><cell>Training Set</cell></row><row><cell>Streaming Data</cell><cell>Test Set</cell></row><row><cell>3DPW</cell><cell></cell></row></table><note>? Corresponding author: Y. Wang, yunbow@sjtu.edu.cn.? Project page: https:// sites.google.com/ view/ dynaboa.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 :</head><label>1</label><figDesc>Comparison of different retrieval strategies. We here use Human3.6M as an example source dataset.</figDesc><table><row><cell>Strategy</cell><cell>Search space</cell><cell># Compare</cell><cell>Time (ms/step)</cell><cell>Memory</cell></row><row><cell cols="3">Complete All source data N S (312 k) Efficient Cluster centers N C (0.1 k)</cell><cell>1.3 ? 10 3 4.3 ? 10 ?1</cell><cell>2.47 GB 0.24 GB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 :</head><label>2</label><figDesc>Typical domain gaps between the source dataset (Human3.6M) and the target datasets used in our experiments in terms of focal length, bone length, camera distance, and camera height<ref type="bibr" target="#b60">[61]</ref>.</figDesc><table><row><cell>Dataset</cell><cell>Type</cell><cell>Capture device/software</cell><cell>#Subjects</cell><cell>#Frames</cell><cell>Bone len. (m)</cell><cell>Focal len. (pixel)</cell><cell>Camera dist. (m)</cell><cell>Camera ht. (m)</cell></row><row><cell>Human3.6M</cell><cell>Indoor</cell><cell>VICON [62]</cell><cell>5</cell><cell>312188</cell><cell cols="2">3.9?0.1 1146.8?2.0</cell><cell>5.2?0.8</cell><cell>1.6?0.1</cell></row><row><cell>3DPW</cell><cell>In-the-wild</cell><cell>Moving camera &amp; IMUs</cell><cell>5</cell><cell cols="3">See Table 3 3.7?0.1 1962.2?1.5</cell><cell>3.5?0.7</cell><cell>0.6?0.8</cell></row><row><cell cols="3">MPI-INF-3DHP Indoor &amp; In-the-wild The Captury [63]</cell><cell>6</cell><cell>2929</cell><cell cols="2">3.7?0.1 1497.9?2.8</cell><cell>3.8?0.8</cell><cell>0.8?0.4</cell></row><row><cell>SURREAL</cell><cell>Synthetic</cell><cell>Blender [64]</cell><cell>30</cell><cell>19500</cell><cell>3.7?0.2</cell><cell>600 ? 0</cell><cell>8.0?1.0</cell><cell>0.9?0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 :</head><label>3</label><figDesc>Illustration of different protocols for data preprocessing on the 3DPW dataset, which have great impacts on the final resuls. #PV/#PS use the SMPL annotations from the original 3DPW, while #PH uses the fitted neutral labels excluding all gender information.</figDesc><table><row><cell>Protocol</cell><cell cols="2">SMPL annotation Valid frames</cell></row><row><cell>#PH (HMMR [67])</cell><cell>The fits (neutral)</cell><cell>26234</cell></row><row><cell>#PV (VIBE [16])</cell><cell>Original</cell><cell>34561</cell></row><row><cell>#PS (SPIN [11])</cell><cell>Original</cell><cell>35515</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 :</head><label>4</label><figDesc>Results on the 3DPW test set using three data preprocessing protocols. First block: End-to-end approaches. ? denotes models that are trained on the 3DPW training split. * denotes models that are fine-tuned on the 3DPW test set under 2D supervisions. Second block: Optimization-based approaches that tune the SMPL parameters on the target domain in an offline manner under the supervision of 2D keypoints.</figDesc><table><row><cell>Method</cell><cell cols="2">Prot. PA-MPJPE ?</cell><cell>MPJPE ?</cell><cell>PVE ?</cell></row><row><cell>HMR [9]</cell><cell>#PH</cell><cell>76.7</cell><cell>130.0</cell><cell>-</cell></row><row><cell>Sim2Real [68]</cell><cell>#PH</cell><cell>74.7</cell><cell>-</cell><cell></cell></row><row><cell>HMMR [67]</cell><cell>#PH</cell><cell>72.6</cell><cell>116.5</cell><cell>-</cell></row><row><cell>? VIBE [16]</cell><cell>#PV</cell><cell>51.9</cell><cell>82.9</cell><cell>99.1</cell></row><row><cell>? ROMP(HRNet-32) [69]</cell><cell>#PV</cell><cell>47.3</cell><cell>76.7</cell><cell>93.4</cell></row><row><cell>GraphCMR [70]</cell><cell>#PS</cell><cell>70.2</cell><cell>-</cell><cell>-</cell></row><row><cell>SPIN [11]</cell><cell>#PS</cell><cell>59.2</cell><cell>96.9</cell><cell>135.1</cell></row><row><cell>PyMAF [71]</cell><cell>#PS</cell><cell>58.9</cell><cell>92.8</cell><cell>110.1</cell></row><row><cell>I2L-MeshNet [15]</cell><cell>#PS</cell><cell>58.6</cell><cell>93.2</cell><cell>-</cell></row><row><cell>DaNet(HRNet-48) [72]</cell><cell>#PS</cell><cell>54.8</cell><cell>85.5</cell><cell>110.8</cell></row><row><cell>HybrIK [73]</cell><cell>#PS</cell><cell>48.8</cell><cell>80.0</cell><cell>94.5</cell></row><row><cell>? METRO [74]</cell><cell>#PS</cell><cell>47.9</cell><cell>77.1</cell><cell>88.2</cell></row><row><cell>? PARE(HRNet-32) [75]</cell><cell>#PS</cell><cell>46.4</cell><cell>79.1</cell><cell>94.2</cell></row><row><cell>? Mesh Graphormer [76]</cell><cell>#PS</cell><cell>45.6</cell><cell>74.7</cell><cell>87.7</cell></row><row><cell>*  SPIN [11]</cell><cell>#PS</cell><cell>46.0</cell><cell>71.4</cell><cell>97.0</cell></row><row><cell>SMPLify [18]</cell><cell>#PH</cell><cell>106.1</cell><cell>199.2</cell><cell>-</cell></row><row><cell>Arnab et al. [77]</cell><cell>#PH</cell><cell>72.2</cell><cell>-</cell><cell>-</cell></row><row><cell>ISO [20]</cell><cell>#PS</cell><cell>70.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Song et al. [78]</cell><cell>#PS</cell><cell>55.9</cell><cell>-</cell><cell>-</cell></row><row><cell>EFT [17]</cell><cell>#PS</cell><cell>54.2</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>#PH</cell><cell>120.8</cell><cell>276.7</cell><cell>307.8</cell></row><row><cell>Base model</cell><cell>#PV</cell><cell>123.8</cell><cell>234.4</cell><cell>257.2</cell></row><row><cell></cell><cell>#PS</cell><cell>123.4</cell><cell>230.3</cell><cell>253.4</cell></row><row><cell></cell><cell>#PH</cell><cell>44.4</cell><cell>77.4</cell><cell>89.0</cell></row><row><cell>DynaBOA</cell><cell>#PV</cell><cell>42.6</cell><cell>69.0</cell><cell>84.4</cell></row><row><cell></cell><cell>#PS</cell><cell>40.4</cell><cell>65.5</cell><cell>82.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5 :</head><label>5</label><figDesc>Results on the test split of 3DHP. In the second column, some models are shown to be trained on the 3DHP training split (D train ), while other models are trained purely on the source dataset but with special designs to improve the generalization ability.Method D train PA-MPJPE ? MPJPE ? PCK ? AUC ?</figDesc><table><row><cell>Mehta et al. [26]</cell><cell>-</cell><cell>-</cell><cell cols="2">72.5 36.9</cell></row><row><cell>Vnect [79]</cell><cell>98.0</cell><cell>124.7</cell><cell cols="2">76.6 40.4</cell></row><row><cell>EpipolarPose [80]</cell><cell>-</cell><cell>109.0</cell><cell>77.5</cell><cell>-</cell></row><row><cell>HMR [9]</cell><cell>89.8</cell><cell>124.2</cell><cell cols="2">72.9 36.5</cell></row><row><cell>SPIN [11]</cell><cell>67.5</cell><cell>105.2</cell><cell cols="2">76.4 37.1</cell></row><row><cell>Zhou et al. [81]</cell><cell>-</cell><cell>137.1</cell><cell cols="2">69.2 32.5</cell></row><row><cell>Habibie et al. [82]</cell><cell>92.0</cell><cell>127.0</cell><cell cols="2">69.6 35.5</cell></row><row><cell>Base model</cell><cell>116.4</cell><cell>199.7</cell><cell cols="2">75.6 38.4</cell></row><row><cell>DynaBOA</cell><cell>66.1</cell><cell>101.5</cell><cell cols="2">79.5 43.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6 :</head><label>6</label><figDesc>Results on the SURREAL test split (D test ). First block: models are trained with no access to the SURREAL data. Second block: models are trained on the SURREAL training split (D train ). Third block: models are trained on D test with 2D supervisions. In particular, * DecoMR is trained with 3D supervisions on D train .</figDesc><table><row><cell>Method</cell><cell>D train</cell><cell>D test</cell><cell>PA-MPJPE ?</cell><cell>MPJPE ?</cell><cell>PVE ?</cell></row><row><cell>SPIN [11]</cell><cell></cell><cell></cell><cell>68.6</cell><cell>105.6</cell><cell>133.7</cell></row><row><cell>GraphCMR [70]</cell><cell></cell><cell></cell><cell>72.0</cell><cell>127.4</cell><cell>72.0</cell></row><row><cell>DecoMR [66]</cell><cell></cell><cell></cell><cell>74.9</cell><cell>138.0</cell><cell>161.0</cell></row><row><cell>? SPIN</cell><cell></cell><cell></cell><cell>43.7</cell><cell>66.7</cell><cell>82.3</cell></row><row><cell>? GraphCMR</cell><cell></cell><cell></cell><cell>63.2</cell><cell>87.4</cell><cell>103.2</cell></row><row><cell>? DecoMR</cell><cell></cell><cell></cell><cell>43.0</cell><cell>52.0</cell><cell>68.9</cell></row><row><cell>*  SPIN</cell><cell></cell><cell></cell><cell>51.9</cell><cell>80.7</cell><cell>99.3</cell></row><row><cell>*  GraphCMR</cell><cell></cell><cell></cell><cell>89.4</cell><cell>167.2</cell><cell>189.2</cell></row><row><cell>*  DecoMR</cell><cell></cell><cell></cell><cell>116.4</cell><cell>175.8</cell><cell>227.4</cell></row><row><cell>Base model</cell><cell></cell><cell></cell><cell>143.1</cell><cell>258.0</cell><cell>278.6</cell></row><row><cell>DynaBOA</cell><cell></cell><cell></cell><cell>34.0</cell><cell>55.2</cell><cell>70.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8 :</head><label>8</label><figDesc>Ablation studies on different forms of temporal constraints used in the upper-level BOA step (#PS on 3DPW).</figDesc><table><row><cell>Model L motion</cell><cell>L teacher</cell><cell cols="2">PA-MPJPE MPJPE</cell><cell>PVE</cell></row><row><cell>B8</cell><cell></cell><cell>49.0</cell><cell>75.2</cell><cell>100.0</cell></row><row><cell>B9</cell><cell></cell><cell>48.2</cell><cell>73.7</cell><cell>98.2</cell></row><row><cell>B10</cell><cell></cell><cell>48.7</cell><cell>74.9</cell><cell>97.9</cell></row><row><cell>BOA</cell><cell></cell><cell>48.1</cell><cell>73.5</cell><cell>96.1</cell></row><row><cell cols="5">occluded by another man, especially in the middle two frames.</cell></row><row><cell cols="5">It represents an extremely out-of-domain scenario, as the source</cell></row><row><cell cols="5">domain of Human3.6M does not contain any occlusion samples.</cell></row><row><cell cols="5">Notably, both ROMP (trained on the target training set with</cell></row><row><cell cols="5">3D annotations) and the original BOA [23] (without the new</cell></row><row><cell cols="5">methods of exemplar guidance and dynamic update) fail to estimate</cell></row><row><cell cols="5">reasonable postures of the occluded man. In contrast, DynaBOA</cell></row><row><cell cols="5">is the only one that successfully survives in this hard case with</cell></row><row><cell cols="5">extraordinary domain gaps. DynaBOA also gives more accurate</cell></row><row><cell cols="5">estimations of head orientation than the compared methods. It is</cell></row><row><cell cols="5">an interesting result, because no 2D head keypoints, e.g., eyes and</cell></row><row><cell cols="5">nose, are used in the online adaptation phase. A possible reason is</cell></row><row><cell cols="5">that DynaBOA effectively transfers the prior knowledge about the</cell></row><row><cell cols="4">3D postures from the retrieved source exemplar.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc>Top: Qualitative showcases of mesh reconstruction on 3DPW streaming data, where the action of playing guitar is not included in the Human3.6M source domain. Note that VIBE and ROMP are specifically designed for videos, and they take the training data of 3DPW (along with the 3D annotations) as a part of the training set. We zoom in on the limbs for better visualization. Bottom: An example of mesh reconstruction under severe occlusion, in which the subject of interest (i.e., the man behind) is heavily occluded by another man. It is an extremely out-of-domain scenario because the source domain does not contain any occlusion samples. Results of optimizing the model N steps on each frame (i.e., repeating the operations in Lines 4-9 in Alg. 1 for N times). As N grows, BOA performs better than its single-level counterpart, showing the ability to prevent overfitting. Experiments are conducted under the #PS protocol on 3DPW.</figDesc><table><row><cell>), which trains the model</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 9 :</head><label>9</label><figDesc>Ablation studies on the new contributions of DynaBOA based on BOA (#PS on 3DPW). "Exemplar" refers to the exemplar guidance method. "Dynamic" refers to the dynamic update strategy.</figDesc><table><row><cell>Model</cell><cell cols="3">Exemplar Dynamic PA-MPJPE MPJPE PVE</cell></row><row><cell>BOA</cell><cell>48.1</cell><cell>73.6</cell><cell>96.1</cell></row><row><cell>B11</cell><cell>45.6</cell><cell>73.1</cell><cell>90.2</cell></row><row><cell>B12</cell><cell>44.0</cell><cell>67.6</cell><cell>89.2</cell></row><row><cell>DynaBOA</cell><cell>40.4</cell><cell>65.5</cell><cell>82.0</cell></row><row><cell>PA-MPJPE</cell><cell>MPJPE</cell><cell>PVE</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">. SPIN is a typical method that unifies an optimization module (e.g., SMPLify<ref type="bibr" target="#b17">[18]</ref>) and a neural network in the training framework.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was funded by NSFC (U19B2035, U20B2072, 61976137, 62106144), the Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), and the Shanghai Sailing Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Soccer on your tabletop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="4738" to="4747" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sportscap: Monocular 3d human motion capture and finegrained understanding in challenging sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11452</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A virtual reality platform for dynamic human-scene interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH ASIA</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards fully mobile 3d face, body, and environment capture using only head-worn cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rewkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TVCG</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2993" to="3004" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Photo wake-up: 3d character animation from a single photo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5908" to="5917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Perceiving 3d human-object spatial arrangements from a single image in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pepose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="34" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human poseitioning system (hps): 3d human pose estimation and self-localization in large scenes from body-mounted sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guzov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4318" to="4329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Populating 3d scenes by learning human-scene interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Endto-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7122" to="7131" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="459" to="468" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via modelfitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structured prediction helps 3D human motion modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aksan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7144" to="7153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Denserac: Joint 3D pose and shape estimation by dense render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7760" to="7770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3D human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">I2l-MeshNet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">VIBE: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5253" to="5263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exemplar Fine-Tuning for 3d human pose fitting towards in-the-wild 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03686</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Expressive Body Capture: 3D Hands, Face, and Body from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="975" to="985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Inference stage optimization for cross-scenario 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Poseaug: A differentiable pose augmentation framework for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8575" to="8584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bilevel online adaptation for out-of-domain human mesh reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">481</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hu-man3.6M: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using IMUs and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="601" to="617" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Estimating human shape and pose from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1381" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Combined discriminative and generative articulated pose and non-rigid shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1337" to="1344" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3D and 2D human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6050" to="6059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Towards accurate marker-less human shape and pose estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Monocular 3D pose and shape estimation of multiple people in natural scenesthe importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="2148" to="2157" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeletondisentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5349" to="5358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="5242" to="5252" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Towards robust RGB-D human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07383</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in 3DV</title>
		<imprint>
			<biblScope unit="page" from="484" to="494" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Chained representation cycling: Learning to estimate 3d human pose and shape by cycling between representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rueegg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Streaming variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Broderick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wibisono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1727" to="1735" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adapting to continuously shifting domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bobu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning to Adapt to Evolving Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Meta-tracker: Fast and robust online adaptation for visual object trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="569" to="585" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Real-time self-adaptive deep stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="195" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to adapt for stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rahnama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9661" to="9670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Online depth learning against forgetting in monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lathuiliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4494" to="4503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Selfsupervised deep visual odometry with online adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6339" to="6348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On the convergence theory of gradient-based model-agnostic meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="page" from="1082" to="1092" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">MetaSets: Meta-learning on point sets for generalizable representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="8863" to="8872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="158" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoderss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2551" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5542" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="5400" to="5409" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Generalizing across domains via crossgradient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Videodg: Generalizing temporal relations in videos to novel domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semisupervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1195" to="1204" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Predicting camera viewpoint improves cross-dataset generalization for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="523" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vicon Mocap System</surname></persName>
		</author>
		<ptr target="https://www.vicon.com/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">The</forename><surname>Captury</surname></persName>
		</author>
		<ptr target="http://www.thecaptury.com" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Blender -a 3D modelling and rendering package</title>
		<ptr target="http://www.blender.org" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Bodynet: Volumetric inference of 3d human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="20" to="36" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">3d human mesh regression with dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7054" to="7063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5614" to="5623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Sim2real transfer learning for 3d human pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="12" to="949" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Monocular, one-stage, regression of multiple 3d people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12272</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4501" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Pymaf: 3d human pose and shape regression with pyramidal mesh alignment feedback loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning 3d human shape and pose from dense body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3383" to="3393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1954" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Pare: Part attention regressor for 3d human body estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><forename type="middle">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Mesh graphormer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3395" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Human body model fitting by learned gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="744" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Vnect: Realtime 3D human pose estimation with a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2d features and intermediate 3d representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="905" to="915" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
