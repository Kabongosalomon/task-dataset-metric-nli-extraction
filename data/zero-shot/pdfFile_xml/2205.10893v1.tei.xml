<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Tworkowski</surname></persName>
							<email>szy.tworkowski@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Warsaw</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Czechowski</surname></persName>
							<email>konrad.czechowski@gmail.com</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Warsaw</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Odrzyg??d?</surname></persName>
							<email>tomaszo@impan.pl</email>
							<affiliation key="aff4">
								<orgName type="department">IDEAS NCBR</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mi?o?</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Polish Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
							<email>yuhuai@google.com</email>
							<affiliation key="aff6">
								<orgName type="department">Google Research &amp;</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateja</forename><surname>Jamnik</surname></persName>
							<email>mateja.jamnik@cl.cam.ac.uk</email>
							<affiliation key="aff7">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In theorem proving, the task of selecting useful premises from a large library to unlock the proof of a given conjecture is crucially important. This presents a challenge for all theorem provers, especially the ones based on language models, due to their relative inability to reason over huge volumes of premises in text form. This paper introduces Thor, a framework integrating language models and automated theorem provers to overcome this difficulty. In Thor, a class of methods called hammers that leverage the power of automated theorem provers are used for premise selection, while all other tasks are designated to language models. Thor increases a language model's success rate on the PISA dataset from 39% to 57%, while solving 8.2% of problems neither language models nor automated theorem provers are able to solve on their own. Furthermore, with a significantly smaller computational budget, Thor can achieve a success rate on the MiniF2F dataset that is on par with the best existing methods. Thor can be instantiated for the majority of popular interactive theorem provers via a straightforward protocol we provide.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In theorem proving, premise selection is the task of identifying useful facts from a large library that enable finding the proof of a given conjecture. It is essential for the discovery of many proofs, and Automated Reasoning in Large Theories (ARLT) depends on having apt methods for premise selection <ref type="bibr" target="#b17">[K?hlwein et al., 2012</ref>. A group of proof methods have been developed inside interactive theorem provers (ITPs) to deal with this task. They use external automated theorem provers (ATPs) to reach the remaining goals, inspect the proofs produced, and pick out the premises involved in them. Such systems are called hammers <ref type="bibr" target="#b1">[Blanchette et al., 2016]</ref>. Hammers are available in many ITPs <ref type="bibr" target="#b21">[Paulson, 2010</ref><ref type="bibr" target="#b14">, Kaliszyk and Urban, 2015</ref><ref type="bibr" target="#b6">, Gauthier and Kaliszyk, 2015</ref><ref type="bibr" target="#b3">, Czajka and Kaliszyk, 2018</ref> and are immensely popular within the theorem proving community.</p><p>Language models have had some successful applications in the area of formal theorem proving <ref type="bibr" target="#b22">[Polu and Sutskever, 2020</ref><ref type="bibr" target="#b13">, Jiang et al., 2021</ref><ref type="bibr" target="#b37">, Polu et al., 2022</ref>. However, we observe that language-model-based reasoning systems are inept at premise selection. The difficulty of premise selection for language models is that they cannot effectively reason over thousands of available facts and their definitions in plain text form. In subsection 2.2, we elaborate on the scale of the problems language models need to deal with for premise selection and provide empirical evidence for this difficulty. Seeing that hammers are very good at finding relevant facts, we propose in our framework to offload the premise selection task to them from language models. The resulting system is Thor, a framework that organically integrates language models and ATPs via the use of hammers.</p><p>The methodology of Thor is simple and can be deployed in any hammer-enabled ITP: we first use the hammer method to attempt to prove every proof state in the training problems, and mark the successful application steps. Then we train the language model on the training problems, predicting a special token (e.g., &lt;hammer&gt;) if the hammer can be applied at the state. When doing evaluation, if the language model emits the special token, we invoke the hammer method. This methodology incurs very little extra computation compared to standard language model training while capitalising on the potential of a hybrid neuro-symbolic model.</p><p>To demonstrate the usefulness of Thor, we instantiate it with a language-model-based reasoning system for the ITP Isabelle and Sledgehammer <ref type="bibr" target="#b21">[Paulson, 2010]</ref>, Isabelle's implementation of the hammer method. We then investigate the performance of the instantiated Thor system on two datasets, PISA <ref type="bibr" target="#b13">[Jiang et al., 2021]</ref> and MiniF2F <ref type="bibr" target="#b37">[Zheng et al., 2022]</ref>. On PISA we dramatically improve the success rate of a language-model-based reasoning system from 39.0% to 57.0% and solve 8.2% of problems that cannot be solved by either language models or Sledgehammer alone. On MiniF2F, <ref type="bibr" target="#b37">Polu et al. [2022]</ref> used expert iteration to improve on a language model and achieved the state-of-the-art 1-pass success rate of 29.6%. With much less computation, Thor increases this rate to 29.9%, slightly exceeding the previous result. It is worth noting that Thor and expert iteration can be used in tandem.</p><p>In this paper, we demonstrate that finding suitable sub-systems for premise selection can benefit the performance of the overall reasoning system. Given Thor's strong performance, computational efficiency, and applicability to many ITPs, we believe it should become a strong baseline as often as possible when language models are used for theorem proving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>1. We created Thor, a theorem proving framework which integrates language models and automated theorem provers via using hammers.</p><p>2. We raised the state-of-the-art success rate of language-model-based reasoning systems on PISA from 39.0% to 57.0%. Thor proved 8.2% theorems which cannot be proved by either language models or Sledgehammer.</p><p>3. We improved the state-of-the-art success rate on MiniF2F from 29.6% to 29.9%, matching the language models trained with expert iteration, but with far less computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Automated and Interactive Theorem Proving</head><p>Mechanising theorem proving has been a grand challenge of artificial intelligence since the late 1950s <ref type="bibr" target="#b7">[Gelernter, 1959]</ref>. A group of systems which we call automated theorem provers attempt to use automated procedures to determine the validity of conjectures (e.g., the DPLL algorithm <ref type="bibr" target="#b4">[Davis et al., 1962]</ref> for SAT problems <ref type="bibr" target="#b31">[Tarski, 1969]</ref>). Popular examples of ATPs include E, SPASS, Z3, CVC4, and Vampire. Although SAT is known to be NP-complete <ref type="bibr" target="#b2">[Cook, 1971]</ref>, modern ATPs can often solve problems with millions of symbols <ref type="bibr" target="#b20">[Ohrimenko et al., 2009]</ref> and are useful practically.</p><p>ATPs are often based on fragments of first-order logic, which limits the type of theorems they can express. Hence, projects such as the formalisation of complicated mathematical results <ref type="bibr" target="#b9">[Gonthier et al., 2008</ref><ref type="bibr" target="#b0">, Avigad et al., 2007</ref><ref type="bibr" target="#b8">, Gonthier et al., 2013</ref><ref type="bibr" target="#b25">, Scholze, 2021</ref> and operating system kernel verification <ref type="bibr" target="#b16">[Klein et al., 2009]</ref> are done in interactive theorem provers, often based on higher-order logic or dependent type theory. ITPs and ATPs have very different objectives: ITPs aim at making it easy to formalise a diverse set of problems in numerous mathematical domains in a sound manner; while ATPs focus on improving the efficiency and performance on very well-defined problems like SAT solving. Prominent ITPs include Isabelle, Mizar, HOL Light, HOL4, Lean, and Coq. Theorem proving in ITPs can be modelled as a sequential decision process: initially a theorem gets declared and the proof state contains some goals; at each step, the user produces a proof step that applies to and transforms the proof state; when all the goals have been discharged, the theorem is considered proven. Large libraries of mathematical knowledge such as the Archive of Formal Proofs 1 and the Mizar Mathematical Library 2 have been built around these ITPs. Because of the size of these mathematical libraries, to find useful premises in them is a difficult problem. In the next subsections we illustrate how two different approaches deal with premise selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Language Models for Theorem Proving</head><p>Language models that automate theorem proving mostly follow the approach of the GPT-f model <ref type="bibr" target="#b22">[Polu and Sutskever, 2020]</ref>: pre-trained causal language models are used to predict a proof step that can be applied, given the current proof state and some optional context. Concretely, a language model can take as input and output, two sequences of the following form:</p><formula xml:id="formula_0">INPUT: &lt;SOS&gt; &lt;CTXT&gt; $(context) &lt;PRF_STT&gt; $(proof state) &lt;PRF_STP&gt; OUTPUT: $(proof step) &lt;EOS&gt;</formula><p>At test time, the reasoning system receives the text representation of the current proof state, samples a proof step from the language model, applies it to the ITP, and repeats until the proof is finished or a computational budget has been reached. A best-first strategy is often used for proof search: a queue of search nodes is maintained with the priority being the accumulated log likelihood of the generated proof steps.</p><p>Language models treat all input and output information as text and they are usually limited to be a few thousands of characters long. To do premise selection well, the language model has to either memorise all the relevant premises during training, or be prompted with available premises in evaluation. It is difficult to do the former because a mathematical corpus can have too many facts for a language model to remember. For example, the Archive of Formal Proofs has more than 200,000 theorems, plus the numerous definitions and their derivations to serve as premises. The latter is no easier because there may be too many premises to fit into the input. For instance, if we use the textual representation of 300 available premises (a usual number used for premise selection with symbolic tools) and their definitions as the context in the input-output format above, the input length can well exceed 10,000 characters and the limit of standard language models. We observe that empirically 1.9% of the steps involving premise selection generated by the language model manage to advance the proof, while the number is 28.2% for steps having no premises. Hence, a good mechanism for premise selection could bring crucial benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hammers</head><p>Blanchette et al.</p><p>[2016] define hammers as methods that "automate reasoning over large libraries developed with formal proof assistants (ITPs)". Consider, for example, Sledgehammer (designed for Isabelle) which is the original and the most popular implementation of hammers. <ref type="figure" target="#fig_1">Figure 1</ref> presents a proof of ? 2 / ? Q in Isabelle. The beauty of using Sledgehammer with Isabelle is that despite the complicated-looking proof, humans only need to sketch the proof in <ref type="figure" target="#fig_1">Figure 1a</ref> and let Sledgehammer find all the necessary premises to complete every single proof step. The final accepted proof is presented in <ref type="figure" target="#fig_1">Figure 1b</ref>. The Sledgehammer proof steps use the internal proof methods metis, meson, smt, auto, simp, fastforce and blast. Conveniently, this tells us which steps in the corpus are generated by Sledgehammer. Note that a human user might also use the proof methods auto, simp, fastforce and blast as these do not contain additional premises. Only the methods metis, meson, smt are exclusive to Sledgehammer.</p><p>We now describe how Sledgehammer performs premise selection: Sledgehammer makes it possible to leverage the advancement of ATP research while using ITPs, and can thus be seen as a bridge between the two <ref type="bibr" target="#b21">[Paulson, 2010]</ref>. When invoked, Sledgehammer translates the current goal together with hundreds of possibly relevant premises into a format (e.g., SMT-LIB, TPTP) that external ATPs can understand <ref type="bibr" target="#b19">[Meng and Paulson, 2008]</ref>. The ATPs are then executed to solve the current goal. Note that Isabelle follows a kernel philosophy (i.e., only a handful of axioms and inference rules are trusted), and external ATPs are used skeptically-should a proof be found by the ATPs, Sledgehammer picks out the useful premises, and reconstructs the proof within the Isabelle kernel (a) The proof sketch produced by the human user. The sledgehammer command indicates that the human invokes the Sledgehammer method at that point.  (e.g., using the primitive inference rules). Here, external ATPs serve as relevance filters of premises rather than trusted oracles. Hammers implemented in other ITPs are largely similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Thor</head><p>In this section we introduce Thor, a framework integrating language models and automated theorem provers via the use of hammers. Thor is motivated by the difficulty for language models to do premise selection and the excellent performance of hammers for it: we should be able to drastically improve automation in theorem proving if we can take the best from both worlds.</p><p>Below we provide the protocol of adopting Thor for a hammer-enabled ITP. We first provide Thor's training data preprocessing procedure in Algorithm 1, and then look at a concrete example to demonstrate its use.</p><p>Algorithm <ref type="formula">1</ref>  Now consider the situation in the proof of ? 2 / ? Q <ref type="figure" target="#fig_1">(Figure 1</ref>) after the step then have "even a": without Thor, it should produce the following datapoint</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INPUT:</head><p>&lt;SOS&gt; &lt;CTXT&gt; $(context) &lt;PRF_STT&gt; $(proof state) &lt;PRF_STP&gt; OUTPUT:</p><p>by (smt (z3) even_power oddE) &lt;EOS&gt;</p><p>With Thor's preprocessing, we apply the hammer method to the proof state and find out that it can be done successfully. Hence, we keep the input the same and change the output to:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OUTPUT: &lt;hammer&gt; &lt;EOS&gt;</head><p>If the hammer method cannot be applied, we leave the datapoint unchanged. We iterate over every datapoint in the training data and apply this preprocessing algorithm.</p><p>We hypothesise that being exposed to training data in this format, the language model is capable of learning a heuristic for when the hammer can be successfully invoked. At evaluation time, whenever the language model outputs the sequence &lt;hammer&gt; &lt;EOS&gt;, instead of applying it directly to the ITP, we call the hammer method. This effectively makes the hammer an invokable method for the language model. This protocol is straightforward to implement for hammer-enabled ITPs.</p><p>The only extra cost of deploying Thor is in the data preprocessing step. Multiplying the hammer time limit by the average number of problems submitted to the Archive of Formal Proofs in one year, we estimate that 7400 CPU hours per year are needed to preprocess one of the largest proof corpora available. This is a modest cost since the process only needs to be done once per dataset and the results can be shared. Better still, for some ITPs, the hammer method leaves a trace, greatly reducing the time needed to figure out which steps can be solved by hammers. For the ITP Coq, all steps containing the keyword sauto are generated by CoqHammer <ref type="bibr" target="#b3">[Czajka and Kaliszyk, 2018]</ref>. For Isabelle, all steps containing the keywords metis, meson, smt are generated by Sledgehammer <ref type="formula">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>Our experiments are intended to answer the following research questions:</p><p>1. Can Thor prove theorems that cannot be proved by language models or automated theorem provers individually? Does Thor improve premise selection for language models?</p><p>2. Does explicitly learning how to select premises hurt the performance of language models?</p><p>3. How important are the context information and the diversity of sequence generation?</p><p>4. How does Thor compare with other methods at improving language models for theorem proving?</p><p>To answer these questions, we create an instance of Thor for the ITP Isabelle. We choose Isabelle for two reasons: (1) Isabelle's Sledgehammer is one of the most mature hammer methods among major ITPs, and may thus showcase Thor's full potential; and (2) Isabelle's Archive of Formal Proofs is one of the world's largest formal mathematical libraries, suitable for data-hungry methods like language models. We make explicit the details of our experimental setup next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Machine specification For pre-training, fine-tuning, and evaluation, we use a TPUVM with 8 cores from Google Cloud Platform. The Isabelle process has access to up to 32 CPU cores. We estimate that reproducing all the experiments in this paper requires a total of 1160 TPU hours.</p><p>Language model architecture We use a decoder-only transformer <ref type="bibr" target="#b34">[Vaswani et al., 2017]</ref> language model, adapting the setup, codebase, and hyperparameters from <ref type="bibr" target="#b35">[Wang and Komatsuzaki, 2021</ref>]. The language model has 700M non-embedding parameters, with 24 layers, 24 attention heads, a hidden dimension of 1536, and a GPT-2 <ref type="bibr" target="#b24">[Radford et al., 2019]</ref> tokenizer with a vocabulary size of 50400. Rotary positional embeddings <ref type="bibr" target="#b28">[Su et al., 2021]</ref> are used. The model is pre-trained on the GitHub + arXiv subsets of The Pile <ref type="bibr" target="#b5">[Gao et al., 2021]</ref>, with a context length of 2048. We use a global batch size of 32 sequences which amounts to 65536 tokens. For the first 3,000 steps, the learning rate linearly increases from 0 to 0.0002, and then it follows a cosine schedule with a final value of 1.2 ? 10 ?5 for 197,000 steps. We use a weight decay rate of 0.05 and no dropout for pre-training. Pre-training takes ? 150 TPU hours. For fine-tuning, we use the procedure described in Section 3 to prepare the PISA dataset. We use the most recent proof step as the context in each datapoint. The same learning rate scheduling strategy is used, with a peak learning rate of 3 ? 10 ?4 after 10,000 steps and a final learning rate of 3 ? 10 ?5 after a further 90,000 steps. We use a dropout rate of 0.15 and a weight decay rate of 0.1. The global batch size is 256 sequences, or 524, 288 tokens. We early-stop fine-tuning and take the checkpoint at 11,000 steps for evaluation as the validation loss reaches a minimum then. Fine-tuning takes ? 50 TPU hours. Sledgehammer configuration To set up Sledgehammer, we mostly follow the default Isabelle2021 configuration. An important default parameter is that the Sledgehammer timeout limit is 30s. Our configuration uses the on-machine versions of the five default ATPs (E, SPASS, Vampire, Z3, and CVC4) to prevent performance deviation caused by network issues.</p><p>Proof search To sample from the language model, we use temperature sampling with the temperature parameter T = 1.2. To search for the proof of a theorem, we use the best-first search strategy described in <ref type="bibr" target="#b22">[Polu and Sutskever, 2020]</ref>. The queue is ordered by the accumulated log likelihoods of the generated proof steps, with a maximum length of 32. Each proof step has a timeout limit of 10s. The search is terminated if and only if one of the following scenarios happens: (1) a valid proof has been found for the theorem;</p><p>(2) the language model is queried 300 times; (3) a wallclock timeout of 500s has been reached; (4) the queue is empty but the theorem is not proved. Empirically, it takes ? 60 TPU hours to evaluate 1, 000 problems.</p><p>Our language model setup is different from Language models of ISAbelle proofs <ref type="bibr" target="#b13">[Jiang et al., 2021, LISA]</ref> in three aspects: (1) our language model has 700M instead of 163M non-embedding parameters (2) the most recent proof step is included in the language model prompt (3) a higher sampling temperature (1.2 instead of 1.0) is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets and Environment</head><p>We use two datasets. The first is the PISA dataset <ref type="bibr" target="#b13">[Jiang et al., 2021]</ref>, which includes the Isabelle/HOL repository 3 under a BSD-style license and the Archive of Formal Proofs version 2021-10-22 4 , whose various entries are under open-source licenses as described on its official page. PISA contains the core higher-order logic library of Isabelle, as well as a diverse library of proofs formalised with Isabelle, mostly concerning mathematics or verification of software and hardware. The PISA dataset contains 2.49 million datapoints in total. The proof states have an average length of 369 characters and the proof steps have an average length of 33 characters. All of the Isabelle/HOL theorems go into the training set as they are considered foundational and might be used by all other repositories. We make a 95%/1%/4% split of theorems from the AFP for the training/validation/test sets. We randomly select 3,000 theorems from the test set (PISA/test) for the evaluation of model performance.</p><p>The second is the Isabelle fraction of the MiniF2F dataset <ref type="bibr" target="#b37">[Zheng et al., 2022]</ref> under an Apache license. The dataset contains 488 high school mathematics competition problems split into a validation set and a test set, each with 244 problems. These problems have been formalised in Lean, Metamath, and Isabelle to provide a benchmark of the same problems in different ITP languages. This allows us to contrast different approaches developed for different ITPs. Since we do not use the validation set for model selection, we do not actually distinguish between the two sets. Hence, we mainly compare with previous work on the test set as the final result.</p><p>We use the codebase by <ref type="bibr" target="#b13">Jiang et al. [2021]</ref>, under a BSD 3-clause license, to interact with the Isabelle server and prove theorems from both datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Thor Against an Ensemble of a Language Model and Sledgehammer</head><p>Because Thor has both a language model and Sledgehammer at its disposal, we wish to investigate how it fares against a simple ensemble of the two. We set out to evaluate the performance of Thor, as well as a language model of the same configuration, and Sledgehammer with a 120s timeout on PISA/test. It takes ? 50 TPU hours to evaluate Thor for 1000 problems. The proof success rates on PISA/test are presented in the second column of Table 1. We can see that the language model alone and Sledgehammer alone can prove 39.0% and 25.7% of the problems respectively. When we take the union of the problems they manage to solve individually, we get a 48.8% success rate. Thor manages to prove 57.0% of the problems. This implies that for 8.2% of the problems, Thor uses both the language model and Sledgehammer to complete the proofs, and it's not possible to achieve this with only the language model or only Sledgehammer. We perform 4 case studies on problems that only Thor can solve in Appendix A.</p><p>Thor's motivation is to solve the premise selection problem for language models. To confirm that Thor helps premise selection, we collect the proofs generated by the language model and Thor respectively and count the number of premises in them. The results are presented in <ref type="figure" target="#fig_3">Figure 2a</ref>: we can see that for proofs requiring 0 or 1 premises, Thor and the language model perform similarly. But for proofs requiring more premises, Thor performs much more robustly, finding several times more proofs than the language model. We also count the number of premises in the ground truth proofs (written by humans) for theorems the language model and Thor can prove. The results are presented in <ref type="figure" target="#fig_3">Figure 2b</ref>: we see that whatever the number of premises the ground truth uses, Thor outperforms the language model in finding proofs, and the more premises the ground truth proof has, the more obvious is the effect. We conclude that Thor is indeed more capable of premise selection than language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The Effect of Learning How to Select Premises</head><p>The procedure we described in Section 3 ensures that the language model learns when to do premise selection, but not how to do it, by replacing the premise selection steps with &lt;hammer&gt;. Here we investigate the effect of making the language model learn both when and how. An easy way to achieve this is to create a variant of Thor: (i) at training time, use the original data; (ii) at evaluation time, when the language model outputs a sequence containing any of the Sledgehammer keywords, invoke Sledgehammer. This further simplifies data preparation and explicitly subjects the language model to perform premise selection. To investigate the effect of this alternative approach, we evaluate a system trained in this way on PISA/test and present its success rate in <ref type="table" target="#tab_3">Table 2</ref>. We can see that it achieves a success rate of 55.4% on PISA/test, 1.6% lower than the base version of Thor, which suggests that explicitly learning how to do premise selection marginally decreases its success rate. This result is expected: since finding how to do premise selection is entrusted to the hammer method, the language model should focus on learning when to invoke the hammer for optimal performance. Making the language model learn an irrelevant additional task only hurts Thor's performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">The Effect of the Proof Context</head><p>Our language model setup differs from that of LISA <ref type="bibr" target="#b13">[Jiang et al., 2021]</ref> in that we use the most recent proof step as the context in the input data, as introduced in Section 3. This is based on the intuition that the most recent proof step information is beneficial for the language model's reasoning ability. In this subsection we perform an ablation study to confirm the effect of this context on Thor. Here a variant of Thor is trained without the context information and evaluated on PISA/test. The results are in <ref type="table" target="#tab_3">Table 2</ref>. We observe that this variant manages to prove 53.6% of theorems on PISA/test, 3.4% fewer than the base version of Thor. The drop in success rate indicates that the context information we use is crucial for the optimal performance of Thor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">The Effect of the Sequence Sampling Diversity</head><p>Our language model setup differs from LISA <ref type="bibr" target="#b13">[Jiang et al., 2021]</ref> also in the sampling temperature. Previous works on language models for theorem proving often use a temperature T = 1.0 <ref type="bibr">Sutskever, 2020, Jiang et al., 2021]</ref> for sampling output sequences, while we use T = 1.2. A higher temperature in the sampling procedure means that the generated sequences are more diverse (having a higher entropy). Here we perform an ablation study on the diversity of Thor-generated sequences. We evaluate Thor with sampling temperature T = 1.0 on PISA/test and the success rate is in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>We can see that the success rate with sampling temperature T = 1.0 is 55.7%, 1.3% lower than with T = 1.2. This suggests a more diverse sampling strategy can improve Thor's performance, and that the optimal diversity in language model samples varies for different systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Comparing Thor with Expert Iteration</head><p>There exist other methods for improving language models for theorem proving like value function training <ref type="bibr" target="#b22">[Polu and Sutskever, 2020]</ref>, proof artifact co-training <ref type="bibr">[Han et al., 2021, PACT]</ref>, and expert iteration <ref type="bibr" target="#b37">[Polu et al., 2022]</ref>. We wish to compare Thor with them. However, these methods operate in ITPs other than Isabelle and are thus hard to compare with directly. Thankfully, <ref type="bibr" target="#b37">Polu et al. [2022]</ref> used expert iteration <ref type="bibr" target="#b26">[Silver et al., 2017]</ref> to improve PACT  and to achieve the state-of-the-art result on MiniF2F, a dataset containing multiple ITP formalisations of the same problems. Hence, we can fairly contrast expert iteration with Thor. We should emphasise that Thor and expert iteration are not incompatible methods: one can use Thor together with expert iteration.</p><p>We start by evaluating Thor, a language model with the same configuration, and Sledgehammer on MiniF2F. The results are presented in <ref type="table" target="#tab_4">Table 3</ref>. We also include the success rates of the language model that <ref type="bibr" target="#b37">Polu et al. [2022]</ref> used (PACT), as well as the language model after expert iteration in the same table. The success rates on the validation set are also included, but we use the rates on the test set as the final results, as the valid set can be used for model selection. We can see that the language model is able to prove 24.2% of the problems on MiniF2F, similar to PACT's 24.6%. Thor increases the success rate of the language model by 5.7% to 29.9%, while expert iteration increases the success rate of PACT by 5.0% to 29.6%. Hence, the improvement in proof success rate brought upon the language model by Thor is comparable to that by expert iteration.</p><p>An important factor in choosing a suitable method is its cost. Expert iteration requires manually creating a set of "curriculum" problems, evaluating the language model on them, and training the language model on a growing training set for one epoch every iteration. We estimate that to perform expert iteration at the same scale as <ref type="bibr" target="#b37">Polu et al. [2022]</ref> for Isabelle, it would cost 100 human hours to formalise 300 maths problems, and 500 TPU hours to evaluate and fine-tune the language model for 8 expert iterations. Thor, on the other hand, incurs little extra computational cost compared with training a standard language model. We conclude that while requiring a much smaller computational budget, Thor can improve language models' success rates to a similar degree as expert iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Language models were first applied to automate theorem proving by <ref type="bibr" target="#b22">Polu and Sutskever [2020]</ref>.</p><p>Since then, there have been a few works <ref type="bibr" target="#b13">, Jiang et al., 2021</ref><ref type="bibr" target="#b37">, Polu et al., 2022</ref> aiming to enhance the ability of language-model-based reasoning systems, or to enable these systems for interactive theorem provers that were not supported before. All of these works used the same framework laid down by <ref type="bibr" target="#b22">Polu and Sutskever [2020]</ref>, namely to iteratively sample from a language model and directly apply the output to the ITP. Thor, to the best of our knowledge, is the first system to explicitly hybridise language models and symbolic reasoning tools (ATPs) for theorem proving. Instead of relying on language models entirely, Thor uses hammers, a well-established tool, to solve premise selection.</p><p>With the growing bodies of formal mathematical libraries, premise selection has become one of the most crucial tasks of theorem proving. The hammer method is one of the many ways that premise selection can be done. We have described how the Isabelle implementation of the hammer method selects premises in Section 2. HOL(y)Hammer <ref type="bibr" target="#b14">[Kaliszyk and Urban, 2015]</ref> and CoqHammer <ref type="bibr" target="#b3">[Czajka and Kaliszyk, 2018]</ref> implement the hammer method for HOL Light and Coq respectively, making it possible for Thor to be instantiated for them. Apart from hammers, SInE <ref type="bibr" target="#b11">[Hoder and Voronkov, 2011]</ref> and SRASS <ref type="bibr" target="#b29">[Sutcliffe and Puzis, 2007]</ref> are both symbolic methods that take on the task of premise selection by ranking the available premises according to their relevance to the current conjecture, measured by syntactic and semantic distances respectively. MaLARea <ref type="bibr" target="#b32">[Urban, 2007]</ref> pioneered having machine learning components in premise selection systems and its later version MaLARea SG1 <ref type="bibr" target="#b33">[Urban et al., 2008]</ref> combines machine learning and formal semantics for premise selection. A few approaches <ref type="bibr" target="#b12">[Irving et al., 2016</ref><ref type="bibr" target="#b36">, Wang et al., 2017</ref><ref type="bibr" target="#b15">, Kaliszyk et al., 2017</ref> use deep learning in the premise selection task. All these diverse methods may have quantitative or qualitative merits over the hammer approach, and thus have the potential to be integrated as the premise selection component for future versions of Thor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this paper we introduced a simple approach to overcome language models' weakness in premise selection for theorem proving: we created Thor, a framework that integrates language models and automated theorem provers via the hammer proof method. We presented a straightforward protocol for deploying Thor on any hammer-enabled ITP. The instance of Thor with Isabelle dramatically increased the number of automatically proved theorems, suggesting that language models' deficiency at premise selection can be effectively compensated by utilising ATPs. Furthermore, approaches like expert iteration <ref type="bibr" target="#b37">[Polu et al., 2022]</ref> or proof artifact co-training  have no contradictions and can be easily incorporated with Thor. Compared with these methods, Thor has the additional advantage of being computationally efficient.</p><p>One limitation of Thor is that it only admits automated theorem provers that directly generate valid proof steps in the ITP via the use of the hammer. In Section 5, we pointed out that there are other premise selection tools with approaches different from the hammer method that the current version of Thor cannot use. Also, there exist methods which assist premise selection but do not directly generate the proof steps. An example of this is SErAPIS <ref type="bibr" target="#b27">[Stathopoulos et al., 2020]</ref>, which performs semantic search over the Isabelle mathematical library with the help of Wikipedia. Thor cannot use this class of methods either. We leave to future work the task of broadening the options for the premise selection tool that Thor uses. Here we only tested Thor on the ITP Isabelle due to the computational costs of experiments. Therefore another future direction is to instantiate Thor with other ITPs and see whether improvements brought by Thor are as significant for other ITPs as we show them here for Isabelle.</p><p>Thor demonstrates how a difficult problem for language models can be solved by borrowing tools from another research domain. We are encouraged by its success and think that more problems like premise selection can be identified and solved similarly. With its strong performance, computational efficiency, and convenient deployment, Thor gives scope to tool hybridisation, which shows promise to be impactful in the field of automated reasoning, and artificial intelligence in general.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b) The proof accepted by Isabelle. The steps containing assume, obtain, have, show are from the original human proof sketch. The steps containing metis, smt, fastforce, blast, auto, fastforce are completed by Sledgehammer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>A proof of ? 2 / ? Q,adapted from the original by Li et al. [2021] with consent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The number of premises in successful proofs found by the language model and Thor. The number of premises in ground truth proofs for problems solved by the language model and Thor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of the number of premises in problems the language model and Thor can solve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Thor's training data preprocessing algorithm.</figDesc><table><row><cell>Require: Proof state s, hammer method h</cell><cell></cell></row><row><cell>INPUT = s.input</cell><cell></cell></row><row><cell>if h(s)? success then</cell><cell>Hammer can be applied to the proof state</cell></row><row><cell>OUTPUT = &lt;hammer&gt; &lt;EOS&gt;</cell><cell></cell></row><row><cell>else</cell><cell>Hammer fails at the proof state</cell></row><row><cell>OUTPUT = s.output</cell><cell></cell></row><row><cell>end if</cell><cell></cell></row><row><cell>return (INPUT, OUTPUT)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>described in Section 2.3). With these traces, deploying Thor on ITPs like Coq or Isabelle incurs little extra computational cost compared to training a standard language model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Proof success rates on PISA/test</figDesc><table><row><cell>Method</cell><cell>Success rate (%)</cell></row><row><cell>LISA [Jiang et al., 2021]</cell><cell>33.2</cell></row><row><cell>Sledgehammer</cell><cell>25.7</cell></row><row><cell>Language model</cell><cell>39.0</cell></row><row><cell>Language model ? Sledgehammer</cell><cell>48.8</cell></row><row><cell>Thor</cell><cell>57.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Proof success rates on PISA/test</figDesc><table><row><cell>Variants of Thor</cell><cell>Success rate (%)</cell></row><row><cell>Base, sampling temperature T = 1.2</cell><cell>57.0</cell></row><row><cell>Learning how to select premises</cell><cell>55.4</cell></row><row><cell>No proof context</cell><cell>53.6</cell></row><row><cell>Sampling temperature T = 1.0</cell><cell>55.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Proof success rates on MiniF2F.</figDesc><table><row><cell>Method</cell><cell cols="2">Valid (%) Test (%)</cell></row><row><cell>PACT [Han et al., 2021]</cell><cell>23.9</cell><cell>24.6</cell></row><row><cell>Expert iteration [Polu et al., 2022]</cell><cell>33.6</cell><cell>29.6</cell></row><row><cell>Sledgehammer</cell><cell>9.9</cell><cell>10.4</cell></row><row><cell>Language model</cell><cell>25.0</cell><cell>24.2</cell></row><row><cell>Language model ? Sledgehammer</cell><cell>27.1</cell><cell>27.5</cell></row><row><cell>Thor</cell><cell>28.3</cell><cell>29.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.isa-afp.org 2 http://mizar.org/library/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://isabelle.in.tum.de/website-Isabelle2021/dist/library/HOL/index.html 4 https://www.isa-afp.org/release/afp-2021-10-22.tar.gz</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Jordan_Normal_Form/Missing_VectorSpace.thy</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>In this section, we present some lemmas solved by Thor only.</p><p>Case 1. The lemma cols_upt_k _insert is from the QR Decomposition entry 5 in the AFP. lemma cols_upt_k_insert:</p><p>fixes A::"'a^'n::{mod_type}^'m::{mod_type}" assumes k: "(Suc k)&lt;ncols A" shows "cols_upt_k A (Suc k) = (insert (column (from_nat (Suc k)) A) (cols_upt_k A k))" unfolding cols_upt_k_def apply (auto) apply (metis Suc_lessD from_nat_mono' from_nat_to_nat_id k less_Suc_eq_le less_le ncols_def to_nat_le) by (metis from_nat_mono' k less_imp_triv less_or_eq_imp_le ncols_def not_less_eq order_trans)</p><p>Here, cols_upt_k A (Suc k) returns the set of columns in the matrix A up to the natural number k+1, while ncols A counts the number of columns in the matrix A. In short, this lemma claims that the set of columns (in a matrix A) up to column index k + 1 is equivalent to that of the same matrix up to column index k inserted with the (k + 1) th column (of A). This will subject to the condition that k + 1 is less than the number of columns in A. With Thor, the LM decided to unfold the goal with the definition of cols_upt_k, which is followed by an auto tactic to simplify the proof state. All remaining subgoals are then discharged by Sledgehammer.</p><p>Case 2. The lemma size_del_max is from theWeight-Balanced Trees entry 6 in the AFP.</p><p>lemma size_del_max: "t = Leaf =? size t = Suc(size(snd(del_max t)))" apply(induction t rule: del_max.induct) apply simp apply (clarsimp split: prod.splits) apply (smt (z3) size_rotateR size_wbt.simps(1)) by simp</p><p>In this lemma, t is a weight-balanced tree, and the size function measures its size (as the name suggests) and del_max deletes the maximum node from it. Essentially, this lemma claims that when a weight-balanced its size will be reduced by one if we remove the largest node from it. For the proof, Thor intelligently performs structural induction with the induction rule del_max.induct and then simplifies the proof state a few times, which includes splitting products with the rule prod.splits. Finally, Thor concludes the remaining goals with Sledgehammer.</p><p>Case 3. The lemma t_list_of_B_log_bound is from the AFP entry named as Priority Queues Based on Braun Trees. 7 lemma t_list_of_B_log_bound: "braun t =? t_list_of_B t ? 3 * (nlog2 (size t + 1) + 1) * size t" apply (induction t rule: measure_induct_rule[where f=size]) apply (case_tac x) apply simp using braun.simps(1) t_list_of_B_braun_simps(1) apply blast by (metis acomplete_if_braun height_acomplete order_refl size1_size t_list_of_B_induct)</p><p>Here, size measures the size of a Braun tree; nlog2 stands for the function ?x. log 2 (x) ; t_list_of_B is another measure of a Braun tree. Basically, this lemma describes the relationship between a normal tree size and a Braun-tree specific measure. The proof starts with an intelligent structural induction, progresses with case analysis, and is concluded with Sledgehammer on each of the remaining subgoals. Here, T is a linear map between two vector spaces. The lemma claims that if the T is injective on the carrier set of the space V, the kernel of T has to be a singleton set with the zero in V. In this proof, Thor naturally performs a sequence of introduction steps by applying the lemma equalityI and subsetI, before unfolds the definition of a kernel (i.e., ker_def ) and uses auto to simplify the proof state. The final remaining goal is closed with Sledgehammer.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A formally verified proof of the prime number theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Avigad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Donnelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Raff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computational Logic (TOCL)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hammering towards QED</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Jasmin Christian Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">C</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Paulson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urban</surname></persName>
		</author>
		<idno type="DOI">10.6092/issn.1972-5787/4593</idno>
		<ptr target="https://doi.org/10.6092/issn.1972-5787/4593" />
	</analytic>
	<monogr>
		<title level="j">J. Formaliz. Reason</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="148" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The complexity of theorem-proving procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third annual ACM symposium on Theory of computing</title>
		<meeting>the third annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="1971" />
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hammer for coq: Automation for dependent type theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Czajka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10817-018-9458-4</idno>
		<ptr target="https://doi.org/10.1007/s10817-018-9458-4" />
	</analytic>
	<monogr>
		<title level="j">J. Autom. Reason</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="423" to="453" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A machine program for theorem-proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Logemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Loveland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="394" to="397" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The pile: An 800gb dataset of diverse text for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2101.00027" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Premise selection and external provers for HOL4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<idno type="DOI">10.1145/2676724.2693173</idno>
		<ptr target="https://doi.org/10.1145/2676724.2693173" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Certified Programs and Proofs</title>
		<editor>Xavier Leroy and Alwen Tiu</editor>
		<meeting>the 2015 Conference on Certified Programs and Proofs<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015-01-15" />
			<biblScope unit="page" from="49" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Realization of a geometry theorem proving machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelernter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFIP congress</title>
		<imprint>
			<date type="published" when="1959" />
			<biblScope unit="page" from="273" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A machine-checked proof of the odd order theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Gonthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Asperti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Avigad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Bertot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Garillot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assia</forename><surname>Mahboubi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidi</forename><forename type="middle">Ould</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Biha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on interactive theorem proving</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="163" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Formal proof-the four-color theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Gonthier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Notices of the AMS</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1382" to="1393" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Proof artifact co-training for theorem proving with language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><forename type="middle">Michael</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rute</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">W</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Polu</surname></persName>
		</author>
		<idno>abs/2102.06203</idno>
		<ptr target="https://arxiv.org/abs/2102.06203" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sine qua non for large theory reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kry?tof</forename><surname>Hoder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Voronkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automated Deduction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="299" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepmath-deep sequence models for premise selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Alexander A Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>E?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><forename type="middle">Michael</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lisa</surname></persName>
		</author>
		<title level="m">Language models of isabelle proofs. 6th Conference on Artificial Intelligence and Theorem Proving</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hol(y)hammer: Online ATP service for HOL light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11786-014-0182-0</idno>
		<ptr target="https://doi.org/10.1007/s11786-014-0182-0" />
	</analytic>
	<monogr>
		<title level="j">Math. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="22" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Holstep: A machine learning dataset for higher-order logic theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00426</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Formal verification of an os kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerwin</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Elphinstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Heiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">June</forename><surname>Andronick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Derrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhammika</forename><surname>Elkaduwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Engelhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Kolanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Norrish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles</title>
		<meeting>the ACM SIGOPS 22nd symposium on Operating systems principles</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="207" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Overview and evaluation of premise selection techniques for large theory mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>K?hlwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeni</forename><surname>Twan Van Laarhoven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Tsivtsivadze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heskes</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-31365-3_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-31365-3_30" />
	</analytic>
	<monogr>
		<title level="m">Automated Reasoning -6th International Joint Conference, IJCAR 2012</title>
		<editor>Bernhard Gramlich, Dale Miller, and Uli Sattler</editor>
		<meeting><address><addrLine>Manchester, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">7364</biblScope>
			<biblScope unit="page" from="378" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Isarstep: a benchmark for high-level mathematical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">C</forename><surname>Paulson</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Pzj6fzU6wkj" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Translating higher-order clauses to first-order clauses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">C</forename><surname>Paulson</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10817-007-9085-y</idno>
		<ptr target="https://doi.org/10.1007/s10817-007-9085-y" />
	</analytic>
	<monogr>
		<title level="j">J. Autom. Reason</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="60" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Propagation via lazy clause generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Ohrimenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stuckey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Codish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Constraints</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="357" to="391" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Three years of experience with sledgehammer, a practical link between automatic and interactive theorem provers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paulson</surname></persName>
		</author>
		<idno type="DOI">10.29007/tnfd</idno>
		<ptr target="https://doi.org/10.29007/tnfd" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Practical Aspects of Automated Reasoning</title>
		<editor>Renate A. Schmidt, Stephan Schulz, and Boris Konev</editor>
		<meeting>the 2nd Workshop on Practical Aspects of Automated Reasoning<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07-14" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>EasyChair</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Generative language modeling for automated theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Polu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/2009.03393</idno>
		<ptr target="https://arxiv.org/abs/2009.03393" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. Formal mathematics statement curriculum learning. CoRR, abs/2202.01344, 2022</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Polu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><forename type="middle">Michael</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunhao</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2202.01344" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Liquid tensor experiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Scholze</surname></persName>
		</author>
		<idno type="DOI">10.1080/10586458.2021.1926016</idno>
		<idno>doi: 10.1080/ 10586458.2021.1926016</idno>
		<ptr target="https://doi.org/10.1080/10586458.2021.1926016" />
	</analytic>
	<monogr>
		<title level="j">Experimental Mathematics</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Mastering chess and shogi by self-play with a general reinforcement learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<idno>abs/1712.01815</idno>
		<ptr target="http://arxiv.org/abs/1712.01815" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Serapis: A concept-oriented search engine for the isabelle libraries based on natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannos</forename><surname>Stathopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Koutsoukou-Argyraki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Paulson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Online proceedings of the Isabelle Workshop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2104.09864" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SRASS -A semantic relevance axiom selection system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Puzis</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-73595-3_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-73595-3_20" />
	</analytic>
	<monogr>
		<title level="m">21st International Conference on Automated Deduction</title>
		<editor>Frank Pfenning, editor</editor>
		<meeting><address><addrLine>Bremen, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">4603</biblScope>
			<biblScope unit="page" from="295" to="310" />
		</imprint>
	</monogr>
	<note>Automated Deduction -CADE-21</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Schulz</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CADE-21 Workshop on Empirically Successful Automated Reasoning in Large Theories</title>
		<meeting>the CADE-21 Workshop on Empirically Successful Automated Reasoning in Large Theories<address><addrLine>Bremen, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-07-17" />
			<biblScope unit="volume">257</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Truth and proof</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Tarski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="63" to="77" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Malarea: a metasystem for automated reasoning in large theories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESARLT</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Malarea SG1-machine learner for automated reasoning with semantic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Pudl?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jir?</forename><surname>Vyskocil</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-71070-7_37</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-71070-7_37" />
	</analytic>
	<monogr>
		<title level="m">Automated Reasoning, 4th International Joint Conference</title>
		<editor>Alessandro Armando, Peter Baumgartner, and Gilles Dowek</editor>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008-08-12" />
			<biblScope unit="volume">5195</biblScope>
			<biblScope unit="page" from="441" to="456" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gpt-J-6b</surname></persName>
		</author>
		<ptr target="https://github.com/kingoflolz/mesh-transformer-jax" />
		<title level="m">A 6 Billion Parameter Autoregressive Language Model</title>
		<imprint>
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Premise selection for theorem proving by deep graph embedding. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">minif2f: a cross-system benchmark for formal olympiad-level mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><forename type="middle">Michael</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Polu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=9ZPegFuFTFv" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
