<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bidi-rectional Self-Training with Multiple Anisotropic Prototypes for Domain Adaptive Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date>October 10-14, 2022. 2022. Oct. 10-14, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Lu</surname></persName>
							<email>luyvlei@163.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
							<email>yaweiluo329@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<email>zhangli@insigma.com.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheyang</forename><surname>Li</surname></persName>
							<email>lizheyang@hikvision.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yangyics@zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
							<email>junx@cs.zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheyang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Zhejiang Insigma Digital Technology Co</orgName>
								<address>
									<settlement>Ltd. Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Hikvision Research Institute Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Zhejiang University Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Zhejiang University Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bidi-rectional Self-Training with Multiple Anisotropic Prototypes for Domain Adaptive Semantic Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 30th ACM Interna-tional Conference on Multimedia (MM &apos;22)</title>
						<meeting>the 30th ACM Interna-tional Conference on Multimedia (MM &apos;22) <address><addrLine>Lisboa, Portugal; Lisboa, Portugal; New York, NY, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<biblScope unit="volume">11</biblScope>
							<date type="published">October 10-14, 2022. 2022. Oct. 10-14, 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3503161.3548225</idno>
					<note>/luyvlei/BiSMAPs. * Yawei Luo is the corresponding author ACM Reference Format:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ! Image segmentation KEYWORDS Semantic Segmentation</term>
					<term>Unsupervised Domain Adaptation</term>
					<term>Gauss- ian Mixture Model</term>
					<term>Self-training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A thriving trend for domain adaptive segmentation endeavors to generate the high-quality pseudo labels for target domain and retrain the segmentor on them. Under this self-training paradigm, some competitive methods have sought to the latent-space information, which establishes the feature centroids (a.k.a prototypes) of the semantic classes and determines the pseudo label candidates by their distances from these centroids. In this paper, we argue that the latent space contains more information to be exploited thus taking one step further to capitalize on it. Firstly, instead of merely using the source-domain prototypes to determine the target pseudo labels as most of the traditional methods do, we bidirectionally produce the target-domain prototypes to degrade those source features which might be too hard or disturbed for the adaptation. Secondly, existing attempts simply model each category as a single and isotropic prototype while ignoring the variance of the feature distribution, which could lead to the confusion of similar categories. To cope with this issue, we propose to represent each category with multiple and anisotropic prototypes via Gaussian Mixture Model, in order to t the de facto distribution of source domain and estimate the likelihood of target samples based on the probability density. We apply our method on GTA5-&gt;Cityscapes and Synthia-&gt;Cityscapes tasks and achieve 61.2% and 62.8% respectively in terms of mean IoU, substantially outperforming other competitive self-training methods. Noticeably, in some categories which severely suer from the categorical confusion such as "truck" and "bus", our method achieves 56.4% and 68.8% respectively, which further demonstrates the eectiveness of our design. The code and model are available at https://github.com</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Semantic segmentation is a ne-grained image understanding task with the goal of assigning a specic category to each pixel. Recently, this task has achieved remarkable progress with the development of deep neural network <ref type="bibr">[1, 3-5, 18, 42]</ref>. The satisfying performance, nevertheless, usually comes with a price of expensive and laborious label annotations. One of the thriving solutions to mitigate this issue has sought to the synthetic datasets rendered from simulators and game engines <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. However, the notorious domain shift <ref type="bibr" target="#b30">[31]</ref> impedes the model trained on synthetic images to be further deployed in a practical environment. To deal with this issue, domain adaptation (DA) approaches <ref type="bibr">[11, 13, 15, 16, 19-23, 26, 30, 33, 34, 43]</ref> are proposed to bridge the gap between the source and target domains. In practice, the unsupervised domain adaptation (UDA), which does not need any labeled examples from the target domain, received more attention since it minimizes human labor ultimately.</p><p>Under the UDA setting, current state-of-the-art methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45]</ref> usually endeavor to generate high-quality pseudo labels and transfer the UDA problem into a self-learning task. The common and pivotal steps in this campaign consist of 1) training an initial adaptation model across domains, which is also called "warmup stage" and 2) generating pseudo labels to self-train the initial model towards target domain. In the warmup stage, adversarial training <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> and style transfer <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38]</ref> techniques are most widely used. Specically, adversarial training utilizes a discriminator to align the distributions of dierent domains while style transfer converts the source-domain images into the target  style to train the model. For the self-training stage, assigning pseudo labels based on the prediction condence is a common practice, yet determining the condence thresholds for variant classes is non-trivial. More recently, applying "feature centroids" (a.k.a prototypes) of the semantic classes to assign pseudo labels <ref type="bibr" target="#b40">[41]</ref> in the latent space mitigates the issue above. Such strategy determines the pseudo label candidates by their distances from feature centroids, which outperforms the approaches of using prediction condence and becomes a vital enabling factor of many competitive methods. This paper follows a self-training paradigm relying on latentspace information. Upon reviewing the recent attempts along this vein, we notice several potential associated issues on the pseudo label assignment mechanism that decline the adaptation performance. First, existing approaches conduct the adaptation using whole source-domain information but ignore the fact that some hard and disturbed source samples do not contribute or even impede the target domain performance. For instance, the GTA5 dataset contains a vast of pixels for mountain areas while Cityscapes does not. To force the domain alignment introducing these pixels would on the contrary drift the target distribution. Second, traditional methods assume that each category obeys an isotropic distribution with the same variance, thus simply using a single feature centroid as the prototype and employing Euclidean distance as the metric to evaluate the similarity of a candidate feature to the current prototype. In this way, a feature that is close enough to a prototype will be assigned a pseudo label. Nevertheless, this assumption does not necessarily hold for the pixel-level features of semantic segmentation. For example, the category "vegetation" is a single class but it actually includes variant parts such as trunk and crown. When using a single prototype to represent "vegetation", the features of trunk might be improperly mapped closer to the prototype of "pole", as shown in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>. Besides, simply using the Euclidean distance as the metric while ignoring the distinct variance of each class may further deteriorate the pseudo label assignment between similar categories, as shown in <ref type="figure" target="#fig_1">Fig. 1(c)</ref>.</p><p>These observations motivate our design of a novel bidirectional self-training method with multiple anisotropic prototypes (dubbed BiSMAP). BiSMAP argues that the latent space actually contains more information to be exploited thus taking one step further to capitalize on it. Firstly, instead of merely using the source-domain prototypes to determine the target pseudo labels as most of the traditional methods do, BiSMAP bidirectionally produces the targetdomain prototypes to degrade those source features which might be too hard or disturbed for self-training. Secondly, instead of simply modeling each category as a single and isotropic prototype while ignoring the variance of the feature distribution that leads to the confusion of similar categories, BiSMAP proposes to represent each category with multiple and anisotropic prototypes via Gaussian Mixture Model, in order to t the de facto distribution of source domain and estimate the likelihood of target samples based on the probability density, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>(e). In this manner, each feature can be assigned to the correct class more accurately during the adaptation.</p><p>We apply BiSMAP on GTA5-&gt;Cityscapes and Synthia-&gt;Cityscapes tasks and achieve 61.2% and 62.8% respectively in terms of mean IoU, substantially outperforming other competitive self-training methods. Noticeably, in some categories which severely suer from the categorical confusion such as "truck" and "bus", BiSMAP achieves 56.4% and 68.8% respectively, which further demonstrates the effectiveness of our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>This section will review the existing works on Unsupervised Domain Adaptation, Self-training, and Gaussian Mixture Model techniques, respectively.</p><p>Unsupervised Domain Adaptation. To deal with the performance gap between domains, numerous works have been explored to align source and target distributions. Feature alignment based on adversarial training is preferred for UDA of segmentation tasks, which use a discriminator to guide the model to generate domaininvariant features <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. In addition to the global feature alignment, category level alignment based on prototype <ref type="bibr" target="#b40">[41]</ref> has emerged as another solution, which directly decreases the Euclidean distance between source and target features. Motivated by the recent image-to-image translation works, some works employ the style transfer technique to alleviate style dierences of images from dierent domains, thus reducing the domain gap before training <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>Self-training. Self-training is a semi-supervised learning method which oers competitive performance for UDA. These methods rst train an initial adaptation model across domains and then relies on the model predictions to assign pseudo labels. Assigning pseudo-labels of high prediction condence is a common practice of self-training. The pixels of common categories tend to be high condence which leads to the rare categories can't be assigned pseudo labels, resulting in the model bias towards easy categories and thus ruining the performance of the rare ones. Nevertheless, it suers from the noise of pseudo labels since pseudo labels with high condence might not always be correct. Recent pseudo-label selection methods are developed to deal with the above problem. Zou et al. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b44">45]</ref> try to nd appropriate thresholds to generate pseudo labels of rare categories under the principle of class balance. Pan et al. <ref type="bibr" target="#b25">[26]</ref> use the image-level entropy to split the target domain data into two groups, then select the group of lower entropy for self-training. Pseudo-label assignment based on the class prototype is another way. Zhang et al. <ref type="bibr" target="#b40">[41]</ref> use the category centroid as a prototype to assign pseudo labels, which could mitigate the side eect of unbalanced data distribution and dig up more samples for self-training. Zhang et al. <ref type="bibr" target="#b39">[40]</ref> dynamically update the pseudo labels to correct the wrong labels, producing better results.</p><p>Gaussian Mixture Model. Gaussian Mixture Model (GMM) <ref type="bibr" target="#b26">[27]</ref> is a probabilistic model that represents the presence of subpopulations within an overall population. A typical scenario of GMM is clustering and density estimation. Wang et al. <ref type="bibr" target="#b35">[36]</ref> cluster the entropy of each class to divide the unlabeled samples into two groups, so as to nd the reliable pseudo labels. Zong et al. <ref type="bibr" target="#b43">[44]</ref> handles the density estimation problem in anomaly detection based on GMM. It can also work as a prototype. Yang et al. <ref type="bibr" target="#b36">[37]</ref> utilizes GMM as multiple prototypes to alleviate the semantic ambiguity caused by single prototype in few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD 3.1 Problem Setting</head><p>First, we formally introduce the problem of domain adaptive semantic segmentation. Under this setting, we are given a source dataset with full annotation {-B , . B } and an unlabeled target dataset {-C }, both of which share the same category set C. The goal is to utilize these two datasets to train a segmentor that can apply to the target domain. In general, the segmentor ? can be divided into a feature extractor ? and a classier :</p><formula xml:id="formula_0">? = ?.<label>(1)</label></formula><p>The source-domain knowledge can be learned under a typical supervised learning paradigm:</p><formula xml:id="formula_1">! B46 = E G?-B ,~?. B [? (? (G),~],<label>(2)</label></formula><p>where ? (., .) denotes a proper loss function such as cross entropy. In the target domain, self-training methods tend to generate the pseudo labels of the target samples {. C } rst and retrain the model</p><formula xml:id="formula_2">on {-C ,. C } together with {-B , . B } reusing Eq. 2.</formula><p>However, as mentioned above, existing attempts in latent space usually ignore the variance and multi-cluster trait of the feature distribution, thus generating the noisy. C . Additionally, some hard or disturbed samples in -B are enrolled in the training process,  These high-quality pseudo labels are then leveraged to retrain the segmentor. In the proposed "backward direction", we retrain the segmentor with both source data and selected target pseudo labels. In this process, we employ the targetdomain prototypes to generate STM and reweight the pixellevel training loss on source samples with STM (i.e., the source pixels in the small-value areas of the STM will be degraded in term of their training losses).</p><p>which would impede the adaptation performance. To deal with these issues, we propose the BiSMAP method. On one hand, it utilizes the "Multiple Anisotropic Prototypes" (MAPs) to generate more accurate pseudo labels for the retraining phase. On the other hand, it bidirectionally introduces the target-domain prototypes to degrade those hard or disturbed source samples. These designs will be detailed in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bidirectional Self-training</head><p>A brief overview of our bidirectional idea is illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. It consists of a "forward direction" that selects reliable pseudo labels in the target domain resorting to the source-domain prototypes, as well as a complementary "backward direction" to degrade those hard or disturbed source samples according to their relations to the target-domain prototypes. In the forward direction, we improve the traditional self-training methods by introducing the "Multiple Anisotropic Prototypes" to generate more accurate pseudo labels.</p><p>In the backward direction, we generate the Source Transferability Map (STM) to represent the importance of each source sample in the adaptation and accordingly reweight the training loss map. For convenience, we begin with the introduction of MAPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multiple Anisotropic Prototypes</head><p>As analyzed above, the de facto feature distribution of a category is exhibited as multi-cluster and anisotropic, which goes beyond the representation ability of a traditional single centroid. To address</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAPs</head><p>Feature extractor of warmup model this limitation, we propose Multiple Anisotropic Prototypes (MAPs), as illustrated in <ref type="figure" target="#fig_3">Fig. 3(a)</ref>. MAPs Generation using Gaussian Mixture Model. To produce MAPs of the source domain, we match the source feature distribution with a Gaussian Mixture Model (GMM). GMM is a classical model which can t the complex distributions and estimate the probability density <ref type="bibr" target="#b26">[27]</ref>. The advantage of choosing GMM is multi-fold. On one hand, GMM employs probability density instead of the isotropic distance to measure the probability that a sample belongs to a cluster. On the other hand, GMM harnesses multiple weighted Gaussian distributions and hence can be extended to non-Gaussian cases. To get MAPs of each category, we leverage the encoder ? to extract the features of each class on the source domain. Specically, only those source features that are correctly classied are considered in this process: <ref type="bibr" target="#b7">8</ref> and represents the feature extracted by ? at pixel index 1 8 of G B , 2 denotes a specic category, G B and B are the source image and its corresponding label. ? denotes the feature extractor of the segmentor which is trained by domain alignment methods, e.g., adversarial training, or style transfer. ? 2 denotes the feature set of category 2 in the source domain.</p><formula xml:id="formula_3">? 2 = {5 B 8 | arg max ? (G B ) 8 = 2,~B 8 = 2, G B 2 -B }, (3) where 5 B 8 is calculated by ? (G B )</formula><p>GMM works in a class-aware mode in which the features ? 2 from each category are tted by a specic GMM. In this manner, we can get ? GMMs corresponding to ? semantic category. Each GMM takes the form of a weighted sum of a series of Gaussian distributions given by:</p><formula xml:id="formula_4">? 2 (5 ) = log( ' :=1 c 2,: # 2,: (5 |`2 ,: , ? 2,: )),<label>(4)</label></formula><formula xml:id="formula_5"># 2,: (5 ) = exp{ 1 2 (5 `2 ,: ) ) ? 1 2,: (5 `2 ,: ) } (2c ) 3/2 |? 2,: | 1/2 ,<label>(5)</label></formula><p>where 5 is a feature vector,`and ? are the mean vector and covariance matrix of the Gaussian distribution, represents the number of Gaussian distributions, 3 is the dimension of feature vector, and c denotes the mixture weight. 2 and : are the category index and the prototype index, respectively. Each category has prototypes and each prototype follows an anisotropic Gaussian distribution. For the feature set ? 2 of a specic category 2 and the initial prototype ? 2 , we can apply Expectation-Maximization algorithm <ref type="bibr" target="#b26">[27]</ref> to solve the GMM equation iteratively and get the prototype ? 2 composed of GMM. Since each category has prototypes and each prototype has its covariance matrix, our design breaks the constraints of the improper "single-centroid and isotropic" distribution assumption. Note that due to the heavy memory footprint of the EM algorithm facing the large-scale pixel-level samples, it is impractical to apply EM straightly on the whole source-domain features. Instead, we sample 300, 000 features from each category, compromising to the scale that an EM algorithm can handle eciently.</p><p>Feature Similarity Measurement. We take the prototype ? 2 corresponding to category 2 as an example to illustrate the measurement of similarity between target-domain features and sourcedomain prototypes. Given a target image G C , we use the feature extractor ? to get the feature map 5 C = ? (G C ). For any given pixellevel feature 5 C 8 , we can measure its similarity to the prototype as ? 2 <ref type="bibr">(5 C 8 )</ref>. Such similarity is based on the probability density rather than the isotropic distance so that it can cope with more complex distributions, and we call {? 2 | 2 2 ?} multiple anisotropic prototypes. Therefore, for each pixel feature we can get a set of probabilities {? 2 (5 C 8 ) | 2 2 ?}. Pseudo Label Assignment. MAPs can estimate the probability of generating a sample of a certain category at any location in latent space. With the probability between each pixel-level feature and the prototypes, we assign pseudo labels based on the log probability density. We take a specic pixel feature 5 C 8 for example. First, we get the similarity set {? 2 (5 C 8 ) | 2 2 ?} using Eq. 4. Then we dene the pseudo label of pixel 8 in target image G C as?C 8 , which is is a one-hot vector or an all-zero vector. The A -th bit of?8 is calculated by the following formula:</p><formula xml:id="formula_6">C 8A = 8 &gt; &lt; &gt; : 1, A = arg max 2 (? 2 (5 C 8 )), ? A (5 C 8 ) X 0, &gt;C?4AF8B4.<label>(6)</label></formula><p>Our pseudo label assignment strategy is based on probability density: samples with log probability density higher than X will be assigned pseudo labels and others will be ignored. The target domain dataset with pseudo labels denotes as {-C ,. C }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Source Sample Degradation</head><p>In traditional self-training methods, some hard or disturbed samples in -B are easily enrolled in the training process, which would impede the adaptation performance. In this section, we would lter out these samples by estimating their transferability and reducing their inuence on adaptation.</p><p>Target Prototypes Generation via Clustering. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, we measure the transferability of each source sample with the aid of target-domain prototypes. Due to the unlabeled characteristic of the target domain, we are not able to use any categorical information to generate the prototypes like traditional methods. Accordingly, a clustering-based method is proposed. First, we employed a network? pre-trained on a third-party dataset, e.g., ImageNet, to extract semantic features from the target domain and get a feature set. Then an unsupervised clustering algorithm, e.g., K-Means <ref type="bibr" target="#b9">[10]</ref>, is applied to cluster these features. Finally, we keep the cluster centers { 9 | 0 &lt; 9 &lt; } as the target prototypes where is the number of clusters used in K-Means. These prototypes are later employed to evaluate the transferability of source domain features, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>Source Transferability Map. Now we have gotten a set of prototypes that could represent the feature distribution of the target domain, in the next step we will evaluate the transferability of source features and gure out those hard or disturbed samples. Here we propose to estimate the transferability of each sample based on both feature distance and category entropy. Formally, we rst dene the distance from the source features to the target domain as follows:</p><formula xml:id="formula_7">? (G B ) 8 = &lt;8={ 5 B 8 9 2 | 0 &lt; 9 &lt; },<label>(7)</label></formula><p>where5 B represents the feature of G B extracted by?, 9 is one of the target prototypes and 8 is the pixel index on feature map5 B . ? (G B ) 8 coarsely depicts the transferability of a pixel-level source feature but suers from the long-tailed distribution: we nd the samples from head classes can always got a relatively high score in the transferability map and vise versa. To balance the classlevel transferability score, we calculate the average entropy of the predicted results for each class in the target domain using the source pre-trained segmentation model. Then we apply Min-Max normalization to those category entropy:</p><formula xml:id="formula_8">4 0 2 = 4 2 4 &lt;8= 4 &lt;0G 4 &lt;8= ,<label>(8)</label></formula><p>where 4 2 denotes the mean entropy of category 2 in the target domain, 4 &lt;8= and 4 &lt;0G represent the maximum and minimum entropy of all categories, respectively. 4 2 is calculated from the softmax layer of the source pre-trained segmentation model on target domain. Since there are no labels for the target domain, we use pseudo-labels to calculate the category entropy. Finally, we dene the transferability map of G B as the follows:</p><formula xml:id="formula_9">F B 8 = &lt;8={exp( ? (G B ) 2 8 3 2 &lt;40= log 2) + 4 0 2 , 1.0},<label>(9)</label></formula><p>where 3 &lt;40= denotes the mean distance calculated by ? (?) in the whole source dataset, F B is the transferability map and F B 8 represents the value of F B at pixel index 8 and 2 is the category of G B 8 according to~B <ref type="bibr" target="#b7">8</ref> . Obviously, Eq. 9 contains a term based on feature distance and a term based on category entropy. The feature distance determines the overall transferability of a source region, while the category entropy serves as a lower bound to balance the class-level transferability. The shape of the transferability map F B is consistent with the shape of the source images, as shown by <ref type="figure" target="#fig_2">Fig. 2</ref>. We will detail the usage of F B during training in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training Pipeline</head><p>The proposed BiSMAP is composed of a warmup stage and a selftraining stage, and the latter can be further divided into a pseudo label generation phase and a retraining phase. We warm up the initial model with Eq. 2, in which we stylize the source images using Global Photometric Alignment <ref type="bibr" target="#b23">[24]</ref> technique following the setting of [24] 2 . These stylized source data will be also employed in the self-training stage.</p><p>In MAPs generation described in Sec. 3.3, the model pre-trained in the warmup stage is applied as the feature extractor ? for producing MAPs. We sample 300, 000 features per class and implement GMM with 8 diagonal covariance Gaussian distributions. Then the pseudo labels are generated via MAPs according to Eq. 6. For STM generation described in Sec. 3.4, we employ a ResNet152 pre-trained by SimCLRv2 <ref type="bibr" target="#b5">[6]</ref> as the feature extractor?.</p><p>The proposed BiSMAP is featured by three loss functions, i.e., the reweighted source loss, the target self-training loss, and the consistency regularization loss. Given an image G B 2 -B of shape 3 ? ? , and a label map~B 2 . B of shape ? ? ? , where ? is the number of semantic classes, the loss of each sample in source  </p><formula xml:id="formula_10">L 24 (G B , \ ? ) = ?, ' 8=1 ? ' 2=1 F B 8~B 82 log ? 82 ,<label>(10)</label></formula><p>where ? 82 represents the probability of class 2 on pixel 8.~B 82 is the ground truth of class 2 on the pixel 8. F B 8 denotes the value of pixel 8 in the transferability map illustrated in Section 3.4. \ ? is the parameter of the model ? to be optimized.</p><p>For the images G C 2 {-C } from the target domain and its corresponding pseudo label?C , we train the model with a symmetric cross-entropy loss <ref type="bibr" target="#b34">[35]</ref>:</p><formula xml:id="formula_11">L B24 (G C , \ ? ) = ?, ' 8=1 ? ' 2=1 U?C 82 log ? 82 V? 82 log?C 82 ,<label>(11)</label></formula><p>We clamp the one-hot label?C to <ref type="bibr">[14 4, 1]</ref> to avoid the numerical issue of log 0. The U and V are set to 0.1 and 1.0 respectively. To further mitigate the noise in the candidate pseudo labels, we apply consistency regularization with KLD loss on the target domain:</p><formula xml:id="formula_12">L 2&gt;=B8BC (G C , i (G C ), \ ? ) = ?, ' 8=1 ? ' 2=1 e ? 82 log e</formula><p>? 82 e ? 82 log ? 82 , <ref type="bibr" target="#b11">(12)</ref> where i represents an image augmentation function and e ? is the output of the exponential moving average (EMA) model with i (G C ) as input as introduced by <ref type="bibr" target="#b31">[32]</ref>. The overall training loss is:</p><formula xml:id="formula_13">L = L 24 + L B24 + _L 2&gt;=B8BC<label>(13)</label></formula><p>where _ controls the relative weight of consistency regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT 4.1 Datasets</head><p>We evaluate BiSMAP together with several state-of-the-art algorithms on two synthetic-to-real domain adaptation tasks: GTA5 <ref type="bibr" target="#b27">[28]</ref> to Cityscapes and SYNTHIA <ref type="bibr" target="#b28">[29]</ref> to Cityscapes. GTA5 contains 24, 966 images with 1, 914 ? 1, 052 resolution. SYNTHIA contains 9, 400 images with 1, 280 ? 760 resolution. We use GTA5 and SYN-THIA as the source domain and Cityscapes as the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We utilize the DeepLab-V3Plus <ref type="bibr" target="#b4">[5]</ref> with ResNet101 <ref type="bibr" target="#b11">[12]</ref> as the segmentor following the setting of <ref type="bibr" target="#b23">[24]</ref>, and the backbone is pre-trained on ImageNet <ref type="bibr" target="#b7">[8]</ref>. The source domain images are converted to the style of the target domain by GPA as introduced in <ref type="bibr" target="#b23">[24]</ref>, and all the following phase is based on the stylized dataset. Images are randomly scaled by ?0.5 ? ?1.5 and cropped to 896 ? 512.</p><p>For the generation of MAPs, we use GMM with 8 Gaussian distributions. The probability density threshold X is set to 100 for  GTA5-&gt;Cityscapes and 50 for Synthia-&gt;Cityscapes. We use the standard color-jittering augmentation in both source and target domains as in <ref type="bibr" target="#b23">[24]</ref>. In consistency regularization, we follow the setting of <ref type="bibr" target="#b31">[32]</ref>, where an EMA model works as a teacher to guide the main model. We use RandAugment <ref type="bibr" target="#b6">[7]</ref> and CutOut <ref type="bibr" target="#b8">[9]</ref> as image augmentation function i. The _ is set to 20, and the smoothing coecient of EMA model is set to 0.999. The batch size is set to 8 for warmup and 4 for self-training, and the iteration of each stage is 90, 000. We use SGD <ref type="bibr" target="#b1">[2]</ref>optimizer with momentum and weightdecay as 0.9 and 0.0005, respectively. The learning rate of SGD is set to 0.0005 and decayed by the poly policy with power 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparative Studies</head><p>Compared with SOTA. We evaluate BiSMAP together with several state-of-the-art methods. It's worth noting that the most of prototype-based methods are implemented on DeepLab-V3Plus since it can better corresponds a feature to a certain pixel. For fair comparison, we also report the results with an additional distillation stage which is proposed in <ref type="bibr" target="#b39">[40]</ref>. The qualitative segmentation examples can be viewed <ref type="figure">Fig. 4</ref>. We show the comparison results on GTA5-&gt;Cityscapes in Tab. 1. The mIoU of BisMAP after the self-training stage is 57.7%, which achieves the state-of-the-art accuracy. Besides, BiSMAP brings signicant improvements to the confusion categories compared to the GPA baseline <ref type="bibr" target="#b23">[24]</ref>, e.g., truck and bus. After the distillation stage, BiSMAP achieve 61.2% mIoU, substantially outperforming the previous state of the arts <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b38">39]</ref>. We show the comparison results on Synthia-&gt;Cityscapes in Tab. 2 and get a 10.8% performance gain compared to the GPA baseline. Similarly, we conduct a distillation stage and achieve SOTA 62.8% mIoU.</p><p>Comparison of pseudo label assignment (PLA) strategies. In Tab. 3, we compare the performance of the proposed MAPs-PLA with both prototype-based and threshold-based approaches on GTA5-&gt;Cityscapes task, including "Category Anchors pseudo label assignment" (CAs-PLA) <ref type="bibr" target="#b40">[41]</ref> and "Instant Adaptive Selector pseudo label assignment" (IAS-PLA) <ref type="bibr" target="#b24">[25]</ref>. All of the results are based on the same warmup model and equipped with our proposed STM for a fair comparison. CAs-PLA assigns pseudo labels using Euclidean distance to the prototypes based on the assumption that the feature distribution is isotropic and single-centered. IAS-PLA is a popular method based on the prediction condence, but it is also signicantly lower than our results.</p><p>By modeling the latent-space information more properly, MAPs-PLA boosts the mIoU with an additional 2.4% compared to CAs-PLA  and 1.9% compared to IAS. We show the visualization results of dierent PLA strategies in <ref type="figure">Fig. 5</ref>. The region of the red dotted box in <ref type="figure">Fig. 5</ref> shows that MAPs can not only x some errors of CAs-PLA, but also achieve good results in pseudo-label screening. Besides, MAPs-PLA achieves less noise than CAs-PLA since the probability density function forms a more accurate estimate of the condence.</p><p>Comparison of T-SNE visualization results. To further demonstrate the eectiveness of MAPs-PLA, we visualize the feature distribution trained by CAs pseudo labels and MAPs pseudo labels as shown in <ref type="figure" target="#fig_6">Fig. 6</ref>, in which the misclassied features are marked as "light red". We mainly focus on two confusing categories: trunk and bus. The single-centroid prototype confuses them since they are naturally closed in latent space, e.g., in the latent space of CAs self-trained model, the feature of bus are multiple clusters but most of the clusters are misclassied. In the latent space of MAPs self-trained model, the bus category also exhibits multi-cluster characteristics but are mostly correctly classied thanks to our Multiple Anisotropic Prototypes. We also show the visualization of latent space before self-training in Appendix. C and similar characteristics of multi-cluster can be observed. In conclusion, MAPs-PLA can better correct errors and achieve improvement in such cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>To assess the importance of various aspects of BiSMAP, we investigate the eects of dierent components on GTA5-&gt;Cityscapes task as shown in Tab. 4. The GPA <ref type="bibr" target="#b23">[24]</ref> baseline reaches 46.3% in our experiment. By introducing the STM, we increase the mIoU by 1.9%. And after the self-training stage with the pseudo labels generated by MAPs, the mIoU improves signicantly, reaching 57.3%. BiSMAP achieves 11.4% performance improvement in one stage of self-training. After the distillation stage, we achieve 61.2% mIoU and signicantly outperforms other methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Parameter Studies</head><p>This section investigates the sensitivity of X and analyzes the relationship between the threshold and the ratio of pseudo labels (PL ratio). As shown in Tab. 5, the probability density threshold X is signicantly negatively correlated with the PL ratio. Therefore, more percentage of the pixel will be assigned pseudo labels with the decrease of X. When X = 100, we achieve highest mIoU by trading o the balance between the PL ratio and accuracy. The threshold used by CAs-PLA is based on spatial distance as designed in <ref type="bibr" target="#b40">[41]</ref>. It got highest performance 54.9% when the threshold is 1.0 and 67.8% pixels are assigned pseudo labels. However, the proposed MAPs-PLA strategy gets 55.7% when only 44.3% pixels are assigned pseudo labels. We show that, at about the same ratio of pseudo labels, MAPs-PLA achieves higher performance. Similar conclusions can be drawn at other scales of PL ratio. By comparing the segmentation results under dierent parameters, we can conclude that MAPs-PLA improves the robustness of pseudo labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we proposed a bidirectional self-training with multiple anisotropic prototypes method for unsupervised domain adaption. Specically, we produce the target-domain prototypes to degrade those source features and develop a robust pseudo label assignment strategy based on the multiple anisotropic prototypes. Extensive experiments verify the eectiveness and superiority of BiSMAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix. A PARAMETER ANALYSIS OF K</head><p>To analyze the eectiveness of multiple prototypes, we use GMMs with dierent (the number of clusters) to generate pseudo-labels during training, and the results are shown in Tab. 6. We nd that as increases, the performance of self-training gradually improves. When is increased to 6, the mIoU achieves 57.3%. Further increasing the number of can no longer improve performance. In conclusion, a larger value can usually stimulates a stronger GMM's ability to t the de facto distribution, yet = 8 is enough to match the distribution in this task. Accordingly, we choose the value of = 8 in the experiment. These experimental results do not contain consistency regularization in order to pinpoint the pure eect of .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix. B DETAILED ABLATION STUDY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix. C T-SNE VISUALIZATION OF LATENT SPACE BEFORE SELF-TRAINING</head><p>In <ref type="figure">Fig. 7</ref>, we show the 2D visualization results of features from "truck" and "bus". It can be seen that the features of semantic segmentation have the characteristics of large variance and multi-class clusters in the latent space.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>source sample, class A source sample, class B feature centroid, class A feature centroid, class B target sample, class A (a) single prototype (b) multiple prototypes (c) isotropic prototype (d) anisotropic prototype (e) multiple anisotropic prototypes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(Best viewed in color.) Comparison of single isotropic prototype and multiple anisotropic prototypes. (a) Traditional single prototype. Since the features of a certain category do not always obey a single-cluster cohesion in the semantic segmentation, the centroid of class A may fall into a low-density region and cause a target sample "+" to locate closer to centroid B. (b) Representing class A with multiple prototypes mitigates this issue, where the target sample "+" can be correctly classied. (c) Traditional isotropic prototype merely considers the centroid of the feature distribution. Because of the ignorance of variance, the target sample "+" is misclassied to centroid B. (d) Anisotropic prototype, where the ellipses represent contour lines of the distribution probability. Taking variance of the feature distribution into account can assign the target sample "+" correctly. (e) BiSMAP takes advantage of both (b) and (d) to model the categorical features with multiple anisotropic prototypes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>(Best viewed in color.) The schematic diagram of BiSMAP, in which the prototypes from both domains are innovatively introduced. In the "forward direction", we utilize the proposed MAPs to do the pseudo labels selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The architecture of BiSMAP. (a) Pseudo label generation phase. The feature extractor ? is pre-trained using the stylized source dataset in the previous warmup stage and kept xed in this phase. In the source training ow, source features extracted by ? are clustered by GMM in a class-ware mode to generate MAPs. In the target inference ow, the target features extracted by ? are evaluated by MAPs and assigned high-quality pseudo labels. (b) Retraining phase. ? is retrained with the source samples together with the target pseudo labels generated in the rst phase. During retraining, the loss of the source samples is reweighted by the source transferability map (STM). To further mitigate the noise of pseudo labels, consistency regularization based on the exponential moving average (EMA) model and data augmentation is applied in this process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Qualitative results of UDA segmentation for GTA5-&gt;Cityscapes. Pseudo label comparison to other PLA strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Latent space visualization of (a) CAs-PLA and (b) MAPs-PLA using T-SNE. We use the "bright red" color to indicate those misclassied features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>2D visualization of the truck and bus features based on T-SNE. Each category shows 3 clusters obtained by GMM clustering, which are represented by dierent colors. All the features are extracted from GTA dataset with the warmup model. Appendix. D MORE PSEUDO-LABEL COMPARISON AND QUALITATIVE RESULTS (a) IAS-PLA (b) CAs-PLA (c) MAPs-PLA (d) Ground truth Pseudo label comparison with other methods: (a) IAS-PLA. (b) CAs-PLA. (c) MAPs-PLA. (d) Ground Truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative results of UDA segmentation for GTA-&gt;Cityscapes. We visualize the results of the GPA baseline, BiSMAP (ours), and the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of domain adaption tasks in GTA to Cityscapes. "*" indicates the results after the distillation stage. Coarse-to-ne [24] 92.5 58.3 86.5 27.4 28.8 38.1 46.7 42.5 85.4 38.4 91.8 66.4 37.0 87.8 40.7 52.4 44.6 41.7 59.0 56.1</figDesc><table><row><cell>Methods</cell><cell>road</cell><cell>side.</cell><cell>buil.</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>vege.</cell><cell>terr.</cell><cell>sky</cell><cell>pers.</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motor</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell>CAG_UDA [41]</cell><cell cols="19">90.4 51.6 83.8 34.2 27.8 38.4 25.3 48.4 85.4 38.2 78.1 58.6 34.6 84.7 21.9 42.7 41.1 29.3 37.2</cell><cell>50.2</cell></row><row><cell>IAST [25]</cell><cell cols="19">94.1 58.8 85.4 39.7 29.2 25.1 43.1 34.2 84.8 34.6 88.7 62.7 30.3 87.6 42.3 50.3 24.7 35.2 40.2</cell><cell>52.2</cell></row><row><cell>FDA [38]</cell><cell cols="19">92.5 53.3 82.4 26.5 27.6 36.4 40.6 38.9 82.3 39.8 78.0 62.6 34.4 84.9 34.1 53.1 16.9 27.7 46.4</cell><cell>50.5</cell></row><row><cell>ProDA [40]</cell><cell cols="16">91.5 52.4 82.9 42.0 35.7 40.0 44.4 43.3 87.0 43.8 79.5 66.5 31.4 86.7 41.1 52.5</cell><cell cols="3">0.0 45.4 53.8</cell><cell>53.7</cell></row><row><cell>MFA [39]</cell><cell cols="16">94.5 61.1 87.6 41.4 35.4 41.2 47.1 45.7 86.6 36.6 87.0 70.1 38.3 87.2 39.5 54.7</cell><cell>0.3</cell><cell cols="2">45.4 57.7</cell><cell>55.7</cell></row><row><cell>GPA(baseline)</cell><cell cols="19">76.8 34.6 68.2 22.7 21.4 40.1 44.1 26.5 85.4 29.4 74.6 67.4 27.6 87.9 37.7 47.5 34.3 29.2 24.7</cell><cell>46.3</cell></row><row><cell>BiSMAP(ours)</cell><cell cols="20">86.2 48.4 83.5 43.8 38.2 41.8 49.5 54.7 87.9 41.7 84.7 63.9 34.4 89.1 49.1 62.2 43.8 37.1 56.6 57.7</cell></row><row><cell>ProDA* [40]</cell><cell cols="16">87.8 56.0 79.7 46.3 44.8 45.6 53.5 53.5 88.6 45.2 82.1 70.7 39.2 88.8 45.5 59.4</cell><cell>1.0</cell><cell cols="2">48.9 56.4</cell><cell>57.5</cell></row><row><cell>MFA* [39]</cell><cell cols="16">93.5 61.6 87.0 49.1 41.3 46.1 53.5 53.9 88.2 42.1 85.8 71.5 37.9 88.8 40.1 54.7</cell><cell>0.0</cell><cell cols="2">48.2 62.8</cell><cell>58.2</cell></row><row><cell>BiSMAP*(ours)</cell><cell cols="20">89.2 54.9 84.4 44.1 39.3 41.6 53.9 53.5 88.4 45.1 82.3 69.4 41.8 90.4 56.4 68.8 51.2 47.8 60.4 61.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of domain adaption tasks in SYNTHIA to Cityscapes. "*" indicates the results after the distillation stage.</figDesc><table><row><cell>Methods</cell><cell>road</cell><cell>side.</cell><cell>buil.</cell><cell>light</cell><cell>sign</cell><cell>vege.</cell><cell>sky</cell><cell>pers.</cell><cell>rider</cell><cell>car</cell><cell>bus</cell><cell>motor</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell>CAG_UDA [41]</cell><cell>84.7</cell><cell>40.8</cell><cell>81.7</cell><cell>13.3</cell><cell>22.7</cell><cell>84.5</cell><cell>77.6</cell><cell>64.2</cell><cell>27.8</cell><cell>80.9</cell><cell>19.7</cell><cell>22.7</cell><cell>48.3</cell><cell>51.5</cell></row><row><cell>IAST [25]</cell><cell>81.9</cell><cell>41.5</cell><cell>83.3</cell><cell>30.9</cell><cell>28.8</cell><cell>83.4</cell><cell>85.0</cell><cell>65.5</cell><cell>30.8</cell><cell>86.5</cell><cell>38.2</cell><cell>33.1</cell><cell>52.7</cell><cell>57.0</cell></row><row><cell>FDA [38]</cell><cell>79.3</cell><cell>35.0</cell><cell>73.2</cell><cell>19.9</cell><cell>24.0</cell><cell>61.7</cell><cell>82.6</cell><cell>61.4</cell><cell>31.1</cell><cell>83.9</cell><cell>40.8</cell><cell>38.4</cell><cell>51.1</cell><cell>52.5</cell></row><row><cell>Coarse-to-ne [24]</cell><cell>75.7</cell><cell>30.0</cell><cell>81.9</cell><cell>18.0</cell><cell>32.7</cell><cell>86.2</cell><cell>90.1</cell><cell>65.1</cell><cell>33.2</cell><cell>83.3</cell><cell>36.5</cell><cell>35.3</cell><cell>54.3</cell><cell>55.5</cell></row><row><cell>ProDA [40]</cell><cell>87.1</cell><cell>44.0</cell><cell>83.2</cell><cell>45.8</cell><cell>34.2</cell><cell>86.7</cell><cell>81.3</cell><cell>68.4</cell><cell>22.1</cell><cell>87.7</cell><cell>50.0</cell><cell>31.4</cell><cell>38.6</cell><cell>58.5</cell></row><row><cell>MFA [39]</cell><cell>85.4</cell><cell>41.9</cell><cell>84.1</cell><cell>22.2</cell><cell>23.9</cell><cell>83.6</cell><cell>80.7</cell><cell>71.5</cell><cell>35.8</cell><cell>86.6</cell><cell>47.6</cell><cell>37.2</cell><cell>62.5</cell><cell>58.7</cell></row><row><cell>GPA(baseline)</cell><cell>75.3</cell><cell>31.3</cell><cell>78.7</cell><cell>22.4</cell><cell>25.2</cell><cell>75.9</cell><cell>82.0</cell><cell>64.3</cell><cell>28.3</cell><cell>80.2</cell><cell>25.9</cell><cell>24.6</cell><cell>27.1</cell><cell>49.3</cell></row><row><cell>BiSMAP(ours)</cell><cell>79.1</cell><cell>36.6</cell><cell>84.7</cell><cell>31.8</cell><cell>41.2</cell><cell>84.7</cell><cell>89.2</cell><cell>65.4</cell><cell>39.0</cell><cell>79.0</cell><cell>47.6</cell><cell>41.8</cell><cell>61.9</cell><cell>60.1</cell></row><row><cell>ProDA* [40]</cell><cell>87.8</cell><cell>45.7</cell><cell>84.6</cell><cell>54.6</cell><cell>37.0</cell><cell>88.1</cell><cell>84.4</cell><cell>74.2</cell><cell>24.3</cell><cell>88.2</cell><cell>51.1</cell><cell>40.5</cell><cell>45.6</cell><cell>62.0</cell></row><row><cell>MFA* [39]</cell><cell>81.8</cell><cell>40.2</cell><cell>85.3</cell><cell>38.0</cell><cell>33.9</cell><cell>82.3</cell><cell>82.0</cell><cell>73.7</cell><cell>41.1</cell><cell>87.8</cell><cell>56.6</cell><cell>46.3</cell><cell>63.8</cell><cell>62.5</cell></row><row><cell>BiSMAP*(ours)</cell><cell>81.9</cell><cell>39.8</cell><cell>84.2</cell><cell>41.7</cell><cell>46.1</cell><cell>83.4</cell><cell>88.7</cell><cell>69.2</cell><cell>39.3</cell><cell>80.7</cell><cell>51.0</cell><cell>51.2</cell><cell>58.8</cell><cell>62.8</cell></row><row><cell cols="7">domain is reweighted according to {F B } during training in the form of multiplication:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison with other pseudo label assign methods.</figDesc><table><row><cell></cell><cell cols="3">CAs-PLA IAS-PLA MAPs-PLA</cell></row><row><cell>mIoU</cell><cell>54.9</cell><cell>55.4</cell><cell>57.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation of each component</figDesc><table><row><cell cols="5">baseline STM MAPs-PLA consistency distillation</cell><cell>mIoU</cell></row><row><cell>X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>46.3</cell></row><row><cell>X</cell><cell>X</cell><cell></cell><cell></cell><cell></cell><cell>48.2(+1.9)</cell></row><row><cell>X</cell><cell></cell><cell>X</cell><cell></cell><cell></cell><cell>56.4(+10.1)</cell></row><row><cell>X</cell><cell>X</cell><cell>X</cell><cell></cell><cell></cell><cell>57.3(+11.0)</cell></row><row><cell>X</cell><cell>X</cell><cell>X</cell><cell>X</cell><cell></cell><cell>57.7(+11.4)</cell></row><row><cell>X</cell><cell>X</cell><cell>X</cell><cell>X</cell><cell>X</cell><cell>61.2(+14.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Parameter study of MAPs-PLA. For MAPs-PLA, threshold is the X. For CAs-PLA, threshold is based on spatial distance. PL ratio is the ratio of pixel assigned pseudo labels.</figDesc><table><row><cell>method</cell><cell></cell><cell cols="2">MAPs-PLA</cell><cell cols="2">CAs-PLA</cell><cell></cell></row><row><cell>threshold</cell><cell>0</cell><cell>50</cell><cell>100 150 0.5</cell><cell>1</cell><cell>1.5</cell><cell>2</cell></row><row><cell>PL ratio</cell><cell cols="6">87.6 77.5 61.3 44.3 82.7 67.8 54.5 42.2</cell></row><row><cell>mIoU</cell><cell cols="6">56.4 57.1 57.3 55.7 53.2 54.9 53.9 53.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>The detailed results of self-training under dierent parameter on GTA-&gt;Cityscapes. 51.1 83.4 38.9 34.6 40.1 48.2 46.6 86.8 35.9 85.4 66.2 36.5 89.4 41.3 54.2 43.1 39.0 57.4 56.1 2 82.3 42.8 82.3 39.9 39.3 38.9 49.5 47.9 87.3 39.7 82.9 66.5 37.3 89.5 41.2 58.7 55.6 39.5 56.0 56.7 4 86.8 51.2 82.0 43.0 38.0 42.4 48.8 50.3 86.4 38.0 86.4 64.0 36.0 88.5 42.7 53.1 53.9 32.7 55.0 56.8 6 85.9 47.5 83.0 41.3 37.2 43.4 51.8 49.2 87.4 41.0 84.9 67.1 38.4 90.0 46.0 57.7 47.4 34.4 55.4 57.3 8 88.4 47.9 85.3 38.1 36.0 41.8 49.7 47.2 86.2 38.6 79.2 67.7 38.9 89.5 49.7 62.5 47.4 37.9 57.6 57.3</figDesc><table><row><cell>K</cell><cell>road</cell><cell>side.</cell><cell>buil.</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>vege.</cell><cell>terr.</cell><cell>sky</cell><cell>pers.</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motor</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell cols="2">1 87.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>The detailed IoU results of ablation study on GTA-&gt;Cityscapes.<ref type="bibr" target="#b33">34</ref>.6 68.2 22.7 21.4 40.1 44.1 26.5 85.4 29.4 74.6 67.4 27.6 87.9 37.7 47.5 34.3 29.2 24.7 46.3 X X 75.6 38.1 70.5 24.7 25.2 40.3 43.2 27.8 84.9 30.9 77.2 66.7 33.7 87.7 38.9 49.5 33.9 28.6 39.4 48.3 X X 87.1 47.1 84.0 38.0 39.0 41.5 49.4 48.2 86.7 39.0 79.2 67.7 37.2 88.7 43.8 59.9 42.1 36.2 57.2 56.4 X X X 88.4 47.9 85.3 38.1 36.0 41.8 49.7 47.2 86.2 38.6 79.2 67.7 38.9 89.5 49.7 62.5 47.4 37.9 57.6 57.3 X X X X 86.2 48.4 83.5 43.8 38.2 41.8 49.5 54.7 87.9 41.7 84.7 63.9 34.4 89.1 49.1 62.2 43.8 37.1 56.6 57.7 X X X X X 89.2 54.9 84.4 44.1 39.3 41.6 53.9 53.5 88.4 45.1 82.3 69.4 41.8 90.4 56.4 68.8 51.2 47.8 60.4 61.2</figDesc><table><row><cell>baseline</cell><cell>STM</cell><cell>MAPs-PLA</cell><cell>consistency</cell><cell>distillation</cell><cell>road</cell><cell>side.</cell><cell>buil.</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>vege.</cell><cell>terr.</cell><cell>sky</cell><cell>pers.</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motor</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell>X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>76.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Unless otherwise noted, we upsample the feature map to the same size of the input image when generating the transferability map and pseudo labels. In this way, a pixel index 8 can refer to the same position in both feature map and input image.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">GPA is also served as our baseline method.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV</title>
		<meeting>the European conference on computer vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georey E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22243" to="22255" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pattern classication and scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">E</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<publisher>Wiley</publisher>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Homan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fsdr: Frequency space domain randomization for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoran</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6891" to="6902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4893" to="4902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeksoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moonsu</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Signicanceaware information bottleneck for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6778" to="6787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial style mining for one-shot unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="20612" to="20623" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Category-level adversarial adaptation for semantic segmentation using puried features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2507" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Coarse-to-Fine Domain Adaptive Semantic Segmentation with Photometric Alignment and Category-Center Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4051" to="4060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Instance adaptive self-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="page" from="415" to="430" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXVI 16</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised intra-domain adaptation for semantic segmentation through selfsupervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkyu</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Rameau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3764" to="3773" />
		</imprint>
	</monogr>
	<note>Seokju Lee, and In So Kweon</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust text-independent speaker identication using Gaussian mixture speaker models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard C</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on speech and audio processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="72" to="83" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Maximum classier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical planning and inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Uncertainty-aware pseudo label renery for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junran</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9092" to="9101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Prototype mixture models for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="763" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multiple Fusion Adaptation: A Strong Framework for Unsupervised Semantic Segmentation Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00295</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12414" to="12424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Category anchorguided unsupervised domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13049</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pyramid Scene Parsing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="598" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeki</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV</title>
		<meeting>the European conference on computer vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
