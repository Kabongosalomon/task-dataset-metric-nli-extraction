<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mask3D for 3D Semantic Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schult</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
						</author>
						<title level="a" type="main">Mask3D for 3D Semantic Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern 3D semantic instance segmentation approaches predominantly rely on specialized voting mechanisms followed by carefully designed geometric clustering techniques. Building on the successes of recent Transformer-based methods for object detection and image segmentation, we propose the first Transformer-based approach for 3D semantic instance segmentation. We show that we can leverage generic Transformer building blocks to directly predict instance masks from 3D point clouds. In our model -called Mask3D -each object instance is represented as an instance query. Using Transformer decoders, the instance queries are learned by iteratively attending to point cloud features at multiple scales. Combined with point features, the instance queries directly yield all instance masks in parallel. Mask3D has several advantages over current state-of-the-art approaches, since it neither relies on (1) voting schemes which require hand-selected geometric properties (such as centers) nor (2) geometric grouping mechanisms requiring manually-tuned hyper-parameters (e.g. radii) and (3) enables a loss that directly optimizes instance masks. Mask3D sets a new state-of-the-art on ScanNet test (+ 6.2 mAP), S3DIS 6-fold (+ 10.1 mAP), STPLS3D (+ 11.2 mAP) and ScanNet200 test (+ 12.4 mAP).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>This work addresses the task of semantic instance segmentation of 3D scenes. That is, given a 3D point cloud, the desired output is a set of object instances represented as binary foreground masks (over all input points) with their corresponding semantic labels (e.g. 'chair', 'table', 'window').</p><p>Instance segmentation resides at the intersection of two problems: semantic segmentation and object detection. Therefore methods have opted to either first learn semantic point features followed by grouping them into separate instances (bottom-up) or detecting object instances followed by refining their semantic mask (top-down). Bottom-up approaches (ASIS <ref type="bibr" target="#b58">[59]</ref>, SGPN <ref type="bibr" target="#b57">[58]</ref>, 3D-BEVIS <ref type="bibr" target="#b11">[12]</ref>) employ contrastive learning, mapping points to a high-dimensional feature space where features of the same instance are close together, and far apart otherwise. Top-down methods (3D-SIS <ref type="bibr" target="#b21">[22]</ref>, 3D-BoNet <ref type="bibr" target="#b60">[61]</ref>) use an approach akin to Mask R-CNN <ref type="bibr" target="#b18">[19]</ref>: First detect instances as bounding boxes and then perform mask segmentation on each box individually. While 3D-SIS <ref type="bibr" target="#b21">[22]</ref> relies on predefined anchor boxes <ref type="bibr" target="#b18">[19]</ref>, 3D-BoNet <ref type="bibr" target="#b60">[61]</ref> proposes an interesting variation that predicts bounding boxes from a global scene descriptor and optimizes an association loss based on bipartite matching <ref type="bibr" target="#b26">[27]</ref>. A major step forward was sparked by powerful feature backbones <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57]</ref> such as sparse convolutional networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref> that improve over existing PointNets <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44]</ref> and dense 3D CNNs <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b59">60]</ref>. <ref type="bibr" target="#b0">1</ref> Computer Vision Group, RWTH Aachen University, Germany. <ref type="bibr" target="#b1">2</ref> Computer Vision and Learning Group, ETH Z?rich, Switzerland. <ref type="bibr" target="#b2">3</ref> ETH AI Center, Z?rich, Switzerland. <ref type="bibr" target="#b3">4</ref> NVIDIA, Santa Clara, USA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input 3D Scene</head><p>Instance Heatmaps 3D Semantic Instances <ref type="figure" target="#fig_1">Fig. 1: Mask3D</ref>. We train an end-to-end model for 3D semantic instance segmentation on point clouds. Given an input 3D point cloud (left), our Transformer-based model uses an attention mechanism to produce instance heatmaps across all points (center) and directly predicts all semantic object instances in parallel (right).</p><p>Well established 2D CNN architectures <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b45">46]</ref> can now easily be adapted to sparse 3D data. These models can process largescale 3D scenes in one pass, which is necessary to capture global scene context at multiple scales. As a result, bottomup approaches which benefit from strong features (MTML <ref type="bibr" target="#b27">[28]</ref>, MASC <ref type="bibr" target="#b31">[32]</ref>) experienced another performance boost. Soon after, inspired by Hough voting approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref>, VoteNet <ref type="bibr" target="#b40">[41]</ref> proposed center-voting for 3D object detection. Instead of mapping points to an abstract high-dimensional feature space (as in bottom-up approaches), points now vote for their object center -votes from the same object are then closer to each other which enables geometric grouping into instance masks. This idea quickly influenced the 3D instance segmentation field, and by now, the vast majority of current state-of-the-art 3D instance segmentation methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b55">56</ref>] make use of both object center-voting and sparse feature backbones. Although 3D instance segmentation has made impressive progress, current approaches have several major problems: typical state-of-the-art models are based on manually-tuned components, such as voting mechanisms that predict handselected geometric properties (e.g., centers <ref type="bibr" target="#b25">[26]</ref>, bounding boxes <ref type="bibr" target="#b6">[7]</ref>, occupancy <ref type="bibr" target="#b17">[18]</ref>), and heuristics for clustering the votes (e.g., dual-set grouping <ref type="bibr" target="#b25">[26]</ref>, proposal aggregation <ref type="bibr" target="#b12">[13]</ref>, set aggregation/filtering <ref type="bibr" target="#b3">[4]</ref>). Another limitation of these models is that they are not designed to directly predict instance masks. Instead, masks are obtained by grouping votes, and the model is trained using proxy-losses on the votes. A more elegant alternative consists of directly predicting and supervising instance masks, such as 3D-BoNet <ref type="bibr" target="#b60">[61]</ref> or DyCo3D <ref type="bibr" target="#b20">[21]</ref>. Recently, this idea gained popularity in 2D object detection (DETR <ref type="bibr" target="#b1">[2]</ref>) and image segmentation (Mask2Former <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>) but so far received less attention in 3D <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b60">61]</ref>. At the same time, in 2D image processing, we observe a strong shift from ubiquitous CNN architectures <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> towards Transformer-based models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">33]</ref>. In 3D, the move towards Transformers is less pronounced with only a few methods focusing on 3D object detection <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref> or 3D semantic segmentation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b62">63]</ref> and no methods for 3D instance segmentation. Overall, these approaches are still behind in terms of performance compared to current state-of-the-art methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>.</p><p>In this work, we propose the first Transformer-based model for 3D semantic instance segmentation of large-scale scenes that sets new state-of-the-art scores over a wide range of datasets, and addresses the aforementioned problems on hand-crafted model designs. The main challenge lies in directly predicting instance masks and their corresponding semantic labels. To this end, our model predicts instance queries that encode semantic and geometric information of each instance in the scene. Each instance query is then further decoded into a semantic class and an instance feature.</p><p>The key idea (to directly generate masks) is to compute similarity scores between individual instance features and all point features in the point cloud <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21]</ref>. This results in a heatmap over the point cloud, which (after normalization and thresholding) yields the final binary instance mask (c.f. <ref type="figure">Fig. 1</ref>). Our model, called Mask3D, builds on recent advances in both Transformers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37]</ref> and 3D deep learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b56">57]</ref>: to compute strong point features, we leverage a sparse convolutional feature backbone <ref type="bibr" target="#b7">[8]</ref> that efficiently processes full scenes and naturally provides multi-scale point features. To generate instance queries, we rely on stacked Transformer decoders <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> that iteratively attend to learned point features in a coarse-to-fine fashion using non-parametric queries <ref type="bibr" target="#b36">[37]</ref>. Unlike voting-based methods, directly predicting and supervising masks causes some challenges during training: before computing a mask loss, we first have to establish correspondences between predicted and annotated masks. A na?ve solution would be to choose for each predicted mask the nearest ground truth mask <ref type="bibr" target="#b20">[21]</ref>. However, this does not guarantee an optimal matching and any unmatched annotated mask would not contribute to the loss. Instead, we perform bipartite graph matching to obtain optimal associations between ground truth and predicted masks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b60">61]</ref>. We evaluate our model on four challenging 3D instance segmentation datasets, ScanNet v2 <ref type="bibr" target="#b8">[9]</ref>, ScanNet200 <ref type="bibr" target="#b46">[47]</ref>, S3DIS <ref type="bibr" target="#b0">[1]</ref> and STPLS3D <ref type="bibr" target="#b2">[3]</ref> and significantly outperform prior art, even surpassing architectures that are highly tuned towards specific datasets. Our experimental study compares various query types, different mask losses, and evaluates the number of queries as well as Transformer decoder steps.</p><p>Our contributions are as follows: (1) We propose the first competitive Transformer-based model for 3D semantic instance segmentation.</p><p>(2) Our model named Mask3D builds on domain-agnostic components, avoiding center voting, nonmaximum suppression, or grouping heuristics, and overall requires less hand-tuning. (3) Mask3D achieves state-of-the-art performance on ScanNet, ScanNet200, S3DIS and STPLS3D. To reach that level of performance with a Transformer-based approach, it is key to predict instance queries that encode the semantics and geometry of the scene and objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>3D Instance Segmentation. Numerous methods have been proposed for 3D instance semantic segmentation, including bottom-up approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref>, top-down approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b60">61]</ref>, and more recently, voting-based approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b55">56]</ref>. MASC <ref type="bibr" target="#b31">[32]</ref> uses a multi-scale hierarchical feature backbone, similar to ours, however, the multi-scale features are used to compute pairwise affinities followed by an offline clustering step. Such backbones are also successfully employed in other fields <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b47">48]</ref>. Another influential work is DyCo3D <ref type="bibr" target="#b20">[21]</ref>, which is among the few approaches that directly predict instance masks without a subsequent clustering step. DyCo3D relies on dynamic convolutions <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b53">54]</ref> which is similar in spirit to our mask prediction mechanism. However, it does not use optimal supervision assignment during training, resulting in subpar performance. Optimal assignment of the supervision signal was first implemented by 3D-BoNet <ref type="bibr" target="#b60">[61]</ref> using Hungarian matching. Similar to ours, <ref type="bibr" target="#b60">[61]</ref> directly predicts all instances in parallel. However, it uses only a single-scale scene descriptor which cannot encode object masks of diverse sizes. Transformers. Initially proposed by Vaswani et al. <ref type="bibr" target="#b54">[55]</ref> for NLP, Transformers have recently revolutionized the field of computer vision with successful models such as ViT <ref type="bibr" target="#b10">[11]</ref> for image classification, DETR <ref type="bibr" target="#b1">[2]</ref> for 2D object detection, or Mask2Former <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> for 2D segmentation tasks. The success of Transformers has been less prominent in the 3D point cloud domain though and recent Transformer-based methods focus on either 3D object detection <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref> or 3D semantic segmentation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b62">63]</ref>. Most of these rely on specific attention modifications to deal with the quadratic complexity of the attention <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b62">63]</ref>. Liu et al. <ref type="bibr" target="#b33">[34]</ref> use vanilla Transformer decoder, but only to refine object proposals, whereas Misra et al. <ref type="bibr" target="#b36">[37]</ref> are the first to show how to apply a vanilla Transformer to point clouds, still relying on an initial learned downsampling stage though. DyCo3D <ref type="bibr" target="#b20">[21]</ref> also uses a Transformer, however at the bottleneck of the feature backbone to increase the receptive field size and is not related to our mechanism for 3D instance segmentation. In this work, we show how a vanilla Transformer decoder can be applied to the task of 3D semantic instance segmentation and achieve state-of-the-art performance.</p><p>III. METHOD <ref type="figure" target="#fig_0">Fig. 2</ref> illustrates our end-to-end 3D instance segmentation model Mask3D. As in Mask2Former <ref type="bibr" target="#b4">[5]</ref>, our model includes a feature backbone (? ?), a Transformer decoder (? ?) built from mask modules (? ?) and Transformer decoder layers used for query refinement (? ?). At the core of the model are instance queries, which each should represent one object instance in the scene and predict the corresponding point-level instance mask. To that end, the instance queries are iteratively refined by the Transformer decoder ( <ref type="figure" target="#fig_0">Fig. 2</ref>, ? ?) which allows the instance queries to cross-attend to point features extracted from the feature backbone and self-attend the other instance queries. This process is repeated for multiple iterations and feature scales, yielding the final set of refined instance queries. A mask module consumes the refined instance queries together with the point features, and returns (for each query) a semantic class and a binary instance mask based on the dot product between point features and instance queries. Next, we describe each of these components in more detail. Sparse Feature Backbone. <ref type="figure" target="#fig_0">(Fig. 2</ref>, ? ?) We use a sparse convolutional U-net backbone with a symmetrical encoder and decoder, based on the MinkowskiEngine <ref type="bibr" target="#b7">[8]</ref>. Given a colored input point cloud P ? R N ?6 of size N , it is first quantized into M 0 voxels V ? R M0?3 , where each voxel is assigned the average RGB color of the points within that voxel as its initial feature. Next to the full-resolution output feature map F 0 ? R M0?D , we also extract a multi-resolution hierarchy of features from the backbone decoder before upsampling to the next finer feature map. At each of these resolutions r ? 0 we can extract features for a set of M r voxels, which we linearly project to a fixed and common dimension D, yielding feature matrices F r ? R Mr?D . We let the queries attend to features from coarser feature maps of the backbone decoder, i.e. r ? 1, and use the full-resolution feature map (r = 0) to compute the auxiliary and final per-voxel instance masks. Mask Module. <ref type="figure" target="#fig_0">(Fig. 2</ref>, ? ?) Given the set of K instance queries X ? R K?D , we predict a binary mask for each instance and classify each of them as one of C classes or as being inactive. To create the binary mask, we map the instance queries through an MLP f mask (?), to the same feature space as the backbone output features. We then compute the dot product between these instance features and the backbone features F 0 . The resulting similarity scores are fed through a sigmoid and thresholded at 0.5, yielding the final binary mask B ? {0, 1} M ?K :</p><formula xml:id="formula_0">B = {b i,j = [?(F 0 f mask (X) T ) i,j &gt; 0.5]}.<label>(1)</label></formula><p>We apply the mask module to the refined queries X at each Transformer layer using the full-resolution feature map F 0 , to create auxiliary binary masks for the masked cross-attention of the following refinement step. When this mask is used as input for the masked cross-attention, we reduce the resolution according to the voxel feature resolution by average pooling.</p><p>Next to the binary mask, we predict a single semantic class per instance. This step is done via a linear projection layer into C + 1 dimensions, followed by a softmax. While prior work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b55">56]</ref> typically needs to obtain the semantic label of an instance via majority voting or grouping over per-point predicted semantics, this information is directly contained in the refined instance queries. Query Refinement. <ref type="figure" target="#fig_0">(Fig. 2</ref>, ? ?) The Transformer decoder starts with K instance queries, and refines them through a stack of L Transformer decoder layers to a final set of accurate, scene specific instance queries by cross-attending to scene features, and reasoning at the instance-level through self-attention. We discuss different types of instance queries in Sec. III-A. Each layer attends to one of the feature maps from the feature backbone using standard cross-attention:</p><formula xml:id="formula_1">X = softmax(QK T ? D)V.<label>(2)</label></formula><p>To do so, the voxel features F r ? R Mr?D are first linearly projected to a set of keys and values of fixed dimensionality K, V ? R Mr?D and our K instance queries X are linearly projected to the queries Q ? R K?D . This cross-attention thus allows the queries to extract information from the voxel features. The cross-attention is followed by a self-attention step between the queries, where the keys, values, and queries are all computed based on linear projections of the instance queries. Without such inter-query communications, the model could not avoid multiple instance queries latching onto the same object, resulting in duplicate instance masks. Similar to most Transformer-based approaches, we use positional encodings for our keys and queries. We use Fourier positional encodings <ref type="bibr" target="#b51">[52]</ref> based on voxel positions. We add the resulting positional encodings to their respective keys before computing the cross-attention. All instance queries are also assigned a fixed (and potentially learned) positional embedding, that is not updated throughout the query refinement process. These positional encodings are added to the respective queries in the cross-attention, as well as to both the keys and queries in the self-attention. Instead of using the vanilla cross-attention (where each query attends to all voxel features in one resolution) we use a masked variant where each instance query only attends to the voxels within its corresponding intermediate instance mask B predicted by the previous layer. This is realized by adding ?? to the attention matrix to all voxels for which the mask is 0. Eq. 2 then becomes:</p><formula xml:id="formula_2">X = softmax(QK T ? D + B ? )V with B ? ij = ?? ? [B ij = 0] (3) where [ ? ]</formula><p>are Iverson brackets. In <ref type="bibr" target="#b4">[5]</ref>, masking out the context from the cross-attention improved segmentation. A likely reason is that the Transformer does not need to learn to focus on a specific instance instead of irrelevant context, but is forced to do so by design.</p><p>In practice, we attend to the 4 coarsest levels of the feature backbone, from coarse to fine, and do this a total of 3 times, resulting in L = 12 query refinement steps. The Transformer decoder layers share weights for all 3 iterations. Early experiments showed that this approach preserves the performance while keeping memory requirements in bound. Sampled Cross-Attention. Point clouds in a training batch typically have different point counts. While MinkowskiEngine can handle this, current Transformer implementations rely on a fixed number of points in each batch entry. In order to leverage well-tested Transformer implementations, in this work we propose to pad the voxel features and mask out the attention where needed. In case the number of voxels exceeds a certain threshold, we resort to sampling voxel features. To allow instances to have access to all voxel features during cross-attention, we resample the voxels in each Transformer decoder layer though, and use all voxels during inference. This can be seen as a form of dropout <ref type="bibr" target="#b49">[50]</ref>. In practice, this procedure saves significant amounts of memory and is crucial for obtaining competitive performance. In particular, since the proposed sampled cross-attention requires less memory, it enables training on higher-resolution voxel grids which is necessary for achieving competitive results on common benchmarks (e.g., 2 cm voxel side-length on ScanNet <ref type="bibr" target="#b8">[9]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training and Implementation Details</head><p>Correspondences. Given that there is no ordering to the set of instances in a scene and the set of predicted instances, we need to establish correspondences between the two sets during training. To that end, we use bipartite graph matching. While such a supervision approach is not new (e.g. <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b60">61]</ref>), recently it has become more common in Transformer-based approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. We construct a cost matrix C ? R K?K , whereK is the number of ground truth instances in a scene. The matching cost for a predicted instance k and a target instancek is given by:</p><formula xml:id="formula_3">C(k,k) = ? dice L dice (k,k) + ? BCE L BCE mask (k,k) + ? cl L CE cl (k,k)<label>(4)</label></formula><p>We set the weights to ? dice = ? cl = 2.0 and ? BCE = 5.0. The optimal solution for this cost assignment problem is efficiently found using the Hungarian method <ref type="bibr" target="#b26">[27]</ref>. After establishing the correspondences, we can directly optimize each predicted mask as follows:</p><formula xml:id="formula_4">L mask = ? BCE L BCE + ? dice L dice ,<label>(5)</label></formula><p>where L BCE is the binary cross-entropy loss (over the foreground and background of that mask) and L dice is the Dice loss <ref type="bibr" target="#b9">[10]</ref>. We use the default multi-class cross-entropy loss L CE cl to supervise the classification. If a mask is left unassigned, we seek to maximize the associated no-object class, for which the L CE cl loss is weighted by an additional ? no-obj. = 0.1. The overall loss for all auxiliary instance predictions after each of the L layers is defined as:</p><formula xml:id="formula_5">L = ? L l L l mask + ? cl L l CE cl<label>(6)</label></formula><p>Prediction Confidence Score. We seek to assign a confidence to each predicted instance. While other existing methods require a dedicated ScoreNet <ref type="bibr" target="#b25">[26]</ref> which is trained to estimate the intersection over union with the ground truth instances, we directly obtain the confidence scores from the refined query features and point features as in Mask2Former <ref type="bibr" target="#b4">[5]</ref>. We first select the queries with a dominant semantic class, for which we obtain the class confidence based on the softmax output c cl ? [0, 1], which we additionally multiply with a mask based confidence:</p><formula xml:id="formula_6">c = c cl ? (? M i m i ? [m i &gt; 0.5]) (? M i [m i &gt; 0.5]),<label>(7)</label></formula><p>where m i ? [0, 1] is the instance mask confidence for the i th voxel given a single query. In essence, this is the mean mask confidence of all voxels falling inside of the binarized mask <ref type="bibr" target="#b4">[5]</ref>. For an instance prediction to have a high confidence, it needs both a confident classification among C classes, and a mask that predominantly consists of high-confidence voxels. Query Types. Methods like DETR <ref type="bibr" target="#b1">[2]</ref> or Mask2Former <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> use parametric queries. During training both the instance query features and the corresponding positional encodings are learned. This thus means that during training the set of K instance queries has to be optimized in such a way that it can cover all instances present in a scene during inference. Misra et al. <ref type="bibr" target="#b36">[37]</ref> propose to initialize queries with sampled point coordinates from the input point cloud based on farthest point sampling. Since this initialization does not involve learned parameters, they are called non-parametric queries. Interestingly, the instance query features are initialized with zeros and only the 3D position of the sampled points is used to set the corresponding positional encoding. We also experiment with a variant where we use sampled point features as instance query features. Similar to <ref type="bibr" target="#b36">[37]</ref>, we observe improved performance when using non-parametric queries although less pronounced. The key advantage of non-parametric queries is that, during inference, we can sample a different number of queries than during training. This provides a trade-off between inference speed and performance, without the need to retrain the model when using more instance queries. Training Details. The feature backbone is a Minkowski Res16UNet34C <ref type="bibr" target="#b7">[8]</ref>. We train for 600 epochs using AdamW <ref type="bibr" target="#b34">[35]</ref> and a one-cycle learning rate schedule <ref type="bibr" target="#b48">[49]</ref> with a maximal learning rate of 10 ?4 . Longer training times (1000 epochs) did not further improve results. One training on 2 cm voxelization takes ?78 hours on an NVIDIA A40 GPU. We perform standard data augmentation: horizontal flipping, random rotations around the z-axis, elastic distortion <ref type="bibr" target="#b45">[46]</ref> and random scaling. Color augmentations include jittering, brightness and contrast augmentations. During training on ScanNet, we reduce memory consumption by computing the dot product between instance queries and aggregated point features within segments (obtained from a graph-based segmentation <ref type="bibr" target="#b14">[15]</ref>, similar to OccuSeg <ref type="bibr" target="#b17">[18]</ref> or Mix3D <ref type="bibr" target="#b37">[38]</ref>). Wrongly merged instances can be separated using connected components <ref type="bibr" target="#b13">[14]</ref> (Sec. IV-C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we compare Mask3D with prior state-of-theart on four publicly available 3D indoor and outdoor datasets (Sec. IV-A). Then, we provide analysis experiments on the proposed model investigating query types and the impact of the number of query refinement steps as well as the number of queries during inference. (Sec. IV-B). Finally, we show qualitative results and discuss limitations (Sec. IV-C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparing with State-of-the-Art Methods</head><p>Datasets and Metrics. We evaluate Mask3D on four publicly available 3D instance segmentation datasets. ScanNet <ref type="bibr" target="#b8">[9]</ref> is a richly-annotated dataset of 3D reconstructed indoor scenes. It contains hundreds of different rooms showing a large variety of room types such as hotels, libraries and offices. The provided splits contain 1202 training, 312 validation and 100 hidden test scenes. Each scene is annotated with semantic and instance segmentation labels covering 18 object categories. The benchmark evaluation metric is mean average precision (mAP). ScanNet200 <ref type="bibr" target="#b46">[47]</ref> extends the original ScanNet scenes with an order of magnitude more classes. ScanNet200 allows to test an algorithm's performance under the natural imbalance of classes, particularly for challenging long-tail classes such as coffee-kettle and pottedplant. We keep the same train, validation and test splits as in the original ScanNet dataset. S3DIS <ref type="bibr" target="#b0">[1]</ref> is a large-scale indoor dataset showing six different areas from three different campus buildings. It contains 272 scans and is also annotated with semantic instance masks over 13 different classes. We follow the common splits and evaluate on Area-5 and 6-fold cross validation. We report scores using the mAP metric from ScanNet and mean precision/recall at IoU threshold 50% (mPrec 50 /mRec 50 ) as initially introduced by ASIS <ref type="bibr" target="#b58">[59]</ref>. Unlike mAP, this metric does not consider confidence scores, therefore we filter out instance masks with a prediction confidence score below 80% to avoid excessive false positives. STPLS3D <ref type="bibr" target="#b2">[3]</ref> is a synthetic outdoor dataset closely mimicking  <ref type="bibr" target="#b0">[1]</ref>. We report mean average precision (mAP) with different IoU threshold (as in <ref type="bibr" target="#b8">[9]</ref>) as well as mean precision (mPrec) and mean recall (mRec) with 50% IoU threshold (as in <ref type="bibr" target="#b58">[59]</ref>) over 13 classes on S3DIS Area 5 and 6-fold cross validation. Scores in light gray are pre-trained on ScanNet <ref type="bibr" target="#b8">[9]</ref> and fine-tuned on S3DIS <ref type="bibr" target="#b0">[1]</ref>.  the data generation process of aerial photogrammetry point clouds. 25 urban scenes totalling 6 km 2 are densely annotated with 14 instance classes. We follow the common splits <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b55">56]</ref>. Results are summarized in Tab. I (ScanNet), Tab. II (S3DIS), Tab. III (left, ScanNet200) and Tab. III (right, STPLS3D). Mask3D outperforms prior work by a large margin on the most challenging metric mAP by at least 6.2 mAP on ScanNet, 6.2 mAP on S3DIS, 10.8 mAP on ScanNet200 and 11.2 mAP on STPLS3D. As in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b55">56]</ref>, we also report scores for models pre-trained on ScanNet and fine-tuned on S3DIS. For Mask3D, pre-training improves performance by 1.2 mAP on Area 5. Mask3D's strong performance on indoor and outdoor datasets as well as its ability to work under challenging class imbalance settings without inherent modifications to the architecture or the training regime highlights its generality. Trained models are available at: https://github.com/JonasSchult/Mask3D</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3DIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis Experiments</head><p>Query Types. Mask3D iteratively refines instance queries by attending to voxel features <ref type="figure" target="#fig_0">(Fig. 2, ?  ?)</ref>. We distinguish two types of query initialization prior to attending to voxel features: 1 parametric and 2 -3 non-parametric initial IV: Ablations. a) We explore two variants for query positions and features. Parametric queries 1 are learned during training. Non-parametric queries consist of FPS point positions 2 and potentially their features 3 , resembling scene-specific queries. b) We optimize the instance mask prediction using the binary crossentropy loss L CE and the dice loss L dice . A weighted combination of dice and cross-entropy loss results in best performance.  queries. Parametric refers to learned positions and features <ref type="bibr" target="#b1">[2]</ref>, while non-parametric refers to point positions sampled with furthest point sampling (FPS) <ref type="bibr" target="#b43">[44]</ref>. When selecting query positions with FPS, we can either initialize the queries to zero ( 2 , as in 3DETR <ref type="bibr" target="#b36">[37]</ref>) or use the point features at the sampled position 3 . Tab. IV (left) shows the effects of using parametric or non-parametric queries on ScanNet validation (5 cm). In line with <ref type="bibr" target="#b36">[37]</ref>, we see that non-parametric queries 2 outperform parametric queries 1 . Interestingly, <ref type="bibr" target="#b2">3</ref> results in degraded performance compared to both parametric 1 and position-only non-parametric queries 2 .</p><p>Number of Queries and Decoders. We analyze the effect of varying numbers of queries K during inference on models trained with K = 100 and K = 200 non-parametric queries sampled with FPS. By increasing K from 100 to 200 during training, we observe a slight increase in performance ( <ref type="figure" target="#fig_1">Fig. 3</ref>, left) at the cost of additional memory. When evaluating with fewer queries than trained with, we observe reduced performance but faster runtime. When evaluating with more queries than trained with, we observe slightly improved performance, typically less than 1 mAP. Our final model uses K = 100 due to memory constraint when using 2 cm voxels in the feature backbone. In this study, we report scores using 5 cm on ScanNet validation. We also analyse the mask quality that we obtain after each Transformer decoder layer in our trained model <ref type="figure" target="#fig_1">(Fig. 3, right)</ref>. We see a rapid increase up to 4 layers, then the quality increases a bit slower. Mask Loss. The mask module <ref type="figure" target="#fig_0">(Fig. 2</ref>, ? ?) generates instance heatmaps for every instance query. After Hungarian matching, the corresponding ground truth mask is used to compute the mask loss L mask . The binary cross entropy loss L BCE is the obvious choice for binary segmentation tasks. However, it does not perform well under large class imbalance (few foreground mask points, many background points). The Dice loss L dice is specifically designed to address such data imbalance. Tab. IV (right) shows scores on ScanNet validation for combinations of both losses. While L dice improves over L BCE , we observe an additional improvement by training our model with a weighted sum of both losses (Eq. 5).  <ref type="figure" target="#fig_2">Fig. 4</ref>, bottom left). As the attention mechanism can attend to the full point cloud, it can happen that two objects with similar semantics and geometry expose similar learned point features and are therefore combined into one instance even if they are far apart in the scene. This is less likely to happen with methods that explicitly encode geometric priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Results and Limitations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we have introduced Mask3D, for 3D semantic instance segmentation. Mask3D is based on Transformer decoders, and learns instance queries that, combined with learned point features, directly predict semantic instance masks without the need for hand-selected voting schemes or hand-crafted grouping mechanisms. We think that Mask3D is an attractive alternative to current voting-based approaches and expect to see follow-up work along this line of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask3D for 3D Semantic Instance Segmentation</head><p>Supplementary Material <ref type="figure">Fig. 5</ref>: Illustration of the full Mask3D model. In the main paper, we showed a simplified version of our model with fewer hierarchical feature levels in the feature backbone (shown in green) and fewer query refinement layers (blue). The feature backbone outputs point features in 5 scales, while the Transformer decoder iteratively refines the instance queries. Given point features and instance queries, the mask module predicts for each query a semantic class and an instance heatmap, which (after thresholding) results in a binary instance mask. We therefore resort to training on 6m?6m blocks randomly cropped from the ground plane to keep the memory requirements in bounds. As Mask3D thus effectively sees less data in each epoch, we train for 1000 epochs. However, during test, we disable cropping and infer full scenes.</p><p>STPLS3D Specific Details. As STPLS3D's evaluation protocol <ref type="bibr" target="#b2">[3]</ref> evaluates on 50m?50m blocks evenly cropped from the full city scene, instances are potentially separated into multiple blocks. We therefore feed slightly larger 54m?54m blocks in our model but only keep the relevant predicted instances of the 50m?50m block. This approach achieves significantly better results, usually roughly 1.2 mAP.</p><p>Model Details. <ref type="figure">Figure 5</ref> shows our full model. Unlike the figure in the main paper, this shows the complete model, including all backbone feature levels and all query refinement steps in the Transformer decoder. We deploy a Minkowski Res16UNet34C <ref type="bibr" target="#b7">[8]</ref> and obtain feature maps F i from all of its 5 scales.  <ref type="bibr" target="#b3">[4]</ref> and SoftGroup <ref type="bibr" target="#b55">[56]</ref> obtained from the official code releases. The most parameters, by far (&gt;90%), are due to the feature-learning backbones <ref type="figure">(Fig. 5, ?  ?)</ref>. In comparison, the remaining number of parameters (including the transformer-decoder) is very  small (&lt;10%). In absolute numbers, the proposed transformerdecoder is larger than the other parts of the baseline methods but still small compared to the size of the feature backbones.</p><p>To verify that the improved performance of Mask3D does not originate from more model parameters, we ran an additional experiment with a smaller feature backbone (Res16UNet18B). The smaller feature backbone results in comparable segmentation performance (40.9 vs. 40.0 mAP) evaluated on ScanNet validation 5 cm. Additional feature backbones are analyzed in Tab. V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison to SoftGroup</head><p>In the following, we qualitatively compare Mask3D with SoftGroup <ref type="bibr" target="#b55">[56]</ref>, the currently best performing voting-based 3D instance segmentation approach. We highlight two error cases for SoftGroup and show our Mask3D for comparison in <ref type="figure">Fig. 6</ref>. Density-Based Clustering. In Section 4.3 (main paper), we described one limitation of Mask3D. A few times, we observed that similarly looking objects are merged into a single instance even if they are apart in the input point cloud (c.f. <ref type="figure">Fig. 7(b)-(c)</ref>). We trace this back to Mask3D's possibility to attend to the full point cloud combined with instances which show similar semantics and geometry. As a solution, we propose to apply DBSCAN <ref type="bibr" target="#b13">[14]</ref> on the output instance masks produced by Mask3D. For each of the K instance masks individually, DBSCAN yields spatially contiguous clusters (c.f. <ref type="figure">Fig. 7(d)</ref>). We treat these dense clusters as new instance masks. We update the confidence score for each newly created instance by applying Equation (7, main paper). In our hyperparameter ablation study in Tab. VII, we achieved overall best results when applying DBSCAN with a minimal distance parameter of 0.9 for ScanNet, 0.6 for S3DIS and 14.0 for STPLS3D. Note that we do not consider noise points, i.e., we set the minimal size of a cluster to 1. <ref type="figure">Fig. 6</ref>: Qualitative Comparison to SoftGroup <ref type="bibr" target="#b55">[56]</ref>. We compare Mask3D with the current top-performing voting-based approach SoftGroup. The top example shows a scene containing a single large U-shaped table, see (e) in pink. SoftGroup is based on center-voting and tries to predict the instance center, shown in (b) in red. However, predicting centers of such very large non-convex shapes can be difficult for voting-based approaches. Indeed, SoftGroup fails to correctly segment the table and returns two partial instances (c). Our Mask3D, on the other side, does not rely on hand-selected geometric properties such as centers and can handle arbitrarily shaped and sized objects. It correctly predicts the tables instance mask (e). In the bottom example, we see that SoftGroup has difficulties to predict precise centers for multiple chairs located next to each other (b). As a result, the manually tuned grouping mechanism aggregates them all into one big instance which is later discarded by the refinement step. It therefore misses to segment all eight chairs (c). Mask3D does not rely on hand-crafted grouping mechanisms and can successfully segment most of the chairs. . In (d), we apply DBSCAN as a postprocessing routine to split erroneously merged instances based on spatial contiguity. We do not see this effect for voting-based methods as they explicitly encode geometric priors (e)-(f).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration of the Mask3D model. The feature backbone outputs multi-scale point features F, while the Transformer decoder iteratively refines the instance queries X. Given point features and instance queries, the mask module predicts for each query a semantic class and an instance heatmap, which (after thresholding) results in a binary instance mask B. ? applies a threshold of 0.5 and spatially rescales if required. ? is the dot product. ? is the sigmoid function. We show a simplified model with fewer layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Number of queries and decoder layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Qualitative Results on ScanNet. We show pairs of predicted instance masks and predicted semantic labels. On the bottom left, we show the heatmap of a failure case of two windows that are wrongly assigned to a single instance. The corresponding point features are visualized as RGB after projecting them to 3D using PCA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4</head><label>4</label><figDesc>shows several representative examples of Mask3D instance segmentation results on ScanNet. The scenes are quite diverse and present a number of challenges, including clutter, scanning artifacts and numerous similar objects. Still, our model shows quite robust results. There are still limitations in our model though. A systematic mistake that we observed are merged instances that are far apart (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( a )Fig. 7 :</head><label>a7</label><figDesc>RGB Point Cloud (b) Mask3D (ours) w/o DBSCAN (c) Instance Heatmap (Windows) (d) Mask3D (ours) with DBSCAN (e) Center Votes (SoftGroup [56]) (f) Prediction (SoftGroup [56]) Qualitative Analysis of DBSCAN Postprocessing. Mask3D occassionally predicts masks containing two instances of the same class. In (b), two windows are merged into a single instance since their underlying point cloud features result in a high response when convolved with the instance query (c.f. heatmap in (c))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>3D Instance Segmentation Scores on ScanNet v2<ref type="bibr" target="#b8">[9]</ref>. We report mean average precision (mAP) with different IoU threshold over 18 classes on the ScanNet validation and test set. The inference speed is averaged over the validation set and computed on a TITAN X GPU (c.f.<ref type="bibr" target="#b55">[56]</ref>), excluding postprocessing. Test scores accessed on 13. September 2022.</figDesc><table><row><cell></cell><cell cols="2">ScanNet Val</cell><cell cols="2">ScanNet Test</cell><cell>Runtime</cell></row><row><cell>Method</cell><cell cols="2">mAP mAP50</cell><cell cols="2">mAP mAP50</cell><cell>(in ms)</cell></row><row><cell>SGPN [58]</cell><cell>-</cell><cell>-</cell><cell>4.9</cell><cell>14.3</cell><cell>158439</cell></row><row><cell>GSPN [62]</cell><cell>19.3</cell><cell>37.8</cell><cell>-</cell><cell>30.6</cell><cell>12702</cell></row><row><cell>3D-SIS [22]</cell><cell>-</cell><cell>18.7</cell><cell>16.1</cell><cell>38.2</cell><cell>-</cell></row><row><cell>MASC [32]</cell><cell>-</cell><cell>-</cell><cell>25.4</cell><cell>44.7</cell><cell>-</cell></row><row><cell>3D-Bonet [61]</cell><cell>-</cell><cell>-</cell><cell>25.3</cell><cell>48.8</cell><cell>9202</cell></row><row><cell>MTML [28]</cell><cell>20.3</cell><cell>40.2</cell><cell>28.2</cell><cell>54.9</cell><cell>-</cell></row><row><cell>3D-MPA [13]</cell><cell>35.5</cell><cell>59.1</cell><cell>35.5</cell><cell>61.1</cell><cell>-</cell></row><row><cell>DyCo3D [21]</cell><cell>35.4</cell><cell>57.6</cell><cell>39.5</cell><cell>64.1</cell><cell>-</cell></row><row><cell cols="2">PointGroup [26] 34.8</cell><cell>56.7</cell><cell>40.7</cell><cell>63.6</cell><cell>452</cell></row><row><cell cols="2">MaskGroup [64] 42.0</cell><cell>63.3</cell><cell>43.4</cell><cell>66.4</cell><cell>-</cell></row><row><cell>OccuSeg [18]</cell><cell>44.2</cell><cell>60.7</cell><cell>48.6</cell><cell>67.2</cell><cell>1904</cell></row><row><cell>SSTNet [31]</cell><cell>49.4</cell><cell>64.3</cell><cell>50.6</cell><cell>69.8</cell><cell>428</cell></row><row><cell>HAIS [4]</cell><cell>43.5</cell><cell>64.1</cell><cell>45.7</cell><cell>69.9</cell><cell>339</cell></row><row><cell>SoftGroup [56]</cell><cell>46.0</cell><cell>67.6</cell><cell>50.4</cell><cell>76.1</cell><cell>345</cell></row><row><cell>Mask3D (Ours)</cell><cell>55.2</cell><cell>73.7</cell><cell>56.6</cell><cell>78.0</cell><cell>339</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>3D Instance Segmentation Scores on S3DIS</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Mask3D (Ours) 57.8 71.9 74.3 63.7 61.8 74.3 76.5 66.2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Area 5</cell><cell></cell><cell></cell><cell cols="3">S3DIS 6-fold CV</cell></row><row><cell>Method</cell><cell cols="8">AP AP50 Prec50 Rec50 AP AP50 Prec50 Rec50</cell></row><row><cell>SGPN [58]</cell><cell>-</cell><cell>-</cell><cell cols="3">36.0 28.7 -</cell><cell>-</cell><cell cols="2">38.2 31.2</cell></row><row><cell>ASIS [59]</cell><cell>-</cell><cell>-</cell><cell cols="3">55.3 42.4 -</cell><cell>-</cell><cell cols="2">63.6 47.5</cell></row><row><cell>3D-Bonet [61]</cell><cell>-</cell><cell>-</cell><cell cols="3">57.5 40.2 -</cell><cell>-</cell><cell cols="2">65.6 47.6</cell></row><row><cell>OccuSeg [18]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">72.8 60.3</cell></row><row><cell>3D-MPA [13]</cell><cell>-</cell><cell>-</cell><cell cols="3">63.1 58.0 -</cell><cell>-</cell><cell cols="2">66.7 64.1</cell></row><row><cell cols="9">PointGroup [26] -57.8 61.9 62.1 -64.0 69.6 69.2</cell></row><row><cell>DyCo3D [21]</cell><cell>-</cell><cell>-</cell><cell cols="3">64.3 64.2 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="9">MaskGroup [64] -65.0 62.9 64.7 -69.9 66.6 69.6</cell></row><row><cell>SSTNet [31]</cell><cell cols="8">42.7 59.3 65.5 64.2 54.1 67.8 73.5 73.4</cell></row><row><cell cols="9">Mask3D (Ours) 56.6 68.4 68.7 66.3 64.5 75.5 72.8 74.5</cell></row><row><cell>HAIS [4]</cell><cell>-</cell><cell>-</cell><cell cols="3">71.1 65.0 -</cell><cell>-</cell><cell cols="2">73.2 69.4</cell></row><row><cell cols="9">SoftGroup [56] 51.6 66.1 73.6 66.6 54.4 68.9 75.3 69.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>3D Instance Segmentation Scores on ScanNet200<ref type="bibr" target="#b46">[47]</ref> and STPLS3D<ref type="bibr" target="#b2">[3]</ref>. We report mean average precision (mAP) with different IoU threshold over 14 classes on the STPLS3D test set. Hidden test scores accessed on 13. September 2022.</figDesc><table><row><cell></cell><cell>ScanNet 200</cell><cell></cell><cell>STPLS3D</cell></row><row><cell>Method</cell><cell>head com tail</cell><cell>Method</cell><cell>mAP mAP50</cell></row><row><cell>CSC [23]</cell><cell>22.3 8.2 4.6</cell><cell cols="2">PointGroup [26] 23.3 38.5</cell></row><row><cell cols="2">Mink34D [8] 24.6 8.3 4.3</cell><cell>HAIS [4]</cell><cell>35.1 46.7</cell></row><row><cell cols="2">LGround [47] 27.5 10.8 6.0</cell><cell cols="2">SoftGroup [56] 46.2 61.8</cell></row><row><cell cols="2">Mask3D (Ours) 38.3 26.3 16.8</cell><cell cols="2">Mask3D (Ours) 57.3 74.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Feature Backbones. We experimented with convolutional and transformer-based feature backbones (c.f.Fig. 5, ? ?).</figDesc><table><row><cell>Backbone Name</cell><cell>Backbone Param.</cell><cell>mAP</cell><cell>mAP 50</cell></row><row><cell>StratifiedFormer [29]</cell><cell>18,798,662</cell><cell>31.1</cell><cell>54.6</cell></row><row><cell>Res16UNet18B [8]</cell><cell>17,204,660</cell><cell>40.0</cell><cell>63.7</cell></row><row><cell>Res16UNet34C [8]</cell><cell>37,856,052</cell><cell>40.9</cell><cell>64.4</cell></row><row><cell cols="3">I. IMPLEMENTATION DETAILS</cell><cell></cell></row><row><cell cols="4">S3DIS Specific Details. As S3DIS [1] contains a few very</cell></row><row><cell cols="4">large spaces, e.g. lecture halls, and also provides a very</cell></row><row><cell cols="4">high point density, scenes can exceed several millions of</cell></row><row><cell>points.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The feature maps have (96, 96, 128, 256, 256) channels (sorted from fine to coarse). As the Transformer decoder expects a feature dimension of 128, we apply a nonshared linear projection after each F i to map the features to the expected dimension. Furthermore, we employ a modified Transformer decoder by Mask2Former<ref type="bibr" target="#b4">[5]</ref> (swapped crossand self-attention) leveraging an 8-headed attention and a feedforward network with 1024-dimensional features. For each intermediate feature map F i with i &gt; 0, we instantiate a dedicated decoder layer. We attend to the backbone features 3 times with Transformer decoders with shared weights. In all our experiments, we use 100 instance queries. Following Misra et al.<ref type="bibr" target="#b36">[37]</ref>, the query positions are calculated from Fourier positional encodings based on relative voxel positions scaled to [?1, 1]. We do not use Dropout. Comparison Feature Backbones. As an additional candidate for non-convolution-based backbones, we deploy the recent StratifiedFormer<ref type="bibr" target="#b28">[29]</ref> which is a Transformer-based feature backbone. The resulting scores are reported in Tab. V. The experiment with the StratifiedFormer shows encouraging results but does not yet reach the performance of the sparse convolutional backbone. However, the experiment clearly shows that our model also runs on different types of feature backbones. We also report scores of another voxelbased feature backbone (Minkowski Res16UNet18B) that is significantly smaller than our original backbone (Minkowski Res16UNet34C) to show robustness towards model size on ScanNet validation. We find that the smaller feature backbone works comparably to the bigger Res16UNet34C. This shows that Mask3D does not overly rely on the specific voxel-based feature extractor.</figDesc><table /><note>Model sizes. Tab. VI shows the model size of Mask3D and two recent top-performing baselines HAIS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI :</head><label>VI</label><figDesc>Model sizes. We compare Mask3D's model size against recent top-performing methods. For all models, most parameters are in the feature backbone and only a small fraction is in the instance segmentation specific part of the models.</figDesc><table><row><cell>Model Name</cell><cell>All Params.</cell><cell>Backbone</cell><cell>Other</cell></row><row><cell>HAIS [4]</cell><cell>30.856M</cell><cell>30.118M</cell><cell>0.738M</cell></row><row><cell>SoftGroup [56]</cell><cell>30.858M</cell><cell>30.118M</cell><cell>0.740M</cell></row><row><cell>Mask3D (Ours)</cell><cell>39.617M</cell><cell>37.856M</cell><cell>1.761M</cell></row><row><cell>Mask3D (Ours -small)</cell><cell>18.958M</cell><cell>17.205M</cell><cell>1.753M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VII :</head><label>VII</label><figDesc>Ablation on DBSCAN postprocessing. To split wrongly merged instances, we employ DBSCAN as an optional postprocessing routine. We report best scores around a minimal distance =0.9 (ScanNet) and =0.6 (S3DIS-A5).</figDesc><table><row><cell></cell><cell cols="3">ScanNet Validation (2 cm)</cell><cell cols="3">S3DIS Area 5 (2 cm)</cell></row><row><cell></cell><cell>AP</cell><cell>AP 50</cell><cell>AP 25</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 25</cell></row><row><cell>-</cell><cell>54.3</cell><cell>73.0</cell><cell>83.4</cell><cell>55.7</cell><cell>69.8</cell><cell>76.1</cell></row><row><cell>0.5</cell><cell>54.1</cell><cell>72.1</cell><cell>82.1</cell><cell>57.6</cell><cell>71.7</cell><cell>77.2</cell></row><row><cell>0.6</cell><cell>54.4</cell><cell>72.4</cell><cell>82.4</cell><cell>57.8</cell><cell>71.9</cell><cell>77.2</cell></row><row><cell>0.7</cell><cell>54.9</cell><cell>73.2</cell><cell>83.1</cell><cell>57.7</cell><cell>71.8</cell><cell>77.2</cell></row><row><cell>0.8</cell><cell>55.0</cell><cell>73.3</cell><cell>83.2</cell><cell>57.5</cell><cell>71.6</cell><cell>77.1</cell></row><row><cell>0.9</cell><cell>55.1</cell><cell>73.7</cell><cell>83.6</cell><cell>57.6</cell><cell>71.6</cell><cell>77.1</cell></row><row><cell>1.0</cell><cell>55.0</cell><cell>73.5</cell><cell>83.5</cell><cell>57.5</cell><cell>71.5</cell><cell>77.2</cell></row><row><cell>1.1</cell><cell>55.0</cell><cell>73.6</cell><cell>83.6</cell><cell>57.5</cell><cell>71.4</cell><cell>77.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work is supported by the ERC Consolidator Grant DeeViSe (ERC-2017-CoG-773161), compute resources from RWTH Aachen University (rwth1238) and the ETH AI Center post-doctoral fellowship. We additionally thank Alexey Nekrasov, Ali Athar and Istv?n S?r?ndi for helpful discussions and feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3D Semantic Parsing of Large-Scale Indoor Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-End Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meida</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hugues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Mccullough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucio</forename><surname>Soibelman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09065</idno>
		<title level="m">STPLS3D: A Large-Scale Synthetic and Real Aerial Photogrammetry 3D Point Cloud Dataset</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical Aggregation for 3D Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Masked-attention Mask Transformer for Universal Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Per-Pixel Classification is Not All You Need for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Box2Mask: Weakly Supervised 3D Semantic Instance Segmentation Using Bounding Boxes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Chibane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><forename type="middle">Anh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to Predict Crisp Boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huibing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinru</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3D Bird&apos;s-eye-view Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cathrin</forename><surname>Elich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodora</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3D-MPA: Multi-Proposal Aggregation for 3D Semantic Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient Graph-Based Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01307</idno>
		<title level="m">Submanifold Sparse Convolutional Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">OccuSeg: Occupancyaware 3D Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DyCo3D: Robust Instance Segmentation of 3D Point Clouds through Dynamic Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Machine Analysis of Bubble Chamber Pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">C</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on High Energy Accelerators and Instrumentation</title>
		<imprint>
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc V</forename><surname>Gool</surname></persName>
		</author>
		<title level="m">Dynamic Filter Networks. Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3D Instance Segmentation via Multi-Task Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin R</forename><surname>Oswald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stratified Transformer for 3D Point Cloud Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust Object Detection with Interleaved Categorization and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ale?</forename><surname>Bastian Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Instance Segmentation in 3D Scenes using Semantic Superpoint Tree Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songcen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04478</idno>
		<title level="m">MASC: Multi-Scale Affinity with Sparse Convolution for 3D Instance Segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Group-Free 3D Object Detection via Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D Convolutional Neural Network for Real-time Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An End-to-End Transformer Model for 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mix3D: Out-of-Context Data Augmentation for 3D Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3D Object Detection With Pointformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuran</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuofan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Erran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast Point Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep Hough Voting for 3D Object Detection in Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pointnet: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Volumetric and Multi-View CNNs for Object Classification on 3D Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Language-Grounded Indoor 3D Semantic Segmentation in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rozenberszki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danila</forename><surname>Rukhovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Vorontsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Konushin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00322</idno>
		<title level="m">Fully Convolutional Anchor-Free 3D Object Detection</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates. In Artificial intelligence and machine learning for multi-domain operations applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholay</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Topin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratul</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nithin</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">KPConv: Flexible and Deformable Convolution for Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Conditional Convolutions for Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">SoftGroup for 3D Instance Segmentation on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kookhoi</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><forename type="middle">Thanh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Associatively Segmenting Instances and Semantics in Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A Deep Representation for Volumetric Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyuk</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Point Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.14662</idno>
		<title level="m">MaskGroup: Hierarchical Point Grouping and Masking for 3D Instance Segmentation</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
