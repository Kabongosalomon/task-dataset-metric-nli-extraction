<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Self-supervised Augmented Knowledge Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanguang</forename><surname>Yang</surname></persName>
							<email>yangchuanguang@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhulin</forename><surname>An</surname></persName>
							<email>anzhulin@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhang</forename><surname>Cai</surname></persName>
							<email>cailinhang19g@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Self-supervised Augmented Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge distillation often involves how to define and transfer knowledge from teacher to student effectively. Although recent self-supervised contrastive knowledge achieves the best performance, forcing the network to learn such knowledge may damage the representation learning of the original class recognition task. We therefore adopt an alternative self-supervised augmented task to guide the network to learn the joint distribution of the original recognition task and self-supervised auxiliary task. It is demonstrated as a richer knowledge to improve the representation power without losing the normal classification capability. Moreover, it is incomplete that previous methods only transfer the probabilistic knowledge between the final layers. We propose to append several auxiliary classifiers to hierarchical intermediate feature maps to generate diverse self-supervised knowledge and perform the one-to-one transfer to teach the student network thoroughly. Our method significantly surpasses the previous SOTA SSKD with an average improvement of 2.56% on CIFAR-100 and an improvement of 0.77% on ImageNet across widely used network pairs. Codes are available at https://github.com/winycg/HSAKD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Orthogonal to efficient network architecture designs <ref type="bibr" target="#b8">Zhu et al., 2019;</ref><ref type="bibr" target="#b6">Yang et al., 2020]</ref>, Knowledge Distillation (KD) <ref type="bibr" target="#b1">[Hinton et al., 2015]</ref> aims to transfer knowledge from a pre-trained high-capacity teacher network to a light-weight student network. The student's performance can often be improved significantly, benefiting from the additional guidance compared with the independent training. The current pattern of KD can be summarized as two critical aspects:</p><p>(1) what kind of knowledge encapsulated in teacher network can be explored for KD; (2) How to effectively transfer knowledge from teacher to student.</p><p>The original <ref type="bibr">KD [Hinton et al., 2015]</ref> minimizes the KLdivergence of predictive class probability distributions be- * Corresponding author (a) Self-supervised contrastive relationship .</p><p>(b) Our introduced self-supervised augmented distribution. <ref type="figure">Figure 1</ref>: Difference of self-supervised knowledge between SSKD and our method. (a) SSKD applies contrastive learning by forcing the image and its transformed version closed against other negative images in the feature embedding space. It defines the contrastive relationship as knowledge. (b) Our method unifies the original task and self-supervised auxiliary task into a joint task and defines the self-supervised augmented distribution as knowledge. tween student and teacher networks, which makes intuitive sense to force the student to mimic how a superior teacher generates the final predictions. However, such a highly abstract dark knowledge ignores much comprehensive information encoded in hidden layers. Later works naturally proposed to transfer feature maps <ref type="bibr" target="#b2">[Romero et al., 2015]</ref> or their refined information <ref type="bibr">[Zagoruyko and Komodakis, 2017;</ref><ref type="bibr" target="#b1">Heo et al., 2019;</ref><ref type="bibr" target="#b0">Ahn et al., 2019]</ref> between intermediate layers of teacher and student. A reasonable interpretation of the success of feature-based distillation lies in that hierarchical feature maps throughout the CNN represent the intermediate learning process with an inductive bias of the final solution. Beyond knowledge alignment limited in the individual sample, more recent works <ref type="bibr" target="#b2">[Peng et al., 2019;</ref><ref type="bibr"></ref>   <ref type="table">Table 1</ref>: Top-1 accuracy on ResNet-18 using rotation as a data augmentation (DA) and a self-supervised augmented label (SAL).</p><p>Tian et al., 2020] leverage cross-sample correlations or dependencies in high-layer feature embedding space. Inspired by the recent success of self-supervised visual representational learning <ref type="bibr" target="#b6">[Chen et al., 2020]</ref>, SSKD  introduces an auxiliary self-supervised task to extract richer knowledge. As shown in <ref type="figure">Fig. 1a</ref>, SSKD proposes transferring cross-sample self-supervised contrastive relationships, making it achieve superior performance in the field of KD. However, forcing the network to learn invariant feature representations among transformed images using a self-supervised pretext task with random rotations from 0 ? , 90 ? , 180 ? , 270 ? utilized in SSKD may destroy the original visual semantics (e.g. 6 v.s. 9). It would increase the difficulty of representation learning for semantic recognition tasks. As validated in <ref type="table">Table 1</ref>, applying random rotation as an additional data augmentation degrades the classification performance, especially on more challenging TinyImageNet.</p><p>To effectively learn knowledge from self-supervised representation learning without interfering with the original fullysupervised classification task, we use a unified task by combining the label spaces of the original task and self-supervised task into a joint label space, as shown in <ref type="figure">Fig. 1b</ref>. This task is partly inspired by the previous seminal self-supervised representation learning <ref type="bibr" target="#b1">[Gidaris et al., 2018;</ref><ref type="bibr" target="#b1">Lee et al., 2020]</ref>. We further introduce these prior works to explore more powerful knowledge for distillation. To verify the effectiveness of the self-supervised augmented label, we also conduct initial exploratory experiments on standard image classification in <ref type="table">Table 1</ref>. We find that the performance can be significantly improved by SAL, which can be attributed to learned better feature representations from an extra well-combined selfsupervised task. The good performance further motivates us to define the self-supervised augmented distribution as a promising knowledge for KD.</p><p>Another valuable problem lies in how to transfer the probabilistic knowledge between teacher and student effectively. Vanilla KD aligns probability distributions only in the final layer but ignores comprehensive knowledge. Feature-based distillation methods provide one-to-one matching between the same convolutional stages of teacher and student. However, matched feature maps may have different semantic abstractions and result in a negative supervisory effect <ref type="bibr">[Passalis et al., 2020]</ref>. Compared with feature information, the probability distribution is indeed a more robust knowledge for KD, especially when existing a large architecture gap between teacher and student <ref type="bibr">[Tian et al., 2020]</ref>.</p><p>However, it is difficult to explicitly derive comprehensive probability distributions from hidden layers over the original architecture. Therefore a natural idea is to append several auxiliary classifiers to the network at various hidden layers to generate multi-level probability distributions from hierar-chical feature maps. It allows us to perform comprehensive one-to-one matching in hidden layers in terms of probabilistic knowledge. Moreover, it is also noteworthy that the gap of abstraction level of any matched distributions would be easily reduced due to the delicately-designed auxiliary classifiers.</p><p>We guide all auxiliary classifiers attached to the original network to learn informative self-supervised augmented distributions. Furthermore, we perform Hierarchical Selfsupervised Augmented Knowledge Distillation (HSAKD) between teacher and student towards all auxiliary classifiers in a one-to-one manner. By taking full advantage of richer selfsupervised augmented knowledge, the student can be guided to learn better feature representations. Note that all auxiliary classifiers are only used to assist knowledge transfer and dropped during the inference period. The overall contributions are summarized as follows:</p><p>? We introduce a self-supervised augmented distribution that encapsulates the unified knowledge of the original classification task and auxiliary self-supervised task as the richer dark knowledge for the field of KD. ? We propose a one-to-one probabilistic knowledge distillation framework by leveraging the architectural auxiliary classifiers, facilitating comprehensive knowledge transfer and alleviating the mismatch problem of abstraction levels when existing a large architecture gap. ? HSAKD significantly refreshes the results achieved by previous SOTA SSKD on standard image classification benchmarks. It can also learn well-general feature representations for downstream semantic recognition tasks.  <ref type="figure">Figure 2</ref>: Overview of our proposed HSAKD. Both teacher and student networks are equipped with several auxiliary classifiers after various convolutional stages to capture diverse self-supervised augmented knowledge from hierarchical feature maps. Mimicry loss is applied from self-supervised augmented distributions of the student {q S l (tj(x); ? )} L l=1 to corresponding that of the teacher {q T l (tj(x); ? )} L l=1 generated from same feature hierarchies in a one-to-one manner. Following the conventional KD, we also force the mimicry from the class probability distribution of student p S (tj(x); ? ) to that of the teacher p T (tj(x); ? ). During the inference period, we only retain the student backbone f S (?) and drop all auxiliary classifiers {c S (?)} L l=1 . Therefore there has no extra inference cost compared with the original student network.</p><p>saw <ref type="bibr" target="#b1">[Noroozi and Favaro, 2016]</ref> and colorization . More recently, [Misra and Maaten, 2020; Chen et al., 2020] learn invariant feature representations under self-supervised pretext tasks by maximizing the consistency of representations among various transformed versions of the same image. Both SSKD and our HSAKD are related to SRL. SSKD uses the latter SRL pattern to extract knowledge. In contrast, HSAKD combines the former classification-based pattern of SRL with the fully-supervised classification task to extract richer joint knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Self-supervised Augmented Distribution</head><p>We present the difference between the original class probability distribution and self-supervised augmented distribution using a conventional classification network of CNN. A CNN can be decomposed into a feature extractor ?(?; ?) and a linear classifier g(?; w), where ? and w are weight tensors. Given an input sample x ? X , X is the training set, z = ?(x; ?) ? R d is the extracted feature embedding vector, where d is the embedding size. We consider a conventional N -way object classification task with the label space N = {1, ? ? ? , N }. The linear classifier attached with softmax normalization maps the feature embedding z to a predictive class probability distribution p(x; ? ) = ?(g(z; w)/? ) ? R N over the label space, where ? is the softmax function, weight matrix w ? R N ?d , ? is a temperature hyper-parameter to scale the smoothness of distribution. We introduce an additional self-supervised task to augment the conventional supervised object class space. Learning such a joint distribution can force the network to generate more informative and meaningful predictions benefiting from the original and auxiliary self-supervised task simultaneously. Assuming that we define M various image transformations {t j } M j=1 with the label space M = {1, ? ? ? , M }, where t 1 denotes the identity transformation, i.e. t 1 (x) = x. To effectively learn composite knowledge, we combine the class space from the original supervised object recognition task and self-supervised task into a unified task. The label space of this task is K = N ? M, here ? is the Cartesian product. |K| = N * M , where |?| is the cardinality of the label collection, * denotes element-wise multiplication.</p><p>Given</p><formula xml:id="formula_0">a transformed samplex ? {t j (x)} M j=1 by apply- ing one transformation on x,z = ?(x; ?) ? R d is the ex- tracted feature embedding vector, q(x; ? ) = ?(g(z; w)/? ) ? R N * M is the predictive distribution over the joint label space K, where weight tensor w ? R (N * M )?d .</formula><p>We use p ? R N to denote the normal class probability distribution and q ? R N * M to denote the self-supervised augmented distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Auxiliary Architecture Design</head><p>It has been widely known that feature maps with various resolutions encode various patterns of representational information. Higher-resolution feature maps often present more fine-grained object details, while lower-resolution ones often contain richer global semantic information. To take full advantage of hierarchical feature maps encapsulated in a single network, we append several intermediate auxiliary classifiers into hidden layers to learn and distill hierarchical selfsupervised augmented knowledge.</p><p>For ease of notation, we denote a conventional classification network as f (?), which maps a input sample t j (x), j ? M to the vanilla class probability distribution p(t j (x); ? ) = ?(f (t j (x))/? ) ? R N over the original class space N . Modern CNNs typically utilize stage-wise convolutional blocks to gradually extract coarser features as the depth of the network increases. For example, popular ResNet-50 for Ima-geNet classification contains consecutive four stages, and extracted feature maps produced from various stages have different granularities and patterns. Assuming that a network contains L stages, we choose to append an auxiliary classifier after each stage, thus resulting in L classifiers {c l (?)} L l=1 , where c l (?) is the auxiliary classifier after l-th stage. c l (?) is composed of stage-wise convolutional blocks, a global average pooling layer and a fully connected layer. Denoting the extracted feature map after l-th stage as F l , we can obtain the self-supervised augmented distribution inferred by c l (?) as q l (t j (x); ? ) = ?(c l (F l ))/? ) ? R N * M over the joint class space K. The overall design of auxiliary classifiers over a 3-stage network is illustrated in <ref type="figure">Fig. 2</ref> for example. The detailed design formulation of various auxiliary classifiers for a specific network can be found in our released codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training the Teacher Network</head><p>We denote the teacher backbone network as f T (?) and L auxiliary classifiers as {c T l (?)} L l=1 . We conduct an end-to-end training process for preparing the teacher network. On the one hand, we train the f T (?) with normal data x by the conventional Cross-Entropy (CE) loss to fit the ground-truth label y ? N , and p T (x; ? ) = ?(f T (x)/? ) ? R N is the predictive class probaility distribution. On the other hand, we aim to train L auxiliary classifiers {c T l (?)} L l=1 for learning hierarchical self-supervised augmented distributions. Given an input sample t j (x), we feed the feature maps {F T l,j } L l=1 generated from backbone f T (?) to {c T l (?)} L l=1 , respectively. The predictive self-supervised augmented distribution inferred by the l-</p><formula xml:id="formula_1">th classifier c T l is q T l (t j (x); ? ) = ?(c T l (F T l,j ))/? ) ? R N * M .</formula><p>We train all auxiliary classifiers using CE loss with selfsupervised augmented labels across {t j (x)} M j=1 as Eq. <ref type="formula" target="#formula_2">(1)</ref>.</p><formula xml:id="formula_2">L T ce SAD = 1 M M j=1 L l=1 L ce (q T l (t j (x); ? ), k j )<label>(1)</label></formula><p>Where ? = 1 and L ce denotes the Cross-Entropy loss. For a bit abuse of notation, we use k j to denote the self-supervised augmented label of t j (x) in joint class space K. The overall loss for training a teacher is shown in Eq.</p><p>(2).</p><formula xml:id="formula_3">L T = E x?X [L ce (p T (x; ? ), y) + L T ce SAD ]<label>(2)</label></formula><p>Note that the two losses in Eq. 2 have different roles. The first loss aims to simply fit the normal data for learning general classification capability. The second loss aims to generate additional self-supervised augmented knowledge by the existing hierarchical features derived from the backbone network. This method facilitates richer knowledge distillation benefiting from the self-supervised task beyond the conventional fully-supervised task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training the Student Network</head><p>We denote the student backbone network as f S (?) and L auxiliary classifiers as {c S l (?)} L l=1 . We conduct an end-toend training process under the supervision of the teacher network. The overall loss includes the task loss from pre-defined ground-truth labels and the mimicry loss from the pre-trained teacher network.</p><p>Task loss. We force the f S (?) to fit the normal data x as the task loss:</p><formula xml:id="formula_4">L task = L ce (p S (x; ? ), y)<label>(3)</label></formula><p>Where p S (x; ? ) = ?(f S (x)/? ) ? R N is the predictive class probaility distribution. We also had tried to force L auxiliary classifiers {c S l (?)} L l=1 to learn the self-supervised augmented distributions from the joint hard label of the original and selfsupervised tasks by L S ce SAD as the additional loss:</p><formula xml:id="formula_5">L S ce SAD = 1 M M j=1 L l=1 L ce (q S l (t j (x); ? ), k j )<label>(4)</label></formula><p>Where q S l (t j (x); ? ) = ?(c S l (F S l,j ))/? ) ? R N * M , and F S l,j is the extracted feature map from the l-th stage of f S (?) for the input t j (x). However, we empirically found that introducing loss (4) into the original task loss (3) damages the performance of student networks, as validated in Section 4.2. Mimicry loss. On the one hand, we consider transferring hierarchical self-supervised augmented distributions generated from L auxiliary classifiers of the teacher network to corresponding L auxiliary classifiers of the student network, respectively. The transfer performs in a one-to-one manner by KL-divergence loss D KL . The loss is formulated as Eq. <ref type="formula">(5)</ref>, where ? 2 is used to retain the gradient contributions unchanged <ref type="bibr" target="#b1">[Hinton et al., 2015]</ref>.</p><formula xml:id="formula_6">L kl q = 1 M M j=1 L l=1 ? 2 D KL (q T l (t j (x); ? ) q S l (t j (x); ? ))</formula><p>(5) Benefiting from Eq. (5), one can expect that the student network gains comprehensive guidances by unified selfsupervised knowledge and the original class full-supervised knowledge. The informative knowledge is derived from multi-scale intermediate feature maps encapsulated in hidden layers of the high-capacity teacher network.</p><p>On the other hand, we transfer the original class probability distributions generated from the final layer between teacher and student. Specifically, we transfer the knowledge derived from both the normal and transformed data {t j (x)} M j=1 , where t 1 (x) = x. This loss is formulated as Eq. <ref type="formula">(6)</ref>.</p><formula xml:id="formula_7">L kl p = 1 M M j=1 ? 2 D KL (p T (t j (x); ? ) p S (t j (x); ? )) (6)</formula><p>We do not explicitly force the student backbone f S (?) to fit the transformed data in task loss for preserving the normal classification capability. But mimicking the side product of predictive class probability distributions inferred from these transformed data from the teacher network is also beneficial for the self-supervised representational learning of student network, as validated in Section 4.2.</p><p>Overall loss. We summarize the task loss and mimicry loss as the overall loss L S for training the student network. L S = E x?X [L task + L kl q + L kl p ] (7) Following the wide practice, we set the hyper-parameter ? = 1 in task loss and ? = 3 in mimicry loss. Besides, we do not introduce other hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>We conduct evaluations on standard CIFAR-100 and Ima-geNet <ref type="bibr" target="#b1">[Deng et al., 2009]</ref>   <ref type="table">Table 2</ref>: Top-1 accuracy (%) comparison of SOTA distillation methods across various teacher-student pairs on CIFAR-100. All results are reproduced by ours using author-provided code. The numbers in Bold and underline denote the best and the second-best results, respectively. 'Teacher' denotes that we first train the backbone f T (?) and then train auxiliary classifiers {c T l (?)} L l=1 based on the frozen f T (?). For a fair comparison, all compared methods and 'Ours' are supervised by 'Teacher'. 'Teacher*' denotes that we train f T (?) and {c T l (?)} L l=1 jointly, leading to a more powerful teacher network. 'Ours*' denotes the results supervised by 'Teacher*' for pursuing better performance. <ref type="bibr">WRN [Zagoruyko S, 2016]</ref>, VGG <ref type="bibr" target="#b4">[Simonyan and Zisserman, 2015]</ref>, MobileNet <ref type="bibr" target="#b3">[Sandler et al., 2018]</ref> and Shuf-fleNet <ref type="bibr" target="#b1">Ma et al., 2018]</ref>. Some representative KD methods including <ref type="bibr">KD [Hinton et al., 2015]</ref>, FitNet <ref type="bibr" target="#b2">[Romero et al., 2015]</ref>, <ref type="bibr">AT [Zagoruyko and Komodakis, 2017]</ref>, <ref type="bibr">AB [Heo et al., 2019]</ref>, <ref type="bibr">VID [Ahn et al., 2019]</ref>, <ref type="bibr">RKD [Park et al., 2019]</ref>, SP <ref type="bibr" target="#b5">[Tung and Mori, 2019]</ref>, CC <ref type="bibr" target="#b2">[Peng et al., 2019]</ref>, <ref type="bibr">CRD [Tian et al., 2020]</ref> and SOTA SSKD  are compared. For a fair comparison, all comparative methods are combined with conventional KD by default, and we adopt rotations {0 ? , 90 ? , 180 ? , 270 ? } as the self-supervised auxiliary task as same as SSKD. We use the standard training settings following  and report the mean result with a standard deviation over 3 runs. The more detailed settings for reproducibility can be found in our released codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>Effect of loss terms. As shown in <ref type="figure" target="#fig_0">Fig. 3 (left)</ref>, applying hierarchical self-supervised augmented knowledge transfer through multiple auxiliary classifiers by loss L kl q substantially boosts the accuracy upon the original task loss L task . We further compare L kl p and L kd upon L task + L kl q to demonstrate the efficacy of transferring class probability distributions from additional transformed images. We find that L kl p results in better accuracy gains than L kd , which suggests that transferring probabilistic class knowledge from those transformed images is also beneficial to feature representation learning. Finally, beyond the mimicry loss, we also explore whether the self-supervised augmented task loss L S ce SAD can be integrated into the overall task loss to train student networks. After adding L S ce SAD upon the above losses, the performance of the student network is dropped slightly. We speculate that mimicking soft self-supervised augmented distribution from the teacher by L kl q is good enough to learn rich self-supervised knowledge. Extra learning from hard one-hot distribution by L S ce SAD may interfere with the process of self-supervised knowledge transfer. Effect of auxiliary classifiers. We append several auxiliary classifiers to the network with various depths to learn and transfer diverse self-supervised augmented distributions extracted from hierarchical features. To examine this practice, we first individually evaluate each auxiliary classifier. As shown in <ref type="figure" target="#fig_0">Fig. 3 (right)</ref>, we can observe that each auxiliary classifier is beneficial to performance improvements. Moreover, the auxiliary classifier attached in the deeper layer often achieves more accuracy gains than that in the shallower layer, which can be attributed to more informative semantic knowledge encoded in high-level features. Finally, using all auxiliary classifiers can maximize accuracy gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-Of-The-Arts</head><p>Results on CIFAR-100 and ImageNet. We compare our HSAKD with SOTA representative distillation methods    across various teacher-student pairs with the same and different architectural styles on CIFAR-100 in <ref type="table">Table 2</ref> and on Ima-geNet in <ref type="table" target="#tab_4">Table 3</ref>. Interestingly, using L T ce SAD as an auxiliary loss can improve the teacher accuracy of the final classification. Compared to the original 'Teacher', 'Teacher*' achieves an average gain of 4.09% across five teacher networks on CIFAR-100 and a top-1 gain of 2.17% for ResNet-34 on Im-ageNet. For downstream student networks, 'Teacher*' leads to an average improvement of 1.15% on CIFAR-100 and an improvement of 0.23% on ImageNet than 'Teacher'. These results indicate that our proposed loss L T can improve the performance of a given network and produce a more suitable teacher for KD to learn a better student.</p><p>Moreover, our HSAKD significantly outperforms the bestcompeting method SSKD across all network pairs with an average accuracy gain of 2.56% on CIFAR-100 and a top-1 gain of 0.77% on ImageNet. Compared with other SOTA methods, the superiority of HSAKD can be attributed to hierarchical self-supervised augmented knowledge distillation by the assistance with well-designed auxiliary classifiers.</p><p>Transferability of Learned Representations. Beyond the accuracy on the object dataset, we also expect the student network can produce the generalized feature representations that transfer well to other unseen semantic recognition datasets. To this end, we freeze the feature extractor pre-trained on the upstream CIFAR-100, and then train two linear classifiers based on frozen pooled features for downstream STL-10 and TinyImageNet respectively, following the common linear classification protocal <ref type="bibr">[Tian et al., 2020]</ref>. As shown in Table 4, we can observe that both SSKD and HSAKD achieve better accuracy than other comparative methods, demonstrating that using self-supervised auxiliary tasks for distillation is conducive to generating better feature representations. Moreover, HSAKD can significantly outperform the best-competing SSKD by 3.63% on STL-10 and 3.50% on TinyImageNet. The results verify that encoding the selfsupervised auxiliary task as an augmented distribution in our HASKD has better supervision quality than the contrastive relationship in SSKD for learning good features.  Transferability for Object Detection. We further evaluate the student network ResNet-18 pre-trained with the teacher ResNet-34 on ImageNet as a backbone to carry out downstream object detection on Pascal VOC. We use Faster- <ref type="bibr">RCNN [Ren et al., 2016]</ref> framework and follow the standard data preprocessing and finetuning strategy. The comparison on detection performance is shown in <ref type="table" target="#tab_6">Table 5</ref>. Our method outperforms the original baseline by 2.27% mAP and the best-competing SSKD by 0.85% mAP. These results verify that our method can guide a network to learn better feature representations for semantic recognition tasks.</p><p>Efficacy under Few-shot Scenario. We compare our method with conventional KD and SOTA CRD and SSKD under few-shot scenarios by retaining 25%, 50% and 75% training samples. For a fair comparison, we use the same data split strategy for each few-shot setting, while maintaining the original test set. As shown in <ref type="table" target="#tab_8">Table 6</ref>, our method can consistently surpass others by large margins under various few-shot settings. Moreover, it is noteworthy that by using only 25% training samples, our method can achieve comparable accuracy with the baseline trained on the complete set. This is because our method can effectively learn general feature representations from limited data. In contrast, previous methods often focus on mimicking the inductive bias from intermediate feature maps or cross-sample relationships, which may overfit the limited set and generalize worse to the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a self-supervised augmented task for KD and further transfer such rich knowledge derived from hierarchical feature maps leveraging well-designed auxiliary classifiers. Our method achieves SOTA performance on the standard image classification benchmarks in the field of KD. It can guide the network to learn well-general feature representations for semantic recognition tasks. Moreover, it has no hyper-parameters to be tuned and is easy to implement.</p><p>A Dataset and Training Settings CIFAR-100. CIFAR-100 is composed of 50K training images and 10K test images in 100 classes. We use the standard data augmentation, i.e. padding to 40 ? 40, random cropping to 32 ? 32 and random horizontal flipping. We train all networks by the SGD optimizer with a momentum of 0.9, a batch size of 64 and a weight decay of 5 ? 10 ?4 . The initial learning rate starts at 0.05 and is decayed by a factor of 10 at 150, 180 and 210 epochs within the total 240 epochs. For MobileNetV2, ShuffleNetV1 and ShuffleNetV2, we use a learning rate of 0.01. We run experiments on an NVIDIA RTX 3090 GPU.</p><p>ImageNet. ImageNet is composed of 1.2 million training images and 50K validation images in 1000 classes. We use the standard data augmentation, i.e. randomly cropping the image into the size ranging from (0.08, 1.0) of the original size and a random aspect ratio ranging from (3/4, 4/3) of the original aspect ratio. The cropped patch is resized to 224 ? 224 and is randomly horizontal flipped with a probability of 0.5. We train all networks by the SGD optimizer with a momentum of 0.9, a batch size of 256 and a weight decay of 1 ? 10 ?4 . The initial learning rate starts at 0.1 and is decayed by a factor of 10 at 30, 60 and 90 epochs within the total 100 epochs. We run experiments on several parallel NVIDIA Tesla V100 GPUs.</p><p>STL-10 and TinyImageNet. STL-10 is composed of 5K labeled training images and 8K test images in 10 classes. TinyImageNet is composed of 100K training images and 10k test images in 200 classes. For data augmentation, we randomly crop the image into the size ranging from (0.08, 1.0) of the original size and a random aspect ratio ranging from (3/4, 4/3) of the original aspect ratio. The cropped patch is resized to 32 ? 32 and is randomly horizontal flipped with a probability of 0.5. We train the linear classifiers by the SGD optimizer with a momentum of 0.9, a batch size of 64 and a weight decay of 0. The initial learning rate starts at 0.1 and is decayed by a factor of 10 at 30, 60 and 90 epochs within the total 100 epochs. We run experiments on an NVIDIA RTX 3090 GPU. All experiments are conducted based on the Pytorch framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Architectural Design of Auxiliary Classifiers</head><p>As discussed in the main paper, we attach one auxiliary classifier after each convolutional stage. The auxiliary classifier is sequentially composed of several convolutional stages, global average pooling (GAP) and a fully-connected (FC) layer, where the design of convolutional stages follows the original architecture. In principle, the convolutional architecture in each auxiliary classifier is the same as the original backbone after the corresponding convolutional stage for architectural identity to ensure the fine-to-coarse feature transformation.</p><p>Assumed that a classification network f (?) contains L convolutional stages 1 ? L, the l-th auxiliary classifier is composed of the l + 1 ? L stages, where l ? [1, L ? 1]. Specifically, the architecture of the final L-th auxiliary classifier is composed of the L-th original convolutional stage but without downsampling for preserving the spatial information. Moreover, because we utilize a self-supervised augmented label with rotations {0 ? , 90 ? , 180 ? , 270 ? }, the class dimension of the FC layer in each auxiliary classifier is 4 times compared with that of the original classification FC layer. We illustrate the overall architecture of various networks with auxiliary classifiers involved in in the main paper, including the family of WRN <ref type="table" target="#tab_10">(Table 7,</ref>  <ref type="bibr">8, 9) [Zagoruyko S, 2016]</ref>, ResNet <ref type="table">(Table 10,</ref>  <ref type="bibr">11, 12, 13, 14, ??) [He et al., 2016]</ref>, VGG <ref type="table" target="#tab_6">(Table 15)</ref>  <ref type="bibr" target="#b4">[Simonyan and Zisserman, 2015]</ref>, MobileNet <ref type="table" target="#tab_8">(Table 16</ref>) <ref type="bibr" target="#b3">[Sandler et al., 2018]</ref> and ShuffleNet <ref type="bibr">(Table 17, 18) [Zhang et al., 2018;</ref><ref type="bibr" target="#b1">Ma et al., 2018]</ref>. For better readability, the style of the illustration of architectural details is followed by the original paper.</p><p>Layer name Output size f (?) c 1 (?) c 2 (?) Classifier 1?1 GAP, 100-D FC GAP, 400-D FC GAP, 400-D FC GAP, 400-D FC Classifier 1?1 GAP, 100-D FC GAP, 400-D FC GAP, 400-D FC GAP, 400-D FC</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Ablation study of loss terms (left) and auxiliary classifiers (right) on the student networks WRN-16-2 and ShuffleNetV1 under the pre-trained teacher network WRN-40-2 on CIFAR-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2107.13715v2 [cs.CV] 23 Jul 2022 (??0.26) 79.76 (?+1.75) TinyImageNet 63.69 62.66 (??1.03) 65.81 (?+2.12)</figDesc><table><row><cell>Dataset</cell><cell cols="2">Baseline +DA (Rotation) +SAL (Rotation)</cell></row><row><cell>CIFAR-100</cell><cell>78.01</cell><cell>77.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>benchmarks across the widely applied network families including ResNet [He et al., 2016], Student 73.57 (?0.23) 71.95 (?0.59) 69.62 (?0.26) 72.95 (?0.24) 73.51 (?0.26) 73.51 (?0.26) 71.74 (?0.35) 72.96 (?0.33) KD 75.23 (?0.23) 73.90 (?0.44) 70.91 (?0.10) 73.54 (?0.26) 75.21 (?0.24) 75.80 (?0.46) 75.83 (?0.18) 75.43 (?0.33) FitNet 75.30 (?0.42) 74.30 (?0.42) 71.21 (?0.16) 75.37 (?0.12) 75.42 (?0.34) 75.41 (?0.07) 76.27 (?0.18) 76.91 (?0.06) AT 75.64 (?0.31) 74.32 (?0.23) 71.35 (?0.09) 75.06 (?0.19)</figDesc><table><row><cell>Teacher</cell><cell>WRN-40-2</cell><cell>WRN-40-2</cell><cell>ResNet56</cell><cell>ResNet32?4</cell><cell>VGG13</cell><cell>ResNet50</cell><cell>WRN-40-2</cell><cell>ResNet32?4</cell></row><row><cell>Student</cell><cell>WRN-16-2</cell><cell>WRN-40-1</cell><cell>ResNet20</cell><cell cols="5">ResNet8?4 MobileNetV2 MobileNetV2 ShuffleNetV1 ShuffleNetV2</cell></row><row><cell>Teacher</cell><cell>76.45</cell><cell>76.45</cell><cell>73.44</cell><cell>79.63</cell><cell>74.64</cell><cell>79.34</cell><cell>76.45</cell><cell>79.63</cell></row><row><cell>Teacher*</cell><cell>80.70</cell><cell>80.70</cell><cell>77.20</cell><cell>83.73</cell><cell>78.48</cell><cell>83.85</cell><cell>80.70</cell><cell>83.73</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>74.08 (?0.21)</cell><cell>76.57 (?0.20)</cell><cell>76.51 (?0.44)</cell><cell>76.32 (?0.12)</cell></row><row><cell>AB</cell><cell cols="4">71.26 (?1.32) 74.55 (?0.46) 71.56 (?0.19) 74.31 (?0.09)</cell><cell>74.98 (?0.44)</cell><cell>75.87 (?0.39)</cell><cell>76.43 (?0.09)</cell><cell>76.40 (?0.29)</cell></row><row><cell>VID</cell><cell cols="4">75.31 (?0.22) 74.23 (?0.28) 71.35 (?0.09) 75.07 (?0.35)</cell><cell>75.67 (?0.13)</cell><cell>75.97 (?0.08)</cell><cell>76.24 (?0.44)</cell><cell>75.98 (?0.41)</cell></row><row><cell>RKD</cell><cell cols="4">75.33 (?0.14) 73.90 (?0.26) 71.67 (?0.08) 74.17 (?0.22)</cell><cell>75.54 (?0.36)</cell><cell>76.20 (?0.06)</cell><cell>75.74 (?0.32)</cell><cell>75.42 (?0.25)</cell></row><row><cell>SP</cell><cell cols="4">74.35 (?0.59) 72.91 (?0.24) 71.45 (?0.38) 75.44 (?0.11)</cell><cell>75.68 (?0.35)</cell><cell>76.35 (?0.14)</cell><cell>76.40 (?0.37)</cell><cell>76.43 (?0.21)</cell></row><row><cell>CC</cell><cell cols="4">75.30 (?0.03) 74.46 (?0.05) 71.44 (?0.10) 74.40 (?0.24)</cell><cell>75.66 (?0.33)</cell><cell>76.05 (?0.25)</cell><cell>75.63 (?0.30)</cell><cell>75.74 (?0.18)</cell></row><row><cell>CRD</cell><cell cols="4">75.81 (?0.33) 74.76 (?0.25) 71.83 (?0.42) 75.77 (?0.24)</cell><cell>76.13 (?0.16)</cell><cell>76.89 (?0.27)</cell><cell>76.37 (?0.23)</cell><cell>76.51 (?0.09)</cell></row><row><cell>SSKD</cell><cell cols="4">76.16 (?0.17) 75.84 (?0.04) 70.80 (?0.02) 75.83 (?0.29)</cell><cell>76.21 (?0.16)</cell><cell>78.21 (?0.16)</cell><cell>76.71 (?0.31)</cell><cell>77.64 (?0.24)</cell></row><row><cell>Ours</cell><cell cols="4">77.20 (?0.17) 77.00 (?0.21) 72.58 (?0.33) 77.26 (?0.14)</cell><cell>77.45 (?0.21)</cell><cell>78.79 (?0.11)</cell><cell>78.51 (?0.20)</cell><cell>79.93 (?0.11)</cell></row><row><cell>Ours*</cell><cell cols="4">78.67 (?0.20) 78.12 (?0.25) 73.73 (?0.10) 77.69 (?0.05)</cell><cell>79.27 (?0.12)</cell><cell>79.43 (?0.24)</cell><cell>80.11 (?0.32)</cell><cell>80.86 (?0.15)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>70.70 69.96 70.62 71.34 71.38 71.62 72.16 72.39 90.00 89.17 89.80 90.37 90.49 90.67 90.85 91.00</figDesc><table><row><cell>Teacher</cell><cell>Student</cell><cell>Acc</cell><cell>Teacher Teacher* Student</cell><cell>KD</cell><cell>AT</cell><cell>CC</cell><cell>SP</cell><cell>RKD CRD SSKD Ours Ours*</cell></row><row><cell cols="2">ResNet-34 ResNet-18</cell><cell cols="3">Top-1 70.66 Top-5 73.31 75.48 69.75 91.42 92.67 89.07 89.88</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Top-1 accuracy (%) comparison on ImageNet. The compared results are from.</figDesc><table><row><cell>Transferred Dataset</cell><cell>Baseline</cell><cell>KD</cell><cell>FitNet</cell><cell>AT</cell><cell>AB</cell><cell>VID</cell><cell>RKD</cell><cell>SP</cell><cell>CC</cell><cell>CRD SSKD Ours</cell></row><row><cell>CIFAR-100? STL-10</cell><cell>67.76</cell><cell cols="9">67.90 69.41 67.37 67.82 69.29 69.74 68.96 69.13 70.09 71.03 74.66</cell></row><row><cell>CIFAR-100? TinyImageNet</cell><cell>34.69</cell><cell cols="9">34.15 36.04 34.44 34.79 36.09 37.21 35.69 36.43 38.17 39.07 42.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Linear classification accuracy (%) of transfer learning on the student MobileNetV2 pre-trained using the teacher VGG-13.</figDesc><table><row><cell>Baseline</cell><cell>KD</cell><cell>CRD SSKD Ours</cell></row><row><cell>76.18</cell><cell cols="2">77.06 77.36 77.60 78.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of detection mAP (%) on Pascal VOC using ResNet-18 as the backbone pre-trained by various KD methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Top-1 accuracy (%) comparison on CIFAR-100 under few-</cell></row><row><cell>shot scenario with various percentages of training samples. We use</cell></row><row><cell>the ResNet56-ResNet20 as the teacher-student pair for evaluation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Architectural details of WRN-40-2 with auxiliary classifiers for CIFAR-100 classification.</figDesc><table><row><cell cols="2">Layer name Output size</cell><cell>f (?)</cell><cell></cell><cell>c 1 (?)</cell><cell></cell><cell>c 2 (?)</cell><cell></cell><cell>c 3 (?)</cell></row><row><cell>conv1</cell><cell>32?32</cell><cell cols="2">3 ? 3, 16</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>conv2 x</cell><cell>32?32</cell><cell>3 ? 3, 16 3 ? 3, 16</cell><cell>? 6</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>conv3 x</cell><cell>16?16</cell><cell>3 ? 3, 32 3 ? 3, 32</cell><cell>? 6</cell><cell>3 ? 3, 32 3 ? 3, 32</cell><cell>? 6</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>conv4 x</cell><cell>8?8</cell><cell>3 ? 3, 64 3 ? 3, 64</cell><cell>? 6</cell><cell>3 ? 3, 64 3 ? 3, 64</cell><cell>? 6</cell><cell>3 ? 3, 64 3 ? 3, 64</cell><cell>? 6</cell><cell>-</cell></row><row><cell>conv5 x</cell><cell>8?8</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>3 ? 3, 64 3 ? 3, 64</cell><cell>? 6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Table 20: Architectural details of ResNet-34 with auxiliary classifiers for ImageNet classification.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Classifier 1?1 GAP, 100-D FC GAP, 400-D FC GAP, 400-D FC GAP, 400-D FC <ref type="table">Table 9</ref>: Architectural details of WRN-16-2 with auxiliary classifiers for CIFAR-100 classification.</p><p>Layer name Output size</p><p>Classifier 1?1 GAP, 100-D FC GAP, 400-D FC GAP, 400-D FC GAP, 400-D FC Layer name Output size</p><p>Classifier 1?1 GAP, 100-D FC GAP, 400-D FC GAP, 400-D FC GAP, 400-D FC Layer name Output size</p><p>Classifier 1?1 GAP, 100-D FC GAP, 400-D FC GAP, 400-D FC GAP, 400-D FC <ref type="table">Table 12</ref>: Architectural details of resnet32?4 with auxiliary classifiers for CIFAR-100 classification.</p><p>Layer name Output size Layer name Output size  </p><p>Layer name Input Operator t c n s t c n s t c n s t c n s t c n s</p><p>x 16 2 ?16 bottleneck 6 32 4 2 6 32 4 2 6 32 4 2 --------8 2 ?32 bottleneck 6 48 3 1 6 48 3 1 6 48 3 1 --------conv5 x 8 2 ?48 bottleneck 6 80 3 2 6 80 3 2 6 80 3 2 6 80 3 2 ----4 2 ?80 bottleneck 6 160 1 1 6 160 1 1 6 160 1 1 6 160 1 1 -</p><p>conv7 4 2 ?160 conv2d 1x1 -640 1 1 -640 1 1 -640 1 1 -640 1 1 -640 1 1 classifier 4 2 ?640 avgpool 7x7 --1 ---1 ---1 ---1 ---1 -1 2 ?640 conv2d 1x1 -100 1 --400 1 --400 1 --400 1 --400 1 -   <ref type="table">Image  32?32  3  Conv1  32?32  3?3  1  1  24   Stage2  16?16  2  1  240  16?16  1  3  240   Stage3  8?8  2  1  2  1  480  8?8  1  7  1  7  480   Stage4  4?4  2  1  2  1  2  1  960  4?4  1  3  1  3  1  3  960   Stage5  4?4  1  1  960  4?4  1  3  960   GlobalPool  1?1  7?7  7?7  7?7  7?7  FC</ref> 100 400 400 400 <ref type="table">Table 17</ref>: Architectural details of ShuffleNetV1 with auxiliary classifiers for CIFAR-100 classification, where the group g = 3.  Layer name Output size</p><p>Classifier 1?1 GAP, 1000-D FC GAP, 4000-D FC GAP, 4000-D FC GAP, 4000-D FC GAP, 4000-D FC Classifier 1?1 GAP, 1000-D FC GAP, 4000-D FC GAP, 4000-D FC GAP, 4000-D FC GAP, 4000-D FC</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational information distillation for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References [ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Maria Tzelepi, and Anastasios Tefas. Heterogeneous knowledge distillation using information flow modeling</title>
		<editor>Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun</editor>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2339" to="2348" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks</title>
	</analytic>
	<monogr>
		<title level="m">Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. ICLR</title>
		<meeting><address><addrLine>Nicolas Ballas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sandler</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zisserman</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Tian et al., 2020] Yonglong Tian</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mori ; Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gated convolutional networks with hybrid connectivity for image classification</title>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<editor>Yang, Zhulin An, Hui Zhu, Xiaolong Hu, Kun Zhang, Kaiqiang Xu, Chao Li, and Yongjun Xu</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12581" to="12588" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<editor>Richard Zhang, Phillip Isola, and Alexei A Efros</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eena: efficient evolution of neural architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
