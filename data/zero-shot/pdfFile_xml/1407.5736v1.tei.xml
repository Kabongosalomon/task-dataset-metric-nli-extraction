<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Rich Features from RGB-D Images for Object Detection and Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
							<email>sgupta@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
							<email>arbelaez@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universidad de los Andes</orgName>
								<address>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
							<email>malik@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Rich Features from RGB-D Images for Object Detection and Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>RGB-D perception</term>
					<term>object detection</term>
					<term>object segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3%, which is a 56% relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and geocentric pose features. Finally, we use the output from our object detectors in an existing superpixel classification framework for semantic scene segmentation and achieve a 24% relative improvement over current state-of-the-art for the object categories that we study. We believe advances such as those represented in this paper will facilitate the use of perception in fields like robotics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We have designed and implemented an integrated system ( <ref type="figure">Figure 1</ref>) for scene understanding from RGB-D images. The overall architecture is a generalization of the current state-of-the-art system for object detection in RGB images, R-CNN <ref type="bibr" target="#b15">[16]</ref>, where we design each module to make effective use of the additional signal in RGB-D images, namely pixel-wise depth. We go beyond object detection by providing pixel-level support maps for individual objects, such as tables and chairs, as well as a pixel-level labeling of scene surfaces, such as walls and floors. Thus our system subsumes the traditionally distinct problems of object detection and semantic segmentation. Our approach is summarized below (source code is available at http://www.cs.berkeley.edu/~sgupta/eccv14/). RGB-D contour detection and 2.5D region proposals: RGB-D images enable one to compute depth and normal gradients <ref type="bibr" target="#b17">[18]</ref>, which we combine with the <ref type="figure">Fig. 1</ref>. Overview: from an RGB and depth image pair, our system detects contours, generates 2.5D region proposals, classifies them into object categories, and then infers segmentation masks for instances of "thing"-like objects, as well as labels for pixels belonging to "stuff"-like categories. structured learning approach in <ref type="bibr" target="#b8">[9]</ref> to yield significantly improved contours. We then use these RGB-D contours to obtain 2.5D region candidates by computing features on the depth and color image for use in the Multiscale Combinatorial Grouping (MCG) framework of Arbel?ez et al. <ref type="bibr" target="#b0">[1]</ref>. This module is state-of-theart for RGB-D proposal generation.</p><p>RGB-D object detection: Convolutional neural networks (CNNs) trained on RGB images are the state-of-the-art for detection and segmentation <ref type="bibr" target="#b15">[16]</ref>. We show that a large CNN pre-trained on RGB images can be adapted to generate rich features for depth images. We propose to represent the depth image by three channels (horizontal disparity, height above ground, and angle with gravity) and show that this representation allows the CNN to learn stronger features than by using disparity (or depth) alone. We use these features, computed on our 2.5D region candidates, in a modified R-CNN framework to obtain a 56% relative improvement in RGB-D object detection, compared to existing methods.</p><p>Instance segmentation: In addition to bounding-box object detection, we also infer pixel-level object masks. We frame this as a foreground labeling task and show improvements over baseline methods.</p><p>Semantic segmentation: Finally, we improve semantic segmentation performance (the task of labeling all pixels with a category, but not differentiating between instances) by using object detections to compute additional features for superpixels in the semantic segmentation system we proposed in <ref type="bibr" target="#b17">[18]</ref>. This approach obtains state-of-the-art results for that task, as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Most prior work on RGB-D perception has focussed on semantic segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>, i.e. the task of assigning a category label to each pixel. While this is an interesting problem, many practical applications require a richer understanding of the scene. Notably, the notion of an object instance is missing from such an output. Object detection in RGB-D images <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref>, in contrast, focusses on instances, but the typical output is a bounding box. As Hariharan et al. <ref type="bibr" target="#b19">[20]</ref> observe, neither of these tasks produces a compelling output representation. It is not enough for a robot to know that there is a mass of 'bottle' pixels in the image. Likewise, a roughly localized bounding box of an individual bottle may be too imprecise for the robot to grasp it. Thus, we propose a framework for solving the problem of instance segmentation (delineating pixels on the object corresponding to each detection) as proposed by <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Recently, convolutional neural networks <ref type="bibr" target="#b26">[27]</ref> were shown to be useful for standard RGB vision tasks like image classification <ref type="bibr" target="#b24">[25]</ref>, object detection <ref type="bibr" target="#b15">[16]</ref>, semantic segmentation <ref type="bibr" target="#b12">[13]</ref> and fine-grained classification <ref type="bibr" target="#b10">[11]</ref>. Naturally, recent works on RGB-D perception have considered neural networks for learning representations from depth images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b34">35]</ref>. Couprie et al. <ref type="bibr" target="#b5">[6]</ref> adapt the multiscale semantic segmentation system of Farabet et al. <ref type="bibr" target="#b12">[13]</ref> by operating directly on four-channel RGB-D images from the NYUD2 dataset. Socher et al. <ref type="bibr" target="#b34">[35]</ref> and Bo et al. <ref type="bibr" target="#b3">[4]</ref> look at object detection in RGB-D images, but detect small prop-like objects imaged in controlled lab settings. In this work, we tackle uncontrolled, cluttered environments as in the NYUD2 dataset. More critically, rather than using the RGB-D image directly, we introduce a new encoding that captures the geocentric pose of pixels in the image, and show that it yields a substantial improvement over naive use of the depth channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">2.5D Region Proposals</head><p>In this section, we describe how to extend multiscale combinatorial grouping (MCG) <ref type="bibr" target="#b0">[1]</ref> to effectively utilize depth cues to obtain 2.5D region proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Contour Detection</head><p>RGB-D contour detection is a well-studied task <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34]</ref>. Here we combine ideas from two leading approaches, <ref type="bibr" target="#b8">[9]</ref> and our past work in <ref type="bibr" target="#b17">[18]</ref>.</p><p>In <ref type="bibr" target="#b17">[18]</ref>, we used gPb-ucm <ref type="bibr" target="#b1">[2]</ref> and proposed local geometric gradients dubbed N G ? , N G + , and DG to capture convex, concave normal gradients and depth gradients. In <ref type="bibr" target="#b8">[9]</ref>, Doll?r et al. proposed a novel learning approach based on structured random forests to directly classify a pixel as being a contour pixel or not. Their approach treats the depth information as another image, rather than encoding it in terms of geocentric quantities, like N G ? . While the two methods perform comparably on the NYUD2 contour detection task (maximum F-measure point in the red and the blue curves in <ref type="figure" target="#fig_2">Figure 3</ref>), there are differences in the the type of contours that either approach produces. <ref type="bibr" target="#b8">[9]</ref> produces better localized contours that capture fine details, but tends to miss normal discontinuities that <ref type="bibr" target="#b17">[18]</ref> easily finds (for example, consider the contours between the walls and the ceiling in left part of the image <ref type="figure" target="#fig_1">Figure 2</ref>). We propose a synthesis of the two approaches that combines features from <ref type="bibr" target="#b17">[18]</ref> with the learning framework from <ref type="bibr" target="#b8">[9]</ref>. Specifically, we add the following features. Normal Gradients: We compute normal gradients at two scales (corresponding to fitting a local plane in a half-disk of radius 3 and 5 pixels), and use these as additional gradient maps. Geocentric Pose: We compute a per pixel height above ground and angle with gravity (using the algorithms we proposed in <ref type="bibr" target="#b17">[18]</ref>. These features allow the decision trees to exploit additional regularities, for example that the brightness edges on the floor are not as important as brightness edges elsewhere. Richer Appearance: We observe that the NYUD2 dataset has limited appearance variation (since it only contains images of indoor scenes). To make the model generalize better, we add the soft edge map produced by running the RGB edge detector of <ref type="bibr" target="#b8">[9]</ref> (which is trained on BSDS) on the RGB image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Candidate Ranking</head><p>From the improved contour signal, we obtain object proposals by generalizing MCG to RGB-D images. MCG for RGB images <ref type="bibr" target="#b0">[1]</ref> uses simple features based on the color image and the region shape to train a random forest regressors to rank the object proposals. We follow the same paradigm, but propose additional geometric features computed on the depth image within each proposal. We compute: (1) the mean and standard deviation of the disparity, height above ground, angle with gravity, and world (X, Y, Z) coordinates of the points in the region;</p><p>(2) the region's (X, Y, Z) extent; (3) the region's minimum and maximum height above ground; (4) the fraction of pixels on vertical surfaces, surfaces facing up, and surfaces facing down; (5) the minimum and maximum standard deviation along a direction in the top view of the room. We obtain 29 geometric features for each region in addition to the 14 from the 2D region shape and color image already computed in <ref type="bibr" target="#b0">[1]</ref>. Note that the computation of these features for a region decomposes over superpixels and can be done efficiently by first computing the first and second order moments on the superpixels and then combining them appropriately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results</head><p>We now present results for contour detection and candidate ranking. We work with the NYUD2 dataset and use the standard split of 795 training images and 654 testing images (we further divide the 795 images into a training set of 381 images and a validation set of 414 images). These splits are carefully selected such that images from the same scene are only in one of these sets. Contour detection: To measure performance on the contour detection task, we plot the precision-recall curve on contours in <ref type="figure" target="#fig_2">Figure 3</ref> and report the standard maximum F-measure metric (F max ) in <ref type="table" target="#tab_0">Table 1</ref>. We start by comparing the performance of <ref type="bibr" target="#b17">[18]</ref>   <ref type="bibr" target="#b8">[9]</ref>. We see that both these contour detectors perform comparably in terms of   F max . <ref type="bibr" target="#b17">[18]</ref> obtains better precision at lower recalls while <ref type="bibr" target="#b8">[9]</ref> obtains better precision in the high recall regime. We also include a qualitative visualization of the contours to understand the differences in the nature of the contours produced by the two approaches ( <ref type="figure" target="#fig_1">Figure 2</ref>). Switching to the effect of our proposed contour detector, we observe that adding normal gradients consistently improves precision for all recall levels and F max increases by 1.2% points ( <ref type="table" target="#tab_0">Table 1</ref>). The addition of geocentric pose features and appearance features improves F max by another 0.6% points, making our final system better than the current state-of-the-art methods by 1.5% points. 1</p><p>Candidate ranking: The goal of the region generation step is to propose a pool of candidates for downstream processing (e.g., object detection and segmentation). Thus, we look at the standard metric of measuring the coverage of ground truth regions as a function of the number of region proposals. Since we are generating region proposals for the task of object detection, where each class ones which we study in this paper, and the ones studied by Lin et al. <ref type="bibr" target="#b28">[29]</ref>. Our depth based region proposals using our improved RGB-D contours work better than Lin et al.'s <ref type="bibr" target="#b28">[29]</ref>, while at the same time being more general. Note that the X-axis is on a log scale.  is equally important, we measure coverage for K region candidates by</p><formula xml:id="formula_0">coverage(K) = 1 C C i=1 ? ? 1 N i ? ? Ni j=1 max k?[1...K] O R l(i,j) k , I i j ? ? ? ? ,<label>(1)</label></formula><p>where C is the number of classes, N i is the number of instances for class i, O(a, b) is the intersection over union between regions a and b, I i j is the region corresponding to the j th instance of class i, l (i, j) is the image which contains the j th instance of class i, and R l k is the k th ranked region in image l. We plot the function coverage(K) in <ref type="figure" target="#fig_3">Figure 4</ref> (left) for our final method, which uses our RGB-D contour detector and RGB-D features for region ranking (black). As baselines, we show regions from the recent work of Lin et al. <ref type="bibr" target="#b28">[29]</ref> with and without non-maximum suppression, MCG with RGB contours and RGB features, MCG with RGB-D contours but RGB features and finally our system which is MCG with RGB-D contours and RGB-D features. We note that there is a large improvement in region quality when switching from RGB contours to RGB-D contours, and a small but consistent improvement from adding our proposed depth features for candidate region re-ranking.</p><p>Since Lin et al. worked with a different set of categories, we also compare on the subset used in their work (in <ref type="figure" target="#fig_3">Figure 4</ref> (right)). Their method was trained specifically to return candidates for these classes. Our method, in contrast, is trained to return candidates for generic objects and therefore "wastes" candidates trying to cover categories that do not contribute to performance on any fixed subset. Nevertheless, our method consistently outperforms <ref type="bibr" target="#b28">[29]</ref>, which highlights the effectiveness and generality of our region proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RGB-D Object Detectors</head><p>We generalize the R-CNN system introduced by Girshick et al. <ref type="bibr" target="#b15">[16]</ref> to leverage depth information. At test time, R-CNN starts with a set of bounding box proposals from an image, computes features on each proposal using a convolutional neural network, and classifies each proposal as being the target object class or not with a linear SVM. The CNN is trained in two stages: first, pretraining it on a large set of labeled images with an image classification objective, and then finetuning it on a much smaller detection dataset with a detection objective.</p><p>We generalize R-CNN to RGB-D images and explore the scientific question: Can we learn rich representations from depth images in a manner similar to those that have been proposed and demonstrated to work well for RGB images?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoding Depth Images for Feature Learning</head><p>Given a depth image, how should it be encoded for use in a CNN? Should the CNN work directly on the raw depth map or are there transformations of the input that the CNN to learn from more effectively?</p><p>We propose to encode the depth image with three channels at each pixel: horizontal disparity, height above ground, and the angle the pixel's local surface normal makes with the inferred gravity direction. We refer to this encoding as HHA. The latter two channels are computed using the algorithms proposed in <ref type="bibr" target="#b17">[18]</ref> and all channels are linearly scaled to map observed values across the training dataset to the 0 to 255 range.</p><p>The HHA representation encodes properties of geocentric pose that emphasize complementary discontinuities in the image (depth, surface normal and height). Furthermore, it is unlikely that a CNN would automatically learn to compute these properties directly from a depth image, especially when very limited training data is available, as is the case with the NYUD2 dataset.</p><p>We use the CNN architecture proposed by Krizhevsky et al. in <ref type="bibr" target="#b24">[25]</ref> and used by Girshick et al. in <ref type="bibr" target="#b15">[16]</ref>. The network has about 60 million parameters and was trained on approximately 1.2 million RGB images from the 2012 ImageNet Challenge <ref type="bibr" target="#b6">[7]</ref>. We refer the reader to <ref type="bibr" target="#b24">[25]</ref> for details about the network. Our hypothesis, to be borne out in experiments, is that there is enough common structure between our HHA geocentric images and RGB images that a network designed for RGB images can also learn a suitable representation for HHA images. As an example, edges in the disparity and angle with gravity direction images correspond to interesting object boundaries (internal or external shape boundaries), similar to ones one gets in RGB images (but probably much cleaner).</p><p>Augmentation with synthetic data: An important observation is the amount of supervised training data that we have in the NYUD2 dataset is about one order of magnitude smaller than what is there for PASCAL VOC dataset (400 images as compared to 2500 images for PASCAL VOC 2007). To address this issue, we generate more data for training and finetuning the network. There are multiple ways of doing this: mesh the already available scenes and render the scenes from novel view points, use data from nearby video frames available in the dataset by flowing annotations using optical flow, use full 3D synthetic CAD objects models available over the Internet and render them into scenes. Meshing the point clouds may be too noisy and nearby frames from the video sequence maybe too similar and thus not very useful. Hence, we followed the third alternative and rendered the 3D annotations for NYUD2 available from <ref type="bibr" target="#b16">[17]</ref> to generate synthetic scenes from various viewpoints. We also simulated the Kinect quantization model in generating this data (rendered depth images are converted to quantized disparity images and low resolution white noise was added to the disparity values).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments</head><p>We work with the NYUD2 dataset and use the standard dataset splits into train, val, and test as described in Section 2.3. The dataset comes with semantic segmentation annotations, which we enclose in a tight box to obtain bounding box annotations. We work with the major furniture categories available in the dataset, such as chair, bed, sofa, table (listed in <ref type="table" target="#tab_1">Table 2</ref>). Experimental setup: There are two aspects to training our model: finetuning the convolutional neural network for feature learning, and training linear SVMs for object proposal classification. Finetuning: We follow the R-CNN procedure from <ref type="bibr" target="#b15">[16]</ref> using the Caffe CNN library <ref type="bibr" target="#b21">[22]</ref>. We start from a CNN that was pretrained on the much larger ILSVRC 2012 dataset. For finetuning, the learning rate was initialized at 0.001 and decreased by a factor of 10 every 20k iterations. We finetuned for 30k iterations, which takes about 7 hours on a NVIDIA Titan GPU. Following <ref type="bibr" target="#b15">[16]</ref>, we label each training example with the class that has the maximally overlapping ground truth instance, if this overlap is larger than 0.5, and background otherwise. All finetuning was done on the train set. SVM Training: For training the linear SVMs, we compute features either from pooling layer 5 (pool5 ), fully connected layer 6 (fc6 ), or fully connected layer 7 (fc7 ). In SVM training, we fixed the positive examples to be from the ground truth boxes for the target class and the negative examples were defined as boxes having less than 0.3 intersection over union with the ground truth instances from that class. Training was done on the train set with SVM hyper-parameters C = 0.001, B = 10, w 1 = 2.0 using liblinear <ref type="bibr" target="#b11">[12]</ref>. We report the performance (detection average precision AP b ) on the val set for the control experiments. For the final experiment we train on trainval and report performance in comparison to other methods on the test set. At test time, we compute features from the fc6 layer in the network, apply the linear classifier, and non-maximum suppression to the output, to obtain a set of sparse detections on the test image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>We use the PASCAL VOC box detection average precision (denoted as AP b following the generalization introduced in [20]) as the performance metric. Results are presented in <ref type="table" target="#tab_1">Table 2</ref>. As a baseline, we report performance of the stateof-the-art non-neural network based detection system, deformable part models (DPM) <ref type="bibr" target="#b13">[14]</ref>. First, we trained DPMs on RGB images, which gives a mean AP b of 8.4% (column A). While quite low, this result agrees with <ref type="bibr" target="#b32">[33]</ref>. <ref type="bibr" target="#b1">2</ref> As a stronger baseline, we trained DPMs on features computed from RGB-D images (by using HOG on the disparity image and a histogram of height above ground in each HOG cell in addition to the HOG on the RGB image). These augmented DPMs (denoted RGBD-DPM) give a mean AP b of 21.7% (column B). We also report results from the method of Girshick et al. <ref type="bibr" target="#b15">[16]</ref>, without and with fine tuning on the RGB images in the dataset, yielding 16.4% and 19.7% respectively (column C and column D). We compare results from layer fc6 for all our experiments. Features from layers fc7 and pool5 generally gave worse performance. The first question we ask is: Can a network trained only on RGB images can do anything when given disparity images? (We replicate each one-channel disparity image three times to match the three-channel filters in the CNN and scaled the input so as to have a distribution similar to RGB images.) The RGB network generalizes surprisingly well and we observe a mean AP b of 11.3% (column E). This results confirms our hypothesis that disparity images have a similar structure to RGB images, and it may not be unreasonable to use an ImageNettest split that they have not made available. Their baseline HOG DPM detection results are significantly higher than those reported in <ref type="bibr" target="#b32">[33]</ref> and this paper, indicating that the split used in <ref type="bibr" target="#b37">[38]</ref> is substantially easier than the standard evaluation split.</p><p>trained CNN as an initialization for finetuning on depth images. In fact, in our experiments we found that it was always better to finetune from the ImageNet initialization than to train starting with a random initialization.</p><p>We then proceed with finetuning this network (starting from the ImageNet initialization), and observe that performance improves to 20.1% (column F), already becoming comparable to RGBD-DPMs. However, finetuning with our HHA depth image encoding dramatically improves performance (by 25% relative), yielding a mean AP b of 25.2% (column G).</p><p>We then observe the effect of synthetic data augmentation. Here, we add 2? synthetic data, based on sampling two novel views of the given NYUD2 scene from the 3D scene annotations made available by <ref type="bibr" target="#b16">[17]</ref>. We observe an improvement from 25.2% to 26.1% mean AP b points (column H). However, when we increase the amount of synthetic data further (15? synthetic data), we see a small drop in performance (column H to I). We attribute the drop to the larger bias that has been introduced by the synthetic data. Guo et al.'s <ref type="bibr" target="#b16">[17]</ref> annotations replace all non-furniture objects with cuboids, changing the statistics of the generated images. More realistic modeling for synthetic scenes is a direction for future research.</p><p>We also report performance when using features from other layers: pool5 (column J) and fc7 (column K). As expected the performance for pool5 is lower, but the performance for fc7 is also lower. We attribute this to over-fitting during finetuning due to the limited amount of data available.</p><p>Finally, we combine the features from both the RGB and the HHA image when finetuned on 2? synthetic data (column L). We see there is consistent improvement from 19.7% and 26.1% individually to 32.5% (column L) mean AP b . This is the final version of our system.</p><p>We also experimented with other forms of RGB and D fusion -early fusion where we passed in a 4 channel RGB-D image for finetuning but were unable to obtain good results (AP b of 21.2%), and late fusion with joint finetuning for RGB and HHA (AP b of 31.9%) performed comparably to our final system (individual finetuning of RGB and HHA networks) (AP b of 32.5%). We chose the simpler architecture.</p><p>Test set performance: We ran our final system (column L) on the test set, by training on the complete trainval set. Performance is reported in <ref type="table" target="#tab_2">Table 3</ref>. We compare against a RGB DPM, RGBD-DPMs as introduced before. Note that our RGBD-DPMs serve as a strong baseline and are already an absolute 8.2% better than published results on the B3DO dataset <ref type="bibr" target="#b20">[21]</ref> (39.4% as compared to 31.2% from the approach of Kim et al. <ref type="bibr" target="#b22">[23]</ref>, detailed results are in the supplementary material <ref type="bibr" target="#b18">[19]</ref>). We also compare to Lin et al. <ref type="bibr" target="#b28">[29]</ref>. <ref type="bibr" target="#b28">[29]</ref> only produces 8, 15 or 30 detections per image which produce an average F 1 measure of 16.60, 17.88 and 18.14 in the 2D detection problem that we are considering as compared to our system which gives an average F max measure of 43.70. Precision Recall curves for our detectors along with the 3 points of operation from <ref type="bibr" target="#b28">[29]</ref> are in the supplementary material <ref type="bibr" target="#b18">[19]</ref>. <ref type="figure">Fig. 5</ref>. Output of our system: We visualize some true positives (column one, two and three) and false positives (columns four and five) from our bed, chair, lamp, sofa and toilet object detectors. We also overlay the instance segmentation that we infer for each of our detections. Some of the false positives due to mis-localization are fixed by the instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Result visualizations:</head><p>We show some of the top scoring true positives and the top scoring false positives for our bed, chair, lamp, sofa and toilet detectors in <ref type="figure">Figure 5</ref>. More figures can be found in the supplementary material <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Instance Segmentation</head><p>In this section, we study the task of instance segmentation as proposed in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">37]</ref>. Our goal is to associate a pixel mask to each detection produced by our RGB-D object detector. We formulate mask prediction as a two-class labeling problem (foreground versus background) on the pixels within each detection window. Our proposed method classifies each detection window pixel with a random forest classifier and then smoothes the predictions by averaging them over superpixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Training</head><p>Learning framework: To train our random forest classifier, we associate each ground truth instance in the train set with a detection from our detector. We select the best scoring detection that overlaps the ground truth bounding box by more than 70%. For each selected detection, we warp the enclosed portion of the associated ground truth mask to a 50 ? 50 grid. Each of these 2500 locations (per detection) serves as a training point.</p><p>We could train a single, monolithic classifier to process all 2500 locations or train a different classifier for each of the 2500 locations in the warped mask. The first option requires a highly non-linear classifier, while the second option suffers from data scarcity. We opt for the first option and work with random forests <ref type="bibr" target="#b4">[5]</ref>, which naturally deal with multi-modal data and have been shown to work well with the set of features we have designed <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32]</ref>. We adapt the open source random forest implementation in <ref type="bibr" target="#b7">[8]</ref> to allow training and testing with on-the-fly feature computation. Our forests have ten decision trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features:</head><p>We compute a set of feature channels at each pixel in the original image (listed in supplementary material <ref type="bibr" target="#b18">[19]</ref>). For each detection, we crop and warp the feature image to obtain features at each of the 50?50 detection window locations. The questions asked by our decision tree split nodes are similar to those in Shotton et al. <ref type="bibr" target="#b31">[32]</ref>, which generalize those originally proposed by Geman et al. <ref type="bibr" target="#b14">[15]</ref>. Specifically, we use two question types: unary questions obtained by thresholding the value in a channel relative to the location of a point, and binary questions obtained by thresholding the difference between two values, at different relative positions, in a particular channel. Shotton et al. <ref type="bibr" target="#b31">[32]</ref> scale their offsets by the depth of the point to classify. We find that depth scaling is unnecessary after warping each instance to a fixed size and scale.</p><p>Testing: During testing, we work with the top 5000 detections for each category (and 10000 for the chairs category, this gives us enough detections to get to 10% or lower precision). For each detection we compute features and pass them through the random forest to obtain a 50 ? 50 foreground confidence map. We unwarp these confidence maps back to the original detection window and accumulate the per pixel predictions over superpixels. We select a threshold on the soft mask by optimizing performance on the val set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>To evaluate instance segmentation performance we use the region detection average precision AP r metric (with a threshold of 0.5) as proposed in <ref type="bibr" target="#b19">[20]</ref>, which extends the average precision metric used for bounding box detection by replacing bounding box overlap with region overlap (intersection over union). Note that this metric captures more information than the semantic segmentation metric as it respects the notion of instances, which is a goal of this paper.</p><p>We report the performance of our system in <ref type="table" target="#tab_2">Table 3</ref>. We compare against three baseline methods: 1) box where we simply assume the mask to be the box for the detection and project it to superpixels, 2) region where we average the region proposals that resulted in the detected bounding box and project this to superpixels, and 3) fg mask where we compute an empirical mask from the set of ground truth masks corresponding to the detection associated with each ground truth instance in the training set. We see that our approach outperforms all the baselines and we obtain a mean AP r of 32.1% as compared to 28.1% for the best baseline. The effectiveness of our instance segmentor is further demonstrated by the fact that for some categories the AP r is better than AP b , indicating that our instance segmentor was able to correct some of the mis-localized detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Semantic Segmentation</head><p>Semantic segmentation is the problem of labeling an image with the correct category label at each pixel. There are multiple ways to approach this problem, like that of doing a bottom-up segmentation and classifying the resulting superpixels <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31]</ref> or modeling contextual relationships among pixels and superpixels <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Here, we extend our approach from <ref type="bibr" target="#b17">[18]</ref>, which produces state-of-the-art results on this task, and investigate the use of our object detectors in the pipeline of computing features for superpixels to classify them. In particular, we design a set of features on the superpixel, based on the detections of the various categories which overlap with the superpixel, and use them in addition to the features preposed in <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results</head><p>We report our semantic segmentation performance in <ref type="table">Table 4</ref>. We use the same metrics as <ref type="bibr" target="#b17">[18]</ref>, the frequency weighted average Jaccard Index f wavacc 3 , but also report other metrics namely the average Jaccard Index (avacc) and average Jaccard Index for categories for which we added the object detectors (avacc* ). <ref type="table">Table 4</ref>. Performance on the 40 class semantic segmentation task as proposed by <ref type="bibr" target="#b17">[18]</ref>: We report the pixel-wise Jaccard index for each of the 40 categories. We compare against 4 baselines: previous approaches from <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b17">[18]</ref> (first three rows), and the approach in <ref type="bibr" target="#b17">[18]</ref> augmented with features from RGBD-DPMs ([18]+DPM) (fourth row). Our approach obtains the best performance fwavacc of 47%. There is an even larger improvement for the categories for which we added our object detector features, where the average performance avacc* goes up from 28.4 to 35.1. Categories for which we added detectors are shaded in gray (avacc* is the average for categories with detectors). As a baseline we consider <ref type="bibr" target="#b17">[18]</ref> + DPM, where we replace our detectors with RGBD-DPM detectors as introduced in Section 3.3. We observe that there is an increase in performance by adding features from DPM object detectors over the approach of <ref type="bibr" target="#b17">[18]</ref>, and the fwavacc goes up from 45.2 to 45.6, and further increase to 47.0 on adding our detectors. The quality of our detectors is brought out further when we consider the performance on just the categories for which we added object detectors which on average goes up from 28.4% to 35.1%. This 24% relative improvement is much larger than the boost obtained by adding RGBD-DPM detectors (31.0% only a 9% relative improvement over 28.4%).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(Gupta et al. CVPR [RGBD]) and Doll?r et al. (SE [RGBD])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Qualitative comparison of contours: Top row: color image, contours from<ref type="bibr" target="#b8">[9]</ref>, bottom row: contours from<ref type="bibr" target="#b17">[18]</ref> and contours from our proposed contour detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>15) gPb?ucm [RGB] (65.77) Silberman et al. [RGBD] (68.66) Gupta et al. CVPR [RGBD] (68.45) SE [RGBD] (70.25) Our(SE + all cues) [RGBD] (69.46) SE+SH [RGBD] (71.03) Our(SE+SH + all cues) [RGBD] Precision-recall curve on boundaries on the NYUD2 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Region Proposal Quality: Coverage as a function of the number of region proposal per image for 2 sets of categories:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>from Gupta et al. Number of candidates Coverage (Average Jaccard Index over Classes) Lin et al. NMS [RGBD] Lin et al. All [RGBD] MCG (RGB edges, RGB feats.) [RGB] MCG (RGBD edges, RGB feats.) [RGBD] Our (MCG (RGBD edges, RGBD feats.)) [RGBD]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>from Lin et al. Number of candidates Coverage (Average Jaccard Index over Classes) Lin et al. NMS [RGBD] Lin et al. All [RGBD] MCG (RGB edges, RGB feats.) [RGB] MCG (RGBD edges, RGB feats.) [RGBD] Our (MCG (RGBD edges, RGBD feats.)) [RGBD]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Segmentation benchmarks on NYUD2. All numbers are percentages.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">ODS (Fmax) OIS (Fmax) AP</cell></row><row><cell>gPb-ucm</cell><cell>RGB</cell><cell>63.15</cell><cell>66.12</cell><cell>56.20</cell></row><row><cell>Silberman et al. [34]</cell><cell>RGB-D</cell><cell>65.77</cell><cell>66.06</cell><cell>-</cell></row><row><cell>Gupta et al. CVPR [18]</cell><cell>RGB-D</cell><cell>68.66</cell><cell>71.57</cell><cell>62.91</cell></row><row><cell>SE [9]</cell><cell>RGB-D</cell><cell>68.45</cell><cell>69.92</cell><cell>67.93</cell></row><row><cell cols="2">Our(SE + normal gradients) RGB-D</cell><cell>69.55</cell><cell>70.89</cell><cell>69.32</cell></row><row><cell>Our(SE + all cues)</cell><cell>RGB-D</cell><cell>70.25</cell><cell>71.59</cell><cell>69.28</cell></row><row><cell>SE+SH [10]</cell><cell>RGB-D</cell><cell>69.46</cell><cell>70.84</cell><cell>71.88</cell></row><row><cell>Our(SE+SH + all cues)</cell><cell>RGB-D</cell><cell>71.03</cell><cell>72.33</cell><cell>73.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Control experiments for object detection on NYUD2 val set. We investigate a variety of ways to encode the depth image for use in a CNN for feature learning. Results are AP as percentages. See Section 3.2.</figDesc><table><row><cell>A</cell><cell>B</cell><cell>C</cell><cell>D</cell><cell>E</cell><cell>F</cell><cell>G</cell><cell>H</cell><cell>I</cell><cell>J</cell><cell>K</cell><cell>L</cell></row><row><cell cols="5">DPM DPM CNN CNN CNN</cell><cell cols="6">CNN CNN CNN CNN CNN CNN</cell><cell>CNN</cell></row><row><cell>finetuned?</cell><cell></cell><cell cols="2">no yes</cell><cell>no</cell><cell>yes</cell><cell cols="5">yes yes yes yes yes</cell><cell>yes</cell></row><row><cell cols="12">input channels RGB RGBD RGB RGB disparity disparity HHA HHA HHA HHA HHA RGB+HHA</cell></row><row><cell>synthetic data?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">2x 15x 2x</cell><cell>2x</cell><cell>2x</cell></row><row><cell>CNN layer</cell><cell></cell><cell cols="2">fc6 fc6</cell><cell>fc6</cell><cell>fc6</cell><cell cols="5">fc6 fc6 fc6 pool5 fc7</cell><cell>fc6</cell></row><row><cell cols="2">bathtub 0.1 12.2</cell><cell cols="2">4.9 5.5</cell><cell>3.5</cell><cell>6.1</cell><cell cols="5">20.4 20.7 20.7 11.1 19.9</cell><cell>22.9</cell></row><row><cell cols="4">bed 21.2 56.6 44.4 52.6</cell><cell>46.5</cell><cell>63.2</cell><cell cols="5">60.6 67.2 67.8 61.0 62.2</cell><cell>66.5</cell></row><row><cell>bookshelf 3.4</cell><cell cols="3">6.3 13.8 19.5</cell><cell>14.2</cell><cell>16.3</cell><cell cols="5">20.7 18.6 16.5 20.6 18.1</cell><cell>21.8</cell></row><row><cell>box 0.1</cell><cell>0.5</cell><cell cols="2">1.3 1.0</cell><cell>0.4</cell><cell>0.4</cell><cell cols="5">0.9 1.4 1.0 1.0 1.1</cell><cell>3.0</cell></row><row><cell cols="4">chair 6.6 22.5 21.4 24.6</cell><cell>23.8</cell><cell>36.1</cell><cell cols="5">38.7 38.2 35.2 32.6 37.4</cell><cell>40.8</cell></row><row><cell cols="4">counter 2.7 14.9 20.7 20.3</cell><cell>18.5</cell><cell>32.8</cell><cell cols="5">32.4 33.6 36.3 24.1 35.0</cell><cell>37.6</cell></row><row><cell>desk 0.7</cell><cell>2.3</cell><cell cols="2">2.8 6.7</cell><cell>1.8</cell><cell>3.1</cell><cell cols="5">5.0 5.1 7.8 4.2 5.4</cell><cell>10.2</cell></row><row><cell>door 1.0</cell><cell cols="3">4.7 10.6 14.1</cell><cell>0.9</cell><cell>2.3</cell><cell cols="5">3.8 3.7 3.4 2.8 3.3</cell><cell>20.5</cell></row><row><cell cols="4">dresser 1.9 23.2 11.2 16.2</cell><cell>3.7</cell><cell>5.7</cell><cell cols="5">18.4 18.9 26.3 13.1 24.7</cell><cell>26.2</cell></row><row><cell cols="4">garbage-bin 8.0 26.6 17.4 17.8</cell><cell>2.4</cell><cell>12.7</cell><cell cols="5">26.9 29.1 16.4 21.4 25.3</cell><cell>37.6</cell></row><row><cell cols="4">lamp 16.7 25.9 13.1 12.0</cell><cell>10.5</cell><cell>21.3</cell><cell cols="5">24.5 26.5 23.6 22.3 23.2</cell><cell>29.3</cell></row><row><cell cols="4">monitor 27.4 27.6 24.8 32.6</cell><cell>0.4</cell><cell>5.0</cell><cell cols="5">11.5 14.0 12.3 17.7 13.5</cell><cell>43.4</cell></row><row><cell cols="2">night-stand 7.9 16.5</cell><cell cols="2">9.0 18.1</cell><cell>3.9</cell><cell>19.1</cell><cell cols="5">25.2 27.3 22.1 25.9 27.8</cell><cell>39.5</cell></row><row><cell cols="2">pillow 2.6 21.1</cell><cell cols="2">6.6 10.7</cell><cell>3.8</cell><cell>23.4</cell><cell cols="5">35.0 32.2 30.7 31.1 31.2</cell><cell>37.4</cell></row><row><cell cols="4">sink 7.9 36.1 19.1 6.8</cell><cell>20.0</cell><cell>28.5</cell><cell cols="5">30.2 22.7 24.9 18.9 23.0</cell><cell>24.2</cell></row><row><cell cols="4">sofa 4.3 28.4 15.5 21.6</cell><cell>7.6</cell><cell>17.3</cell><cell cols="5">36.3 37.5 39.0 30.2 34.3</cell><cell>42.8</cell></row><row><cell cols="2">table 5.3 14.2</cell><cell cols="2">6.9 10.0</cell><cell>12.0</cell><cell>18.0</cell><cell cols="5">18.8 22.0 22.6 21.0 22.8</cell><cell>24.3</cell></row><row><cell cols="4">television 16.2 23.5 29.1 31.6</cell><cell>9.7</cell><cell>14.7</cell><cell cols="5">18.4 23.4 26.3 18.9 22.9</cell><cell>37.2</cell></row><row><cell cols="4">toilet 25.1 48.3 39.6 52.0</cell><cell>31.2</cell><cell>55.7</cell><cell cols="5">51.4 54.2 52.6 38.4 48.8</cell><cell>53.0</cell></row><row><cell cols="4">mean 8.4 21.7 16.4 19.7</cell><cell>11.3</cell><cell>20.1</cell><cell cols="5">25.2 26.1 25.6 21.9 25.3</cell><cell>32.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Test set results for detection and instance segmentation on NYUD2: First four rows correspond to box detection average precision, AP b , and we compare against three baselines: RGB DPMs, RGBD-DPMs, and RGB R-CNN. The last four lines correspond to region detection average precision, AP r . See Section 3.3 and Section 4.2. mean bath bed book box chair count-desk door dress-garba-lamp monit-night pillow sink sofa table tele toilet DPM 23.9 19.3 56.0 17.5 0.6 23.5 24.0 6.2 9.5 16.4 26.7 26.7 34.9 32.6 20.7 22.8 34.2 17.2 19.5 45.1 RGB R-CNN 22.5 16.9 45.3 28.5 0.7 25.9 30.4 9.7 16.3 18.9 15.7 27.9 32.5 17.0 11.1 16.6 29.4 12.7 27.4 44.1 Our 37.3 44.4 71.0 32.9 1.4 43.3 44.0 15.1 24.5 30.4 39.4 36.5 52.6 40.0 34.8 36.1 53.9 24.4 37.5 46.8 18.9 66.1 10.2 1.5 35.5 32.8 10.2 22.8 33.7 38.3 35.5 53.3 42.7 31.5 34.4 40.7 14.3 37.4 50.5</figDesc><table><row><cell></cell><cell>tub</cell><cell>shelf</cell><cell>-er</cell><cell cols="2">-er -ge bin</cell><cell>-or stand</cell><cell>vision</cell></row><row><cell cols="5">RGB DPM 9.0 0.9 27.6 9.0 0.1 7.8 7.3 0.7 2.5 1.4</cell><cell cols="2">6.6 22.2 10.0</cell><cell>9.2</cell><cell>4.3 5.9 9.4 5.5 5.8 34.4</cell></row><row><cell>RGBD-box</cell><cell cols="5">14.0 5.9 40.0 4.1 0.7 5.5 0.5 3.2 14.5 26.9 32.9</cell><cell>1.2 40.2 11.1</cell><cell>6.1 9.4 13.6 2.6 35.1 11.9</cell></row><row><cell>region</cell><cell cols="6">28.1 32.4 54.9 9.4 1.1 27.0 21.4 8.9 20.3 29.0 37.1 26.3 48.3 38.6 33.1 30.9 30.5 10.2 33.7 39.9</cell></row><row><cell>fg mask</cell><cell cols="6">28.0 14.7 59.9 8.9 1.3 29.2 5.4 7.2 22.6 33.2 38.1 31.2 54.8 39.4 32.1 32.0 36.2 11.2 37.4 37.5</cell></row><row><cell>Our</cell><cell>32.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Doll?r et al.<ref type="bibr" target="#b9">[10]</ref> recently introduced an extension of their algorithm and report performance improvements (SE+SH[RGBD] dashed red curve inFigure 3). We can also use our cues with<ref type="bibr" target="#b9">[10]</ref>, and observe an analogous improvement in performance (Our(SE+SH + all cues) [RGBD] dashed blue curve inFigure 3). For the rest of the paper we use the Our(SE+all cues)[RGBD] version of our contour detector.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Wang et al.<ref type="bibr" target="#b37">[38]</ref> report impressive detection results on NYUD2, however we are unable to compare directly with their method because they use a non-standard train-</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We calculate the pixel-wise intersection over union for each class independently as in the PASCAL VOC semantic segmentation challenge and then compute an average of these category-wise IoU numbers weighted by the pixel frequency of these categories.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements : This work was sponsored by ONR SMARTS MURI N00014-09-1-1051, ONR MURI N00014-10-1-0933 and a Berkeley Fellowship. The GPUs used in this research were generously donated by the NVIDIA Corporation. We are also thankful to Bharath Hariharan, for all the useful discussions. We also thank Piotr Doll?r for helping us with their contour detection code.</p><p>Learning Rich Features from RGB-D Images for Detection and Segmentation</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">CPMC-3D-O2P: Semantic segmentation of RGB-D images using CPMC and second order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Banica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<idno>abs/1312.7715</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised Feature Learning for RGB-D Based Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ISER</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Indoor semantic segmentation using depth information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1301.3572</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<ptr target="http://www.image-net.org/challenges/LSVRC/2012/" />
		<title level="m">ImageNet Large Scale Visual Recognition Competition 2012 (ILSVRC2012)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Piotr&apos;s Image and Video Matlab Toolbox (PMT)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<ptr target="http://vision.ucsd.edu/~pdollar/toolbox/doc/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fast edge detection using structured forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1406.5549</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMRL</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Joint induction of shape features and tree classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Support surface prediction in indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Perceptual organization and recognition of indoor scenes from RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning rich features from RGB-D images for object detection and segmentation : Supplementary material</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<ptr target="http://www.cs.berkeley.edu/~sgupta/pdf/rcnn-depth-supp.pdf" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A category-level 3d object dataset: Putting the kinect to work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Janoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Consumer Depth Cameras for Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://caffe.berkeleyvision.org/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Accurate localization of 3d objects from RGB-D data using segmentation hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Soo Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semantic labeling of 3d point clouds for indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A large-scale hierarchical multi-view rgb-d object dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICRA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Sketch tokens: A learned mid-level representation for contour and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Holistic scene understanding for 3D object detection with RGBD cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Discriminatively trained sparse code gradients for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">RGB-(D) scene labeling: Features and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Building part-based object detectors via 3d geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Convolutionalrecursive deep learning for 3d object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Bath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Histogram of oriented normal vectors for object recognition with a depth sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Skubic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Scene parsing with object instances and occlusion ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning structured hough voting for joint object detection and occlusion reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Object Detection in RGB-D Indoor Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Ye</surname></persName>
		</author>
		<ptr target="http://www.eecs.berkeley.edu/Pubs/TechRpts/2013/EECS-2013-3.html" />
		<imprint>
			<date type="published" when="2013-01" />
			<pubPlace>University of California, Berkeley</pubPlace>
		</imprint>
	</monogr>
	<note>Master&apos;s thesis, EECS Department</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
