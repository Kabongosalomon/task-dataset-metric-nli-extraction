<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MSeg: A Composite Dataset for Multi-domain Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lambert</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
						</author>
						<title level="a" type="main">MSeg: A Composite Dataset for Multi-domain Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-robust vision</term>
					<term>semantic segmentation</term>
					<term>instance segmentation</term>
					<term>panoptic segmentation</term>
					<term>domain generalization !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present MSeg, a composite dataset that unifies semantic segmentation datasets from different domains. A naive merge of the constituent datasets yields poor performance due to inconsistent taxonomies and annotation practices. We reconcile the taxonomies and bring the pixel-level annotations into alignment by relabeling more than 220,000 object masks in more than 80,000 images, requiring more than 1.34 years of collective annotator effort. The resulting composite dataset enables training a single semantic segmentation model that functions effectively across domains and generalizes to datasets that were not seen during training. We adopt zero-shot cross-dataset transfer as a benchmark to systematically evaluate a model's robustness and show that MSeg training yields substantially more robust models in comparison to training on individual datasets or naive mixing of datasets without the presented contributions. A model trained on MSeg ranks first on the WildDash-v1 leaderboard for robust semantic segmentation, with no exposure to WildDash data during training. We evaluate our models in the 2020 Robust Vision Challenge (RVC) as an extreme generalization experiment. MSeg training sets include only three of the seven datasets in the RVC; more importantly, the evaluation taxonomy of RVC is different and more detailed. Surprisingly, our model shows competitive performance and ranks second. To evaluate how close we are to the grand aim of robust, efficient, and complete scene understanding, we go beyond semantic segmentation by training instance segmentation and panoptic segmentation models using our dataset. Moreover, we also evaluate various engineering design decisions and metrics, including resolution and computational efficiency. Although our models are far from this grand aim, our comprehensive evaluation is crucial for progress. Moreover, we share all the models and code with the community.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>When Papert first proposed computer vision as a summer project in 1966 <ref type="bibr" target="#b0">[1]</ref>, he described the primary objective as "...a system of programs which will divide a vidisector picture into regions such as likely objects, likely background areas and chaos." Five decades later, computer vision is a thriving engineering field, and the task described by Papert is known as semantic segmentation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>Have we delivered on Papert's objective? A cursory examination of the literature would suggest that we have. Hundreds of papers are published every year that report ever-higher accuracy on semantic segmentation benchmarks such as Cityscapes <ref type="bibr" target="#b7">[8]</ref>, Mapillary <ref type="bibr" target="#b8">[9]</ref>, COCO <ref type="bibr" target="#b9">[10]</ref>, ADE20K <ref type="bibr" target="#b10">[11]</ref>, and others. Yet a simple exercise can show that the mission has not been accomplished. Take a camera and begin recording as you traverse a sequence of environments: for example, going about your house to pack some supplies, getting into the car, driving through your city to a forest on the outskirts, and going on a hike. Now perform semantic segmentation on the recorded video. Is there a model that will successfully perform this task?</p><p>A computer vision professional will likely resort to multiple models, each trained on a different dataset. Perhaps a model trained on the NYU dataset for the indoor portion <ref type="bibr" target="#b11">[12]</ref>, a model trained on Mapillary for the driving portion, and a model trained on ADE20K for the hike. Yet this is not a satisfactory state of affairs. It burdens practitioners with developing multiple models and implementing a controller that decides which model should be used at any given time. It also indicates that we haven't yet arrived at a satisfactory vision system: after all, an animal can traverse the same environments with a single visual apparatus that continues to perform its perceptual duties throughout.</p><p>A natural solution is to train a model on multiple datasets, hoping that the result will perform as well as the best dedicated model in any given environment. As has previously been observed, and confirmed in our experiments, the results are far from satisfactory. A key underlying issue is that different datasets have different taxonomies: that is, they have different definitions of what constitutes a 'category' or 'class' of visual entities. Taxonomic clashes and inconsistent annotation practices across datasets from different domains (e.g., indoor and outdoor, urban and natural, domain-specific and domain-agnostic) substantially reduce the accuracy of models trained on multiple datasets.</p><p>In this paper, we take steps towards addressing these issues. We present MSeg, a composite dataset that unifies semantic segmentation datasets from different domains: COCO <ref type="bibr" target="#b9">[10]</ref>, ADE20K <ref type="bibr" target="#b10">[11]</ref>, Mapillary <ref type="bibr" target="#b8">[9]</ref>, IDD <ref type="bibr" target="#b12">[13]</ref>, BDD <ref type="bibr" target="#b13">[14]</ref>, Cityscapes <ref type="bibr" target="#b7">[8]</ref>, and SUN RGB-D <ref type="bibr" target="#b14">[15]</ref>. A naive merge of the taxonomies of the seven datasets would yield more than 300 classes, with substantial internal inconsistency in definitions. Instead, we reconcile the taxonomies, merging and splitting classes to arrive at a unified taxonomy with 194 categories. To bring the pixel-level annotations in conformance with the unified taxonomy, we conduct a largescale annotation via the Mechanical Turk platform and produce compatible annotations across datasets by relabeling object masks.</p><p>The resulting composite dataset enables training unified semantic segmentation models that come to a step closer to delivering on Papert's vision. MSeg training yields models that exhibit much better generalization to datasets that were not seen during This enables training models that perform consistently across domains and generalize better. Input images in this figure were taken from datasets that were not seen during training.</p><p>training. We adopt zero-shot cross-dataset transfer as a proxy for a model's expected performance in the "real world" <ref type="bibr" target="#b15">[16]</ref>. We train models on MSeg and test on datasets that are disjoint from MSeg. In this mode, MSeg training is substantially more robust than training on individual datasets or training on multiple datasets without the reported taxonomic reconciliation.</p><p>Accurate semantic segmentation is typically not enough for a typical computer vision practitioner as it lacks any sense of an object. Instance segmentation extends semantic segmentation with finding semantically meaningful instances, and panoptic segmentation <ref type="bibr" target="#b16">[17]</ref> combines instance segmentation with semantic segmentation. Similarly, we evaluate the capabilities of MSeg dataset for these problems as the component training datasets have labels for the instances. Moreover, the practical applications typically come with computational efficiency requirements. For this purpose, we further evaluate the computational efficiency of our models at various resolutions to understand the computational efficiency and accuracy trade-offs. Although the results of these experiments are still far from the ultimate requirements of the practitioners, it gives a complete understanding of the MSeg dataset and models. Moreover, we share all trained models, data, and source code with the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Cross-domain semantic segmentation</head><p>Annotation for semantic segmentation is an exceptionally costly process as pixel-level annotations are needed. Utilizing multiple or additional existing semantic segmentation datasets is a sensible direction to ease this cost and effectively increase the dataset size. Mixing segmentation datasets has primarily been done within a single domain and application, such as driving. Ros et al. <ref type="bibr" target="#b17">[18]</ref> aggregated six driving datasets to create the MDRS3 meta-dataset, with approximately 30K images in total. Bevandic et al. <ref type="bibr" target="#b18">[19]</ref> mix Mapillary Vistas, Cityscapes, the WildDash validation set, and ImageNet-1K-BB (a subset of ImageNet <ref type="bibr" target="#b19">[20]</ref> for which bounding box annotations are available) for joint segmentation and outlier detection on WildDash <ref type="bibr" target="#b20">[21]</ref>. On a smaller scale, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> mix Mapillary, Cityscapes, and the German Traffic Sign Detection Benchmark. In contrast to these works, we focus on semantic segmentation across multiple domains and resolve inconsistencies between datasets at a deeper level, including relabeling incompatible annotations.</p><p>Varma et al. <ref type="bibr" target="#b12">[13]</ref> evaluate the transfer performance of semantic segmentation datasets for driving. They only use 16 common classes, without any dataset mixing. They observe that crossdataset transfer is significantly inferior to "self-training" (i.e., training on the target dataset). We have observed the same outcomes when models are trained on individual datasets, or when datasets are mixed naively.</p><p>Liang et al. <ref type="bibr" target="#b23">[24]</ref> train a model by mixing Cityscapes, ADE20K, COCO Stuff, and Mapillary, but do not evaluate cross-dataset generalization. Kalluri et al. <ref type="bibr" target="#b24">[25]</ref> mix pairs of datasets (Cityscapes + CamVid, Cityscapes + IDD, Cityscapes + SUN RGB-D) for semi-supervised learning.</p><p>An underlying issue that impedes progress on unified semantic segmentation is the incompatibility of dataset taxonomies. In contrast to the aforementioned attempts, we directly address this issue by deriving a consistent taxonomy that bridges datasets from multiple domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Domain adaptation and generalization</head><p>Training datasets are biased and deployment in the real world presents the trained models with data that is unlike what had been seen during training <ref type="bibr" target="#b25">[26]</ref>. This is known as covariate shift <ref type="bibr" target="#b26">[27]</ref> or selection bias <ref type="bibr" target="#b27">[28]</ref>, and can be tackled in the adaptation or the generalization setting. In adaptation, samples from the test distribution (deployment environment) are available during training, albeit without labels. In generalization, we expect models to generalize to previously unseen environments after being trained on data from multiple domains. An extensive collection of these methods have recently been summarized and benchmarked in various settings <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, and we refer the interested reader to these studies. We summarize the most relevant ones and discuss their relationship with our work.</p><p>We operate in the generalization mode and aim to train robust models that perform well in new environments, with no data from the target domain available during training. Many domain generalization approaches are based on the assumption that learning features that are invariant to the training domain will facilitate generalization to new domains <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Volpi et al. <ref type="bibr" target="#b32">[33]</ref> use distributionally robust optimization by considering domain difference as noise in the data distribution space. Bilen and Vedaldi <ref type="bibr" target="#b33">[34]</ref> propose to learn a unified representation and eliminate domainspecific scaling factors using instance normalization. Mancini et al. <ref type="bibr" target="#b31">[32]</ref> modify batch normalization statistics to make features and activations domain-invariant. Recently, the tools from causality has been applied to the problem of domain generalization <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. The causal relationship between images and their labels are expected to generalize to novel domains whereas environment specific relationships are expected to be anti-causal.</p><p>The aforementioned domain generalization methods assume that the same classifier can be applied in all environments. This relies on compatible definitions of visual categories which our work enables. Our work is complementary and can facilitate future research on domain generalization by providing a compatible taxonomy and consistent annotations across semantic segmentation datasets from different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visual learning over diverse domains</head><p>In order to study the problem of learning from multiple datasets, the Visual Domain Decathlon <ref type="bibr" target="#b36">[37]</ref> introduced a benchmark over ten image classification datasets. However, it allows training on all of them. More importantly, its purpose is not training a single classifier. Instead, they hope domains will assist each other by transferring inductive biases in a multi-task setting. Triantafillou et al. <ref type="bibr" target="#b37">[38]</ref> proposed a meta-dataset for benchmarking few-shot classification algorithms.</p><p>For the problem of monocular depth estimation, Ranftl et al. <ref type="bibr" target="#b15">[16]</ref> use multiple datasets and mix them via a multi-task learning framework. We are inspired by this work and aim to facilitate progress on dataset mixing and cross-dataset generalization in semantic segmentation. Unlike the work of Ranftl et al., which dealt with a geometric task (depth estimation), we are confronted with inconsistencies in semantic labeling across datasets, and make contributions towards resolving these.</p><p>A recent robust vision challenge <ref type="bibr" target="#b38">[39]</ref> addresses robustness and generalization by learning from multiple datasets. The challenge is using our dataset but in a few-shot generalization setting instead of zero-shot generalization. Although our models are trained in the zero-shot regime, the resulting performance is competitive with few-shot models showing our approach is a promising direction to develop general and robust vision models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Beyond Semantic Segmentation</head><p>Semantic segmentation divides an image into meaningful subregions; however, it fails to provide any information about instance and object properties. For example, a large crowd is segmented as people regardless of the number of people within the region. In  <ref type="bibr" target="#b10">[11]</ref> Everyday objects 22,210 MAPILLARY <ref type="bibr" target="#b8">[9]</ref> Driving (Worldwide) 20,000 IDD <ref type="bibr" target="#b12">[13]</ref> Driving (India) 7,974 BDD <ref type="bibr" target="#b13">[14]</ref> Driving (United States) 8,000 CITYSCAPES <ref type="bibr" target="#b7">[8]</ref> Driving (Germany) 3,475 SUN RGBD <ref type="bibr" target="#b14">[15]</ref> Indoor 5,285 Test PASCAL VOC <ref type="bibr" target="#b50">[51]</ref> Everyday objects 1,449 PASCAL CONTEXT <ref type="bibr" target="#b51">[52]</ref> Everyday objects 5,105 CAMVID <ref type="bibr" target="#b52">[53]</ref> Driving (U.K.) 101 WILDDASH-V1 1 <ref type="bibr" target="#b20">[21]</ref> Driving (Worldwide) 70 KITTI <ref type="bibr" target="#b53">[54]</ref> Driving (Germany) 200 SCANNET-20 <ref type="bibr" target="#b54">[55]</ref> Indoor <ref type="bibr" target="#b4">5,</ref><ref type="bibr">436</ref> order to overcome this limitation, object detection and segmentation have been jointly studied under the term of image parsing <ref type="bibr" target="#b39">[40]</ref>. This problem has further been studied using graphical models <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>.</p><p>Recently the problem of instance segmentation has gained interest. It segments an image into semantically meaningful instances <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>. However, it only consider things, not stuff <ref type="bibr" target="#b48">[49]</ref>. Panoptic segmentation <ref type="bibr" target="#b16">[17]</ref> is an emerging scene understanding task which jointly performs instance segmentation and semantic segmentation over both thing and stuff classes.</p><p>The vast majority of train datasets in our composite dataset include instance labels; hence, we extend our dataset to instance segmentation and panoptic segmentation. We show successful panoptic and instance segmentation results using our dataset, validating the usefulness of MSeg for these future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE MSEG DATASET</head><p>Our proposed dataset, MSeg, is a composite dataset as it is composed of multiple diverse datasets after a label unification process. We consider a large number of semantic segmentation datasets and select a subset to create MSeg. We list the semantic segmentation datasets used in MSeg in <ref type="table" target="#tab_0">Table 1</ref>. The datasets that were not used, and reasons for not including them, are listed in the <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Our guiding principle for selecting a training/test dataset split is that large, modern datasets are most useful for training, whereas older and smaller datasets are good candidates for testing. We test zero-shot cross-dataset performance on the validation subsets of these datasets. Note that the data from the test datasets (including their training splits) is never used for training in MSeg. For validation, we use the validation subsets of the training datasets listed in <ref type="table" target="#tab_0">Table 1</ref>. Next, we describe the dataset-specific choices we make.</p><p>We use the free, academic version of Mapillary Vistas <ref type="bibr" target="#b8">[9]</ref>. In this we forego highly detailed classification of traffic signs, traffic lights, and lane markings in favor of broader access to MSeg.</p><p>For COCO <ref type="bibr" target="#b9">[10]</ref>, we use the taxonomy of COCO Panoptic as a starting point, rather than COCO Stuff <ref type="bibr" target="#b49">[50]</ref>. The COCO Panoptic taxonomy merges some of the material-based classes of COCO Stuff into common categories that are more compatible with other We include only fully-manually labeled datasets in MSeg. ApolloScape uses a semi-automatic label generation pipeline. VIPER <ref type="bibr" target="#b56">[57]</ref> Driving (Synthetic) 134,097</p><p>We do not use synthetic data. INTERIORNET <ref type="bibr" target="#b57">[58]</ref> Indoor (Synthetic) 20,000,000</p><p>We do not use synthetic data. MATTERPORT3D <ref type="bibr" target="#b58">[59]</ref> Indoor 194,400 Annotated in 3D, rather than in 2D. STANFORD 2D-3D-S <ref type="bibr" target="#b59">[60]</ref> Indoor <ref type="bibr" target="#b69">70,</ref><ref type="bibr">496</ref> Data is semantically annotated on 3D point clouds, rather than in 2D; the dataset includes only 13 object classes, which is few for an indoor environment. OPENIMAGES V5 <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref> Everyday Objects 957,561 Provides only object segmentations (no segmentation of stuff categories). A2D2 <ref type="bibr" target="#b63">[64]</ref> Driving (Germany) 40,000</p><p>Was not available at time of paper preparation. MINC OPENSURFACES <ref type="bibr" target="#b64">[65]</ref> Materials Annotation categories are material-centric, instead of object-or stuffcentric. REPLICA DATASET <ref type="bibr" target="#b65">[66]</ref> Indoor (Synthetic) N/A Real data is not available (images are synthetically rendered).</p><p>datasets. (E.g., floor-marble, floor-other, and floor-tile are merged into floor.) Naively combining the component datasets yields roughly 200K images with 316 semantic classes (after merging classes with synonymous names). We found that training on naively combined datasets yields low accuracy and poor generalization. We believe the main cause for this failure is inconsistency in the taxonomies and annotation practices in the different datasets. The following subsections explain these issues and our solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Taxonomy</head><p>In order to train a cross-domain semantic segmentation model, we need a unified taxonomy. In order to unify the training taxonomies, we need to perform merging and splitting operations. Merging and splitting of classes from component datasets have different drawbacks. Merging is easy and can be performed programmatically, with no additional labeling. The disadvantage is that labeling effort that was invested into the original dataset is sacrificed and the resulting taxonomy has coarser granularity. Splitting, on the other hand, is labor-intensive. To split a class from a component dataset, all masks with that class need to be relabeled. This provides finer granularity for the resulting taxonomy, but costs time and labor.</p><p>In order to make these decisions, we consider two primary objectives. First, as many classes should be preserved as possible. Merging classes can reduce the discriminative ability of the resulting models. Second, the taxonomy should be flat, rather than hierarchical, to maximize compatibility with standard training methods. In order to satisfy these objectives, we followed a sequence of decision rules, summarized in <ref type="figure" target="#fig_1">Figure 2</ref>, to determine split and merge operations on taxonomies of the component datasets. Although the choice of classes in any taxonomy is somewhat arbitrary (e.g. a taxonomy for a fashion dataset versus a wildlife preservation dataset will be highly-task dependent), our decision tree allows us to make objective choices.</p><p>Given a query category, if a category with a similar or related name is found in any one of the other datasets' taxonomies, we manually inspect the image masks. If we determine the category meaning to be equivalent, (e.g. Mapillary "On Rails" and COCO-1. A second version of the benchmark, WildDash-v2, was introduced after the completion of our experiments and publication of our CVPR 2020 paper. Panoptic "train"'), we combine class masks under a common name.</p><p>If no similar or related name is found in any other dataset's taxonomy, we must inspect a large number of images from each such dataset in order to determine if the category is present, but simply unannotated or annotated differently. If a category is truly unique to this dataset (i.e. present in no other dataset), we add this class to the unified taxonomy (e.g. IDD's "auto-rickshaw" or COCO-Panoptic's "surfboard" or "tie").</p><p>In the case that a similar category name exists, but was not equivalent (e.g. COCO-Panoptic "person" and Mapillary "person"), or a category is present in another dataset's images, a more challenging task remains. In such a scenario, we seek to answer the question, is there a category in any other dataset that is a strict superset or subset of query category? If superset/subsets were to be merged, current boundaries must keep unrelated classes separate. If the answer is no, (e.g. Mapillary "manhole" vs. COCO-Panoptic's "road, or IDD's "non-drivable fallback" and Cityscapes "terrain"), we mark masks as "unlabeled." Given there is no hierarchical relationship, one would need to re-draw new boundaries (an endeavor which we do not undertake). However, if there is a category in another dataset that is a strict superset or subset of the query category (e.g. ADE20K "glass" and COCO-Panoptic's "wine glass"), there is a possibility to "shatter" the superset category. Shattering the superset must be enforceable to an MTurk worker, given a typical field of view, without drawing a new boundary (e.g. ADE20K "swivelchair" and COCO-Panoptic "chair" is possible since boundaries are shared, and the field of view generally accommodates this, unlike "building" vs. "house"). If some shattered masks do not fall into any pre-existing subset, we add an "-other" category. However, if such shattering is impossible without drawing a new boundary, (e.g. ADE20K's "flower", "tree", "palm" vs. Cityscapes "vegetation"), we merge both into the superset. If such shattering is impossible because the classification is not enforceable to a Mechanical Turk worker given a typical field-of-view (e.g. COCO-Panoptic's "dining table" and "table-merged"), we merge both into the superset category.</p><p>Following this process, we condensed the 316 classes obtained by merging the component datasets into a unified taxonomy of 194 classes. The full list and class statistics are given in <ref type="figure" target="#fig_2">Figure 3</ref> and further described and visualized in the appendix. Each of these classes is derived from classes in the component datasets. We also report the statistics of the number of relabeled masks in the appendix.</p><p>After this process, an MSeg category can have one of the following relationships to classes in a component dataset: (a) it can be in direct correspondence to a class in a component taxonomy, (b) it can be the result of merging a number of classes from a component taxonomy, (c) it can be the result of splitting a class in a component taxonomy (one-to-many mapping), or (d) it can be the union of classes which are split from different classes in the component taxonomy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relabeling Instances of Split Classes</head><p>We utilize Amazon Mechanical Turk (AMT) to relabel masks of classes that need to be split. We re-annotate only the datasets used for learning, leaving the evaluation datasets intact. Instead of recomputing boundaries, we formulate the problem as multiway classification and ask annotators to classify each mask into   finer-grained categories from the MSeg taxonomy. We include an example labeling screen, workflow and labeling validation process in the appendix In total, we split 31 classes and relabel 221,323 masks. We visualize some of these split operations in <ref type="figure" target="#fig_3">Figure 4</ref> and provide additional details in the appendix. AMT workers sometimes submit inaccurate, random, or even adversarial decisions <ref type="bibr" target="#b66">[67]</ref>. To ensure annotation quality, we embed 'sentinel' tasks within each batch of work <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>, constituting at least 10% of each batch. These sentinels are tasks for which the ground truth is unambiguous and is manually annotated by the authors. We use the sentinels to automatically evaluate the reliability of each annotator so that we can direct work towards more reliable annotators. In order to guarantee acceptable accuracy, five workers annotate each batch, and the work is resubmitted until all submitted batches meet a 100% sentinel accuracy. Afterwards, the category is determined by majority vote; categories that do not meet these criteria are manually labeled inhouse by an expert annotator (one of the authors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>We quantitatively and qualitatively study MSeg for learning multidomain scene understanding models. We start with discussing the implementation details in Section 4.1. Then, we evaluate the performance of semantic segmentation models trained using MSeg data and the impact of our universal taxonomy in Section 4.2. Finally, we study the practical multi-domain scene understanding problem beyond semantic segmentation. We train multi-domain semantic, instance, and panoptic segmentation models using larger compute and evaluate them in terms of accuracy and efficiency. We share the source code, trained models, and the dataset for full reproducibility 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>Our semantic segmentation experiments largely follow the protocol introduced by Zhao et al. <ref type="bibr" target="#b70">[71]</ref>. However, we used the HRNet-V2-W48 architecture <ref type="bibr" target="#b71">[72]</ref>, where W48 indicates the width of the high-resolution convolution. We use HRNet without Object Contextual Representations (OCR) and with synchronized BN. For instance segmentation, we use Mask R-CNN <ref type="bibr" target="#b72">[73]</ref> with a ResNeXt-101-32x8d-FPN <ref type="bibr" target="#b73">[74]</ref> backbone. For panoptic segmentation, we use Panoptic-FPN <ref type="bibr" target="#b74">[75]</ref> with a ResNet-101 backbone. We use 1080p resolution in our experiments with the crop size of 713 ? 713. When we further evaluate the impact of resolution, we use crop sizes of 473 ? 473 for 480p and 593 ? 593 for 720p.</p><p>We use SGD with momentum 0.9 and a weight decay of 10 ?4 . We use a polynomial learning rate with power 0.9. Base learning rate is set to 10 ?2 . For multi-scale inference, we use a multi-scale accumulation of probabilities, with scales of 0.5 to 1.75 with 0.25 increments.</p><p>When forming a minibatch of size m from multiple datasets, we evenly split the minibatch by the number of training datasets n, meaning each dataset will contribute m /n examples to each minibatch. Accordingly, there is no notion of "epoch" for the unified dataset during our training, but rather only total samples seen from each dataset. For example, in a single effectual "COCO epoch", Mapillary will complete more than 6 effectual epochs, as its dataset is less than <ref type="bibr">1 6</ref> th the size of COCO. For all of our quantitative studies, we train until one million crops from 2. Trained models, data, and source code: https://github.com/mseg-dataset each dataset's images have been seen. We further train models until three million crops have been seen to share them with the community. We use a mini-batch size m = 35.</p><p>Image resolution is inconsistent across component datasets. For example, Mapillary contains many images of resolution ?2000 ? 4000, while most ADE20K images have resolution ?300 ? 400. Before training, we use 2? or 3? super-resolution <ref type="bibr" target="#b75">[76]</ref> to first upsample the training datasets with lower resolution to a higher one (at least 1000p). At training time, we resize images from different datasets to a consistent resolution of 1080p. Before training, resizing images from different datasets to a canonical size is important: during training, a crop of a fixed size is fed to the network, and such crops could represent dramatically different relative portions of the image if images' sizes differ drastically. Specifically, in our experiments, we resize all images such that their shorter side is 1080 pixels (while preserving aspect ratios) and use a crop size of 713 ? 713px. At test time, we resize the image to one of three different resolutions (360/720/1080 as the images' shorter side), perform inference, and then interpolate the prediction maps back to the original resolutions for evaluation. The resolution level (360/720/1080) is set per dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Using the MSeg taxonomy on a held-out dataset</head><p>At inference time, at each pixel we obtain a vector of probabilities over the unified taxonomy's m u categories. These unified taxonomy probabilities must be allocated to test dataset taxonomy buckets. For example, we have three separate probabilities in our unified taxonomy for 'motorcyclist', 'bicyclist', and 'rider-other'. We sum these three together to compute a Cityscapes 'rider' probability. We implement this remapping from m u classes to m t classes as a linear mapping P from R mu to R mt . The weights P ij are binary 0/1 values and are fixed before training or evaluation; the weights are determined manually by inspecting label maps of the test datasets. P ij is set to 1 if unified taxonomy class j contributes to evaluation dataset class i, otherwise P ij = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Zero-shot Quantitative Results</head><p>We evaluate zero-shot generalization only for semantic segmentation since our test datasets typically do not have instance labels. We use the MSeg training set to train a unified semantic segmentation model using 1 million crops. <ref type="table" target="#tab_4">Table 3</ref> lists the results of zero-shot transfer of the model to MSeg test datasets. Note that none of these datasets were seen by the model during training. For comparison, we list the performance of corresponding models that were trained on a single training dataset, among the training datasets that were used to make up MSeg. For reference, we also list the performance of 'oracle' models that were trained on the training splits of the test datasets. Note that WildDash does not have a training set, thus no 'oracle' performance is provided for it.</p><p>The results in <ref type="table" target="#tab_4">Table 3</ref>   When compared with oracle models, training on MSeg is competitive with oracle models on many datasets, validating the importance of learning from a diverse collection of datasets. One interesting failure case is ScanNet <ref type="bibr" target="#b54">[55]</ref>, as it is especially difficult in a zero-shot, cross-dataset test regime because its scenes are captured while the photographer is in motion; an oracle model trained on this data learns to account for the motion blur, yielding a 13.8 point improvement over MSeg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">WildDash benchmark</head><p>The WildDash benchmark <ref type="bibr" target="#b20">[21]</ref> specifically evaluates the robustness of semantic segmentation models. Images mainly contain  road scenes with unusual and hazardous conditions (e.g., poor weather, noise, distortion). The benchmark is intended for testing the robustness of models trained on other datasets, and does not provide a training set of its own. A small set of 70 annotated images is provided for validation. The primary mode of evaluation is a leaderboard, with a testing server and a test set with hidden annotations. The main evaluation measure is Meta Average mIoU, which combines performance metrics associated with different hazards and per-frame IoU.</p><p>We submitted results from a model trained on MSeg to the WildDash test server, with multi-scale inference. Note that Wild-Dash is not among the MSeg training sets and the submitted model has never seen WildDash images during training. The results are reported in <ref type="table" target="#tab_5">Table 5</ref>. Our model is ranked 1st on the leaderboard for the WildDash-v1 Benchmark. Remarkably, our model outperforms methods that were trained on multiple datasets and utilized the WildDash validation set during training. In comparison to the best prior model that (like ours) did not leverage WildDash data during training, our model improves accuracy by 9.3 percentage points: a 24% relative improvement. On ScanNet, our model can accurately parse cabinets, kitchen counters, and kitchen sinks, whereas COCO cannot because COCO counter masks are not labeled accurately. Because we relabeled COCO counter masks, we are able to effectively use them for learning. In comparison, ADE20K models are also blind to counters and Mapillary-trained models completely fail in ScanNet's indoor regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Zero-Shot Qualitative Results</head><p>On KITTI, the ADE20K and COCO-trained models cannot recognize the sidewalk along the road edge under difficult illumination conditions, whereas the MSeg and Mapillary models can. On WildDash, our model is the only one to correctly identify trucks on both sides of a road, while the other models confuse them for buildings. On another WildDash image, our MSeg model alone can parse the road surface under snowy conditions; a Mapillary-trained model predicts the snow-covered road surface as sky, and a COCO-trained model predicts it as wall. <ref type="table" target="#tab_6">Table 4</ref> lists the accuracy of trained models on the MSeg training datasets. We test on the validation sets and compute IoU on a subset of classes that are jointly present in the dataset and MSeg's taxonomy. Except for Cityscapes and BDD100K, results on validation sets of all training datasets are not directly comparable to the literature since the MSeg taxonomy involves merging multiple classes. As expected, individually-trained models generally demonstrate good accuracy when tested on the same dataset: a model trained on COCO performs well on COCO, etc. Remarkably, the MSeg model matches or outperforms the respective individually-trained models across almost all datasets. On a number of training datasets, the MSeg-trained model attains higher accuracy than the individually-trained model, sometimes by a significant margin. The aggregate performance of the MSeg model is summarized by the harmonic mean across datasets. It is 76% higher than the best individually-trained baseline (COCO). <ref type="table" target="#tab_7">Table 6</ref> reports a controlled evaluation of two of our contributions: the unified taxonomy (Section 3.1) and compatible relabeling (Section 3.2). The 'Naive merge' baseline is a model trained on a composite dataset that uses a naively merged taxonomy in which the classes are a union of all training classes, and each test class is only mapped to an universal class if they share the same name. The 'MSeg (w/o relabeling)' baseline uses the unified MSeg taxonomy, but does not use the manually-relabeled data for split classes (Section 3.2). The model trained on the presented composite dataset ('MSeg') achieves better performance than the baselines. Using MSeg's unified taxonomy gives a large boost over the naive taxonomy, and relabeling the data further helps.    each submission must be a single, unified model. The RVC creates an aggregate rank for methods based on their rank per dataset on 7 pre-defined evaluation datasets <ref type="bibr" target="#b77">[78]</ref>. The competition winner, SwiftNet <ref type="bibr" target="#b76">[77]</ref>, was specifically trained on the 7 RVC test datasets (see <ref type="table" target="#tab_8">Table 7</ref>); as a result, SwiftNet suffered from no domain gap, whereas our MSeg model saw only 3 of the 7 datasets during training. For models that are trained and evaluated on the same datasets, we find MSeg outperforms SwiftNet in all cases except Mapillary Vistas as seen in <ref type="table" target="#tab_8">Table 7</ref>. For Mapillary, relative resolution of SwiftNet's taxonomy and our taxonomy are different. We merged 19 finegrained Mapillary classes into our universal classes, causing us to receive a score of zero when formulating a mapping backwards. In order to quantify the impact of this taxonomy difference, we re-compute the mIoU by ignoring these 19 classes in <ref type="table" target="#tab_10">Table 9</ref>. As seen from the table, when a matching taxonomy is used, our mIoU score is 5% higher than SwiftNet's. We visualize this taxonomy difference qualitatively in <ref type="figure">Figure 6</ref> for the WildDash-v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance on Training Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study for Relabeling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Practical Semantic Segmentation for Practitioners</head><p>Our main motivation behind MSeg is developing and sharing practical multi-domain models which practitioners can use without any additional training. In order to ease this adoption, we perform two crucial steps. Firstly, we go beyond the training duration of 1 million crops and obtain better models with longer training. Specifically, we train models until 3 million crops per dataset are seen. Secondly, we go beyond 1080p resolution, and train 480p and 720p resolution models. This is especially crucial for mobile applications where computation is scarcer. In order to understand the accuracy-computation trade-off, we analyze the accuracy and computational efficiency of these models. We tabulate these results in <ref type="table" target="#tab_9">Table 8</ref>.</p><p>One interesting observation is the superior zero-shot generalization of the 480p model in the test datasets. On the other hand, the higher resolution is helpful for performance in training datasets. Although this is promising for practitioners as they could use the low resolution models and obtain the best of both worlds, it is somewhat puzzling. Although explaining this discrepancy is vastly beyond the scope of this paper, we state a conjecture which might help explain this phenomenon. One promising perspective would be the distributional distances, as they are strongly related with the out-of-distribution generalization <ref type="bibr" target="#b78">[79]</ref>. It is reasonable to conjecture that the distribution of different training sets gets closer in a lower-resolution setting. As a supporting thought experiment, consider classifying an image into respective datasets (similar to <ref type="bibr" target="#b25">[26]</ref>). It would be harder to classify which dataset the image is coming from at a lower resolution.</p><p>In order to quantify the computational efficiency, we provide an analysis of the feedforward inference runtime of our semantic segmentation models in <ref type="table" target="#tab_0">Table 11</ref>. We average runtime over 100 single-scale forward passes. Realtime single-scale inference is possible on an NVIDIA RTX 2080 Ti desktop GPU. We only perform single-scale analysis as these experiments are performed on specially optimized single-scale inference code which we share with the community 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Going Beyond Semantic Segmentation</head><p>Semantic segmentation problem has no sense of an object or instance. Instance and panoptic segmentation extend this problem to introduce the concept of objects. Instance segmentation segments an image into semantically meaningful instances. However, it only considers object categories, not "stuff'. On the other hand, panoptic segmentation jointly performs instance segmentation and semantic segmentation.</p><p>We train instance and panoptic segmentation models using MSeg since the vast majority of training datasets have instance labels. However, we only evaluate these models on the MSeg training datasets since our test datasets typically do not have instance labels. During our training and evaluation, we exclude BDD and SUN RGBD because they do not provide instance labels. We use validation sets of these datasets in our evaluation. To differentiate between "thing" and "stuff" classes in the universal taxonomy, we deem the class to be a "thing" if all classes mapped to that universal class are "thing" classes in their original datasets. This step is necessary since instance segmentation detects and segments "thing" objects, while panoptic segmentation will also segment "stuff" pixels.</p><p>We evaluate instance segmentation performance by reporting box average precision and mask average precision in <ref type="table" target="#tab_0">Table 10</ref>    train and test distributions due to data augmentation following Touvron et al. <ref type="bibr" target="#b79">[80]</ref>. We further evaluate the resulting models in the joint instance and semantic segmentation task called panoptic segmentation. We use panoptic quality as a metric and tabulate the results in <ref type="table" target="#tab_0">Table 12</ref>. We also present qualitative results in <ref type="figure">Figure 7</ref>. Although all resolutions perform similarly in our quantitative analysis, high resolution models perform best. Moreover, the qualitative results in <ref type="figure">Figure 7</ref> are promising.</p><p>Although answering the question of whether MSeg-relabeling improves instance and panoptic segmentation is not directly possible due to the lack of instance labels in many datasets we use, the results presented in this section suggests that practitioners can use these models in domain-free sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We presented a composite dataset for multi-domain scene understanding. To construct the composite dataset, we reconciled the taxonomies of seven semantic segmentation datasets. In cases where categories needed to be split, we performed large-scale mask relabeling via the Mechanical Turk platform. We showed that the resulting composite dataset enables training a unified semantic segmentation model that delivers consistently high performance across domains. The trained model generalizes to previously unseen datasets and was ranked first on the WildDash leaderboard for robust semantic segmentation, with no exposure to WildDash data during training. Moreover, we submitted the resulting models to the Robust Vision Challange (RVC) without any additional training as an extreme generalization experiment. Our model performed competitively and ranked second. MSeg training sets only include three out of the seven datasets in the RVC, making this competitive performance surprising and promising. In order to guide practical deployment of our models, we also evaluated the role of resolution in the trade-off between computational efficiency and accuracy. Finally, we trained instance and panoptic segmentation models to go beyond semantic segmentation. We see the presented work as a step towards broader deployment of robust computer vision systems and hope that it will support future work on zero-shot generalization. Code, data, and trained models are available at https://github.com/mseg-dataset. Ozan Sener is a research scientist at Intel Labs. In 2017, he earned his Ph.D. degree from Cornell University. Between 2016 and 2018, he worked as a visiting Ph.D. student and later as a PostDoc at Stanford University. He is interested in developing machine learning methods and theory, with applications in computer vision and robotics. His past research focused on transfer and multi-task learning, derivative-free optimization, and unsupervised learning. James Hays is an associate professor of computing at Georgia Institute of Technology and a Principal Scientist at Argo AI. James received his Ph.D. from Carnegie Mellon University in 2009 and was a postdoc at Massachusetts Institute of Technology. James's research interests span computer vision, robotics, and machine learning. James is the recipient of the NSF CAREER award and Sloan Fellowship.</p><p>Vladlen Koltun is the Chief Scientist for Intelligent Systems at Intel. He directs the Intelligent Systems Lab, which conducts high-impact basic research in computer vision, machine learning, robotics, and related areas. He has mentored more than 50 PhD students, postdocs, research scientists, and PhD student interns, many of whom are now successful research leaders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>In this appendix, we provide additional details and experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELABELING INTERFACE AND WORKFLOW</head><p>In <ref type="figure" target="#fig_8">Figure 8</ref>, we provide a sample Amazon Mechanical Turk interface for three tasks: cabinet-merged mask splitting, curtain mask splitting, and animal mask splitting. We distribute human intelligence tasks (HITs) in batches of 100 masks at a time to each worker. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ADDITIONAL DETAILS ON MSEG</head><p>In <ref type="figure" target="#fig_1">Figure 2</ref> of the main text, we visualized the mapping between the meta-training set taxonomies and the MSeg taxonomy for subset of the classes. We visualize the remaining classes in <ref type="figure" target="#fig_0">Figures  9 &amp; 10</ref>. We also list the number of split and merge operations in <ref type="table" target="#tab_0">Table 13</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ADDITIONAL QUALITATIVE COMPARISON WITH SWIFTNET</head><p>In addition to the qualitative study on WildDash-v2 in the main paper, we qualitatively compare the segmentation performance of our method with SwiftNet on ADE20K in <ref type="figure" target="#fig_0">Figure 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">PANOPTIC CLASS MAPPING</head><p>Resolving Stuff-Thing Inconsistencies. As discussed in the main text, each of our training datasets has its own set of 'stuff'  and 'thing' classes and these inconsistencies must be resolved for unified training. This occurs because the delineation between 'stuff' and 'thing' classes can be ambiguous as times.</p><p>In <ref type="table" target="#tab_0">Table 21</ref>, we show the stuff vs. thing assignments in MSeg. As also discussed in the main text, to differentiate between "thing" and "stuff" classes in the universal taxonomy, we deem the class to be a "thing" if all classes mapped to that universal class are "thing" classes in their original datasets. If a class is considered 'stuff' in even a single dataset, this pollutes the class, and must be considered 'stuff' for all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">ADDITIONAL RE-LABELING FOR DENOISING</head><p>In our qualitative analysis of the datasets, we observed abundance of erroneous annotations for some specific classes in IDD <ref type="bibr" target="#b12">[13]</ref>. We added such an example error in <ref type="figure" target="#fig_0">Figure 12</ref> to visualize the issue. Hence, to solve these errors, we performed additional re-labelling for the IDD for rider, bicycle, and motorcycle classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Original Relabeled         The mask above is labeled as class 8 ("rider"), which we relabel to class 10 ("bicycle")</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>MSeg unifies multiple semantic segmentation datasets by reconciling their taxonomies and resolving incompatible annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Procedure for determining the set of categories in the MSeg taxonomy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Semantic classes in MSeg. Left: pixel counts of MSeg classes, in log scale. Right: percentage of pixels from each component dataset that contribute to each class. Any single dataset is insufficient for describing the visual world.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>visualizes these relationships for 40 classes. For example, the class 'person' in COCO and ADE20K corresponds to four classes ('person', 'rider-other', 'bicyclist', and 'motorcyclist') in the Mapillary dataset. Thus the 'person' labels in COCO and ADE20K need to be split into one of the aforementioned four Mapillary categories depending on the context. (See boxes COCO-E and ADE20K-D in Figure 4.) Mapillary is much more finegrained than other driving datasets and classifies Pothole, Parking, Road, Bike Lane, Service Lane, Crosswalk-Plain, Lane Marking-General, Lane Marking-Crosswalk separately. These classes are merged into a unified MSeg 'road' class. (See box Mapillary-C in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 :</head><label>4</label><figDesc>Visualization of a subset of the class mapping from each dataset to our unified taxonomy. This figure shows 40 of the 194 classes; see the appendix for the full list. Each filled circle means that a class with that name exists in the dataset, while an empty circle means that there is no pixel from that class in the dataset. A rectangle indicates that a split and/or merge operation was performed to map to the specified class in MSeg. Rectangles are zoomed-in in the right panel. Merge operations are shown with straight lines and split operations are shown with dashed lines. (Best seen in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5</head><label>5</label><figDesc>provides qualitative semantic segmentation results on images from different test datasets. Unlike the baselines, the MSeg model is successful in all domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Example Mechanical Turk Interface. We provide binary mask classification problems to MTurk annotators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :Fig. 10 :Fig. 12 :</head><label>91012</label><figDesc>Visualization of a subset of the class mapping from each dataset to our unified taxonomy. This figure shows the classes omitted in the main textFigure 2. Each filled circle means that a class with that name exists in the dataset, while an empty circle means that there is no pixel from that class in the dataset. A rectangle indicates that a split and/or merge operation was performed to map to the specified class in MSeg. Rectangles are zoomed-in in the below panel. Merge operations are shown with straight lines and split operations are shown with dashed lines. (Best seen in color.) Visualization of a subset of the class mapping from each dataset to our unified taxonomy. This figure shows the classes omitted in the main textFigure 2. Each filled circle means that a class with that name exists in the dataset, while an empty circle means that there is no pixel from that class in the dataset. (Best seen in color.) An example error from IDD<ref type="bibr" target="#b12">[13]</ref> segmentation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Component datasets in MSeg.</figDesc><table><row><cell>Dataset name</cell><cell>Origin domain</cell><cell># Images</cell></row><row><cell>Training &amp; Validation</cell><cell></cell><cell></cell></row><row><cell cols="2">COCO [10] + COCO STUFF [50] Everyday objects</cell><cell>123,287</cell></row><row><cell>ADE20K</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 :</head><label>2</label><figDesc>List of datasets we do not currently include in MSeg.</figDesc><table><row><cell>Dataset Name</cell><cell>Origin Domain</cell><cell># Images</cell><cell>Reason for Exclusion</cell></row><row><cell>Excluded Datasets</cell><cell></cell><cell></cell><cell></cell></row><row><cell>APOLLOSCAPE [56]</cell><cell>Driving (Worldwide)</cell><cell>147,000</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table Drawers Table Bath</head><label>DrawersBath</label><figDesc></figDesc><table><row><cell></cell><cell>Building Phone Booth</cell><cell>Building</cell><cell>Pothole Parking</cell><cell></cell></row><row><cell>Bag Tunnel</cell><cell>Sidewalk Curb Curb Cut Pedestrian Area</cell><cell>Pavement</cell><cell>Road Bike Lane Service Lane Crosswalk Lane Marking</cell><cell>Road Road</cell></row><row><cell>Building Television</cell><cell>Rug Floor -Wood Floor -Other</cell><cell>Floor Rug</cell><cell>Crosswalk</cell><cell>Armchair</cell></row><row><cell>Floor</cell><cell></cell><cell>Chandelier</cell><cell></cell><cell>Basket</cell></row><row><cell>Rug</cell><cell></cell><cell>Lamp</cell><cell></cell><cell>Bench</cell></row><row><cell>Armchair</cell><cell>Light</cell><cell>Sconce Streetlight</cell><cell></cell><cell>Chair-Other</cell></row><row><cell>Bench</cell><cell></cell><cell>Light-other</cell><cell></cell><cell>Ottoman</cell></row><row><cell>Chair-other</cell><cell>Pavement</cell><cell>Runway</cell><cell>Chair</cell><cell>Seat</cell></row><row><cell>Seat Ottoman</cell><cell>Road</cell><cell>Pavement -Sidewalk Road</cell><cell></cell><cell>Object Slow Wheeled</cell></row><row><cell>Stool</cell><cell></cell><cell>Non-rider</cell><cell></cell><cell>Stool</cell></row><row><cell>Swivelchair</cell><cell>Person</cell><cell>Bicyclist Motorcyclist</cell><cell></cell><cell>Swivel Chair</cell></row><row><cell>Sofa</cell><cell></cell><cell>Rider -other</cell><cell></cell><cell>. Counter</cell></row><row><cell>Chandelier Lamp Light-other Sconce Streetlight</cell><cell>Curtain Building Skyscraper Booth Hovel</cell><cell>Other Curtain Shower Curtain Building</cell><cell>Counter Cabinet Table Dining Table</cell><cell>Counter -other Kitchen Island Desk Nightstand Table Pool Table</cell></row><row><cell>Runway</cell><cell>Tower Grandstand</cell><cell></cell><cell>Sidewalk Path</cell><cell>Sidewalk</cell></row><row><cell>Pavement-sidewalk</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Road Snow</cell><cell>Person</cell><cell>Non-rider Bicyclist Motorcyclist</cell><cell>Mountain Hill</cell><cell>Snow Mountain</cell></row><row><cell></cell><cell></cell><cell>Rider -other</cell><cell></cell><cell></cell></row><row><cell>Mountain</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Person (non-rider)</cell><cell>Boat Ship</cell><cell>Boat &amp; Ship</cell><cell></cell><cell></cell></row><row><cell>Bicyclist Motorcyclist</cell><cell>Curtain</cell><cell>Other Curtain Shower Curtain</cell><cell></cell><cell></cell></row><row><cell>Rider -Other</cell><cell>Sidewalk</cell><cell></cell><cell>Road</cell><cell></cell></row><row><cell>Bicycle</cell><cell>Curb</cell><cell>Sidewalk</cell><cell>Parking</cell><cell>Road</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Drivable Fallback</cell><cell></cell></row><row><cell>Motorcycle</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Non-rider</cell></row><row><cell>Other -Curtain</cell><cell></cell><cell></cell><cell></cell><cell>Bicyclist</cell></row><row><cell>Shower-Curtain</cell><cell></cell><cell></cell><cell>Rider</cell><cell>Motorcyclist</cell></row><row><cell>Boat-Ship</cell><cell></cell><cell></cell><cell></cell><cell>Rider -other</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Bicycle</cell></row><row><cell>Bathroom counter</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Counter -other Kitchen Island</cell><cell></cell><cell>Armchair Bench Chair-other</cell><cell>Lamp</cell><cell>Chandelier Lamp Light -other</cell></row><row><cell>Desk</cell><cell></cell><cell>Ottoman</cell><cell></cell><cell>Sconce</cell></row><row><cell>Nightstand</cell><cell>Chair</cell><cell>Seat Stool</cell><cell></cell><cell>Bath. Counter Counter -other</cell></row><row><cell>Table</cell><cell></cell><cell>Swivel Chair</cell><cell>Counter</cell><cell>Kitchen Island</cell></row><row><cell>Pool Table</cell><cell></cell><cell>Sofa</cell><cell></cell><cell>Desk</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>indicate that good performance on a particular test dataset sometimes is obtained by training on a training dataset that has compatible priors. For example, training on COCO yields good performance on VOC, and training on Mapillary yields good performance on KITTI. But no individual training dataset yields good performance across all test datasets.In contrast, the model trained on MSeg performs consistently across all datasets. This is evident in the aggregate performance, summarized by the harmonic mean across datasets. The harmonic mean mIoU achieved by the MSeg-trained model is 28% higher than the accuracy of the best individually-trained baseline (COCO).</figDesc><table><row><cell>Input image</cell><cell>ADE20K model</cell><cell>Mapillary model</cell><cell>COCO model</cell><cell>MSeg model</cell></row></table><note>Fig. 5: Zero-shot generalization on images from MSeg test datasets; ScanNet-20 (top two rows), KITTI (middle row), and WildDash (bottom two rows). In the fourth row the objects on the left are semi-truck trailers, and only the MSeg model recognizes them.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Train/Test</cell><cell cols="7">VOC Context CamVid WildDash KITTI ScanNet h. mean</cell></row><row><cell>COCO</cell><cell>73.4</cell><cell>43.3</cell><cell>58.7</cell><cell>38.2</cell><cell>47.6</cell><cell>33.4</cell><cell>45.8</cell></row><row><cell>ADE20K</cell><cell>35.4</cell><cell>23.9</cell><cell>52.6</cell><cell>38.6</cell><cell>41.6</cell><cell>42.9</cell><cell>36.9</cell></row><row><cell>Mapillary</cell><cell>22.5</cell><cell>13.6</cell><cell>82.1</cell><cell>55.4</cell><cell>67.7</cell><cell>2.1</cell><cell>9.3</cell></row><row><cell>IDD</cell><cell>14.6</cell><cell>6.5</cell><cell>72.1</cell><cell>41.2</cell><cell>51.0</cell><cell>1.6</cell><cell>6.5</cell></row><row><cell>BDD</cell><cell>14.4</cell><cell>7.1</cell><cell>70.7</cell><cell>52.2</cell><cell>54.5</cell><cell>1.4</cell><cell>6.1</cell></row><row><cell>Cityscapes</cell><cell>13.3</cell><cell>6.8</cell><cell>76.1</cell><cell>30.1</cell><cell>57.6</cell><cell>1.7</cell><cell>6.8</cell></row><row><cell>SUN RGBD</cell><cell>10.0</cell><cell>4.3</cell><cell>0.1</cell><cell>1.9</cell><cell>1.1</cell><cell>42.6</cell><cell>0.3</cell></row><row><cell>MSeg</cell><cell>70.7</cell><cell>42.7</cell><cell>83.3</cell><cell>62.0</cell><cell>67.0</cell><cell>48.2</cell><cell>59.2</cell></row><row><cell cols="2">MSeg-w/o relabeling 70.2</cell><cell>42.7</cell><cell>82.0</cell><cell>62.7</cell><cell>65.5</cell><cell>43.2</cell><cell>57.6</cell></row><row><cell>Oracle</cell><cell>77.8</cell><cell>45.8</cell><cell>78.8</cell><cell>-</cell><cell>58.4</cell><cell>62.3</cell><cell>-</cell></row></table><note>Semantic segmentation accuracy (mIoU) on MSeg test datasets using 1 million crops. (Zero-shot cross-dataset generaliza- tion.) Top: performance of models trained on individual training datasets. Middle: the same model trained on MSeg (our result). Bottom: for reference, performance of 'oracle' models trained on the test datasets. Numbers within 1% of the best are in bold (excluding 'oracle' models). The rightmost column is a summary measure: harmonic mean across datasets. Inference is performed at a single scale.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 :</head><label>5</label><figDesc>Results from the WildDash-v1 leaderboard at the time of submission. Our model, transferred zero-shot, ranks 1st and outperforms models that utilized WildDash data during training.</figDesc><table><row><cell></cell><cell>Meta AVG mIoU</cell><cell>Seen WildDash data?</cell></row><row><cell>MSeg-1080 (Ours)</cell><cell>48.3</cell></row><row><cell>LDN BIN-768 [19]</cell><cell>46.9</cell></row><row><cell>LDN OE [19]</cell><cell>42.7</cell></row><row><cell>DN169-CAT-DUAL</cell><cell>41.0</cell></row><row><cell>AHiSS [22]</cell><cell>39.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 :</head><label>4</label><figDesc>Semantic segmentation accuracy (mIoU) on MSeg training datasets. Models trained with 1 million crops. Evaluated on validation sets. Top: performance of models trained on individual datasets. Bottom: the same model trained on MSeg (our result). Numbers within 1% of the best are in bold. The rightmost column is a summary measure: harmonic mean across datasets. Inference is performed at a single scale.</figDesc><table><row><cell>Train/Test</cell><cell cols="8">COCO ADE20K Mapillary IDD BDD Cityscapes SUN h. mean</cell></row><row><cell>COCO</cell><cell>52.7</cell><cell>19.1</cell><cell>28.4</cell><cell>31.1</cell><cell>44.9</cell><cell>46.9</cell><cell>29.6</cell><cell>32.4</cell></row><row><cell>ADE20K</cell><cell>14.6</cell><cell>45.6</cell><cell>24.2</cell><cell>26.8</cell><cell>40.7</cell><cell>44.3</cell><cell>36.0</cell><cell>28.7</cell></row><row><cell>Mapillary</cell><cell>7.0</cell><cell>6.2</cell><cell>53.0</cell><cell>50.6</cell><cell>59.3</cell><cell>71.9</cell><cell>0.3</cell><cell>1.7</cell></row><row><cell>IDD</cell><cell>3.2</cell><cell>3.0</cell><cell>24.6</cell><cell cols="2">64.9 42.4</cell><cell>48.0</cell><cell>0.4</cell><cell>2.3</cell></row><row><cell>BDD</cell><cell>3.8</cell><cell>4.2</cell><cell>23.2</cell><cell>32.3</cell><cell>63.4</cell><cell>58.1</cell><cell>0.3</cell><cell>1.6</cell></row><row><cell>Cityscapes</cell><cell>3.4</cell><cell>3.1</cell><cell>22.1</cell><cell>30.1</cell><cell>44.1</cell><cell>77.5</cell><cell>0.2</cell><cell>1.2</cell></row><row><cell>SUN RGBD</cell><cell>3.4</cell><cell>7.0</cell><cell>1.1</cell><cell>1.0</cell><cell>2.2</cell><cell>2.6</cell><cell>43.0</cell><cell>2.1</cell></row><row><cell>MSeg-w/o relabeling</cell><cell>50.4</cell><cell>45.4</cell><cell>53.1</cell><cell cols="2">65.1 66.5</cell><cell>79.5</cell><cell>49.9</cell><cell>56.6</cell></row><row><cell>MSeg</cell><cell>50.7</cell><cell>45.7</cell><cell>53.1</cell><cell cols="2">65.3 68.5</cell><cell>80.4</cell><cell>50.3</cell><cell>57.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6 :</head><label>6</label><figDesc>Controlled evaluation of unified taxonomy and mask relabeling. Zero-shot transfer to MSeg test datasets. Both contributions make a positive impact on generalization accuracy.</figDesc><table><row><cell>Train/Test</cell><cell cols="6">VOC Context CamVid WildDash KITTI ScanNet h. mean</cell></row><row><cell>Naive merge</cell><cell>17.8 19.4</cell><cell>56.3</cell><cell>55.9</cell><cell>61.6</cell><cell>45.6</cell><cell>33.1</cell></row><row><cell cols="2">MSeg w/o relabeling 70.2 42.7</cell><cell>82.0</cell><cell>62.7</cell><cell>65.5</cell><cell>43.2</cell><cell>57.6</cell></row><row><cell>MSeg</cell><cell>70.7 42.7</cell><cell>83.3</cell><cell>62.0</cell><cell>67.0</cell><cell>48.2</cell><cell>59.2</cell></row></table><note>4.6 Performance in the 2020 Robust Vision Challenge As an extreme generalization experiment, we submit an MSeg model to the 2020 Robust Vision Challenge (RVC) semantic6: Qualitative results from SwiftNet (RVC 2020 champion) and our MSeg model (RVC 2020 runner-up without any retraining on RVC datasets) on images from the WildDash-v2 RVC test dataset. Taxonomies greatly influence predictions; SwiftNet and WildDash- v2 introduce a separate 'van' class (see row 1), whereas we group 'van' instances with the 'car' category. SwiftNet and WildDash-v2 also introduce a separate 'lane marking' class (see row 3), which we merge into 'road'. Note that SwiftNet's taxonomy incorporates an 'ego-vehicle' class, whereas we treat 'ego-vehicle' as 'unlabeled' when training MSeg models (See row 3).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7 :</head><label>7</label><figDesc>Robust Vision Challenge (RVC) 2020 results in the semantic segmentation track, as measured by class mIoU. We report zero-shot cross-dataset generalization in red and testing on the test split of datasets that were already seen at training time in blue. The best number in each column is highlighted in bold. It is important to clarify that RVC uses a different set of evaluation classes than our model was trained for. Moreover, RVC allows training on seven datasets, four of which are not among the MSeg training datasets. Hence, our model uses a Prior to the start of the competition, we provided our MSeg codebase and models to the organizers of the competition, who used it to prepare the RVC devkit for competitors. A new dataset, WildDash-v2, was</figDesc><table><row><cell>RVC TEST DATASETS</cell></row></table><note>different taxonomy and does not utilize the majority of the datasets in the RVC, making this evaluation significantly biased against our submission. Success in such a setting is only possible if our model can truly generalize beyond MSeg. Surprisingly, our submission ranks 2nd without any retraining. In the rest of this section, we analyze this experiment in more detail. The RVC is a biennial workshop and set of competitions held in conjunction with the ECCV conference.introduced for the August 2020 competition, replacing WildDash- v1 and introducing new evaluation classes such as 'pickup-truck', 'lane marking', and 'van'. Rather than measuring robustness via cross-dataset zero-shot generalization, the RVC competition uses the test split of 7 training datasets, as opposed to the validation split we use in Section 4.4. Therefore, submissions trained in an "oracle" mode (see Section 4.2) are permitted, although</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8 :</head><label>8</label><figDesc>Semantic segmentation accuracy (mIoU) on MSeg train and test datasets after a longer training. Performance of models trained on MSeg using 3 million crops. Numbers within 1% of the best are in bold. The rightmost column is a summary measure: harmonic mean across datasets. Inference is performed at a single scale.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Test Datasets (Zero-Shot Generalization)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Train Datasets</cell><cell></cell></row><row><cell>Train/Test</cell><cell cols="6">VOC Context CamVid WildDash KITTI ScanNet h. mean</cell><cell cols="6">COCO ADE20K Mapillary IDD BDD Cityscapes SUN h. mean</cell></row><row><cell cols="2">MSeg-3m-480p 76.4 45.9</cell><cell>81.2</cell><cell>62.7</cell><cell>68.2</cell><cell>49.5</cell><cell>61.2</cell><cell>56.1</cell><cell>49.6</cell><cell>53.5</cell><cell>64.5 67.8</cell><cell>79.9</cell><cell>49.2 58.5</cell></row><row><cell cols="2">MSeg-3m-720p 74.7 44.0</cell><cell>83.5</cell><cell>60.4</cell><cell>67.9</cell><cell>47.7</cell><cell>59.8</cell><cell>53.3</cell><cell>48.2</cell><cell>53.5</cell><cell>64.8 68.6</cell><cell>79.8</cell><cell>49.3 57.8</cell></row><row><cell cols="2">MSeg-3m-1080p 72.0 44.0</cell><cell>84.5</cell><cell>59.9</cell><cell>66.5</cell><cell>49.5</cell><cell>59.8</cell><cell>53.6</cell><cell>49.2</cell><cell>54.9</cell><cell>66.3 69.1</cell><cell>81.5</cell><cell>50.1 58.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9 :</head><label>9</label><figDesc></figDesc><table><row><cell cols="3">Quantifying the effect of taxonomy resolution on</cell></row><row><cell cols="3">performance on Mapillary Vistas in the RVC 2020 challenge. A</cell></row><row><cell cols="3">total of 19 Mapillary Vistas (Public) classes are merged in our</cell></row><row><cell cols="3">universal taxonomy. We report mIoU of competing methods for</cell></row><row><cell cols="3">our matching taxonomy (MAPILLARY-46) and the finer-grained</cell></row><row><cell>taxonomy (MAPILLARY-65).</cell><cell></cell><cell></cell></row><row><cell>METHOD NAME</cell><cell>MAPILLARY-65</cell><cell>MAPILLARY-46</cell></row><row><cell>SWIFTNET (SN RN152PYRX8 RVC) [77]</cell><cell>40.43</cell><cell>43.53</cell></row><row><cell>MSEG1080 RVC</cell><cell>34.19</cell><cell>48.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 11</head><label>11</label><figDesc></figDesc><table><row><cell></cell><cell>: Average</cell></row><row><cell cols="2">frame rate for different</cell></row><row><cell>resolutions.</cell><cell></cell></row><row><cell></cell><cell>RTX 2080 Ti</cell></row><row><cell>420p</cell><cell>27.19</cell></row><row><cell>720p</cell><cell>19.29</cell></row><row><cell>1080p</cell><cell>15.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 10 :</head><label>10</label><figDesc>Instance segmentation accuracy on MSeg training datasets. (Evaluated on validation sets.) Performance of models trained on MSeg. Inference is performed at a single scale.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Box Average Precision</cell><cell></cell><cell></cell><cell cols="3">Mask Average Precision</cell><cell></cell></row><row><cell>Resolution</cell><cell cols="5">COCO ADE20K Mapillary IDD Cityscapes</cell><cell cols="5">COCO ADE20K Mapillary IDD Cityscapes</cell></row><row><cell>MSeg-3m-480p</cell><cell>35.1</cell><cell>26.2</cell><cell>13.9</cell><cell>33.4</cell><cell>30.2</cell><cell>32.1</cell><cell>21.7</cell><cell>11.9</cell><cell>29.4</cell><cell>24.6</cell></row><row><cell>MSeg-3m-720p</cell><cell>35.8</cell><cell>26.2</cell><cell>15.7</cell><cell>35.4</cell><cell>30.2</cell><cell>32.6</cell><cell>21.3</cell><cell>13.7</cell><cell>31.2</cell><cell>24.8</cell></row><row><cell>MSeg-3m-1080p</cell><cell>35.2</cell><cell>24.7</cell><cell>17.0</cell><cell>35.4</cell><cell>30.2</cell><cell>32.0</cell><cell>20.3</cell><cell>15.1</cell><cell>31.2</cell><cell>24.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 12 :</head><label>12</label><figDesc></figDesc><table><row><cell cols="6">Panoptic segmentation accuracy (PQ) on MSeg train-</cell></row><row><cell cols="4">ing datasets. (Evaluated on validation sets.)</cell><cell></cell><cell></cell></row><row><cell>Resolution</cell><cell cols="5">COCO ADE20K Mapillary IDD Cityscapes</cell></row><row><cell>MSeg-3m-480p</cell><cell>35.7</cell><cell>32.7</cell><cell>14.0</cell><cell>47.5</cell><cell>49.5</cell></row><row><cell>MSeg-3m-720p</cell><cell>35.7</cell><cell>32.9</cell><cell>11.9</cell><cell>44.2</cell><cell>50.8</cell></row><row><cell cols="2">MSeg-3m-1080p 38.8</cell><cell>33.7</cell><cell>14.9</cell><cell>45.0</cell><cell>51.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>John Lambert is a Ph.D student in computer vision and robotics at the Georgia Institute of Technology, advised by Dr. James Hays and Dr. Frank Dellaert. He received his M.S. and B.S. degrees in Computer Science in 2018 and 2017 from Stanford University, where he was advised by Dr. Silvio Savarese. His research interests center around scene understanding and 3d reconstruction and mapping for mobile robotics.</figDesc><table><row><cell>Zhuang Liu is a Ph.D. student at University of</cell></row><row><cell>California, Berkeley, advised by Prof. Trevor Dar-</cell></row><row><cell>rell. He received his B.S. degree in Computer</cell></row><row><cell>Science from Institute for Interdisciplinary Infor-</cell></row><row><cell>mation Sciences (Yao Class), Tsinghua Univer-</cell></row><row><cell>sity in 2017. He spent time as an intern at Cornell</cell></row><row><cell>University, Intel Labs and Adobe Research. His</cell></row><row><cell>research mainly focuses on the computational</cell></row><row><cell>and data efficiency of deep learning.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>, 14, 15, 16, 17, 18, 19, 20.</figDesc><table><row><cell>Dataset</cell><cell>Original</cell><cell></cell><cell>Relabeled</cell></row><row><cell>Class</cell><cell></cell><cell>Count</cell><cell>Class</cell><cell>Count</cell></row><row><cell></cell><cell></cell><cell></cell><cell>bridge</cell><cell>1466</cell></row><row><cell></cell><cell></cell><cell></cell><cell>building</cell><cell>23</cell></row><row><cell></cell><cell></cell><cell></cell><cell>unlabel</cell><cell>77</cell></row><row><cell>bridge</cell><cell></cell><cell>1763</cell><cell>pier wharf</cell><cell>194</cell></row><row><cell></cell><cell></cell><cell></cell><cell>platform</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>runway</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>vegetation</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>nightstand</cell><cell>210</cell></row><row><cell></cell><cell></cell><cell></cell><cell>chest of drawers</cell><cell>106</cell></row><row><cell></cell><cell></cell><cell></cell><cell>cabinet</cell><cell>7332</cell></row><row><cell cols="3">cabinet-merged 8071</cell><cell>desk counter other</cell><cell>14 6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>unlabel (incorrect)</cell><cell>371</cell></row><row><cell></cell><cell></cell><cell></cell><cell>wardrobe</cell><cell>19</cell></row><row><cell></cell><cell></cell><cell></cell><cell>bookshelf</cell><cell>13</cell></row><row><cell></cell><cell></cell><cell></cell><cell>armchair</cell><cell>9205</cell></row><row><cell></cell><cell></cell><cell></cell><cell>basket</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>bench</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>chair other</cell><cell>20661</cell></row><row><cell>chair</cell><cell></cell><cell>39830</cell><cell>unlabel ottoman</cell><cell>211 25</cell></row><row><cell></cell><cell></cell><cell></cell><cell>seat</cell><cell>7398</cell></row><row><cell>COCO Panoptic</cell><cell></cell><cell></cell><cell>slow wheeled object</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell></cell><cell>stool</cell><cell>995</cell></row><row><cell></cell><cell></cell><cell></cell><cell>swivel chair</cell><cell>1319</cell></row><row><cell></cell><cell></cell><cell></cell><cell>bathroom counter</cell><cell>738</cell></row><row><cell></cell><cell></cell><cell></cell><cell>counter other</cell><cell>3823</cell></row><row><cell cols="2">counter</cell><cell>4751</cell><cell>kitchen island</cell><cell>28</cell></row><row><cell></cell><cell></cell><cell></cell><cell>nightstand</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>unlabel</cell><cell>161</cell></row><row><cell>curtain</cell><cell></cell><cell>5306</cell><cell>curtain other shower curtain</cell><cell>4629 677</cell></row><row><cell></cell><cell></cell><cell></cell><cell>box</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>counter other</cell><cell>218</cell></row><row><cell></cell><cell></cell><cell></cell><cell>desk</cell><cell>29</cell></row><row><cell></cell><cell></cell><cell></cell><cell>kitchen island</cell><cell>135</cell></row><row><cell cols="2">dining table</cell><cell>16306</cell><cell>table</cell><cell>15851</cell></row><row><cell></cell><cell></cell><cell></cell><cell>unlabel</cell><cell>68</cell></row><row><cell></cell><cell></cell><cell></cell><cell>nightstand</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>chest of drawers</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>bathroom counter</cell><cell>1</cell></row><row><cell cols="2">fence-merged</cell><cell>4856</cell><cell>fence guardrail</cell><cell>4733 123</cell></row><row><cell></cell><cell></cell><cell></cell><cell>chandelier</cell><cell>260</cell></row><row><cell></cell><cell></cell><cell></cell><cell>lamp</cell><cell>4527</cell></row><row><cell>light</cell><cell></cell><cell>12047</cell><cell>light other unlabel (incorrect)</cell><cell>3793 86</cell></row><row><cell></cell><cell></cell><cell></cell><cell>sconce</cell><cell>939</cell></row><row><cell></cell><cell></cell><cell></cell><cell>streetlight</cell><cell>2442</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 13 :</head><label>13</label><figDesc>We record the number of masks from each COCO Panoptic class that are relabeled (here a-f classes are shown alphabetically).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE 14 :</head><label>14</label><figDesc>We record the number of masks from each COCO Panoptic class that are re-labeled (classes g-z, listed alphabetically.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE 15 :</head><label>15</label><figDesc>We record the number of masks from each ADE20K class that are re-labeled.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Original</cell><cell>Relabeled</cell></row><row><cell></cell><cell cols="2">Class Count</cell><cell>Class</cell><cell>Count</cell></row><row><cell></cell><cell></cell><cell></cell><cell>bathroom counter</cell><cell>78</cell></row><row><cell></cell><cell></cell><cell></cell><cell>cabinet</cell><cell>2</cell></row><row><cell></cell><cell>counter</cell><cell>461</cell><cell>counter other desk</cell><cell>322 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>kitchen island</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>unlabel</cell><cell>50</cell></row><row><cell></cell><cell></cell><cell></cell><cell>armchair</cell><cell>877</cell></row><row><cell></cell><cell></cell><cell></cell><cell>bench</cell><cell>12</cell></row><row><cell></cell><cell></cell><cell></cell><cell>cabinet</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>chair other</cell><cell>1103</cell></row><row><cell></cell><cell></cell><cell></cell><cell>door</cell><cell>1</cell></row><row><cell>SUN RGBD</cell><cell></cell><cell></cell><cell>unlabel</cell><cell>469</cell></row><row><cell></cell><cell>chair</cell><cell>3236</cell><cell>ottoman</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>seat</cell><cell>72</cell></row><row><cell></cell><cell></cell><cell></cell><cell>sofa</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>stool</cell><cell>62</cell></row><row><cell></cell><cell></cell><cell></cell><cell>swivel chair</cell><cell>626</cell></row><row><cell></cell><cell></cell><cell></cell><cell>table</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>wall</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>chandelier</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>lamp</cell><cell>708</cell></row><row><cell></cell><cell>lamp</cell><cell>775</cell><cell>light other</cell><cell>7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>unlabel</cell><cell>34</cell></row><row><cell></cell><cell></cell><cell></cell><cell>sconce</cell><cell>25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>TABLE 16 :</head><label>16</label><figDesc>We record the number of masks from each SUN RBGD class that are re-labeled.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Original</cell><cell>Relabeled</cell></row><row><cell></cell><cell cols="2">Class Count</cell><cell>Class</cell><cell>Count</cell></row><row><cell></cell><cell></cell><cell></cell><cell>motorcyclist</cell><cell>3</cell></row><row><cell></cell><cell cols="2">person 2808</cell><cell cols="2">bicyclist person nonrider 2740 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>unlabel</cell><cell>61</cell></row><row><cell>BDD</cell><cell></cell><cell></cell><cell>motorcyclist</cell><cell>120</cell></row><row><cell></cell><cell></cell><cell></cell><cell>bicyclist</cell><cell>233</cell></row><row><cell></cell><cell>rider</cell><cell>389</cell><cell>person nonrider</cell><cell>24</cell></row><row><cell></cell><cell></cell><cell></cell><cell>rider other</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>unlabel</cell><cell>4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>TABLE 17 :</head><label>17</label><figDesc>We record the number of masks from each BDD class that are re-labeled.</figDesc><table><row><cell>Dataset</cell><cell>Original</cell><cell></cell><cell cols="2">Relabeled</cell></row><row><cell>Class</cell><cell></cell><cell>Count</cell><cell>Class</cell><cell>Count</cell></row><row><cell></cell><cell></cell><cell></cell><cell>fountain</cell><cell>18</cell></row><row><cell></cell><cell></cell><cell></cell><cell>river lake</cell><cell>343</cell></row><row><cell>Water</cell><cell></cell><cell>500</cell><cell>sea</cell><cell>44</cell></row><row><cell></cell><cell></cell><cell></cell><cell>water other</cell><cell>92</cell></row><row><cell></cell><cell></cell><cell></cell><cell>unlabel</cell><cell>3</cell></row><row><cell>Mapillary</cell><cell></cell><cell></cell><cell>animal other</cell><cell>9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>bird</cell><cell>3</cell></row><row><cell cols="2">Ground Animal</cell><cell>411</cell><cell>cat cow</cell><cell>2 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>dog</cell><cell>308</cell></row><row><cell></cell><cell></cell><cell></cell><cell>horse</cell><cell>26</cell></row><row><cell></cell><cell></cell><cell></cell><cell>unlabeled</cell><cell>60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>TABLE 18 :</head><label>18</label><figDesc>We record the number of masks from each Mapillary Vistas class that are re-labeled.</figDesc><table><row><cell>Dataset</cell><cell>Original</cell><cell>Relabeled</cell><cell></cell></row><row><cell></cell><cell>Class Count</cell><cell>Class</cell><cell>Count</cell></row><row><cell></cell><cell></cell><cell>motorcyclist</cell><cell>31020</cell></row><row><cell></cell><cell></cell><cell>bicyclist</cell><cell>286</cell></row><row><cell></cell><cell></cell><cell>person nonrider</cell><cell>527</cell></row><row><cell></cell><cell></cell><cell>rider other</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell>unlabel</cell><cell>134</cell></row><row><cell>IDD</cell><cell>rider 32080</cell><cell>pole</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>backpack</cell><cell>7</cell></row><row><cell></cell><cell></cell><cell>bag</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>motorcycle</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>box</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell>bicycle</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>TABLE 19 :</head><label>19</label><figDesc>We record the number of masks from each IDD class that are re-labeled.</figDesc><table><row><cell>Dataset</cell><cell>Original</cell><cell>Relabeled</cell></row><row><cell></cell><cell>Class Count</cell><cell>Class</cell><cell>Count</cell></row><row><cell></cell><cell></cell><cell>motorcyclist</cell><cell>268</cell></row><row><cell></cell><cell></cell><cell>bicyclist</cell><cell>2010</cell></row><row><cell cols="2">Cityscapes rider 2363</cell><cell>person nonrider rider other</cell><cell>25 48</cell></row><row><cell></cell><cell></cell><cell>unlabel</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell>bicycle</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>TABLE 20 :</head><label>20</label><figDesc>We record the number of masks from each Cityscapes class that are re-labeled.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>TABLE 21 :</head><label>21</label><figDesc>The classification into 'stuff' and 'things' by origin datasets creates inconsistencies. Below, 'thing' classes are shown in red, and 'stuff' is shown in blue. BDD and SUN RGBD do not provide instance labels.</figDesc><table><row><cell cols="2">Unified Class Name ADE20K</cell><cell>Cityscapes</cell><cell>COCO-Panoptic</cell><cell>IDD</cell><cell>Mapillary</cell></row><row><cell>building</cell><cell>booth, house, building</cell><cell>building</cell><cell cols="2">building-other-merged, house, roof building</cell><cell>Phone Booth, Building</cell></row><row><cell></cell><cell>skyscraper, hovel, tower, grandstand</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>food other</cell><cell>food</cell><cell></cell><cell>food-other-merged</cell><cell></cell><cell></cell></row><row><cell>table</cell><cell>coffee table, table</cell><cell></cell><cell>dining table, table-merged</cell><cell></cell><cell></cell></row><row><cell>counter other</cell><cell>counter</cell><cell></cell><cell>counter</cell><cell></cell><cell></cell></row><row><cell>door</cell><cell>door, screen door</cell><cell></cell><cell>door stuff</cell><cell></cell><cell></cell></row><row><cell>light other</cell><cell>light</cell><cell></cell><cell>light</cell><cell></cell><cell></cell></row><row><cell>mirror</cell><cell>mirror</cell><cell></cell><cell>mirror stuff</cell><cell></cell><cell></cell></row><row><cell>shelf</cell><cell>shelf</cell><cell></cell><cell>shelf</cell><cell></cell><cell></cell></row><row><cell>stairs</cell><cell>stairs</cell><cell></cell><cell>stairway, stairs</cell><cell></cell><cell></cell></row><row><cell>cabinet</cell><cell>cabinet, buffet</cell><cell></cell><cell>cabinet-merged</cell><cell></cell><cell></cell></row><row><cell>road</cell><cell>road</cell><cell>road</cell><cell>road</cell><cell cols="2">road, parking, drivable fallback Crosswalk-Plain, Lane Marking-Crosswalk</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pothole, Parking, Road, Bike Lane,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Service Lane, Lane Marking-General</cell></row><row><cell>box</cell><cell>box</cell><cell></cell><cell>cardboard</cell><cell></cell><cell></cell></row><row><cell>traffic sign</cell><cell></cell><cell>traffic sign</cell><cell>stop sign</cell><cell>traffic sign</cell><cell>Traffic Sign (Back), Traffic Sign (Front)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Traffic Sign Frame</cell></row><row><cell>traffic light</cell><cell>traffic light</cell><cell>traffic light</cell><cell>traffic light</cell><cell>traffic light</cell><cell>Traffic Light</cell></row><row><cell>billboard</cell><cell>trade name</cell><cell></cell><cell></cell><cell>billboard</cell><cell>Billboard</cell></row><row><cell>pole</cell><cell>pole</cell><cell>pole</cell><cell></cell><cell>pole, polegroup</cell><cell>Utility Pole,Pole</cell></row><row><cell>fence</cell><cell>fence</cell><cell>fence</cell><cell>fence-merged</cell><cell>fence</cell><cell>Fence</cell></row><row><cell>banner</cell><cell></cell><cell></cell><cell>banner</cell><cell></cell><cell>Banner</cell></row><row><cell>curtain other</cell><cell>curtain</cell><cell></cell><cell>curtain</cell><cell></cell><cell></cell></row><row><cell>pillow</cell><cell></cell><cell cols="2">pillow, cushion pillow</cell><cell></cell><cell></cell></row><row><cell>towel</cell><cell>towel'</cell><cell></cell><cell>towel'</cell><cell></cell><cell></cell></row><row><cell>vegetation</cell><cell>flower,palm tree</cell><cell>vegetation</cell><cell>flower, tree-merged</cell><cell>vegetation</cell><cell>Vegetation</cell></row><row><cell>train</cell><cell></cell><cell>train</cell><cell>train</cell><cell>train</cell><cell>On Rails</cell></row><row><cell>window</cell><cell>windowpane</cell><cell></cell><cell>window-other</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. All trained models as well as training and inference codes are available at http://github.com/mseg-dataset/mseg-semantic</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank Marin Or?i?, Petra Bevandi?, and Sini?a?egvi? for sharing their SwiftNet label map predictions from the 2020 Robust Vision Challenge.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The summer vision project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Papert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">TextonBoost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with Gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Mapillary Vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">IDD: A dataset for exploring problems of autonomous navigation in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">BDD100K: A diverse driving video database with scalable annotation tooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04687</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot crossdataset transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01341</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Training constrained deconvolutional networks for road scene semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Alcantarilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Watanabe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01545</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simultaneous semantic segmentation and outlier detection in presence of domain shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kre?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Or?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>?egvi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wilddash -creating hazard-aware benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murschitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steininger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G. Fernandez</forename><surname>Dominguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Training of convolutional networks on multiple heterogeneous datasets for street scene semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meletis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dubbelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Training efficient semantic segmentation CNNs on multiple datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leonardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mazzini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic-structured semantic propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Universal semisupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sample selection bias as a specification error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Heckman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Econometric Society</title>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
	<note>Econometrica</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">In search of lost domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=lQdXeXDoWtI" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Wilds: A benchmark of in-the-wild distribution shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stavness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Earnshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning</title>
		<editor>Research, M. Meila and T. Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning, ser. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="5637" to="5664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust place categorization with deep domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Universal representations: The missing link between faces, text, planktons, and cat breeds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07275</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Invariant risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Domain adaptation under structural causal models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>B?hlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03096</idno>
		<title level="m">Metadataset: A dataset of datasets for learning to learn from few examples</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Robust vision challenge 2020</title>
		<ptr target="http://www.robustvision.net/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image parsing: Unifying segmentation, detection, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="140" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Finding things: Image parsing with regions and per-exemplar detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scene parsing with object instances and occlusion ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pixelwise instance segmentation with a dynamically instantiated network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SGN: sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Instancecut: From edges to instances with multicut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Polytransform: Deep polygon transformer for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">The plenoptic function and the elements of early vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">The Pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">IJCV</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">ScanNet: Richly-annotated 3D reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The apolloscape dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Interiornet: Mega-scale multisensor photo-realistic indoor scenes dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzoumanikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Matterport3d: Learning from rgb-d data in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Joint 2D-3D-Semantic Data for Indoor Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Large-scale interactive object segmentation with human annotators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Openimages: A public dataset for large-scale multilabel and multi-class image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="https://storage.googleapis.com/openimages/web/index.html" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kassahun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahmudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ricou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Durgesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hauswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?hlegg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>J?nicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mirashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Savani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vorobiov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schuberth</surname></persName>
		</author>
		<ptr target="http://www.a2d2.audi" />
		<title level="m">A2d2: Aev autonomous driving dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Material recognition in the wild with the materials in context database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Budge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Briales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gillingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pesqueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Nardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05797</idno>
		<title level="m">The replica dataset: A digital replica of indoor spaces</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Cheap and fast -but is it good? Evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Playing for benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Pytorch semantic segmentation</title>
		<ptr target="https://github.com/hszhao/semseg,ac-cessed" />
		<imprint>
			<date type="published" when="2019-09" />
			<biblScope unit="page">274</biblScope>
			<pubPlace>Commit</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Feedback network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Multi-domain semantic segmentation with pyramidal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Or?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Grubi?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>?ari?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>?egvi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.01636</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A new monotonic, clone-independent, reversal symmetric, and condorcet-consistent single-winner election method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schulze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Choice and Welfare</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="303" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-009-5152-4</idno>
		<ptr target="https://doi.org/10.1007/s10994-009-5152-4" />
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou ; H. Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/" />
	</analytic>
	<monogr>
		<title level="m">d03a857a23b5285736c4d55e0bb067c8-Paper.pdf Input Image SwiftNet MSeg Input Image SwiftNet MSeg Fig. 11: Qualitative results from SwiftNet (RVC 2020 champion) and our MSeg model (RVC 2020 runner-up without any re-training on RVC datasets) on images from the ADE20K RVC test dataset</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems. test split). Once again taxonomies greatly influence predictions; We found that Mechanical Turk workers were unable to consistently separate skyscrapers, buildings, and house instances, therefore our taxonomy merges them and our model avoids confusion between the three</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
