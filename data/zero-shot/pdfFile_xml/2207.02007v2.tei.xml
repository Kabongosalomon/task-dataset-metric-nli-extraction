<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The StarCraft Multi-Agent Challenges + : Learning of Multi-Stage Tasks and Environmental Factors without Precise Reward Functions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihwan</forename><surname>Oh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongsik</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonkee</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonghwan</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Chong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Se-Young</forename><surname>Yun</surname></persName>
						</author>
						<title level="a" type="main">The StarCraft Multi-Agent Challenges + : Learning of Multi-Stage Tasks and Environmental Factors without Precise Reward Functions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel benchmark called the StarCraft Multi-Agent Challenges + , where agents learn to perform multi-stage tasks and to use environmental factors without precise reward functions. The previous challenges (SMAC) recognized as a standard benchmark of Multi-Agent Reinforcement Learning are mainly concerned with ensuring that all agents cooperatively eliminate approaching adversaries only through fine manipulation with obvious reward functions. This challenge, on the other hand, is interested in the exploration capability of MARL algorithms to efficiently learn implicit multi-stage tasks and environmental factors as well as microcontrol. This study covers both offensive and defensive scenarios. In the offensive scenarios, agents must learn to first find opponents and then eliminate them. The defensive scenarios require agents to use topographic features. For example, agents need to position themselves behind protective structures to make it harder for enemies to attack. We investigate MARL algorithms under SMAC + and observe that recent approaches work well in similar settings to the previous challenges, but misbehave in offensive scenarios. Additionally, we observe that an enhanced exploration approach has a positive effect on performance but is not able to completely solve all scenarios. This study proposes a new axis of future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The StarCraft Multi-Agent Challenges (SMAC) is recognized as the standard benchmark simulator of Multi-Agent Reinforcement Learning (MARL) studies <ref type="bibr">(Samvelyan et al., 2019)</ref>. Tasks in the SMAC mainly require micro-managed * Equal contribution 1 KAIST AI, Seoul, South Korea. Correspondence to: Se-Young Yun &lt;yunseyoung@kaist.edu&gt;.</p><p>AI for Agent-Based Modelling Workshop at ICML2022, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author <ref type="bibr">(s)</ref> control of agents in defensive situations, where all opponents naturally approach the trained agents. This allows agents to obtain rewards directly. In these environments, the majority of algorithms concentrated on determining the relevance of each agent during training <ref type="bibr">(Sunehag et al., 2017;</ref><ref type="bibr" target="#b10">Lowe et al., 2017;</ref><ref type="bibr">Rashid et al., 2018;</ref><ref type="bibr" target="#b6">Hu et al., 2021;</ref><ref type="bibr" target="#b7">Iqbal et al., 2021;</ref><ref type="bibr" target="#b9">Liu et al., 2021;</ref><ref type="bibr">Sun et al., 2021;</ref><ref type="bibr">Qiu et al., 2021)</ref>. Some difficult scenarios, such as 2c vs 64zg and corridor, on the other hand, require agents to indirectly learn environmental factors, such as exploiting different levels of terrains or discover multi-stage tasks like avoiding rushing enemies first and then eliminating individuals without a specific reward for them. Recently, efficient exploration approaches for MARL algorithms were reported to drastically increase performance in those tough scenarios <ref type="bibr">(Sun et al., 2021;</ref><ref type="bibr">Son et al., 2022)</ref>. However, those scenarios do not allow quantitative assessment of the algorithm's exploration capabilities, as they do not accurately reflect the difficulty of the task, which depends on the complexity of multi-stage tasks and the significance of environmental factors.</p><p>To address this issue, we propose a new class of the StarCraft Multi-Agent Challenges + (SMAC + ) that encompasses advanced and sophisticated multi-stage tasks, and involves environmental factors agents must learn to accomplish, as seen in <ref type="table" target="#tab_0">Table 1</ref>. We present three defensive scenarios to encourage agents to employ topographical features such as positioning themselves behind structures to lower the probability of being attacked by enemies. In addition, offensive scenarios require agents to initially find the adversaries while also considering topographical obstacles and then rapidly defeating each of the adversarial troops. Like previous challenges, SMAC, in these situations, agents are still rewarded just for eliminating enemies, indicating that they indirectly learn how to do multi-stage tasks and use environmental factors. In the experiment results, we compare the performance of 11 MARL algorithms across all scenarios to establish a benchmark. We find that existing approaches perform well in similar settings to the previous challenge, but when environments need to complete sophisticated subtasks, most algorithms fail to learn adequately even when the training time is significantly extended. To summarize, we make the following contributions: arXiv:2207.02007v2 <ref type="bibr">[cs.</ref>LG] 7 Jul 2022 ? We present an extensive benchmark of MARL algorithms on SMAC + . We find that recent MARL algorithms with enhanced exploration demonstrate stable performance in the proposed scenarios, but other baselines cannot be efficiently trained.</p><p>? We suggest the most challenging environments that demand simultaneously learning micro-control and multistage tasks. These scenarios are an open-ended problem for efficient exploration toward MARL domains because no algorithm attain satisfactory performance on the challenging scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The StarCraft Multi-Agent Challenges +</head><p>We propose a novel multi-agent environment referred to as StarCraft Multi-Agent Challenges + that features a quantitative evaluation of the exploration abilities of MARL algorithms. This challenge offers more advanced and sophisticated environmental factors such as destructible structures that can be used to conceal enemies and terrain features, such as a hill, that may be used to mitigate damages. Also, we newly introduce offensive scenarios that demand sequential completion of multi-stage tasks requiring finding adversaries initially and then eliminating them. Like in SMAC, both defensive and offensive scenarios in SMAC + employ the reward function proportional to the number of enemies removed. For unit combination, we select units to necessitate environmental factors and completion of multi-stage tasks cooperatively so that the trained agents can validate that they make effective use of these properties. In SMAC + , agents must implicitly discover multi-stage tasks and factors by relying on their exploration strategy. Hence, it provides the evaluation of the exploration capability of MARL algorithms by comparing performance in similar but diverse scenarios. We describe the details of our environments in the rest of this section. We design topographical features like trees, and stones, which block each unit's sight equally for both allies and opponents. Another topographic feature is the hill, which provides a stochastic environment for damage dealing, as seen in <ref type="figure" target="#fig_0">Figure 1c</ref>. When an attacker stationed on the hill strikes opponents in the plain, the yellow line indicates deterministic damage dealing with a 100% probability. On the other hand, the orange line shows probabilistic damage with a 50% when an attacker positioned on the plain assaults an opponent placed on the hill. This is quite reasonable in the sense that someone in a topographically high position may easily hurt someone, but not vice versa. Hence, exploration of MARL algorithms must encourage agents to find these factors during the training phase, as agents are not explicitly rewarded for using these features. To determine the aforementioned factors, we devise a combination of units that can validate the cooperative decision-making of several agents. The role of each type of agent is presented in Table 2. Each unit in SMAC + has a unique set of attributes, such as shooting range, sight range, and firepower. Thus, it becomes important to determine how to combine and locate agents in battles considering various unit types. We explain more details in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Defensive Scenarios</head><p>In defensive scenarios, we place allies on the hill and adversaries on the plain. We emphasize the importance of agents defeating adversaries utilizing topographical factors. The defensive scenarios in SMAC + are almost identical to those in SMAC. However, our environment expands the exploration range of allies to scout for the direction of offense by allowing enemies to attack in several directions and adding The defense troop is positioned on the hill in preparation for combat, while the offensive troop placed on the lower side goes forward to the defense troops. The yellow line describes an attack by troops on the hill, while the orange line indicates an attack from below the hill.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Offensive Scenarios</head><p>Offensive scenarios provide learning of multi-stage tasks without direct incentives in MARL challenges. We suggest that agents should accomplish goals incrementally, such as eliminating adversaries after locating them. To observe a clear multi-stage structure, we allocate thirteen supplies to the allies more than the enemies. Hence, as soon as enemies are located, the agents rapidly learn to destroy enemies. As detailed in <ref type="table" target="#tab_0">Table 1</ref>, in SMAC + , agents will not have a chance to get a reward if they do not encounter adversaries. This is because there are only three circumstances in which agents can get rewards: when agents defeat an adversary, kill an adversary, or inflict harm on an adversary. As a result, the main challenges necessitate not only micro-management, but also exploration to locate enemies. For instance, the agents learn to separate the allied troops, locate the enemies, and effectively use armored troops like a long-ranged siege Tank. We measure the exploration strategy of effectively finding the enemy through this scenario. In this study, we examine the efficiency with which MARL algorithms explore to identify enemies by altering distance from them. In addition, to create more challenging scenarios, we show how enemy formation affects difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In this section, we describe experimental results to answer three key questions: 1) Do the proposed scenarios provide a quantitative assessment of exploration capability by varying complexity of multi-stage tasks and the significance of environmental factors? 2) Do existing MARL algorithms efficiently utilize exploration to discover environmental factors and the completion of multi-stage tasks? 3) Can these algorithms reliably perform well on SMAC + despite repeated training?</p><p>To demonstrate the need for assessment of exploration capabilities, we choose eleven algorithms of MARL algorithms classified into three categories; policy gradient algorithms, typical value-based algorithms, and distributional valuebased algorithms. First, as an initial study of the MARL domain, policy gradient algorithms such as COMA <ref type="bibr" target="#b3">(Foerster et al., 2018)</ref>, MASAC <ref type="bibr" target="#b4">(Haarnoja et al., 2018)</ref>, MADDPG <ref type="bibr" target="#b10">(Lowe et al., 2017)</ref>  In order to look into exploration capability, all baselines are trained in this experiment until the total number of cumulative episode steps respectively reaches five million steps for a sequential episodic setting. We respectively train each baseline three times and report average win-rates as the evaluation metric by conducting 32 test-runs in the episodic setting. Initially, we demonstrate the SMAC + benchmark utilizing sequential episodic buffers. We report experimental results by choosing a representative algorithm from each category because of the massive training time; MADDPG <ref type="bibr" target="#b10">(Lowe et al., 2017)</ref>, COMA <ref type="bibr" target="#b3">(Foerster et al., 2018)</ref>, <ref type="bibr">QMIX (Rashid et al., 2018)</ref> and <ref type="bibr">DRIMA (Son et al., 2022)</ref>. We also conducted on sequential episodic buffer setting and note that the tendency of experimental results remains unchanged as shown in <ref type="figure">Figure 2</ref>, allowing us to analyze the exploration capabilities of MARL algorithms based on results of the parallel episodic buffer. More training information on sequential episodic buffer, details about algorithms and exper-  <ref type="figure">Figure 2</ref>: Average win-rates of QMIX and DRIMA according to the cumulative episodic steps during training. Two baselines are respectively trained three times. (a) learning curves of the sequential episodic setting for 5 million steps. (b) learning curves of parallel setting for 10 million steps. The horizontal axis means the total number of cumulative episode steps during the training, and the vertical axis is the win-rate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Benchmark on Sequential Episodic Buffer</head><p>We first look into defensive scenario experiments on SMAC + to test whether MARL algorithms not only ad-equately employ environmental factors but also learn microcontrols. As seen in <ref type="table" target="#tab_5">Table 5</ref>, we find that supply difference and opponents' approach manipulate the difficulty, as the majority of baselines perform worse in response to those variants. In terms of algorithmic performance, we observe COMA and QMIX drastically degrade, but MADDPG gradually degrades. This fact reveals that MADDPG enables agents to effectively learn micro-control. However, among baselines, DRIMA achieves the highest score and retains performance even when the supply difference significantly increases. This is due to the fact that DRIMA efficiently explores micro-control but also environmental factors. This finding indicates that effective exploration uncovers intrinsic environmental factors. Regarding to offensive scenarios, we notice considerable performance differences of each baseline. Overall, even if an algorithm attains high scores at a trial, with exception of DRIMA, it is not guaranteed to train reliably in other trials. As mentioned, offensive scenarios do not require as much high micro-control as defensive scenarios, instead, it is important to locate enemies without direct incentives, such that when agents find enemies during training, the win-rate metric immediately goes to a high score. However, the finding enemies during training is decided by random actions drawn by -greedy or probabilistic policy, resulting in considerable variance in test outcome. In contrast, we see a perfect convergence of DRIMA in all offensive scenarios by employing its efficient exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation on Challenging Scenarios</head><p>As previously stated, we identify that DRIMA reliably solves all offensive scenarios. To provide open-ended problems for the MARL domain, we suggest more challenging scenarios as shown in <ref type="table" target="#tab_3">Table 4</ref>. In these scenarios, the agents are required to simultaneously learn completion of multistage tasks and micro-control during training. DRIMA produces remarkable performance in past experiments, so that we provide the results acquired by DRIMA to verify that the proposed scenarios are sufficiently challenging. We use the same experimental condition as in the earlier experiments. As you can see <ref type="table" target="#tab_6">Table 6</ref>, DRIMA still solves Off hard, although its performance on Off superhard is negligible. We argue that this scenario requires more sophisticated fine manipulation compared to other offensive scenarios. This is due to the fact that not only the strength of allies is identical to that of opponents, but also Gathered enables opponents to intensively strike allies at once. This indicates the necessity of more efficient exploration strategies for the completion of multi-stage tasks and micro-control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>We propose SMAC + , a suite of environments to learn multistage tasks and environmental factors without specific rewards. We develop new MARL environments based on the previous challenge <ref type="bibr">(Samvelyan et al., 2019)</ref>. This point allows us to ensure that all baselines are completely compatible with our environments. Consequently, we evaluate a total eleven MARL algorithms on both defensive and offensive scenarios, and their experimental results show that an efficient exploration strategy is required to learn multi-stage tasks and environmental factors. We hope this work serves as a valuable benchmark to evaluate the exploration capabilities of MARL algorithms and give guidance for future research.</p><p>Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and Whiteson, S. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Statement</head><p>The authors bear all responsibility in case of violation of rights, etc., and confirmation of the data license.</p><p>Accessibility SMAC + environment is publicly available at the Github repository: https://github.com/osilab-kaist/smac_ plus and we will maintain the released code for long-term accessibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>License</head><p>The original SMAC environment and PyMARL code follow the MIT license and Apache 2.0 license respectively. The proposed SMAC + environment and the modified PyMARL code are also released under the MIT license and Apache 2.0 license each. The details of licenses can be found in our repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility</head><p>The README file in the repository serves as a guide for installation and training. Several yaml files in pymarl/src/ config directory contains the parameters we used for the paper. Further training details can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Specification of StarCraft Multi-Agent Challenge +</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Environment</head><p>State and Observation features The features of original SMAC environment (Samvelyan et al., 2019) are described in from <ref type="table" target="#tab_0">Table A.1 to Table A.</ref>2. ut means the number of different unit types in the given scenarios. Each agent has 6 moving actions and attacks a particular enemy, so the the dimension of action space is 6 plus the number of enemies. The global state information is provided during the centralized training phase while agents must utilize its own local observation during the decentralized execution phase.  <ref type="table" target="#tab_8">Table A</ref>.3. The StarCraft2 in-game properties intrinsically provide additional information of terrain levels, such as pathing grid and terrain height. We consider terrain features like hill and entrance, so that agents must distinguish opponents are located higher or not in these scenarios. Furthermore, all agents can receive the last actions of all units within its sight range as a part of observations. These features are also provided in SMAC but not included in default setting. In addition, we incorporate the coordinate information perpendicular to the terrain surface and the agent can have its own coordinate values. The unit type represents a total of eight entities in this scenario; Marine, Marauder, General Tank (which cannot switch to siege mode), Siege Tank in general mode, Siege Tank in siege mode (because siege tank may pick their mode) and three neutral building. The dimension of the last action is specifically described in <ref type="table" target="#tab_3">Table A.4.</ref> Overall, an agent's observation space is composed of {movement feature, agent's own feature, enemies feature, allies feature, neutral buildings' feature} which are all visible when objects are within the agent's sight range. As the global state information, SMAC + provides two options. We might use same global information of SMAC, or a concatenation of all agents' observations. We empirically found that a concatenation of all observations gave better performance across SMAC + scenarios, hence we use concatenated observations as the global state information in default.   <ref type="table" target="#tab_8">Table A</ref>.4. The basic action is almost identical to SMAC, but we additionally consider an units skill. The unit skill is designed for a general tank to change to siege mode and vise versa. If others units choose unit skill action, it regards as stop action. We also add the number of neutral buildings into the action set because those can be removed by agents if needed. Therefore, the action space of SMAC + is much larger than SMAC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basic action</head><p>Attack enemies Attack neutral buildings North, South, East, West, No-op, Stop, Skill Enemy 1, ? ? ? Enemy n Builidng 1, ? ? ? , Building n Unit features All units sight range and shooting range are different as shown <ref type="table" target="#tab_8">Table Table A</ref>.5. The fire power has two types. Basically, every unit has its own fire power. Except marines, the remaining units give a special damage according to unit types. Tank has a machinery and heavy armor character, Marauder has a heavy armor character and Marine has nothing special. When seeing the <ref type="table" target="#tab_8">Table Table A</ref>.5, Marauder has a enhanced fire power on units who have machinery attributes.</p><p>Similarly, Tank has a enhanced fire power on units who have heavy armor attributes. We reduced fire power of Siege Tank because of the property of long-ranged and splash damage for game balance. So according to the scenario and training step, usage of Siege Tank might be different. Communication among agents In this paper, we study a new type option of MARL referred as communicating with all agents. It means all agents share partial information of observations for improving cooperation. For example, all units can access opponent's position when one of allies observe even though opponents are located outside of the unit's sight range. This assumption still assures a decentralized evaluation because none of agents utilize absolute state information and only share a small fraction of observations throughout the test time. We argue that this assumption more accurately reflects the actual challenge because real-world communication technology is being rapidly developed. Specifically, the communication option allows agents to share a portion of their observations when they are within certain agents' sight range. When you see <ref type="figure">Figure A</ref>.1a, due to limited field of view, armored troops that are capable of long-distance assaults, such as tank and siege-armed tank, cannot engage the adversaries. However, the communication option enables armored troops attack distant enemies by utilizing ally sights as seen <ref type="figure">Figure A.</ref>1b. These attributes help each agent to move in a more cooperative manner. We technically describe a communication option. The original SMAC environment uses a visibility matrix which is similar to adjacency matrix to indicate visible units for each agent. The dimension of the row is the number of the agents while the dimension of the column is the all the units in the environment including the agents. The entity of matrix (i, j) is 1 if unit j is in the sight range of the agent i and 0 otherwise.  Even though there is no significant performance gap, the learned policies are quite different. When the agents are not allowed to communicate, as shown in <ref type="figure">Figure A</ref>.2b they show standing alongside at the entrance of the hill and directly engage the enemies. Even though the agents can benefit the stochastic damage dealing from feature of hill, not utilizing the feature makes it hard to win due to the supply difference between enemies and allies. In contrast, in <ref type="figure">Figure A</ref>.2c, the Marauder distracts the enemies while the Marines hide behind the neutral buildings which block the sight of the enemies. This implies that the agents learned to utilize the sight-blocking capabilities of the neutral building blocks with the communication option. The possible reason is that the agents can behave apart from one another due to the observation sharing. It would be an interesting challenge to design an algorithm to learn such polices without communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. The Detail Information of Scenarios</head><p>Defensive scenarios By the extended exploration problem, allies should scout nearby the hill where allies are located to find out the enemies' offense direction as shown in <ref type="figure">Figure A</ref>.3b. If trained well, the agents won't try to get out of the hill which make enemies' offense as a stochastic damage. Plus, allies remove the trees (neutral buildings) that block the agents' sight for making it easy to secure original sight range and find the enemies' offense direction which means eliminating the uncertainty which can be found in <ref type="figure">Figure A</ref>.3a. After finding the enemies offense direction, allies need to be located at some proper location that provide reduce-able damage from enemies offense shown as <ref type="figure">Figure A</ref>.3c which shows that allies stand on the hill not allowing enemies to enter the hill. If agents are not trained well, they failed to find the enemies offense direction allowing occupation of the allies' respawn place to enemies. Offensive scenarios In the offensive scenarios in SMAC + , allies must find the place where the enemies are located. However, finding enemies far away from the allies is not an easy problem. Because the action space in SMAC + becomes much larger than the original SMAC (Samvelyan et al., 2019) environment caused by the neutral buildings. Therefore, it is difficult for agents to find enemies at the beginning of learning. To find the enemies, allies should endure the damage from the enemies like shown as <ref type="figure">Figure</ref>    <ref type="bibr" target="#b3">(Foerster et al., 2018)</ref>, MASAC <ref type="bibr" target="#b4">(Haarnoja et al., 2018)</ref>, MADDPG <ref type="bibr" target="#b10">(Lowe et al., 2017)</ref> are chosen for the policy-based category by general usage in MARL domain. Additionally, we report the four most effective baselines based on the results of parallel training in <ref type="table" target="#tab_8">Table D</ref>.13. For the fair comparison with the MADDPG algorithm <ref type="bibr" target="#b10">(Lowe et al., 2017)</ref> which is only compatible to the episodic setting, we respectively select the best performing algorithms in value-based, distribution-based and policy-based algorithms such as QMIX (Rashid et al.,  <ref type="bibr" target="#b3">(Foerster et al., 2018)</ref> and train those algorithms using the episodic buffer setting. We run each algorithm for a total of ten million timesteps with 3 different random seeds for parallel training using 20 runners and 5 million timestep for episodic training. The trained model is tested at every ten thousand timesteps during 32 episodes for episodic training and 20 episodes for parallel training. As a evaluation metric, the percentage of winning episodes referred as to win-rate is employed. In these experiments, we set all rewards to positive values.</p><p>Hyperparameters We describe about hyperparameters we used. Hyperparmeters are almost same with experiments conducted in the other papers for the fairness except training steps. Throughout the training, we anneal from 1.0 to 0.05 over 50000 training steps and fix the during the rest of the training time. We fix ? = 0.99. Replay buffer is capable for containing most recent 5000 episodes and we sample 32 size of batch from the replay buffer randomly. We update target network with current network every 200 time steps. We have 10050000 steps for each training. The details about hyperparameters are on <ref type="table" target="#tab_8">Table B</ref>.8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Algorithms</head><p>In this section, we describe how each algorithm works. We divided algorithms in detail taxonomies as Value-based, Policybased, and Distribution-based in this section. Value-based and Distribution-based algorithms showed better performance so far than policy based algorithms. We think that this was induced by a sample efficiency and suitable action space for discrete action choice which is advantages of value-based algorithms. We will describe about Distribution-based algorithms which got popular from <ref type="bibr" target="#b0">(Bellemare et al., 2017)</ref> that achieved state-of-the-art performance in Atari and Mujoco environments and also in SMAC's challenging hard scenario.</p><p>Partially Observable MDP In Markov Decision Process (MDP), agents can observe all the environments like <ref type="bibr" target="#b5">(Hausknecht &amp; Stone, 2015)</ref> if we don't use screen flickering technique where environment is stochastic. But in real world, MDP is not achievable generally. Instead, Partially Observable MDP (POMDP) is observable in the real world rather than MDP. For example, human being can observe his around where he is located but cannot observe where he is not located. So, POMDP is an MDP that agents can observe things only in their sight and agents decide their actions based on their observations. This is called Decentralized-POMDP (Dec-POMDP). In MARL settings, we consider simulator's environment as a Dec-POMDP setting.</p><p>Notation Formally, Dec-POMDP is given with a tuple G = S, U, P, r , Z , O, n, ? . s ? S is the true state that the environment provides. a ? A ? {1, ..., n} is an agent that chooses an action u a ? U which forms a joint action space u ? U where n is the number of agents. P(s | s, u) : S ? U ? S ? [0, 1] is a transition probability function. All agents in Dec-POMDP receive shared reward, so the reward function is r(s, u) :</p><formula xml:id="formula_0">S ? U ? R. Observation function is O(s, a) : S ? A ? Z that determines agents' observation z a ? Z. ? ? [0, 1)</formula><p>is a discount factor for the reward. Value-based algorithms were developed for environments that require discrete action space. So, SMAC environment had started with value-based algorithms naturally and showed better performance than policy-based algorithms. Value-based algorithms use n-step (1-step in general) temporal difference error (TD-error) for updating critic parameters. Basically these algorithms follow below equation as a basis of loss function:</p><formula xml:id="formula_1">? td?error = (R + ? * max a Q ? ? (s , a ) ? Q ? (s, a)) 2 (C.1)</formula><p>where s and a represent state and action. Prime means next step and ?, ? ? means parameters of behavior and target network each. In general, state and action are denoted as s, a, but from now we will use the notation of h and u for state and action which are also utilized as a method of notation state and action in RL domains. Before entering to the value-based algorithms, we have to know about the condition which is needed for stable learning and avoiding lazy agents that is called IGM(Individual Global Max) condition which means individual maxiaml Q-values consist of total (or global) maximal Q-values. It can be represented as follows: </p><formula xml:id="formula_2">arg max u Q total (h, u) = ? ? ? arg max u1 Q 1 (h 1 , u 1 ) . . . arg max uN Q N (h N , u N ) ? ? ? (C.</formula><formula xml:id="formula_3">Q joint (h, u) = N i=1 Q i (h i , u i ) (C.3)</formula><p>where Q and N means Q-value and number of agents. In this algorithm all agents share parameters and the performance and the performance start to increase substantially compared to IQL algorithm. satisfied with the monotonocity that make the parameters of mixing network positive values and enforce the joint-Q-value as monotonic in the per agent Q-values, which can be represented as:</p><formula xml:id="formula_4">?Q joint ?Q i ? 0, ?i ? N (C.4)</formula><p>which enable tractable optimization of the joint Q-value in off-policy learning. To make joint Q-value using mixing network, algorithm utilizes value factorization like VDN, meanwhile it uses a mixing network to compute the total value function. In the Equation C.5, ? means a linear combination function by a mixing network which can be denoted as follows:.</p><formula xml:id="formula_5">Q total (h, u) = ? Q 1 (h 1 , u 1 ) , . . . , Q n (h N , u N ) (C.5)</formula><p>By the mixing network and monotonicity, QMIX outperformed other algorithms like IQL, VDN, COMA etc. QMIX algorithms has become the most popular algorithms in MARL and still lots of variants of QMIX have continued to emerge.</p><p>QTRAN Former VDN and QMIX algorithms can handle the MARL issue as a way of being trapped in a constrained structures which are called additivitiy and monotonicity. QTRAN tried to relax the constraints on architecture's structure for abundant expressiveness by transforming the Q joint into an easily factorizable value. Mitigating the constraints, QTRAN proved guarantee for general factorization better than VDN and QMIX. The relaxed condition can be represented as:</p><formula xml:id="formula_6">N i=1 Q i (h i , u i ) ? Q joint (h, u) + V joint (h) = 0 u =? ? 0 u =? (C.6) where, V joint (h) = max u Q joint (h, u) ? N i=1 Q i (h i ,? i ) (C.7)</formula><p>where? means optimal action and V (h joint ) is a discrepancy that occurs from the Partially Observable MDP (POMDP) environment which can corrects the discrepancy between centralized Q joint and the sum of individual Q which is</p><formula xml:id="formula_7">N i=1 Q i (h i , u i ).</formula><p>Despite of the expressive power that relaxed constraints on architecture's structure, QTRAN shows poor performance in many complex scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Distribution-based</head><p>These days policy-based distributional RL algorithms are emerging in MARL domain but by the the fact that SMAC demands discrete control optimization, usually distributional RL (DRL) for SMAC is based on the value-based RL algorithm so far. DRL algorithm has been developing substantially with great attention since <ref type="bibr" target="#b0">(Bellemare et al., 2017)</ref>, <ref type="bibr" target="#b0">(Bellemare et al., 2017)</ref>, <ref type="bibr" target="#b2">(Dabney et al., 2018b)</ref> suggested new concept of value-based RL which outputs a distribution of return per action. <ref type="bibr" target="#b0">(Bellemare et al., 2017)</ref> fixed possible return value and made model to predict probability of returns with projected KL-divergence loss function. <ref type="bibr" target="#b2">(Dabney et al., 2018b)</ref> approximate quantile regression as a output of return distribution per action and fixed probability of each returns with 1 N where N means number of returns per action and made model to predict value of returns. And <ref type="bibr" target="#b2">(Dabney et al., 2018b)</ref> utilize Wasserstein metric as a loss function measuring distance of TD-error between Q(s t , a t ) and R(s t+1 , a t+1 ) + ? * max at+1 Q(s t+1 , a t+1 ). <ref type="bibr" target="#b1">(Dabney et al., 2018a)</ref> samples quantile fractions uniformly in [0, 1] which were fixed in <ref type="bibr" target="#b2">(Dabney et al., 2018b)</ref>. And also model outputs value of returns corresponds to quantile fractions embeded with cosine function. <ref type="bibr">(Yang et al., 2019)</ref> approximates both of probability and value of returns contrary to former algorithms that only approximate probability or value of returns. So, DRL algorithms develop following the sequence of C51 <ref type="bibr" target="#b0">(Bellemare et al., 2017)</ref> DFAC DFAC <ref type="figure" target="#fig_0">(Sun et al., 2021)</ref> is a first algorithm that combines distributional RL and multi-agent RL which is based on the IQN <ref type="bibr" target="#b1">(Dabney et al., 2018a)</ref> algorithm. Especially, IQN <ref type="bibr" target="#b1">(Dabney et al., 2018a)</ref> was used for distributional output sampling quantile fractions from U[0, 1] and approximating return values with quantile regression. By mean-shape decomposition, authors integrated distributional perspective in the multi-agent setting not violating the IGM condition which can be written as:</p><formula xml:id="formula_8">arg max u E[Z joint (h, u)] = ? ? ? arg max u1 E[Z 1 (h 1 , u 1 )]</formula><p>. . .</p><formula xml:id="formula_9">arg max uN E[Z N (h N , u N )] ? ? ? (C.8)</formula><p>that can be satisfied by the following DFAC Theorem (mean-shape decomposition):</p><formula xml:id="formula_10">Z joint (h, u) = E[Z joint (h, u)] + Z joint (h, u) ? E[Z joint (h, u)] = Z mean (h, u) + Z shape (h, u) = ?(Q 1 (h 1 , u 1 ) , . . . , Q N (h N , u N )) + ?(Z 1 (h 1 , u 1 ) , . . . , Z N (h N , u N )) (C.9)</formula><p>which is proved to satisfy IGM condition. <ref type="bibr">DFAC(Sun et al., 2021)</ref> shows outperforming performance than any other algorithms especially in Hard scenarios. Also this algorithm can be adapted to IQL, <ref type="bibr">VDN(Sunehag et al., 2017)</ref>, <ref type="bibr">QMIX(Rashid et al., 2018)</ref>. So the DFAC algorithm's variants are named as DIQL, DDN, DMIX that were used as our baselines.</p><p>DRIMA DFAC only considers a simple risk source but DRIMA <ref type="bibr">(Son et al., 2022)</ref> considers separating risk sources into agent-wise risk and environment-wise risk which makes another hyperparameter contrary to DFAC algorithm that has a hyperparameter in risk setting, agent-wise risk. Environment-wise risk can be interpreted as transition stochasticity and agent-wise risk can be seen as the randomness induced by the other agents' action which can't be modeled by environment MDP (Markov Decision Process). In distributional multi-agent reinforcement learning algorithms <ref type="bibr">((Sun et al., 2021)</ref>, <ref type="bibr">(Qiu et al., 2021)</ref>), models take risk level as an input to the agent utility function that output a distribution of return per an action which can be considered as a randomness by agents. But in DRIMA, agent receives agent-wise risk w agt and in the process of making joint distribution of returns, joint action-value network takes w env as input where agent utility function and joint action-value network composes of hierarchical architecture that resembles with QTRAN(Son et al., 2019) structure.</p><p>Network architecture of DRIMA consists of agent-wise utility function, true action-value network, and transformed actionvalue network. Agent-wise utility function is structured by DRQN <ref type="bibr" target="#b5">(Hausknecht &amp; Stone, 2015)</ref> taking w agt as a input. True action-value network approximates true distribution of returns with additional representation power that receives environment-wise risk w env , state s and utility functions' outputs Z i . Transformed action-value network is similar to the QMIX's(Rashid et al., 2018) mixing network as follows:</p><formula xml:id="formula_11">Z trans (s, ?, u, w agt ) = f mix (z 1 (? 1 , u 1 , w agt ) , . . . , z N (? N , u N , w agt ) ; ? mix (s, w agt )) (C.10)</formula><p>where not considering environment-wise risk and ? mix (s, w agt ) consists of a non-negative values obtained from hypernetwork like QMIX. DRIMA outperforms other algorithms especially in offense scenarios and in our experiments, we follow the default risk setting, agent-wise risk to be seeking and environment-wise risk to be averse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Policy-based</head><p>Policy-based algorithm directly optimize the parameters ? to approximate the optimal policy ? which is especially specialized in continuous action control setting like robot system and autonomous vehicle. The simplest way of training policy-based model is independently taking policy gradient per agent. As expected, this works poorly more than IQL algorithm. Therefore, centralized training and decentralized execution is main framework for training models in policy-based algorithms. Policy based algorithms maximizes objective function that take expectation on initial state value as follows which is usually noted as J(?):</p><formula xml:id="formula_12">J(?) = E ? ? [V 0 ] (C.11)</formula><p>where the gradient of objective is represented as ? ? J(?) = E s,a [? ? log ? ? (a|s)G(s)] which update actor's parameter realized in algorithm <ref type="bibr">REINFORCE(Sutton et al., 1999)</ref>. In actor-critic algorithm, Return G(s) is substituted by Q-value Q ? (s, a) represented as same as Equation C.1 or advantage function A ? (s, a) which demands another parameterized model to approximate critic. In multi-agent setting, we derive gradient of objective as:</p><formula xml:id="formula_13">? ?i J(? i ) = E si,ai [? ?i log ? i (a i |s i )Q ? i (s, a 1 , . . . , a N )] (C.12)</formula><p>where Q ? i is a centralized individual critic that takes as inputs other agents' actions and additional information like global state or gathering all observations of the agents'.</p><p>COMA COMA <ref type="bibr" target="#b3">(Foerster et al., 2018)</ref> is policy-based method that uses actor-critic algorithm that takes contribution to solving credit assignment problem by using concept of Difference rewards <ref type="bibr">(Wolpert &amp; Tumer, 2002)</ref>. In COMA <ref type="bibr" target="#b3">(Foerster et al., 2018)</ref>, a new form of advantage function was proposed to solve the 'credit assignment' problem in a multi-agent environment. Original advantage function is calculated through differences between state-value functions and action-state value function, but in COMA <ref type="bibr" target="#b3">(Foerster et al., 2018)</ref>, all other agents' actions are fixed and use the average value of the action-state value function for a particular agent's action as follows:</p><formula xml:id="formula_14">A a (h, u) = Q (h, u) ? u a ? a u a |h a Q h, u ?a , u a (C.13)</formula><p>which estimate credit r <ref type="figure">(s, (u ?a , c a )</ref>) where c means default action of agents that is hard to estimate. This is why COMA uses Equation C.13 as a estimation. The average of the action-state value function for each agent's action becomes the reference value for that agent's action. It serves to determine how good an agent's action is compared to the average of action-state value.</p><p>MASAC MASAC <ref type="bibr" target="#b12">(Pu et al., 2021)</ref> is soft actor critic (SAC) <ref type="bibr" target="#b4">(Haarnoja et al., 2018)</ref> algorithm for multi-agent system. SAC is a actor-critic algorithm that maximizes exploration in action space utilizing entropy of distribution of action probability based on the maximum entropy RL theory. Like the original SAC for single agent, MASAC substitute individual Q-value with Q total and calculate TD-error and objective gradient using same value network with <ref type="bibr">QMIX(Rashid et al., 2018)</ref>. So, the TD-error is calculated as follows:</p><formula xml:id="formula_15">? td?error = R + ?min u Q target ? ? (h , u ) ? Q total ? (h, u) 2 (C.14)</formula><p>which update the critic parameters. And for optimal policy, derived from soft policy iteration, objective is as follows:</p><formula xml:id="formula_16">J(?) = E D [? log ?(u|h) ? Q tot ? (h, u)] (C.15)</formula><p>where ? controls exploration and exploitation trade-off that if ? is near to 1, it means exploration more, on the contrary ? is near to 0, it means exploitation more.</p><p>MADDPG MADDPG <ref type="bibr" target="#b10">(Lowe et al., 2017)</ref> has main contribution of reducing uncertainty by taking input as actions by other agents and learns centralized individual critic and decentralized actor for policy that enable both of cooperative and competitive strategy by splitting centralized critic in individual manner. MADDPG <ref type="bibr" target="#b10">(Lowe et al., 2017)</ref> has emerged for continuous actions like MASAC <ref type="bibr" target="#b12">(Pu et al., 2021)</ref> algorithm contrary to COMA <ref type="bibr" target="#b3">(Foerster et al., 2018)</ref> algorithm. MAD-DPG <ref type="bibr" target="#b10">(Lowe et al., 2017)</ref> is based on the DDPG <ref type="bibr" target="#b8">(Lillicrap et al., 2015)</ref> algorithm that select deterministic action which is different from general policy-based algorithm that sample actions based on the action's distribution. The MADDPG <ref type="bibr" target="#b10">(Lowe et al., 2017)</ref>'s gradient of objective function can be written as a new version of Equation C.12 adapting deterministic policy a = ?(s) as follows:</p><formula xml:id="formula_17">? ?i J(? i ) = E h,u?D [? ?i ? i (u i |h i )? ui Q ? i (h, u 1 , . . . , u N )| ui=?i(ui) ] (C.16)</formula><p>where replay buffer D contains tuples of (h, u, r, h ) that also utilized for critic update like Equation C.1 as: <ref type="bibr">hj )</ref> . Updating actor parameters only needs for state and action information from replay buffer not next state and next action. From this point, MADDPG takes off-policy algorithm that enable to have replay buffer which makes sample efficient algorithms like value-based. This is why MADDPG showed good performance than general actor-critic algorithm that it has similar algorithm architecture with value-based RL. And because of the nature of the MADDPG algorithm, actor and critic should be updated at every step, making that distributed training (known as a parallel runner) in SMAC(Samvelyan et al., 2019)) unable. In SMAC(Samvelyan et al., 2019), we can select distributed training with multiple simulator or basic training with single simulator. In this paper, we use only non-distributed setting for MADDPG.</p><formula xml:id="formula_18">L(? i ) = E u,h,r,h [(Q ? i (h, u 1 , . . . , u N ) ? y) 2 ] (C.17) where y = r i + ?Q ? i (h', u 1 , . . . , u N )| u j =? j (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Completed Experimental Results of All Possible Cases</head><p>We present a part of the experimental results in the manuscript. Here, we show experiment results in all possible cases and report the win-rate performances. As mentioned, due to the constraint on MADDPG that is not compatible with training in the parallel method, we respectively show the experimental results based on both sequential and parallel episodic buffers. Prior to the explanation of all results, we note the update interval difference between the sequential episodic buffer and the parallel episodic buffer as shown in <ref type="table" target="#tab_8">Table D</ref> We pick numerous benchmark algorithms based on the selection of traditional value-based and policy-based algorithms as well as contemporary algorithms exhibiting state-of-the-art performance in the MARL domain. Consequently, we chose IQN, VDN, QMIX, and QTRAN for value-based algorithms, COMA, MASAC, and MADDPG for policy-based algorithms, and DMIX, DDN, DIQL, and DRIMA for more modern algorithms. In this part, we discuss the performance of each algorithm trained on the parallel episodic buffer and sequential episodic buffer. As demonstrated in <ref type="table" target="#tab_8">Table D</ref>.12, and <ref type="figure">Figure D</ref>.6 -D.16, for 5 million training time steps, we provide experiment results in an sequential episodic buffer for most of the SMAC + scenarios using the all algorithms. In episodic buffer settings, the performance in defense scenarios tends to decline as the supply differential increases, while the performance in offensive scenarios is marginally getting worse due to the complexity of the path from the starting place of allied troops to enemy units. Whereas many of the scenarios are quite solved by the algorithms, all the algorithms struggle to solve offense hard and superhard scenarios. Only QMIX and DRIMA can solve the offense hard scenario, as shown in <ref type="figure">Figure D.7g</ref> and <ref type="figure">Figure D</ref>.9g whereas none of the algorithms can solve offense superhard scenario. We report only defensive scenarios for some algorithms for the time limitation.</p><p>As seen in <ref type="table" target="#tab_8">Table D</ref>.13, and <ref type="figure">Figure D</ref>.17 -D.26, for 10 million steps, we present the experimental outcomes of a parallel training setup with 20 parallel runners for each scenario and algorithms except MADDPG for not compatible with parallel episodic buffer setting. This setting contains 20 times fewer updates than episodic training, which may result in a performance decrease. With a parallel buffer, the overall tendency of performance is the same as that of episodic buffer setting, which shows that the performance in defense scenarios tends to decline as the supply differential increases. But in offensive scenarios, the pattern that the performance decreases as the complexity of the scenario increases is strongly shown than in sequential episodic settings. But likewise sequential episodic setting, none of the algorithms can solve the offense hard and superhard scenarios, which are also very challenging in a parallel setting. The only algorithm which solved the offense hard scenario are DRIMA, as shown in <ref type="figure">Figure D</ref>.26g, which uses risk-based exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Benchmark on Parallel Episodic Buffer</head><p>While the majority of studies have reported performance using the sequential episodic buffer, as we mentioned above, the parallel episodic buffer would be more practical due to reduced training time. For this reason, we provide a comprehensive benchmark of MARL algorithms in this setting. Since the previous challenge (Samvelyan et al., 2019) already provided technical implementation of the parallel setup, we merely determine the parallelism level. With consideration of computation resources, we intend to run twenty simulators simultaneously to gather episodes. In these experiments, we evaluate 10 MARL algorithms on SMAC + . Overall, we observe that the tendency of experimental results is identical to those of the results from the episodic buffer as seen in <ref type="table" target="#tab_8">Table D.</ref>13. In addition, this result supports that both defensive and offensive scenarios can also be adjusted by the complexity of multi-tasks and environmental factors. Instead, as stated, due to the reduced update frequency, we observe that the performance of all baselines is marginally decreased and learning instability is increased. For example, in relatively simple scenarios like off near and off distant, typical value-based algorithms such as VDN and QMIX outperform other algorithms, whereas as complexity increases, DRIMA with an enhanced exploration capability by risk-based information attains the highest scores only in off complicated and def outnumbered. When we look closely at distributional value-based algorithms, we find that enhanced exploration by distributional value functions positively affects the performance on SMAC + , meanwhile the DMIX shows unstable outcomes despite that it occasionally gains the best scores in complex scenarios. Contrary, we see that DRIMA captures robust scores compared to   other distributional algorithms.</p><p>In another aspect, we find the moment at which win-rate begins to rise is intriguing. As you can see <ref type="figure">Figure 2</ref>, the learning curves of QMIX obviously depicts distinct tendency according to the buffer setting, on the other hand, the learning curves of  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>We additionally investigate the reason why some algorithms perform worse in offensive scenarios than the defensive scenarios. We point out that the exploration is a main issue to increase performance in offensive scenarios. For this reason, we try to find the optimal hyper-parameter setting with respect to exploration perspective by sweeping out -greedy steps and adjusting risk-sensitiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Optimization for Exploration</head><p>To determine the optimal hyperparameters for algorithms that do not solve scenarios requiring exploration, we conducted multiple types of exploration-exploitation trade-off balancing in value-based, distribution-based, and policy-based algorithms with -greedy decaying, various risk levels, and entropy term control. We choose QMIX, DFAC (DMIX, DDN), and MASAC for each category that can be trained in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1.1. VALUE-BASED ALGORITHMS</head><p>We explore for the optimal hyperparamater for exploration with -greedy in space {10k, 50k, 100k, 500k, 5000k} for QMIX, particularly in difficult defensive and offensive scenarios that demand extensive explorations. In addition to the linear decaying approach, we utilize the exponential and piece-wise decaying methods for a total of 50k steps. We discover that there is no discernible trend in adjusting -greedy, exponential, and piece-wise decaying steps for exploration in scenarios requiring exploration. We find that more exploratory factors are required for value-based algorithms to win offensive scenarios. The results are depicted in <ref type="figure">Figure Figure E</ref>.27.  In this section, we discuss the main concept of Distribution-based algorithms, followed by the outcomes of controlling risk-sensitivity.</p><p>Quantile function Distributioanl deep reinforcement learning gained popularity when <ref type="bibr" target="#b0">(Bellemare et al., 2017)</ref> proposed the C51 method, which generates output as a distribution of return given state and action. In this domain, algorithms compute TD-error between distributional bellman's updated distribution and the current distribution of return using the Wasserstein metric. It differs among methods, but the majority of them estimate the return distribution in order to compute the Wasserstein distance between the present distribution and the desired distribution (which is updated by distributional bellman update   We name each portion as risk-averse, risk-neutral-averse, risk-neutral-seeking and risk-seeking sampling. The results are shown in <ref type="figure">Figure E</ref>.29. Looking the results, we notice that risk-neutral-averse criteria with DMIX work best in most of the setting except some scenarios that no training has developed. Also, we can find that risk-seeking criteria in DMIX and DDN never worked in this setting. With these experiments, we knew that distributional RL is very fragile to the sampling ? or We choose MASAC as a representative model among policy-based algorithms since SAC was designed to improve exploration strategy by adding an entropy term to the loss function with coefficient ?, as described in Equation C.15. The default value for ? is 0.01, which favors exploration over exploitation. We set ? space to the values {0.01, 0.1, 0.5, 0.9}. Results indicate that increasing the entropy term for exploration in our scenario has little effect, and algorithms may require additional parameters for exploration. The outcome is in <ref type="figure">Figure E</ref>.28</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Reward Engineering</head><p>We evaluate the extent to the reward function engineering can solve the offense scenarios. The alternative reward function is designed to reward as agents get closer to the enemies. Since the offensive scenarios require agents to find the enemies before defeating them, agents with the explicit reward for finding the enemies to agents early in training can solve tasks sequentially. In detail, agents are rewarded by how much they get close to the enemies with respect to euclidean distance in the map until the 100k training time step. After that, they get the basic reward that is used in SMAC, SMAC + . The equation of the alternative reward function is as follows</p><formula xml:id="formula_19">R alt (h, u) = ? 1 |e| a e distance(pos a ? pos e ) (E.18)</formula><p>where pos is 2D cartesian coordinates, a and e denote agents and enemies respectively.</p><p>We test QMIX in Off distant with parallel episodic buffer and identical hyper-parameter setting. As in <ref type="figure">Figure E</ref>.30a, the results show that reward engineering makes significant improvements both in final win-rate and convergence speed compared to QMIX with a basic reward function. We argue that heat-maps of agent movement in offensive scenarios prove efficient exploration of MARL algorithms in ??. In this subsection, we show all heat-maps generated by four algorithms like COMA, QMIX, MADDPG, and DRIMA trained on the sequential episode buffer. As previously indicated, as the number of training steps increases, all agents (lower side) must move forward to the enemies positioned on the hill (upper side). As shown in <ref type="figure">Figure E</ref>.31, we identify that DRIMA can make all agents go for the enemies in all scenarios meanwhile, the other algorithms do not succeed in making agents move forward in some scenarios. When compared of <ref type="table" target="#tab_8">Table D</ref>.12 and <ref type="figure">Figure E</ref>.31, we can see that the win-rate performance at the test phase is highly correlated to agents going for enemies without direct incentives. Therefore, we claim that the exploration capability of MARL algorithms enables agents to train to discover enemies even though an algorithm at early stage does not move agents forward to enemies.</p><p>Additionally, we observe that these heat-maps might be used to determine the complexity of offensive scenarios. In the Off near, Off near, two algorithms, at least, seems to find enemies, however all algorithms except DRIMA fail to find enemies in the Off complicated, Off hard and Off superhard. When you see the results of QMIX in the Off complicated and Off hard, the agents appear to be approaching enemies, but they are not capable of precisely locating enemies. In these situations, where the enemies are scattered two-sided in front of each entrance of the hill, QMIX removes just one-sided opponents and does not find the remaining regions. On the other hand, DRIMA perfectly complete to locate enemies scattered and eliminate them in this situation. Especially in the Off superhard, the other algorithms do not completely train, but DRIMA makes agents move forward to the opponents and begin to beat them. Nonetheless, as seen in <ref type="table" target="#tab_8">Table D</ref>.12, DRIMA attains about 10% win-rate performance on Off superhard, leading us to conclude that a longer training horizon is necessary in order to simultaneously learn fine-manipulation and enemy detection.  <ref type="figure">Figure E</ref>.27: Sweeping out -decaying steps with QMIX algorithm. The numbers next to the EPSILON letter means the decaying steps. In piece-wise setting, decaying line consist of combination of two linear line and set decaying point to 0.1. Arriving that point, again decay epsilon to 0.5. The number '10000 to 50000' in label means how to use piece-wise decaying strategy That is, for example, '10000 to 50000' means decay epsilon from 1.0 to 0.1 for 10000 steps and then decay from 0.1 to 0.05 for 40000 steps </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) General terrain (b) Complex terrain (c) Hill advantage Summary of environmental factors in the SMAC + .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>{( 4</head><label>4</label><figDesc>+ 8) + (4 + ut) + (The number of allies ? (14+7+ The number of enemies and neutrals)) + 14 ? (The number of enemies + The number of neutrals)} Action space The total action set in SMAC + consists of basic action, number of enemies, and number of neutral buildings as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Initial sight of siege tanks (b) Distant attack by communication Figure A.1: The explanation of environmental factors in the SMAC + . The yellow line describes range attack by armored troops. The blue rectangle represents the alliance sights between the agent and the alliance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure</head><label></label><figDesc>A.2a shows the training curve of VDN algorithm in the def infantry with and without the communication option.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Training curve (b) No communication (c) Inter-agent communication Figure A.2: Case study of communication option. We present the results of def infantry by VDN and its associated test trial.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>to scouting Figure A.3: Sequential screenshots in Defensive Scenarios</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>A.4a. When finding the location and deployment of enemies, allies begin to attack the enemies with proper strategy if agents trained well as shown inFigureA.4b and Figure A.4c. But if agents are not trained well in finding the enemies, agents get overfitted to the bad samples resulting the bad behavior as shown in Figure A.4d.Offensive scenario is hard to solve for agents because of the sparse rewards problem and multi-goal objectives, we set the number of ally units quite larger than defensive scenarios' enemies' number for observing the training development. We can reduce the number of ally units for future work, if proper algorithms are developed.(a) Finding enemies (b) Before communication (c) After communication (d) Failed to scouting Figure A.4: Sequential screenshots in Offensive Scenarios</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Centralized</head><label></label><figDesc>Training &amp; Decentralized Execution In early stage of MARL, decentralized training &amp; decentralized execution (DTDE) is the main framework of training model like training single agent RL model. But it was very hard to train model with the DTDE framework in multi-agent setting, because of the more enhanced randomness by other agents' action selection, Partially Observable MDP setting (POMDP), and insufficient information depending only on one's observation during training the model's parameters. So, now centralized training &amp; decentralized execution (CTDE) (Oliehoek et al., 2008) framework is highly employed in Multi-Agent Reinforcement Learning (MARL) domain because of the advantage of informative training data like global state or gathering all of the observation of the agents. Most of the recent MARL algorithms have developed on the basis of CTDE learning framework which has access on all information at training step but has access on individual information only at execution step as shown in Figure C.5. Gathering all agents' information at the center enables learning in POMDP and complex setting. We describe the development of the algorithms based on CTDE from VDN(Sunehag et al., 2017) to DFAC(Sun et al., 2021) which uses distributional RL algorithm. (a) Centralized training (b) Decentralized execution Figure C.5: CTDE Framework The StarCraft Multi-Agent Challenges Plus C.1. Value-based</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>QMIX</head><label></label><figDesc>VDN(Sunehag et al., 2017)  has limitation on expressing complexity of the centralized joint Q-value that can ignore the additional global state or gathered observational information by just summing up the all individual Q-values. QMIX(Rashid et al., 2018)  develop the architecture of VDN which utilizes additivity for making Q joint satisfying IGM condition. QMIX adapt mixing network as a mixer for weighted linear summation of individual Q-values rather than simple linear summation like VDN where the parameters for mixing network are made by hypernetwork. And the IGM condition is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>??QR-DQN(Dabney et al., 2018b)??IQN(Dabney et al., 2018a)??FQF(Yang et al., 2019).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>total cumulative 2.41 million episode steps during training 2) Takes total cumulative 7.85 million episode steps during training 3) Takes total cumulative 8.15 million episode steps during training 4) Takes total cumulative 8.10 million episode steps during training DRIMA are comparable in both settings if convergence occurs. Therefore, we claim that learning multi-stage tasks and environmental factors without direct incentives are influenced by the exploration capability of MARL algorithms. QMIX trained on the sequential episodic buffer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure D. 8 :</head><label>8</label><figDesc>MADDPG trained on the sequential episodic buffer E.1.2. DISTRIBUTION-BASED ALGORITHMS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure</head><label></label><figDesc>10: IQL trained on the sequential episodic buffer characteristics, we may train a model with risk-sensitive behavior criteria.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>14: DDN trained on the sequential episodic buffer risk-sensitive criteria so as to make the needs for research about the risk-sensitive behavior in distributional reinforcement learning which is very critical in performance.(a) Defense infantry (b) Defense armored (c) Defense outnumbered Figure D.15: DMIX trained on the sequential episodic buffer (a) Defense infantry (b) Defense armored (c) Defense outnumbered Figure D.16: MASAC trained on the sequential episodic buffer E.1.3. POLICY-BASED ALGORITHMS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Figure E.30b illustrate agents find the enemies properly in the early stage of training. Even though the reward function is changed drastically at 100k time step, the agents utilize the information learned through the alternative reward function until the end of trainingFigure E.30c. 17: MASAC trained on the parallel episodic buffer E.3. Heat-map Analysis of Offensive Scenarios</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Figure D.18: COMA trained on the parallel episodic buffer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>28: Sweeping out coefficient ? of entropy term in loss function with MASAC algorithm. The numbers next to the COEF letter means the ?. As the coefficient is close to the 0 it indicates less exploration, vice versa. 29: Risk-sensitive experiments with DFAC algorithm according to the sampling methods. DDN(0.25) and DDN(0.5) means sampling ? ? U([0, 0.25]), U([0.25, 0.5]) each using DDN algorithm. (a) QMIX Training Curve (b) Early stage (c) Last stage Figure E.30: (a) is the result of Reward engineering experiment. The line QMIX RE 100K indicate QMIX trained with reward engineering until 100k training time step. (b), (c) are heat-maps of all agents movements at 500k, 10m training time step each. The StarCraft Multi-Agent Challenges Plus (a) Off near scenario (b) Off distant scenario (c) Off complicated scenario (d) Off hard scenario (e) Off superhard scenario Figure E.31: Heat-maps of agents movement by COMA, QMIX, MADDPG and DRIMA in all offensive scenarios. All baselines are trained on the sequential episodic buffer. The result of early stage is captured at six hundred thousands steps while The end of training is 5 million cumulative steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>List of environmental factors and multi-stage tasks for both SMAC and SMAC + . Both SMAC and SMAC + employ the same final objective like eliminating all enemies. Off complicated scenario ? We propose a novel MARL environment; SMAC + . This intends to identify how agents explore to learn sequential completion of multi-stage tasks and environmental factors when reward functions are designed only towards the final objective.</figDesc><table><row><cell></cell><cell cols="2">SMAC</cell><cell cols="2">SMAC +</cell></row><row><cell></cell><cell>2c vs 64zg</cell><cell>corridor</cell><cell>Defense</cell><cell>Offense</cell></row><row><cell>Environmental Factors</cell><cell>Different levels of the terrain</cell><cell>Limited sight range of enemies</cell><cell>Destroy obstacles hiding enemies Place in less damage zones</cell><cell>Approach enemies strategically Discover a detour  *  Destroy moving impediments  *</cell></row><row><cell>Multi-Stage Tasks</cell><cell>-</cell><cell>Avoid enemies first, eliminate individually</cell><cell>-</cell><cell>Identify where enemies locate, then exterminate enemies</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* :</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>List of units and their roles in SMAC + Limited range of fire Siege Tank Firepower against groups / Limited range of sight / Protection from close-range attack 2.1. General Description of SMAC + : Terrain and Unit Combination</figDesc><table><row><cell>Unit</cell><cell>Role</cell></row><row><cell>Marine</cell><cell>Intensive firepower / Search for opponent positions</cell></row><row><cell>Marauder</cell><cell>Damage absorption from enemy / Enemy's movement restriction</cell></row><row><cell>Tank</cell><cell>Strong firepower against individuals /</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Taxonomy of offensive and defensive scenarios.</figDesc><table><row><cell></cell><cell>Defensive scenario</cell><cell></cell><cell cols="2">Offensive scenario</cell></row><row><cell>Scenario</cell><cell cols="2">Supply difference Opponents approach</cell><cell>Scenario</cell><cell>Distance from opponents</cell></row><row><cell>Def infantry</cell><cell>-2</cell><cell>One-sided</cell><cell>Off near</cell><cell>Near</cell></row><row><cell>Def armored</cell><cell>-6</cell><cell>Two-sided</cell><cell>Off distant</cell><cell>Distant</cell></row><row><cell>Def outnumbered</cell><cell>-9</cell><cell>Two-sided</cell><cell>Off complicated</cell><cell>Complicated</cell></row></table><note>The term "Supply difference" refers to the gap in popula- tions between opponents and allies in the StarCraft2. Ene- mies are scattered in all offensive scenarios such that they cannot fire fiercely, allowing agents to easily defeat them when they engage.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The list of the most challenging offensive scenarios. New scenarios require to simultaneously learn micro-control and multi-stage tasks.</figDesc><table><row><cell>Scenario</cell><cell>Supply difference</cell><cell>Distance from opponents</cell><cell>Opponents formation</cell></row><row><cell>Off hard</cell><cell>0</cell><cell>Complicated</cell><cell>Spread</cell></row><row><cell>Off superhard</cell><cell>0</cell><cell>Complicated</cell><cell>Gather</cell></row><row><cell cols="2">topographical changes.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Average win-rate (%) performance of QMIX, DRIMA, COMA and MADDPG. All methods used sequential episodic buffers. Note that MADDPG is only compatible with the sequential episodic buffer.</figDesc><table><row><cell></cell><cell>Trial</cell><cell cols="2">Defensive scenarios</cell><cell></cell><cell></cell><cell cols="2">Offensive scenarios</cell></row><row><cell></cell><cell></cell><cell cols="6">infantry armored outnumbered near distant complicated</cell></row><row><cell></cell><cell>1</cell><cell>75.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>COMA(Foerster et al., 2018)</cell><cell>2</cell><cell>28.1</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell></cell><cell>3</cell><cell>21.9</cell><cell>0.0 1)</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0 2)</cell></row><row><cell></cell><cell>1</cell><cell>100</cell><cell>100</cell><cell>3.1</cell><cell>0.0</cell><cell>0.0</cell><cell>100</cell></row><row><cell>QMIX(Rashid et al., 2018)</cell><cell>2</cell><cell>93.8</cell><cell>0.0</cell><cell>0.0</cell><cell>100.0</cell><cell>100.0</cell><cell>87.5</cell></row><row><cell></cell><cell>3</cell><cell>96.9</cell><cell>0.0</cell><cell>0.0</cell><cell>90.6</cell><cell>93.8</cell><cell>0.0 3)</cell></row><row><cell></cell><cell>1</cell><cell>100</cell><cell>96.9</cell><cell>81.3</cell><cell>0.0</cell><cell>90.6</cell><cell>0.0</cell></row><row><cell>MADDPG(Lowe et al., 2017)</cell><cell>2</cell><cell>100</cell><cell>84.4</cell><cell>81.3</cell><cell>75.0</cell><cell>0.0</cell><cell>75.0</cell></row><row><cell></cell><cell>3</cell><cell>100</cell><cell>90.6</cell><cell>71.9</cell><cell>100.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell></cell><cell>1</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>93.8</cell><cell>100 4)</cell><cell>96.9</cell></row><row><cell>DRIMA(Son et al., 2022)</cell><cell>2</cell><cell>100</cell><cell>96.9</cell><cell>96.9</cell><cell>93.8</cell><cell>100</cell><cell>100</cell></row><row><cell></cell><cell>3</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>96.9</cell></row><row><cell cols="2">The total number of win-rate ? 80%</cell><cell>9</cell><cell>7</cell><cell>5</cell><cell>6</cell><cell>6</cell><cell>5</cell></row><row><cell cols="4">1) Takes total cumulative 3.29 million episode steps during training</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">2) Takes total cumulative 4.21 million episode steps during training</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">3) Takes total cumulative 4.59 million episode steps during training</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">4) Takes total cumulative 2.53 million episode steps during training</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(a) Sequential episodic buffer (b) Parallel episodic buffer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Average win-rate (%) performance on more challenging offensive scenarios. These scenarios require to address multi-stage tasks and micro-control at once.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Episodic buffer</cell><cell></cell><cell>Parallel buffer</cell></row><row><cell></cell><cell>Trial</cell><cell cols="4">Off hard Off superhard Off hard Off superhard</cell></row><row><cell></cell><cell>1</cell><cell>96.9</cell><cell>15.6</cell><cell>100</cell><cell>0.0</cell></row><row><cell>DRIMA(Son et al., 2022)</cell><cell>2</cell><cell>93.8</cell><cell>3.1</cell><cell>80.0</cell><cell>0.0</cell></row><row><cell></cell><cell>3</cell><cell>93.8</cell><cell>15.6</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell cols="6">iment results are respectively documented at Appendix B,</cell></row><row><cell cols="5">Appendix C, Appendix D and Appendix E.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A</head><label>A</label><figDesc></figDesc><table><row><cell>Feature</cell><cell>Description</cell><cell>Dimension</cell></row><row><cell>Ally</cell><cell>relative y, unit type, last action} {health, cooldown, relative x,</cell><cell>The number of allies</cell></row><row><cell>Feature</cell><cell>Description</cell><cell>Dimension</cell></row><row><cell>Move</cell><cell>basic movement</cell><cell>4</cell></row><row><cell>Own</cell><cell>{health, unit type}</cell><cell>1 + ut</cell></row><row><cell>Ally</cell><cell>{visibility, health, distance, relative x, relative y, unit type}</cell><cell>The number of allies ?(5 + ut)</cell></row><row><cell>Enemy</cell><cell>{visibility, health, distance, relative x, relative y, unit type}</cell><cell>The number of enemies ?(5 + ut)</cell></row></table><note>.1: Global state information in SMAC?(4 + ut + 6+number of enemies) Enemy {health, relative x, relative y, unit type} The number of enemies ?(3 + ut) Total {Ally + Enemy} {The number of allies ?(4 + ut + 6+number of enemies) + The number of enemies ?(3 + ut)} Table A.2: Observation information in SMACTotal {Move, Own, Ally, Enemy} {4 + (1 + ut) + (The number of allies ?(5 + ut)) (The number of enemies ?(5 + ut))} The observation features in SMAC + are listed in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A</head><label>A</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">.3: Observation information in SMAC +</cell></row><row><cell>Feature</cell><cell>Description</cell><cell>Dimension</cell></row><row><cell>Move</cell><cell>{basic movement, pathing grid value}</cell><cell>4 + 8</cell></row><row><cell>Own</cell><cell>{health, x, y, z, unit type}</cell><cell>4 + ut</cell></row><row><cell>Ally</cell><cell>{visibility, health, distance, relative x, relative y, unit type, last action }</cell><cell>The number of allies ? (14 + 7+ The number of enemies and neutrals)</cell></row><row><cell>Enemy</cell><cell>{visibility, health, distance, relative x, relative y, relative z, unit type}</cell><cell>The number of enemies ? 14</cell></row><row><cell>Neutral</cell><cell>{visibility, health, distance, relative x, relative y, relative z, unit type}</cell><cell>The number of neutrals ? 14</cell></row><row><cell>Total</cell><cell>{Move, Own, Ally, Enemy, Neutral building }</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A .</head><label>A</label><figDesc></figDesc><table /><note>4: Action space in SMAC +</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table A .</head><label>A</label><figDesc>5: Shooting, Sight range &amp; Fire power in SMAC + . Fire powers are categorized into two types. The left one is default fire power and the right one is enhanced fire power according to the opponent's characteristic.</figDesc><table><row><cell>Range</cell><cell cols="4">Marine Marauder Tank Siege Tank</cell></row><row><cell>Shooting</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>17</cell></row><row><cell>Sight</cell><cell>9</cell><cell>9</cell><cell>9</cell><cell>9</cell></row><row><cell>Fire power</cell><cell>6</cell><cell>10 / 30</cell><cell>15 / 25</cell><cell>5 / 10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>The agent's observation only receives the features of visible units. To allow communication among the agents in SMAC + , we re-design the visibility matrix by setting communication range. If agent i and j is in the communication range of each other, then they share the visible units. For example, if the enemy k is in the sight range of the agent j but not in the agent i's, visibility matrix entity (i, k) is changed from 0 to 1. The agents use this modified visibility matrix to get observation features. The sight range is 9 for all agents, and the communication range is 16 for the Siege Tank and 12 for other units. Researchers can easily turn on or off the communication function by setting obs communicate info = True or False in the environment setting file. The default value is True. Note that this setting does not violate CTDE assumption, as the agent receives additional Enemy feat, Ally feat, Neutral building feat ofTable A.3 according to the updated visibility matrix, but the global state or the combination of the entire observations are not accessible during a test phase. obs broadcast info is another communication function that has no limit on communication range. Hence, when we set either one option to True, the communication option is active, but turning off all these functions makes no communication among agents.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table A .</head><label>A</label><figDesc>6: SMAC + defensive scenarios. All units are only from Terran race for the reflecting realistic part of the world. SG, Mar and M mean each Siege Tank, Marauder, Marine We evaluate the following eleven MARL algorithms using the CTDE paradigm. For the value-based category, widely-adopted baselines employing value function factorization (IQL, VDN (Sunehag et al., 2017), QMIX (Rashid et al., 2018), QTRAN (Son et al., 2019)) and more recent state-of-the-art algorithms (DIQL, DMIX, DDN (Sun et al., 2021), DRIMA (Son et al., 2022)) are selected. COMA</figDesc><table><row><cell></cell><cell>Defense</cell><cell></cell></row><row><cell>Scenario</cell><cell>Ally Units</cell><cell>Enemy Units</cell></row><row><cell>Infantry</cell><cell>1 Mar &amp; 4 M</cell><cell>1 Mar &amp; 6 M</cell></row><row><cell>Armored</cell><cell cols="2">1 SG Tank, 1 Tank, 1 Mar &amp; 5 M 2 Tank, 2 Mar &amp; 9 M</cell></row><row><cell cols="3">Outnumbered 1 SG Tank, 1 Tank, 1 Mar &amp; 5 M 2 Tank, 3 Mar &amp; 10 M</cell></row><row><cell>B. Training Details</cell><cell></cell><cell></cell></row><row><cell>Training Protocol</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table A .</head><label>A</label><figDesc>7: SMAC + offensive scenarios.</figDesc><table><row><cell></cell><cell></cell><cell>Offense</cell><cell></cell></row><row><cell>Scenario</cell><cell>Ally Units</cell><cell>Enemy Units</cell><cell>Distance &amp; formation</cell></row><row><cell>Near</cell><cell>{3 SG Tank,</cell><cell>{1 SG Tank,</cell><cell>Near &amp; Spread</cell></row><row><cell>Distant</cell><cell>3 Tank,</cell><cell>2 Tank,</cell><cell>Distant &amp; Spread</cell></row><row><cell>Complicated</cell><cell>3 Mar &amp; 4 M}</cell><cell>2 Mar &amp; 4 M}</cell><cell>Complicated &amp; Spread</cell></row><row><cell>Hard</cell><cell cols="2">{1 SG Tank, 2 Tank, {1 SG Tank, 2 Tank,</cell><cell>Complicated &amp; Spread</cell></row><row><cell>Superhard</cell><cell>2 Mar &amp; 4 M}</cell><cell>2 Mar &amp; 4 M}</cell><cell>Complicated &amp; Gathered</cell></row><row><cell cols="2">2018), DRIMA (Son et al., 2022) and COMA</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table B</head><label>B</label><figDesc>Computational CostWe use a machine containing 512GB memory, 4 GPUs(GeForce RTX 3080) with 10240MB memory each and AMD EPYC 7543 Processor with 32 cores. On the basis of an offensive scenario, SMAC + requires about 70-80GB of main memory and 3000-7000 MB of GPU capacity per scenario with parallel 20 simulators settings, and we are able to obtain results within 12-24 hours. Meanwhile, when we train for total 5 million cumulative episode steps via the sequential episodic buffer, it takes at least 24 hours and grows up to 48 hours. The detailed information of computation cost is listed inTable B.9.Table B.9: Approximate training hours of SMAC + . Results are evaluated with sequential episodic buffers during 5 million training timesteps and parallel episodic buffers during 10 million training timesteps.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">.8: Hyperparameters of Network</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Hyperparameter</cell><cell>Value</cell><cell></cell><cell cols="2">Description</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Training step</cell><cell>10050000</cell><cell cols="3">how many steps we trained the model</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Discount factor</cell><cell>0.99</cell><cell></cell><cell cols="2">how we estimate the future rewards</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Learning rate</cell><cell>5 ? 10 ?4</cell><cell></cell><cell cols="2">learning rate by RMSProp optimizer</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Target update period</cell><cell>200</cell><cell></cell><cell cols="2">update period of target network</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Replay buffer size</cell><cell>5000</cell><cell cols="4">maximum container size of the past samples</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Batch size</cell><cell>32</cell><cell></cell><cell cols="2">number of samples for each update</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Batch size run</cell><cell>20</cell><cell></cell><cell cols="2">number of parallel simulator</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1.0 to 0.05</cell><cell cols="4">-greedy exploration over 50000 training steps</cell><cell></cell></row><row><cell></cell><cell cols="2">Number of sampling ?</cell><cell>32</cell><cell cols="4">number of quantile fraction samples in DFAC</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Defensive scenarios</cell><cell></cell><cell></cell><cell cols="2">Offensive scenarios</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="8">infantry armored outnumbered near distant complicated hard superhard</cell></row><row><cell>Episode</cell><cell>23</cell><cell>35</cell><cell>36</cell><cell>36</cell><cell>38</cell><cell>46</cell><cell>44</cell><cell>47</cell></row><row><cell>Parallel</cell><cell>5</cell><cell>8</cell><cell>8</cell><cell>13</cell><cell>13</cell><cell>16</cell><cell>9</cell><cell>9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>.10.Table D.10: Comparing model update interval between episodic and parallel episodic buffer when training 10 million steps.</figDesc><table><row><cell></cell><cell cols="2">Model update interval</cell><cell cols="2">The number of model update</cell></row><row><cell></cell><cell cols="4">Target network Behavior network Target network Behavior network</cell></row><row><cell>Episode</cell><cell>200 episodes</cell><cell>1 episode</cell><cell>420</cell><cell>80000</cell></row><row><cell>Parallel</cell><cell>200 episodes</cell><cell>20 episodes</cell><cell>420</cell><cell>4240</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table D .</head><label>D</label><figDesc>11: Average win-rate (%) performance of QMIX, DRIMA, COMA and MADDPG. All methods used sequential episodic buffers. Note that MADDPG is only compatible with the sequenced experience buffer.</figDesc><table><row><cell>Trial</cell><cell>Defensive scenarios</cell><cell>Offensive scenarios</cell></row><row><cell></cell><cell cols="2">infantry armored outnumbered near distant complicated hard superhard</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table D .</head><label>D</label><figDesc>13: Average win-rate (%) performance of 10 algorithms in both defensive and offensive scenarios using parallel episodic buffers with 20 parallel simulators.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Defensive scenarios</cell><cell>Offensive scenarios</cell></row><row><cell>Category</cell><cell>Algorithm</cell><cell>Trial</cell><cell cols="2">infantry armored outnumbered near distant complicated hard superhard</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>). Consequently, we may pretend that we have a model that approximates a quantile function with domain [0, 1] and return value range (? inf, + inf). A model's output may approximate the inverse of the cumulative density function.Risk-sensitive criteriaWe can observe that utilizing return distributions can result in risk-sensitive reinforcement learning. When a model approximates the quantile function, sampling ? uniformly from U[0, 0.25] yields relatively low output values. This is known as risk-averse behavior since we anticipate low rewards. If, on the other hand, we randomly pick ? from U[0.75, 1], we will obtain relatively high values, which may be viewed as an optimistic behavior. This is referred to as risk-seeking behavior. If we randomly choose ? from U[0, 1], we get a risk-neutral distribution that is not skewed toward risk-averse or risk-seeking criteria. Also, we may consider risk-sensitive behavior from the perspective of variance, also known as uncertainty. Lower variance of an action's return distribution indicates lower uncertainty, which indicates risk-averse action, whereas higher variance of an action's return distribution indicates greater uncertainty, which indicates risk-seeking action, because it is extremely difficult to predict what return we will receive. With these distributional RL</figDesc><table><row><cell>(a) Defense infantry</cell><cell cols="2">(b) Defense armored</cell><cell>(c) Defense outnumbered</cell></row><row><cell>(d) Offense near</cell><cell></cell><cell>(e) Offense distant</cell><cell>(f) Offense complicated</cell></row><row><cell cols="2">(g) Offense hard</cell><cell cols="2">(h) Offense superhard</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head></head><label></label><figDesc>Risk-sensitive experimentsWe conducted risk-sensitive criteria experiments in our setting, and we adapted DFAC variants for distributional RL named DDN and DMIX. In DRIMA algorithm which is also risk-based distributional RL, we just follow the default setting of the hyperparameter not conducted with further experiments.</figDesc><table><row><cell>We divide the sampling</cell></row><row><cell>quantile fractions into 4 portion ([0, 0.25], [0.25, 0.5], [0.5, 0.75], [0.75, 1.0]) from uniform distribution each ? ? U([?, ?]).</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was conducted by Center for Applied Research in Artificial Intelligence (CARAI) grant funded by DAPA and ADD (UD190031RD).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Implicit quantile networks for distributional reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1096" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distributional reinforcement learning with quantile regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Counterfactual multi-agent policy gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1861" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep recurrent q-learning for partially observable mdps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 aaai fall symposium series</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Updet: Universal multi-agent reinforcement learning via policy decoupling with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08001</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Randomized entity-wise factorization for multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A S</forename><surname>De Witt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>B?hmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sha</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4596" to="4606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coach-player multi-agent reinforcement learning for dynamic team composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anandkumar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6860" to="6870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-agent actor-critic for mixed cooperative-competitive environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">I</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pieter Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimal and approximate q-value functions for decentralized pomdps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Spaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="289" to="353" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Decomposed soft actor-critic method for cooperative multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06655</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning risk-sensitive policies forcooperative reinforcement learning agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Obraztsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rmix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
