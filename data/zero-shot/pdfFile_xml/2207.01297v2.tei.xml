<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transferring Textual Knowledge for Visual Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transferring Textual Knowledge for Visual Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transferring knowledge from task-agnostic pre-trained deep models for downstream tasks is an important topic in computer vision research. Along with the growth of computational capacity, we now have open-source Vision-Language pretrained models in large scales of the model architecture and amount of data. In this study, we focus on transferring knowledge for vision classification tasks. Conventional methods randomly initialize the linear classifier head for vision classification, but they leave the usage of the text encoder for downstream visual recognition tasks undiscovered. In this paper, we revise the role of the linear classifier and replace the classifier with the embedded language representations of the object categories. These language representations are initialized from the text encoder of the vision-language pre-trained model to further utilize its well-pretrained language model parameters. The empirical study shows that our method improves both the performance and the training speed of video classification, with a negligible change in the model. In particular, our paradigm achieves the state-of-the-art accuracy of 87.8% on Kinetics-400. Code: https://github.com/whwu95/Text4Vis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-training a task-agnostic model using large-scale general datasets and then transferring its learning feature representations to downstream tasks is a paradigm in many computer vision applications <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. While in the last decade, the convolutional-based models that are optimized on the ImageNet [3] (more precisely, ILSVRC-2012) dataset with a supervised style dominated this field. Owing to the dramatically increasing computational capacity, now we can train models that have several magnitude more model parameters and FLOPs on significantly larger datasets in either supervised [4, 2, 5], weakly-supervised <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref> or self-supervised <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> style. Recently, contrastive learning-based visionlanguage pre-training [1] manifest their superior capabilities in improving down-streaming tasks performance such as classification [1], captioning [9], image generation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, to name a few. These models are powerful for two reasons: i) the employed large-scale weakly-related datasets provide rich semantics and diverse representations of concepts; ii) the representation vectors of images and texts are roughly aligned in the semantic embedding space. However, the most common approach to using these models is fine-tuning the visual encoder on specific tasks. Although the rich semantics and diverse representations of concepts benefit the downstream tasks, the usage of the textual encoder is still left undiscovered.</p><p>In this study, we aim to improve the transferability of such vision-language pre-training models for downstream classification tasks, with the help of their textual encoders. Our motivation comes from the semantic similarity among the ground-truth labels. To demonstrate this, we employ the kinetics video recognition dataset [12] for the analysis. We extract the embedded textual vectors of class labels using the textual encoder released by CLIP <ref type="bibr" target="#b0">[1]</ref>. We then calculate the correlation between the embedded textual vectors. The plot is shown on the left of <ref type="figure">Figure 1</ref>. Not surprisingly, the extracted Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Inter-class correlation maps of "embeddings of class labels" for 20 categories on Kinetics-400. Left: The extracted textual vectors of class labels, Right: The "embeddings" from learned classifier. The color thresholds are adjusted for better understandability. Please zoom in for best view. textual vectors of class labels exhibit certain inter-class correlations, since part of them include the same verbs in their labels, such as playing &lt;something&gt;. Meanwhile, the labels with different verbs show a negligible inter-class correlation, such as drinking and driving. Next, we examine the final projection head of a vanilla visual recognition framework. We conduct the visual-only fine-tuning progress with the visual encoder that is also released by CLIP <ref type="bibr" target="#b0">[1]</ref>. The detailed configurations are provided in Section 4.3. The projection head is a matrix of d ? c to compute the pre-softmax values (or logits) from the d-dimensional feature vectors for the c classes. Non-rigorously, we can consider the d-dimensional row vectors as the embeddings of the class labels, allowing us to explore the inter-class correlation between these learned "embeddings", as shown on the right side of <ref type="figure">Figure 1</ref>. Interestingly, these learned "embeddings" also reveal certain correlations after the training progress, despite being initialized randomly and optimized without knowing any textual information <ref type="bibr" target="#b0">1</ref> .</p><p>Therefore, we suppose that the semantic information contained in the samples (images and videos) does correlate with inter-classes. Following this motivation, we replace the projection matrix with several variants: i) A projection matrix whose row vectors are randomly sampled (trivial correlation); ii) A projection matrix whose row vectors are orthogonal to each other (non-correlated). Then we replace the projection matrix with fixed embedded textual vectors that provide the "proper" correlation. In the empirical studies, we find that the textual knowledge significantly improves the transferability of pre-trained models, regarding both the classification accuracy and the convergence speed. Our main contributions are summarized as follows:</p><p>? We build a new recognition paradigm to improve the transferability using knowledge from the textual encoder of the well-pretrained vision-language model.</p><p>? We conduct extensive experiments on popular video and image datasets (i.e., Kinetics-400 <ref type="bibr" target="#b11">[12]</ref>, UCF-101 <ref type="bibr" target="#b12">[13]</ref>, HMDB-51 <ref type="bibr" target="#b13">[14]</ref> and ImageNet <ref type="bibr" target="#b2">[3]</ref>) to demonstrate the transferability of our solution in many types of transfer learning, i.e., image/video recognition, zero-shot recognition, few-shot recognition. Our approach democratizes the training on large-scale video/image datasets and achieves state-of-the-art performance on video recognition tasks, e.g., 87.3% top-1 accuracy on Kinetics-400.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Denotations. In the rest of the paper, we use bold letters to denote Vector, and capital italic letters to denote Tensor or Matrix. For instance, we employ z ? R d to denote the feature vector extracted from a pre-trained model of dimension d, we employ W ? R d?c to denote the projection matrix for the c?class linear classifier. Without ambiguity, we also use capital italic letters to denote the   <ref type="figure">Figure 2</ref>: Illustration of (a) standard visual recognition paradigm, (b) vision-language pretraining paradigm, and (c) our proposed recognition paradigm. modality in subscripts, especially we employ V and T to denote the Visual modality and Textual modality, respectively. We further employ lowercase italic letters to denote functions or neural networks. For instance, we employ g V (?, ? V ) and g T (?, ? T ) to denote the visual encoder and textual encoder, respectively. Additionally, we employ calligraphic letters, e.g., D, to denote sets of elements.</p><formula xml:id="formula_0">Classifier W b x c b x d b x d b x b b x d c x d b x c g T g V g V g V g T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Revisiting of the standard paradigm and the vision-language pre-training</head><p>Standard visual feature transferring paradigm. We start with the most ordinary scenario, where a visual feature encoder model g V is optimized using a large-scale dataset D that contains visual samples with or without ground-truth labels. On our labeled downstream dataset D = {(x 1 , y 1 ), (x 2 , y 2 ), . . .}, our empirical learning target can be written as</p><formula xml:id="formula_1">g * V , W * = argmin ? V ,W E x,y?D H(y|?(W ? g V (x))) ,<label>(1)</label></formula><p>where H(p|p) stands for the CrossEntropy between the predicted distribution p and the ground-truth distributionp, ? denotes the softmax operation, W ? R c?d denotes the linear projection matrix for classification. The formulation in Eq. 1 is a standard visual feature transferring paradigm, where the visual encoder g V and the projection matrix (classifier) W are learned simultaneously.</p><p>Vision-language pre-training in CLIP. As shown in <ref type="figure">Figure 2</ref>(b), we then review the contrastive pre-training paradigm of the vision-language models in <ref type="bibr" target="#b0">[1]</ref>. Given a weakly related image-text pair dataset D = {(x V,1 , x T,1 ), (x V,2 , x T,2 )...}. With slight abuse of the notations, we employ the x V , x T to denote a mini-batch of size b, then we minimize the following target,</p><formula xml:id="formula_2">g * V , g * T = argmin ? V ,? T E x V ,x T ?D H(Q|?(g V (x V ) T ? g T (x T ))) ,<label>(2)</label></formula><p>where Q is the set that contains b one-hot labels of size c, with their 1, 2, . . . , b -th element being 1 (b &lt; c, denoting the positive image-text pairs. Here we clarify that, the definition in Eq. 2 is not the rigorous form of the Noise-Contrastive Estimation (NCE) loss proposed in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Instead, we employ the cross entropy version implementation in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>. This implementation depicts a connection between the standard feature transferring paradigm and ours. In which, the g T (x T ) can be considered as the projection matrix that map the visual feature g V (x V ) to the given label set Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Our proposed paradigm</head><p>As discussed in Section 1, we replace the learnable randomly initialized linear projection matrix W with pre-defined matrixW . Similarly, the training target can be written as</p><formula xml:id="formula_3">g * V = argmin ? V E x,y?D H(y|?(W ? g V (x))) .<label>(3)</label></formula><p>Note thatW is not in the optimization targets, since we freeze it from updating during the fine-tuning on the downstream tasks. We do this for two reasons: Firstly, it could preserve the textual knowledge from being disturbed by the randomness brought by the mini-batch. For instance, when some classes are missing, their embedded feature vector might be broken by the other classes; Secondly, we want to provide a fair comparison between different initializations ofW (The unfrozen results are given in the Appendix ?A.4). Now we consider how to initializeW . To examine how the correlation between the semantic information contained in the samples helps, we investigate the following four types of initialization, where the forth is our proposed initialization.</p><p>Randomized matrix For the most simple randomized matrix case, we set each row of theW with a random Gaussian vector of zero mean and standard deviation, that is</p><formula xml:id="formula_4">W ? N (0, I d ),<label>(4)</label></formula><p>where I d denotes the identity matrix of dimension d ? d. Arithmetically, a trivial "correlation" would appear between the row of theW , since the sampling size is significantly small to be biased. Evidently, the trivial "correlation" cannot indicate the real correspondence between the classes due to its stochasticity. Therefore we expect the model to have inferior performance since it needs to avoid these incorrect correlations when learning the visual feature representation.</p><p>Randomized Orthogonal matrix We follow the approach of the randomized matrix. We then remove the correlation by ensuring the row vectors are orthogonal. This is achieved by QR decomposition. Concretely, since d &gt; c, we first generate a random matrix of size d ? d and select the first c rows as our projection matrix. Formally, we have,</p><formula xml:id="formula_5">W j ? QR(U ) j , j = 1, 2, . . . , c, U i ? N (0, I d ), i = 1, 2, . . . , d,<label>(5)</label></formula><p>where U is the intermediate randomized matrix, QR(U ) is the row orthogonal matrix obtained through the QR decomposition. Similar to the randomized matrix, we also expect this initialization to have inferior performance. Given the fact that the one-hot label vectors are also orthogonal to each other, it will not be helpful to project the visual feature vectors with an orthogonal matrix, which increases the difficulty of learning meaningful visual features.</p><p>Linear discriminant projection We consider another way of initializing the projection matrix. We employ the multi-class Fisher's linear discriminant analysis (LDA) to learn a linear classifier, then employ the weight matrix of the classifier as our initialization of the projection matrix. The LDA is optimized using the visual embeddings from the pre-trained model of samples in the train split. Then we compute the projection matrix following previous work <ref type="bibr" target="#b17">[18]</ref>. Intuitively, the LDA first projects the feature vectors into a lower dimension space that maximizes the inter-class covariance and then estimates the likelihood of a sample to the class distributions. We, therefore, term this as the maximal correlation initialization. As an essential classifier, this type of initialization delivers reasonable performance, but it is largely dependent on the data employed to compute the projection matrix. When the data is limited, the estimated correlation will be biased. On the other hand, in our proposed paradigm, the pretrained textual encoder provides unbiased correlations for fine-tuning.</p><p>Textual embedding vectors We finally describe our proposed feature transferring paradigm. Briefly, the projection weightW is composed of the embedded textual feature vectors of the labels. Given a set of tokenized class labels L = {l 1 , l 2 , . . . , l c }, we hav?</p><formula xml:id="formula_6">W i ? g T (l i ), i = 1, 2, . . . , c,<label>(6)</label></formula><p>whereW i the i-th row vector in matrixW . AndW i is initialized using the textual encoder output of the textual label of the i-th class. In the experimental analysis, we investigate two types of textual feature encoders: i) The encoder that is trained with a visual encoder in the contrastive style; ii) The encoder that is trained solely using only textual samples on tasks such as masked language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Works</head><p>Visual Recognition. Convolutional networks have long been the standard for backbone architectures in image recognition <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> and video recognition <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. Inspired by the Transformer <ref type="bibr" target="#b31">[32]</ref> scaling successes in Natural Language Processing, Vision Transformer (ViT) <ref type="bibr" target="#b32">[33]</ref> applies a standard Transformer directly to images, which delivers impressive performance on image recognition. Since then, ViT <ref type="bibr" target="#b32">[33]</ref> has led a new trend in image recognition backbone architectures, shifting from CNNs to Transformers. To improve performance, follow-up studies (e.g., DeiT <ref type="bibr" target="#b33">[34]</ref>, Swin <ref type="bibr" target="#b34">[35]</ref>) have been developed. Also, many works has begun to adopt transformers in video recognition, such as TimeSFormer <ref type="bibr" target="#b35">[36]</ref>, ViViT <ref type="bibr" target="#b36">[37]</ref>, VideoSwin <ref type="bibr" target="#b37">[38]</ref>, and MViT <ref type="bibr" target="#b38">[39]</ref>.</p><p>Vision-language Pre-training. Recently, CLIP <ref type="bibr" target="#b0">[1]</ref> provides good practice in learning the coordinated vision-language pretraining models using the image-text InfoNCE contrastive loss <ref type="bibr" target="#b39">[40]</ref>. Based on CLIP, several variants <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> have been proposed by combining more types of learning tasks such as image-text matching and masked image/language modeling. These contrastively learned models have two deserved properties for downstream tasks: the abundant visual feature representations and the aligned textual feature representations. Yet another study <ref type="bibr" target="#b45">[46]</ref> merged the downstream classification task into the pretraining progress, which demonstrates a decent improvement of accuracy over the standard cross-entropy loss. Recently, many video-text retrieval methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b52">53]</ref> have benefited from vision-language pre-training as well. Moreover, a few recent works <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref> transfer the CLIP <ref type="bibr" target="#b0">[1]</ref> pre-trained image-text matching model to the downstream video-text matching framework for video recognition with contrastive loss. Specifically, ActionClip <ref type="bibr" target="#b53">[54]</ref> extends the CLIP <ref type="bibr" target="#b0">[1]</ref> to train a downstream video-text matching model and then perform video recognition indirectly using the similarity between learned video and text encoders during inference. <ref type="bibr" target="#b54">[55]</ref> focus on efficient prompting and learning the continuous prompt template as text input for video recognition. Instead of these matching-based approaches, we aim to propose a new recognition paradigm that directly transfers textual knowledge for visual recognition. Our approach can balance performance and efficiency, and experiments demonstrate that our approach can reduce computational power requirements while democratizing training on large-scale video/image datasets (see <ref type="table" target="#tab_10">Table 9</ref> and 12 for more information).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments: Video Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setups</head><p>To evaluate our method for video recognition, we conduct experiments on three widely used benchmarks, i.e., Kinetics-400 <ref type="bibr" target="#b11">[12]</ref>, UCF-101 <ref type="bibr" target="#b12">[13]</ref> and HMDB-51 <ref type="bibr" target="#b13">[14]</ref>. See Appendix ?A.1 for more details.</p><p>Training &amp; Inference. We utilize ResNet <ref type="bibr" target="#b19">[20]</ref> and ViT <ref type="bibr" target="#b32">[33]</ref> as the visual encoders since they are the representative backbones of CNN and vision transformer, respectively. We employ the pre-trained visual and textual encoder released by CLIP <ref type="bibr" target="#b0">[1]</ref> in most experiments for simplicity. Given a video, we first uniformly sampled T (e.g., <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32)</ref> frames over the entire video. Then image patches with the resolution of 224?224 are randomly resized cropped from the sampled frames to form the input. We use RandAugment for the data augmentation. The model is optimized using AdamW with momentum set to 0.9. We use an initial learning rate of 5e ?6 , a cosine learning rate schedule with a 5-epoch linear warmup and a batch size of 256 for experiments on all datasets. For fast training, we set the total training epoch to 30 unless specified otherwise.</p><p>To trade off accuracy and speed, we consider two evaluation protocols. (1) Single View: We use only 1 clip per video and the center 224?224 crop for efficient evaluation, (e.g., as in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3). (2)</head><p>Multiple Views: This is a widely used setting in previous works <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b56">57]</ref> to sample multiple clips per video (e.g., 10 clips) with several spatial crops (e.g., 3 crops) in order to get higher accuracy. For comparison with SOTAs, we use four clips with three 224?224 crops ("4?3 Views") in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results.</head><p>Comparison to state-of-the-art. In <ref type="table" target="#tab_1">Table 1</ref>, on Kinetics-400, we compare to state-of-the-arts that are pre-trained on large-scale datasets such as ImageNet-21K <ref type="bibr" target="#b2">[3]</ref>, IG-65M <ref type="bibr" target="#b62">[63]</ref>, JFT-300M <ref type="bibr" target="#b1">[2]</ref>, FLD-900M <ref type="bibr" target="#b43">[44]</ref> and JFT-3B <ref type="bibr" target="#b4">[5]</ref>. The suffix represents the magnitude of the dataset, e.g., JFT-3B consists of nearly 3 billion annotated images. We include the details of these web-scale datasets in Appendix ?B.1. To the best of our knowledge, up to now, none of the three largest datasets (i.e., JFT-300M, FLD-900M, JFT-3B) are open-sourced and also do not provide pre-trained models. Thus, we use the CLIP <ref type="bibr" target="#b0">[1]</ref> checkpoints, which are publicly available 2 and have been trained on 400 million web image-text pairs (namely WIT-400M). Observe that we achieve state-of-the-art results. Specifically, our model outperforms all JFT300M-pretrained methods in terms of Top-1 and Top-5 accuracy. We achieve 87.3%, which improves even further by 0.8% over Florence <ref type="bibr" target="#b43">[44]</ref>, although their model and data scale are both 2?larger. Besides, our model is even better than JFT3B-pretrained Few-shot video recognition. Video recognition using only a few samples is known as few-shot video recognition. We study a more challenging K-shot C-way situation instead of the conventional 5-shot 5-way configuration. We scale the task up to categorize all categories in the dataset with just K samples per category for training. The upper bound of this situation is denoted by the term "All-shot". <ref type="table" target="#tab_2">Table 2</ref> reports the top-1 accuracy for the three datasets. In this extreme scenario of few data, we use 200 epochs to train models with ViT-B/16 for few-shot video recognition. For temporal modeling, we use TAP. We can observe that our method provides amazing transferability on diverse domain data in these extreme data-poor circumstances.  Zero-shot video recognition. We conduct experiments on two open-set settings: 1) Intra-dataset: The Kinetics-400 was divided into two parts: 300 categories (K300) for training and 100 categories (K100) for zero-shot recognition. 2) Cross-dataset: We train our models on K400 and then evaluate them on UCF101. To avoid catastrophic forgetting <ref type="bibr" target="#b63">[64]</ref>, here we train our models with few epochs. As shown in <ref type="table" target="#tab_3">Table 3</ref>, unlike the traditional recognition paradigm, ours can achieve zero-shot recognition for unseen categories by replacing the offline classifiers. Appropriately tweaking the pre-trained model slightly can boost performance even further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablations on Kinetics.</head><p>In this section, we conduct extensive ablation experiments to demonstrate our method with the instantiation. Models in this section use 8-frame input, ViT-B/16 as the visual backbone, 30 epochs for training and a single view for testing on Kinetics-400, unless specified otherwise.</p><p>Comparison with vision-only framework. <ref type="figure">Figure 2(a)</ref> illustrates the standard visual recognition framework. As a comparison with our method, we train the unimodality video model, which consists of the same visual encoder and a learnable classifier with random initialization. To produce video embedding, we just apply temporal average pooling (TAP) to frame embeddings. As presented in <ref type="figure" target="#fig_1">Figure 3</ref>, our method surpasses Vision-Only baselines across multiple label fractions on Kinetics-400.</p><p>Especially when just only 10% labeled data is available for training, demonstrating that the advantage of our paradigm is more profound when the labeled data is limited. Also, when training with full data, our Vision-Text method leads to an additional 5% improvement with the same training recipe. <ref type="figure" target="#fig_2">Figure 4</ref> further demonstrates our paradigm significantly improves convergence speed.   Different assignments to the offline classifier. We set different initializations described in section 2.2 to the offline classifier W ? R d?c and then train our visual encoder on Kinetics-400. <ref type="table" target="#tab_5">Table 4</ref> lists their comparisons. We show that feeding the offline classifier a random d-by-c matrix with a normal distribution reduces performance significantly. Then we assign the orthogonal matrix to the classifier, and we can see that having different classes that are orthogonal will result in inferior performance. Also, we choose DistilBERT <ref type="bibr" target="#b64">[65]</ref> as the textual encoder to pre-extract the text embeddings of c categories. The resulting performance is the same as that of the CLIP's textual encoder. Furthermore, we term the linear discriminate projection as the maximal correlation initialization, as stated in Section 2.2. To do so, we first sample 60 videos from each class in the training set and utilize the pre-trained visual encoder to extract visual embeddings from these 24,000 videos. Finally, we learn the linear classifier by performing linear discriminant analysis on these visual embeddings and their ground-truth labels. We can see that the result of the LDA projection is consistent with our statement. More visualizations of these classifiers are in Appendix ?A.3.  Temporal modeling. Here we explore more temporal modelings for ViT <ref type="bibr" target="#b32">[33]</ref> and ResNet <ref type="bibr" target="#b19">[20]</ref>:</p><p>(1) TAP: Temporal average pooling is the most straightforward temporal modeling.</p><p>(2) T1D: The channel-wise temporal 1D convolutions, is a common strategy <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b66">67]</ref>, to perform efficient temporal interaction in the latter stages (i.e., res 4?5 ) of ResNet. (3) T-Trans: The embeddings of frames are fed to a multi-layer (e.g., 6-layer) temporal transformer encoder. (4) TokenT1D:</p><p>We use T1D to model temporal relations for [class] token features that are aggregated from local features via attention in the vision transformer. We perform the TokenT1D in multiple positions of a vision transformer. Results are shown in <ref type="table" target="#tab_6">Table 5</ref>. On both backbones, TAP provides simple baselines and T-Trans exhibits the best top-1 accuracy. Both of them maintain the original frame-level representations and then perform temporal modeling. An interesting thing we observed is that T1D does not seem to work in this scenario. The reason lies in that T1D may have the potential to break the learned strong representations provided by CLIP. TokenT1D is another internal-backbone temporal modeling, and it does not yield a performance drop, and even slightly improves the TAP baseline. We believe this is because TokenT1D is only imposed on the global [class] token features instead of patches features, resulting in minimal modifications on pre-trained features.</p><p>Visual encoder with different pre-training. Besides CLIP-pretrained visual encoders, we further explore our paradigm with different pre-trained visual encoders. As shown in <ref type="table" target="#tab_7">Table 6</ref>, equipped with ImageNet-pretrained visual encoder, our method helps to improve the vision-only counterpart by 0.9%. We can see that the CLIP-pretrained visual encoder achieves more significant performance, which is probably because CLIP provides the coarse initial alignment between frames and category names, as well as covers rich visual concepts.  Text input forms. Intuitively, the name of a class appears to be the most straightforward text information. We can see that only using the label text can yield good results in <ref type="table" target="#tab_8">Table 7</ref>. Then following the prompt engineering in CLIP <ref type="bibr" target="#b0">[1]</ref>, we utilize the prompt template "a video of a person {label}." to help specify the text is about the content of the video. This only slightly increases performance over the baseline of using the label text. We further use multiple prompt templates as the text augmentation during training. Performance decreases by 0.64% on Kinetics-400. This may be because different prompt templates may introduce extra noise for the training. In addition to the hand-crafted prompt, we also adopt an automated prompt <ref type="bibr" target="#b67">[68]</ref> to describe a prompt's context using a set of learnable vectors. The results suggest that different templates have little impact on our model. More instantiations. We assess different instantiations of our paradigm, in terms of different visual encoders, more input frames, and larger spatial resolution. See Appendix ?B.2 for more details on architectures. In <ref type="table" target="#tab_9">Table 8</ref>, we present the results of our method with two typical evaluation protocols. In general, more frames, larger spatial resolution, and deeper backbones lead to higher accuracy. Our recognition paradigm vs. Matching paradigm. Here we make a comparison with the matching-based method mentioned in Section 3. The matching paradigm treats the recognition task as a video-text matching problem with contrastive loss, thus requiring a batch gathering to collect embeddings of all batches across all GPUs and calculate cosine similarity for a given batch across all other batches. See Appendix ?B.3 for details about the batch gathering. In <ref type="table" target="#tab_10">Table 9</ref>, we try to compare with the matching paradigm <ref type="bibr" target="#b53">[54]</ref> as fairly as we can. We can see that the matching paradigm does not work well without batch gather. This is due to contrastive learning favors a large batch size. Besides, involving batch gather will multiply the training time. Also, in this case, the pre-trained textual encoder still needs to be updated, which requires larger GPU memory. However, our paradigm employs pre-extracted text embeddings as our classifier, so the only thing we need to fine-tune is the visual encoder. Results show that our method achieves the best accuracy-cost trade-off. Specifically, our method achieves the performance of 81.52% with VIT-B/16, which takes only 10 hours to run the training using 8 GPUs (2?faster than the matching counterpart).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments: Image Recognition</head><p>We also evaluate our approach to the image recognition task. Here we conduct experiments on ImageNet <ref type="bibr" target="#b2">[3]</ref> and share the same training recipe in section 4.1 with ImageNet.</p><p>Few-shot image recognition. Here we also use the challenging K-shot C-way setting on ImageNet. Specifically, the models are trained using K images (shots) from the training set for each image category and then measure performance on the corresponding standard 1000-class testing set. As shown in <ref type="table" target="#tab_1">Table 10</ref>, the results reveal that our method has strong transferability under data-poor conditions, whereas the standard unimodality paradigm is ineffective in comparison to ours.  Zero-shot image recognition. Here we split the ImageNet-1K into two parts, with 600 categories (IN600) for training, and the remaining unseen 400 categories (IN400) for evaluation. <ref type="table" target="#tab_1">Table 11</ref> demonstrates the zero-shot image recognition ability of our method.</p><p>Efficient training. For readers' reference, we provide the performance of our approach with different visual backbones on ImageNet in Tabel 12. Notably, using 8 GPUs, we can train the VIT-B/16 to achieve 82.25% in 90 minutes, while the ViT-L/14 only takes 6 hours to achieve 86.47%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present a new paradigm for improving the transferability of visual recognition that is based on the knowledge from the textual encoder of the well-trained vision-language model. The empirical study shows that our method improves both the performance and the convergence speed of visual classification. The proposed approach has superior performance on both general and zero-shot/few-shot recognition and achieves state-of-the-art performance on video recognition tasks, and democratizes training on large-scale video/image datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this appendix, ?A contains further results for video recognition: the statistics of video datasets ( ?A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Results on Video Recognition</head><p>A.1 Video datasets</p><p>? Kinetics-400 (K400) <ref type="bibr" target="#b11">[12]</ref> is a large-scale video dataset, which consists of 240k training videos and 20k validation videos in 400 different human action categories.</p><p>? UCF-101 <ref type="bibr" target="#b12">[13]</ref> contains 13k videos spanning over 101 human actions.</p><p>? HMDB-51 <ref type="bibr" target="#b13">[14]</ref> contains approximately 7k videos belonging to 51 action class categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Comparison with state-of-the-arts on UCF-101 and HMDB-51</head><p>We also evaluate our method on the UCF-101 and HMDB-51 datasets to demonstrate its capacity to generalize to smaller datasets. We finetune our models on these two datasets using the pre-trained ViT-L model on Kinetics-400 and present the mean class accuracy over three splits utilizing 8 frames as inputs and 30 epochs for training.   <ref type="bibr" target="#b69">[70]</ref> 94.3% 70.9% I3D <ref type="bibr" target="#b26">[27]</ref> 95.6% 74.8% R(2+1)D <ref type="bibr" target="#b30">[31]</ref> 96.8% 74.5% S3D-G <ref type="bibr" target="#b29">[30]</ref> 96.8% 75.9% TSM <ref type="bibr" target="#b70">[71]</ref> 95.9% 73.5% STM <ref type="bibr" target="#b71">[72]</ref> 96.2% 72.2% TEINet <ref type="bibr" target="#b66">[67]</ref> 96.7% 72.1% MVFNet <ref type="bibr" target="#b56">[57]</ref> 96.6% 75.7% TDN <ref type="bibr" target="#b65">[66]</ref> 97.4% 76.4%</p><p>Ours 98.2% 79.0%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 More visualizations of different classifiers</head><p>Here we provide more visualizations of different classifiers in <ref type="figure">Figure A</ref>.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Comparison with the unfrozen classifier</head><p>As we described in Section 2.2 of the submission, we freeze the classifier from updating during the fine-tuning of the downstream tasks for the reason: It could preserve the textual knowledge from being disturbed by the randomness brought by the mini-batch. By doing so, we can replace the offline classifier and do zero-shot recognition.</p><p>Here, we test the unfrozen classifier with the same textual embeddings as the frozen classifier. The unfrozen results are given in <ref type="table" target="#tab_16">Table A.</ref>2. We can see that the unfrozen setting causes the original textual knowledge to be broken, resulting in a decrease in performance.  ? JFT-300M: JFT-300M is an internal Google dataset used to train image classification models. The dataset consists of 300M images that are labeled with 18,291 categories. Image labels are generated using a complex algorithm that combines raw web signals, web page connections, and user feedback. However, the dataset and the pre-trained weights are not open-source.</p><p>? FLD-900M: FLD-900M is a large image-caption dataset from Microsoft, which includes 900M Images and 900M Free form text (From one word, Phrase to sentence). By now, the dataset and the pre-trained weights are not open-source.</p><p>? JFT-3B: JFT-3B is an internal Google dataset and a larger version of the JFT-300M. It has over 3 billion images that have been annotated with a class structure of around 30k labels using a semi-automated procedure. Also, the dataset and the pre-trained weights are not open-source.</p><p>? WIT-400M: WIT-400M is a dataset that contains 400 million web image-text pairs, and is used to train CLIP <ref type="bibr" target="#b0">[1]</ref>. CLIP does not release the dataset, but made all of the pre-trained models available 5 . In this paper, we utilize the CLIP-pretrained models in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Visual encoder architectures</head><p>In this paper, we use the visual encoder and textual encoder as shown in   # The logits are then used as inputs for N*M-way (e.g., 128-way) classification, # resulting in a loss value corresponding to N inputs in each GPU. # Then Distributed Data Parallel mechanism takes care of averaging these across GPUs, # which becomes equivalent to calculating the loss over NMxNM (e.g.,128x128) similarities.</p><p>Hence, we develop the Distributed InfoNCE based on DDP for large batch size and fast training. The core of the Distributed InfoNCE implementation is batch gathering. Say there are M GPUs and each GPU gets N input pairs, we need to calculate the NM?NM similarity matrix across the GPUs for InfoNCE loss. Without batch gathering, each GPU only computes a local N?N matrix, s.t. N NM, Then the cosine similarity and the InfoNCE loss would be calculated only for the pairs within a single GPU and later their gradients would be averaged and synced. That's obviously not what we want.</p><p>The batch gathering for Distributed InfoNCE is presented as follows. When calculating the similarity matrix (and thus the logit scores across text inputs for each image/video), a GPU only needs to hold M vision features, and perform matrix product with NM text features, yielding an M?NM matrix. This computation is distributed (i.e., sharded) across N GPUs, and we have calculated NM?NM similarities across the GPUs in total. The loss we employ is symmetric and the same happens w.r.t.</p><p>text inputs. As shown in Algorithm 1, we also give an example pseudocode to help you understand the statement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Text template</head><p>In Table4 of the submission, we study several text input forms, including class names, single hard template, multiple hard templates, and learnable templates. More details are as follows:</p><p>Class name To build textual embeddings, we utilize the category names of the dataset as the text input, e.g., "eating hotdog", "driving car", etc.</p><p>Single hard template We employ the hand-crafted template "a video of a person {class name}." to form a sentence as input.</p><p>Multiple hard templates CLIP 6 provides 28 templates for Kinetics, one of which is the above single template. We use these multiple templates as the text augmentation during training. At each iteration, we choose one template at random as text input. Then, using the above single hard template as input, we perform the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learnable templates</head><p>We adopt the automated prompt CoOp <ref type="bibr" target="#b67">[68]</ref> to describe a prompt's context using a set of learnable vectors. Specifically, the prompt given to the text encoder is designed with the following form,</p><formula xml:id="formula_7">t = [V] 1 [V] 2 . . . [V] M [class name],<label>(7)</label></formula><p>where each [V] m (m ? {1, . . . , M }) is a vector of the same size as word embeddings, and M is a hyperparameter indicating the number of context tokens. We set the M to 4.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Vision-Text v.s.Vision-only framework under different label fractions on Kinetics-400.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The training loss of Vision-Text and Vision-only framework on Kinetics-400.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure A. 5 :</head><label>5</label><figDesc>Inter-class correlation maps of "embeddings of class labels" for 20 categories on Kinetics-400. The color thresholds are adjusted for better understandability. Please zoom in for best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>#</head><label></label><figDesc>text_encoder: encoder network for text input # vision_encoder: encoder network for vision input, e.g., images or videos. # V: minibatch of vision inputs # T: minibatch of text inputs # N: the local batch size of each GPU, e.g.,16 # M: the number of GPUs, e.g.,8 # N * M: the global batch size for multi-gpu training, e.g.,128 # extract feature representations of each modality local_vision_features = vision_encoder(V) # shape: [N, embed_dim] local_text_features = text_encoder(T) # shape: [N, embed_dim] # normalization local_vision_features = l2_normalize(local_vision_features, axis=1) local_text_features = l2_normalize(local_text_features, axis=1) # batch_gather is a function gathering and concatenating the tensors across GPUs. all_vision_features = batch_gather(local_vision_features) # shape: [N * M, embed_dim] all_text_features = batch_gather(local_text_features) # shape: [N * M, embed_dim] # scaled pairwise cosine similarities # shape = [N, N * M] logits_per_image = logit_scale * image_features @ all_text_features.t() # shape = [N, N * M] logits_per_text = logit_scale * text_features @ all_image_features.t()</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>CLS 2 , ..., CLS c</figDesc><table><row><cell>Images / Videos</cell><cell>Visual Encoder</cell><cell cols="2">Classifier W c x d Emb. logits b x d</cell><cell>CLS 1 , Images / Videos</cell><cell>Textual Encoder Visual Encoder</cell><cell>Emb.</cell><cell>Emb.</cell><cell>logits</cell></row><row><cell>a photo of a {CLS}</cell><cell>Textual Encoder</cell><cell>Emb.</cell><cell></cell><cell>CLS</cell><cell></cell><cell cols="2">Encoder</cell></row><row><cell>Images / Videos</cell><cell>Visual Encoder</cell><cell>Emb.</cell><cell>logits</cell><cell>class name</cell><cell>dot product</cell><cell cols="3">Offline encoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Textual embedding</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison to SOTAs on Kinetics-400. "Views" indicates # temporal clip ? # spatial crop. The magnitudes are Giga (10 9 ) and Mega (10 6 ) for FLOPs and Param. "IN" denotes ImageNet.CoVeR<ref type="bibr" target="#b61">[62]</ref>, and their data scale is 7.5?larger. See Appendix ?A.2 for more results on UCF-101 and HMDB-51 datasets.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell>Pre-train</cell><cell cols="4">Top-1 Top-5 FLOPs?Views Param</cell></row><row><cell>NL I3D-101 [27]</cell><cell>128?224 2</cell><cell>IN-1K</cell><cell>77.7</cell><cell>93.3</cell><cell>359?10?3</cell><cell>61.8</cell></row><row><cell>MVFNet En [57]</cell><cell>24?224 2</cell><cell>IN-1K</cell><cell>79.1</cell><cell>93.8</cell><cell>188?10?3</cell><cell>-</cell></row><row><cell>SlowFast NL101 [56]</cell><cell>16?224 2</cell><cell>Scratch</cell><cell>79.8</cell><cell>93.9</cell><cell>234?10?3</cell><cell>59.9</cell></row><row><cell>X3D-XXL [58]</cell><cell>16?440 2</cell><cell>Scratch</cell><cell>80.4</cell><cell>94.6</cell><cell>144?10?3</cell><cell>20.3</cell></row><row><cell>MViT-B, 64?3 [39]</cell><cell>64?224 2</cell><cell>Scratch</cell><cell>81.2</cell><cell>95.1</cell><cell>455?3?3</cell><cell>36.6</cell></row><row><cell cols="2">Methods with large-scale pre-training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TimeSformer-L [36]</cell><cell>96?224 2</cell><cell>IN-21K</cell><cell>80.7</cell><cell>94.7</cell><cell>2380?1?3</cell><cell>121.4</cell></row><row><cell>ViViT-L/16?2 [37]</cell><cell>32?320 2</cell><cell>IN-21K</cell><cell>81.3</cell><cell>94.7</cell><cell>3992?4?3</cell><cell>310.8</cell></row><row><cell>VideoSwin-L [38]</cell><cell>32?384 2</cell><cell>IN-21K</cell><cell>84.9</cell><cell>96.7</cell><cell>2107?10?5</cell><cell>200.0</cell></row><row><cell>ip-CSN-152 [59]</cell><cell>32?224 2</cell><cell>IG-65M</cell><cell>82.5</cell><cell>95.3</cell><cell>109?10?3</cell><cell>32.8</cell></row><row><cell>ViViT-L/16?2 [37]</cell><cell>32?320 2</cell><cell>JFT-300M</cell><cell>83.5</cell><cell>95.5</cell><cell>3992?4?3</cell><cell>310.8</cell></row><row><cell>ViViT-H/16?2 [37]</cell><cell>32?224 2</cell><cell>JFT-300M</cell><cell>84.8</cell><cell>95.8</cell><cell>8316?4?3</cell><cell>647.5</cell></row><row><cell cols="2">TokLearner-L/10 [60] 32?224 2</cell><cell>JFT-300M</cell><cell>85.4</cell><cell>96.3</cell><cell>4076?4?3</cell><cell>450</cell></row><row><cell>MTV-H [61]</cell><cell>32?224 2</cell><cell>JFT-300M</cell><cell>85.8</cell><cell>96.6</cell><cell>3706?4?3</cell><cell>-</cell></row><row><cell>CoVeR [62]</cell><cell>16?448 2</cell><cell>JFT-300M</cell><cell>86.3</cell><cell>-</cell><cell>-?1?3</cell><cell>-</cell></row><row><cell>Florence [44]</cell><cell cols="2">32?384 2 FLD-900M</cell><cell>86.5</cell><cell>97.3</cell><cell>-?4?3</cell><cell>647</cell></row><row><cell>CoVeR [62]</cell><cell>16?448 2</cell><cell>JFT-3B</cell><cell>87.2</cell><cell>-</cell><cell>-?1?3</cell><cell>-</cell></row><row><cell>Ours ViT-L/14</cell><cell>32?224 2</cell><cell>WIT-400M</cell><cell>87.1</cell><cell>97.4</cell><cell>1662?4?3</cell><cell>230.7</cell></row><row><cell>Ours ViT-L/14</cell><cell>32?336 2</cell><cell>WIT-400M</cell><cell>87.8</cell><cell>97.6</cell><cell>3829?1?3</cell><cell>230.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Few-shot video recognition on three popular datasets under K-shot C-way setting.</figDesc><table><row><cell cols="4">K-shot K400 UCF101 HMDB51</cell></row><row><cell>1</cell><cell>63.16</cell><cell>88.77</cell><cell>65.17</cell></row><row><cell>3</cell><cell>67.50</cell><cell>92.78</cell><cell>69.99</cell></row><row><cell>5</cell><cell>69.89</cell><cell>93.87</cell><cell>71.03</cell></row><row><cell>All</cell><cell>80.13</cell><cell>95.24</cell><cell>73.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Zero-shot video recognition under intradataset and cross-dataset settings. {A}?{B} indicates we train the model on dataset A then perform zero-shot recognition on dataset B.</figDesc><table><row><cell></cell><cell cols="2">K300?K100 K400?UCF</cell></row><row><cell>Ours w/o train</cell><cell>63.35</cell><cell>63.01</cell></row><row><cell>Ours w/ train</cell><cell>66.38</cell><cell>74.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Exploration of different generation methods for the frozen classifier.</figDesc><table><row><cell>Offline classifier from</cell><cell>Top 1</cell></row><row><cell>Textual encoder of CLIP</cell><cell>81.52</cell></row><row><cell>Random normal matrix</cell><cell>59.30</cell></row><row><cell>Random orthogonal matrix</cell><cell>59.44</cell></row><row><cell>DistilBERT</cell><cell>81.45</cell></row><row><cell cols="2">Linear discriminant projection 80.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Temporal modeling for video encoders.</figDesc><table><row><cell>Backbone</cell><cell cols="2">Modeling Top-1 Top-5</cell></row><row><cell></cell><cell>TAP</cell><cell>71.20 90.37</cell></row><row><cell>ResNet-50</cell><cell>T1D</cell><cell>67.18 88.45</cell></row><row><cell></cell><cell>T-Trans</cell><cell>74.26 91.67</cell></row><row><cell></cell><cell>TAP</cell><cell>80.13 94.98</cell></row><row><cell>VIT-B/16</cell><cell cols="2">TokenT1D 80.42 95.03</cell></row><row><cell></cell><cell>T-Trans</cell><cell>81.52 95.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Study on different pre-training.</figDesc><table><row><cell>Visual encoder</cell><cell>Paradigm</cell><cell>Top-1</cell></row><row><cell>CLIP-pretrained</cell><cell cols="2">Vision-Only 75.27 Vision-Text 80.13</cell></row><row><cell>ImageNet-pretrained</cell><cell cols="2">Vision-Only 74.78 Vision-Text 75.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Study on various text input forms.</figDesc><table><row><cell>Text input from</cell><cell>Top 1</cell></row><row><cell>class name</cell><cell>81.37</cell></row><row><cell>"a video of a person" + class name</cell><cell>81.52</cell></row><row><cell cols="2">multiple fixed templates + class name 80.88</cell></row><row><cell>learnable template + class name</cell><cell>81.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Different instantiations of our method on Kinetics-400. "Single View" indicates one temporal clip with one spatial crop, whereas "4?3 Views" indicates 4 temporal clips with 3 spatial crops.</figDesc><table><row><cell>Encoder</cell><cell cols="2">Resolution Frames</cell><cell>Single View Top-1 Top-5 Top-1 Top-5 4?3 Views</cell></row><row><cell>ResNet-50</cell><cell>224?224</cell><cell>8 16</cell><cell>74.26 91.67 75.50 92.61 75.50 92.20 76.60 93.12</cell></row><row><cell>VIT-B/32</cell><cell>224?224</cell><cell>8 16</cell><cell>78.54 94.14 80.00 94.84 79.25 94.31 80.51 95.10</cell></row><row><cell>VIT-B/16</cell><cell>224?224</cell><cell>8 16</cell><cell>81.52 95.49 82.90 96.28 82.32 95.90 83.58 96.38</cell></row><row><cell>VIT-L/14</cell><cell>224?224</cell><cell>8 16</cell><cell>85.52 96.72 86.37 97.23 85.94 96.96 86.72 97.28</cell></row><row><cell>VIT-L/14</cell><cell>336?336</cell><cell>8 16</cell><cell>86.33 97.06 87.09 97.38 86.79 97.24 87.56 97.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Ours vs. Matching paradigm with ViT-B/16 on Kinetics-400. The number of V100-days is the number of V100 GPU used for training multiplied by the training time in days. * indicates the official result<ref type="bibr" target="#b53">[54]</ref> via "Data-parallel training" on 3090 GPUs. For efficient training and fair comparison, we implement all experiments with "Distributed Data-parallel training" in theTable.</figDesc><table><row><cell>Method</cell><cell cols="3">Batch gather Textual encoder Top-1 Top-5 V100-days</cell></row><row><cell></cell><cell>online</cell><cell>81.15 95.42</cell><cell>6.7 (10  *  )</cell></row><row><cell>Matching paradigm [54]</cell><cell>offline online</cell><cell>80.73 95.36 77.77 94.79</cell><cell>6.6 3.5</cell></row><row><cell></cell><cell>offline</cell><cell>76.13 94.57</cell><cell>3.3</cell></row><row><cell>Our paradigm</cell><cell>offline</cell><cell>81.52 95.49</cell><cell>3.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Few-shot image recognition on ImageNet. "Zeroshot" and "All-shot" denote the lower and upper bounds of the task respectively. Top-1 accuracy is reported here.</figDesc><table><row><cell>K-shot</cell><cell>0</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>All</cell></row><row><cell>Ours</cell><cell cols="5">66.73 71.50 73.64 74.99 82.25</cell></row><row><cell>Vision-Only</cell><cell>0</cell><cell cols="4">4.71 30.44 41.70 79.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Zero-shot image recognition.</figDesc><table><row><cell cols="2">We train the model on IN600 then per-</cell></row><row><cell cols="2">form evaluation on IN400.</cell></row><row><cell></cell><cell>IN600?IN400</cell></row><row><cell>Ours w/o train</cell><cell>70.28</cell></row><row><cell>Ours w/ train</cell><cell>72.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Study on various backbones. Models are trained with 10 epochs.</figDesc><table><row><cell cols="6">Backbone Resolution Top-1 Top-5 FLOPs Params A100-days</cell></row><row><cell>VIT-B/16</cell><cell>224?224</cell><cell>83.10 96.94</cell><cell>11.3G</cell><cell>57.3M</cell><cell>0.5</cell></row><row><cell>VIT-L/14</cell><cell>224?224</cell><cell>86.72 98.24</cell><cell cols="2">51.9G 202.1M</cell><cell>2.0</cell></row><row><cell>VIT-L/14</cell><cell>336?336</cell><cell cols="3">87.54 98.43 116.5G 202.1M</cell><cell>5.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>1), comparison with SOTAs on UCF-101 and HMDB-51 ( ?A.2), more visualizations of different classifier ( ?A.3) and more ablations ( ?A.4). ?B contains additional implementation details for: details of several large-scale datasets for pre-training ( ?B.1), visual encoder architectures ( ?B.2), Batch Gather ( ?B.3) and Text template ( ?B.4).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Table A.1 reveals that our model has a pretty transfer capability, with mean class accuracy of 98.2% on UCF-101 and 79.0% on HMDB-51, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table A .</head><label>A</label><figDesc>1: Mean class accuracy on UCF-101 and HMDB-51 achieved by different methods which are transferred from their Kinetics models with RGB modality (over 3 splits).</figDesc><table><row><cell>Method</cell><cell cols="2">UCF-101 HMDB-51</cell></row><row><cell>ECO En [69]</cell><cell>94.8%</cell><cell>72.4%</cell></row><row><cell>ARTNet</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table A</head><label>A</label><figDesc>.2: Frozen classifier vs. Unfrozen classifier. Large-scale datasets for pre-training Here we describe the large-scale web-scale datasets used in other video recognition methods for pre-training. The suffix of the name represents the magnitude of the dataset. Facebook has proposed the IG-65M dataset, which contains approximately 65 million public, user-generated Instagram videos with hashtags. Due to label and temporal noise, the dataset is used for weakly-supervised training. This dataset is not open-source, but several pre-trained R(2+1)D [31] and CSN [59] models are provided 4 .</figDesc><table><row><cell>Setting</cell><cell>Top-1 Top-5</cell></row><row><cell>frozen</cell><cell>81.52 95.49</cell></row><row><cell cols="2">unfrozen 79.16 93.55</cell></row><row><cell cols="2">B Additional Implementation Details</cell></row><row><cell cols="2">B.1 ? ImageNet-1K/21K: The ImageNet-1K dataset was used to pre-train models for computer</cell></row><row><cell cols="2">vision transfer learning. It was first released for the ILSVRC2012 visual recognition</cell></row><row><cell cols="2">challenge. The ImageNet-1K dataset is a subset of the larger ImageNet dataset, which</cell></row><row><cell cols="2">contains 14,197,122 images split into 21,841 categories. The whole dataset is known to</cell></row><row><cell cols="2">as ImageNet-21K (sometimes referred to as ImageNet-22K) and has been open-source 3 .</cell></row><row><cell cols="2">ImageNet-1K was created by selecting a subset of 1.2M images from ImageNet-21K, that</cell></row><row><cell cols="2">belong to 1000 mutually exclusive classes.</cell></row><row><cell>? IG-65M:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table A</head><label>A</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.3 and A.4.</cell></row><row><cell></cell><cell cols="5">Table A.3: CLIP-ResNet hyperparameters</cell><cell></cell></row><row><cell></cell><cell>Embedding</cell><cell>Input</cell><cell>ResNet</cell><cell></cell><cell cols="3">Text Transformer</cell></row><row><cell>Model</cell><cell>dimension</cell><cell>resolution</cell><cell>blocks</cell><cell cols="4">width layers width heads</cell></row><row><cell>RN50</cell><cell>1024</cell><cell>224</cell><cell cols="2">(3, 4, 6, 3) 2048</cell><cell>12</cell><cell>512</cell><cell>8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table A</head><label>A</label><figDesc>Numpy-like Pseudocode that illustrates the role of Batch Gather in Distributed InfoNCE.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">.4: CLIP-ViT hyperparameters</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Embedding</cell><cell>Input</cell><cell cols="3">Vision Transformer</cell><cell cols="3">Text Transformer</cell></row><row><cell>Model</cell><cell>dimension</cell><cell cols="7">resolution layers width heads layers width heads</cell></row><row><cell>ViT-B/32</cell><cell>512</cell><cell>224</cell><cell>12</cell><cell>768</cell><cell>12</cell><cell>12</cell><cell>512</cell><cell>8</cell></row><row><cell>ViT-B/16</cell><cell>512</cell><cell>224</cell><cell>12</cell><cell>768</cell><cell>12</cell><cell>12</cell><cell>512</cell><cell>8</cell></row><row><cell>ViT-L/14</cell><cell>768</cell><cell>224</cell><cell>24</cell><cell>1024</cell><cell>16</cell><cell>12</cell><cell>768</cell><cell>12</cell></row><row><cell>ViT-L/14-336px</cell><cell>768</cell><cell>336</cell><cell>24</cell><cell>1024</cell><cell>16</cell><cell>12</cell><cell>768</cell><cell>12</cell></row><row><cell cols="3">B.3 Batch Gather for Distributed InfoNCE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Instead of Data-Parallel Training (DP), which is single-process, multi-thread, and only works on</cell></row><row><cell cols="9">a single machine, Distributed Data-Parallel Training (DDP) is a widely adopted single-program</cell></row><row><cell cols="9">multiple-data training paradigm for single-and multi-machine training. Due to GIL contention across</cell></row><row><cell cols="9">threads, per-iteration replicated model, and additional overhead introduced by scattering inputs and</cell></row><row><cell cols="7">gathering outputs, DP is usually slower than DDP even on a single machine.</cell><cell></cell><cell></cell></row><row><cell>Algorithm 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">That is, optimized with cross-entropy loss with one-hot labels</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/openai/CLIP/blob/main/clip/clip.py</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.image-net.org 4 https://github.com/facebookresearch/vmz 5 https://github.com/openai/CLIP</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/openai/CLIP/blob/main/data/prompts.md</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10972</idno>
		<title level="m">Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Scaling vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04560</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Self-supervised pretraining of visual features in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lefaudeux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01988</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bermano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clipcap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09734</idno>
		<title level="m">Clip prefix for image captioning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Est?baliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1807</biblScope>
		</imprint>
	</monogr>
	<note>Representation learning with contrastive predictive coding. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Contrastive predictive coding based feature for automatic speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01575</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9640" to="9649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using discriminant analysis for multi-class classification: an experimental investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsunori</forename><surname>Ogihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and information systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="453" to="472" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies for action recognition with a biologically-inspired deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yemin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurips</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="813" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<title level="m">Video swin transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1807</biblScope>
		</imprint>
	</monogr>
	<note>Representation learning with contrastive predictive coding. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12086</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05208</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11432</idno>
		<title level="m">A new foundation model for computer vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Legg</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Seyedhosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01917</idno>
		<title level="m">Coca: Contrastive captioners are image-text foundation models</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Unified contrastive learning in image-text-label space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03610</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9879" to="9889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1728" to="1738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Clip4clip: An empirical study of clip for end to end video clip retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8746" to="8755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">T2vlad: global-local sequence alignment for text-video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5079" to="5088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Centerclip: Token clustering for efficient text-video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7331" to="7341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiazheng</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08472</idno>
		<title level="m">Actionclip: A new paradigm for video action recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Prompting visual-language models for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04478</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mvfnet: Multi-view fusion network for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Video classification with channelseparated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5552" to="5561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Tokenlearner: What can 8 learned tokens do for images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angelova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11297</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Multiview transformers for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Shen Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.04288</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Co-training transformer with videos and images improves action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07175</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Large-scale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12046" to="12055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Tdn: Temporal difference networks for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1895" to="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Teinet: Towards an efficient architecture for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11669" to="11676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Learning to prompt for vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01134</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamaljeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
