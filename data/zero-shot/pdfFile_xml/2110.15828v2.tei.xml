<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Resampling Base Distributions of Normalizing Flows</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Stimper</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Miguel Hern?ndez-Lobato</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Resampling Base Distributions of Normalizing Flows</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Normalizing flows are a popular class of models for approximating probability distributions. However, their invertible nature limits their ability to model target distributions whose support have a complex topological structure, such as Boltzmann distributions. Several procedures have been proposed to solve this problem but many of them sacrifice invertibility and, thereby, tractability of the log-likelihood as well as other desirable properties. To address these limitations, we introduce a base distribution for normalizing flows based on learned rejection sampling, allowing the resulting normalizing flow to model complicated distributions without giving up bijectivity. Furthermore, we develop suitable learning algorithms using both maximizing the log-likelihood and the optimization of the Kullback-Leibler divergence, and apply them to various sample problems, i.e. approximating 2D densities, density estimation of tabular data, image generation, and modeling Boltzmann distributions. In these experiments our method is competitive with or outperforms the baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Inferring and approximating probability distributions is a central problem of unsupervised machine learning. A popular class of models for this task are normalizing flows <ref type="bibr" target="#b38">(Tabak and Vanden-Eijnden, 2010;</ref><ref type="bibr" target="#b37">Tabak and Turner, 2013;</ref><ref type="bibr" target="#b33">Rezende and Mohamed, 2015)</ref>, which are given by an invertible map transforming a simple base distribution such as a Gaussian to obtain a complex distribution matching our target. Normalizing flows have been applied successfully to a variety of problems, such as image generation <ref type="bibr">(Dinh et al., 2015</ref><ref type="bibr">(Dinh et al., , 2017</ref><ref type="bibr" target="#b20">Kingma and Dhariwal, 2018;</ref><ref type="bibr" target="#b12">Ho et al., 2019;</ref><ref type="bibr">Grci? et al., 2021)</ref>, audio synthesis (van den <ref type="bibr" target="#b39">Oord et al., 2018)</ref>, variational inference <ref type="bibr" target="#b33">(Rezende and Mohamed, 2015)</ref>, semi-supervised learning <ref type="bibr" target="#b14">(Izmailov et al., 2020)</ref> and approximating Boltzmann distributions <ref type="bibr" target="#b28">(No? et al., 2019;</ref><ref type="bibr" target="#b41">Wu et al., 2020;</ref><ref type="bibr" target="#b40">Wirnsberger et al., 2020)</ref> among others <ref type="bibr" target="#b29">(Papamakarios et al., 2021)</ref>. However, with respect to some performance measures they are still outperformed by autoregressive models <ref type="bibr" target="#b7">(Chen et al., 2018;</ref><ref type="bibr" target="#b31">Parmar et al., 2018;</ref><ref type="bibr" target="#b8">Child et al., 2019)</ref>, generative adversarial networks (GANs) <ref type="bibr">(Gulrajani et al., 2017;</ref><ref type="bibr" target="#b17">Karras et al., 2019</ref><ref type="bibr" target="#b16">Karras et al., , 2020a</ref>, and diffusion based models <ref type="bibr" target="#b36">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b21">Kingma et al., 2021)</ref>. One reason for this is an architectural limitation. Due to their bijective nature the normalizing flow transformation leaves the topological structure of the support of the base distribution unchanged and, since it is usually simple, there is a topological mismatch with the often complex target distribution <ref type="bibr" target="#b9">(Cornish et al., 2020)</ref>, thereby diminishing the modeling performance and even causing exploding inverses <ref type="bibr" target="#b3">(Behrmann et al., 2021)</ref>. Several solutions have been proposed, e.g. augmenting the space the model operates on <ref type="bibr" target="#b13">(Huang et al., 2020)</ref>, continuously indexing the flow layers <ref type="bibr" target="#b9">(Cornish et al., 2020)</ref>, and adding stochastic <ref type="bibr" target="#b41">(Wu et al., 2020)</ref> or surjective layers <ref type="bibr" target="#b27">(Nielsen et al., 2020)</ref>. However, these approaches sacrifice the bijectivity of the flow transformation, which means in most cases that the model is no longer tractable, memory savings during training are no longer possible <ref type="bibr">(Gomez et al., 2017)</ref>, and the model is no longer a perfect encoder-decoder pair. Some work has been done on using multimodal base distributions <ref type="bibr" target="#b14">(Izmailov et al., 2020;</ref><ref type="bibr" target="#b0">Ardizzone et al., 2020;</ref><ref type="bibr" target="#b10">Hagemann and Neumayer, 2021)</ref>, but the intention was to do classification or solve inverse problems with flow-based models and not to capture the inherent multimodal nature of the target distribution. <ref type="bibr" target="#b30">Papamakarios et al. (2017)</ref> took a mixture of Gaussians as base distribution and showed that this can improve arXiv:2110.15828v2 [stat.ML] 24 Feb 2022 the performance.</p><p>In this work, we develop a method to obtain a more expressive base distribution through learned accept/reject sampling (LARS) <ref type="bibr" target="#b1">(Bauer and Mnih, 2019)</ref>. It can be estimated jointly with the flow map by either maximum likelihood (ML) learning or minimizing the Kullback-Leibler (KL) divergence, matching the topological structure of the target's support. Moreover, we propose how the method can be scaled up to high dimensional datasets and demonstrate the effectiveness of our procedure on the tasks of learning 2D densities, estimating the density of tabular data, generating images, and the approximation of a 22 atom molecule's Boltzmann distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Normalizing Flows</head><p>Let z be a random variable taking values in R d , having the density p ? (z) parameterized by ?. Furthermore, let F ? : R d ? R d be a bijective map parameterized by ?. We can compute the tractable density of the new random variable x := F ? (z) with the change of variables formula</p><formula xml:id="formula_0">p(x) = p ? (z) |det(J F ? (z))| ?1 ,<label>(1)</label></formula><p>where J F ? is the Jacobian matrix of F ? . This way of constructing a complex probability distribution p(x) from a simple base distribution p(z) is called a normalizing flow. We can use them to approximate a target density p * (x), which is done by optimizing a training objective. If the target density is unknown but samples from the corresponding distribution are available, we maximize the expected log-likelihood (LL) of the model LL(?, ?) = E p * (x) [log (p(x))] .</p><p>Conversely, if the target density is given, we minimize the (reverse) KL divergence 1 <ref type="bibr" target="#b29">(Papamakarios et al., 2021</ref>)</p><formula xml:id="formula_2">KLD(?, ?) := E p(x) [log p(x)] ? E p(x) [log p * (x)] ,<label>(3)</label></formula><p>or another difference measure for probability distributions such as the ?-divergence <ref type="bibr" target="#b11">(Hern?ndez-Lobato et al., 2016)</ref>.</p><p>To deal with high dimensional data, such as images, the multiscale architecture was introduced by Dinh et al. <ref type="bibr">(2017)</ref>. As sketched in <ref type="figure" target="#fig_7">Figure 5</ref>, at the first level, the entire input x is transformed by several flow layers F 1 . The result is split up into two parts, h</p><p>1 and h</p><p>1 .</p><p>1 For simplicity, we will call it just KL divergence from now on. For images, this is typically done by first squeezing the image, i.e. reducing its height and width by a factor 2 and adding the surplus pixels as additional channels, and then splitting the resulting tensor along the channel dimension. h</p><p>1 is immediately factored out in the density, while h</p><p>(2) 2 is further transformed by the next set of flow layers F 2 . The process is then repeated until a desired depth is reached. The full density for a multiscale architecture with n levels is given by</p><formula xml:id="formula_6">p(x) = n i=1 |det (J Fi (h i?1 ))| p(z i ),<label>(4)</label></formula><p>where we set h 0 = x.</p><p>Normalizing flows can compete with other machine learning models on many benchmarks <ref type="bibr" target="#b29">(Papamakarios et al., 2021)</ref>. However, their performance is still impaired by an architectural weakness. The transformations defining a normalizing flow are invertible and such maps leave the topology of the sets they map unchanged <ref type="bibr" target="#b34">(Runde, 2005)</ref>. Consequently, the topological structure of the support of p(z) is the same as that of p(x). Usually, the base distribution is a Gaussian, which has only one mode, so its support consists of one connected component but the target distribution might be multimodal with the density between the modes being close to zero or even numerically zero due to finite precision so that the support consists of multiple disconnected components. As an exemplification we fit a real-valued non-volume preserving (real NVP) flow model with 8 coupling layers to a multimodal target distribution, see <ref type="figure" target="#fig_1">Figure 1</ref>. The density of the trained model consists of one connected component covering the modes of the target, but connecting them via a density filament. Certain flow-based models, such as the residual flow <ref type="bibr" target="#b6">Chen et al., 2019)</ref>, can only converge to the target if they become non-invertible due to the topological mismatch <ref type="bibr" target="#b9">(Cornish et al., 2020)</ref>, thereby causing unstable training behaviour <ref type="bibr" target="#b3">(Behrmann et al., 2021)</ref>. Proposed solutions include increasing the model size significantly , but this increases the computational cost and memory demand while the stability issues persist. Training can be stabilized via a suitable regularization, but this reduces the performance <ref type="bibr" target="#b3">(Behrmann et al., 2021)</ref>. Other approaches are discussed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learned accept/reject sampling</head><p>Learned accept/reject sampling (LARS) is a method to approximate a d-dimensional distribution q(z) by reweighting a proposal distribution ?(z) through a learned acceptance function a ? : R d ? [0, 1], where ? are the learned parameters <ref type="bibr" target="#b1">(Bauer and Mnih, 2019)</ref>. Given a sample z i from ?, we will accept it with a probability a ? (z i ), otherwise we reject it and draw a new sample until we accept one of the proposed samples. The resulting distribution is given by</p><formula xml:id="formula_7">p ? (z) = ?(z)a ? (z) Z ; Z := ?(z)a ? (z)dz. (5)</formula><p>In order to limit the computational cost caused by high rejection rates, Bauer and Mnih (2019) introduced a truncation parameter T ? N. If the first T ? 1 samples from the proposal get rejected, we accept the T th sample no matter the value of the learned acceptance probability. Through this intervention, we alter the final sampling distribution to become</p><formula xml:id="formula_8">p T (z) = (1 ? ? T ) a ? (z)?(z) Z + ? T ?(z),<label>(6)</label></formula><p>where ? T := (1 ? Z) T ?1 , which reduces to (5) for T ? ?. The integral (5) defining Z is not tractable, so we cannot compute it directly. Instead, it is estimated via Monte Carlo sampling, i.e.</p><formula xml:id="formula_9">Z ? 1 S S s=1 a ? (z s ),<label>(7)</label></formula><p>where z s ? ?(z), which needs to be recomputed in every training iteration, as parameter changes in a ? cause a change in Z.</p><p>LARS was first used to create a more expressive prior for variational autoencoders (VAEs) <ref type="bibr" target="#b22">(Kingma and Welling, 2014)</ref>, making it closer to the aggregate posterior distribution, thereby bringing the approximate posterior distribution closer to the ground truth. The resampled priors are trained jointly with the likelihood and the approximate posterior via maximization of the evidence lower bound. Since this only requires to evaluate the density of the prior at the data points, it is not even required to perform rejection sampling during training; therefore, the computational cost of training the whole model is only increased slightly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Resampled base distributions</head><p>In Section 2.1, we argued that the topological structure of the support of the base distribution equals that of the overall flow distribution. To avoid artefacts resulting from mismatches between them, we aim to make the latter closer to the former. Therefore, we resample the base distribution with LARS, i.e. use it as our proposal so that its density becomes (6). Since there are no restrictions on the acceptance function a ? , we can use an arbitrarily complex neural network to model any desired topological structure. The resulting logprobability of the model is given by</p><formula xml:id="formula_10">log p(x) = log ?(z) + log ? T + (1 ? ? T ) a ? (z) Z ? log |det J F ? (z)| ,<label>(8)</label></formula><p>where F ? is the flow transformation, i.e. the composition of all flow layers, and z = F ?1 ? (x). In our case, the proposal is a Gaussian but it could be any other distribution or a more complicated model, such as a mixture of Gaussians or an autoregressive model. Depending on the application, a ? will be a fully connected or a convolutional neural network, and details about how the architecture can be chosen are given in Appendix C.1. Since the evaluation of a ? can be parallelized over the number of dimensions of the data d, we only add a constant computational overhead to our model. In contrast, autoregressive models scale linearly with d. We can sample from the model by performing LARS and propagating the accepted values through the flow map. The rejection rate, and hence the sampling speed, can be controlled via the truncation parameter T , which we set to 100 in our experiments unless otherwise stated, but also through adding Z to our loss function, which is discussed in Appendix C.2.</p><p>Usually, the base distribution of normalizing flows has mean and variance parameters being trained with the flow layer parameters. Our proposal is simply a standard normal distribution, i.e. a diagonal Gaussian with mean zero and variance one. Thereby, we ensure that the samples from the proposal, which are the input for the neural network representing the learned acceptance probability a ? , come from a distribution which does not change during training. Instead, the mean and variance of the distribution can be altered after the resampling process by applying an affine flow layer with scale and shift being learnable parameters.</p><p>Note that while we retain the invertiblility of the flow, the probability distribution (8) cannot be evaluated exactly since Z needs to be estimated via (7). However, for large T the base distribution reduces to <ref type="formula">(5)</ref> and, hence, we are only off by a constant meaning there would not be a bias when doing importance sampling, which is crucial for applications such as Boltzmann generators, see Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning algorithms</head><p>The resampled base distribution can be trained jointly with the flow layers of our model. Both, the expected LL and the KL divergence, can be used as objectives. The former corresponds to maximizing (2), which is done via stochastic gradient decent. As done by <ref type="bibr" target="#b1">Bauer and Mnih (2019)</ref>, we sample from the proposal in each iteration to estimate the gradient of Z with respect to the parameters, see <ref type="bibr">(7)</ref>. To stabilize training, we estimate the value of the normalization constant by an exponential moving average, see Appendix A.1 for more details.</p><p>When the unnormalized target densityp * (x) is known, we can use the KL divergence (3) as our objective. However, because sampling from the base distribution includes an acceptance/rejection step, we cannot apply the reparameterization trick <ref type="bibr" target="#b22">(Kingma and Welling, 2014)</ref> to obtain the gradients with respect to the model parameters. Instead, we derive an expression of the gradients of the KL divergence similar to that introduced by <ref type="bibr">Grover et al. (2018)</ref>.</p><p>Theorem 1. Let p ? (z) be the base distribution of a normalizing flow, having parameters ?, and F ? be the respective invertible mapping, depending on its parameters ?, such that the density of the model is log (p(x)) = log (p ? (z)) ? log |det J F ? (z)| , (9) with x = F ? (z). Then, the gradients of the KL divergence with respect to the parameters are given by</p><formula xml:id="formula_11">? ? KLD(?, ?) = Cov p ? (z) ? ? log p ? (z), log (p ? (z)) ? log |det J F ? (z)| ? logp * (F ? (z)) (10) ? ? KLD(?, ?) = ?E p ? (z) ? ? logp * (F ? (z)) + log |det J F ? (z)|<label>(11)</label></formula><p>The proof is given in Appendix A.2. We will use (10) and (11) to compute the gradients of the KL divergence in our experiments and, thereby, demonstrate its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Application to multiscale architecture</head><p>LARS cannot be applied to very high dimensional distributions because we have to estimate Z and its gradients via Monte Carlo sampling and the number of samples needed grows exponentially with the number of dimensions <ref type="bibr" target="#b1">(Bauer and Mnih, 2019)</ref>. Although the base distribution of a normalizing flow must have the same number of dimensions as the target, we can reduce the number of dimensions significantly by factorization. Therefore, we extend the multiscale architecture, see Section 2.1 and <ref type="bibr">(Dinh et al., 2017)</ref>, by further subdividing the base distribution at each level into factors with less than 100 dimensions. First, we squeeze the feature map until the product of height and width is smaller than 100. Then, each channel is treated as a separate factor, see <ref type="figure" target="#fig_2">Figure 2</ref>. To reduce the complexity of the model, we use parameter sharing to express the distribution of factors, i.e. there is one neural network per level with multiple outputs, each representing the acceptance probability a ? for one channel. This also has the advantage that we can estimate the normalization constant and its gradient of all factors of one level in parallel by sampling from a Gaussian, passing the samples through the neural network and computing the average for each output dimension separately. As mentioned in Section 3.1, the mean and variance is added via a constant coupling layer. Furthermore, the base distribution can be made class-conditional by making the mean and variance and/or a ? dependent on the class. The latter can be efficiently achieved by adding more outputs to the neural network to have one value for a ? per class and distribution if needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">2D distributions</head><p>In this section, we aim to demonstrate that our method is indeed capable of modeling complicated distributions. Our code for all experiments is publicly available on GitHub at https://github.com/ VincentStimper/resampled-base-flows.</p><p>We start with simple <ref type="formula" target="#formula_1">2D</ref>    with two modes, one with eight modes, and one with two rings, see <ref type="figure" target="#fig_3">Figure 3</ref> and <ref type="table" target="#tab_9">Table 8</ref>. We use both learning algorithms discussed in Section 3.2. To train our flows via ML, we draw samples from our distributions via rejection sampling. As flow architectures, we choose real NVP (Dinh et al., 2017) and residual flow <ref type="bibr" target="#b6">Chen et al., 2019)</ref> with 16 layers each. For each flow architecture, we train models with a Gaussian, a mixture of 10 Gaussians, and a resampled base distribution, having a Gaussian proposal and a neural network with 2 hidden layers with 256 hidden units each as well as a sigmoid output function as acceptance probability.</p><p>The densities of the trained real NVP and residual flow models are show in the <ref type="figure" target="#fig_3">Figures 3 and 9</ref>, respectively. With a Gaussian base distribution, the flows struggle to model the complex topological structure. For the trained real NVP models it is especially visible in <ref type="figure" target="#fig_3">Figure 3</ref> that the density essentially consists of one connected component since there are density filaments between the modes and rings are not closed.</p><p>The multimodal distributions can be fitted much better when using a mixture of Gaussians as base distribution, but especially the ring distribution can still not be represented properly. With a resampled base distribution the flow models the target distributions accurately without any artefacts. The base distribu- 0.68 ? 0.01 13.08 ? 0.00 ?13.83 ? 0.10 ?9.93 ? 0.06 RBD-NSF (ours) 0.69 ? 0.01 13.29 ? 0.05 ?14.02 ? 0.12 ?9.45 ? 0.03 tions assume the respective topological structure of the target while the flow transformation does the fine adjustment of the density. We also estimate the KL divergences of the target and the model distributions which are listed in <ref type="table" target="#tab_1">Table 1</ref>. In all cases the flow model with the resampled base distribution outperforms the respective baselines.</p><p>Moreover, we train real NVP models with Gaussian and resampled base distributions with the KL divergence using the gradient estimators derived in Theorem 1. The same architecture as the models trained with ML learning are used and their resulting densities are shown in <ref type="figure">Figure 7</ref>. In addition, we also computed the KL divergences listed in <ref type="table" target="#tab_3">Table 3</ref>. As for the previous experiments, the flow with the resampled base distribution clearly outperforms its baseline visually and quantitatively for all the three targets.  <ref type="bibr" target="#b9">(Cornish et al., 2020)</ref>, and one with a resampled base distribution. The LL of the models are shown in <ref type="table" target="#tab_2">Table 2</ref>. More details about the setup and the architecture as well as results for real NVP flows on the same datasets are given in Appendix E.</p><p>There is no significant performance difference of the three methods on the power dataset. On Hepmass, the resampled base distributions achieves similar performance to CIF, while both are better than the vanilla NSF. For the Gas and Miniboone dataset, the flow with the resampled base distribution clearly outperforms its baselines. When using real NVP, the difference is even larger on all datasets but Miniboone, as can be seen in <ref type="table" target="#tab_11">Table 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Image generation</head><p>To model images with our method, we train Glow <ref type="bibr" target="#b20">(Kingma and Dhariwal, 2018)</ref> on the CIFAR-10 dataset <ref type="bibr" target="#b24">(Krizhevsky, 2009)</ref>. We use the multiscale architecture introduced in Section 3.3, where we compare a Gaussian with a respective resampled base distribution. As done by <ref type="bibr" target="#b20">Kingma and Dhariwal (2018)</ref>, we use 3 levels, but train models with 8, 16, and 32 layers per level with each base distribution, with more details provided in Appendix F. For each model architecture, we do three seeded training runs and report bits per dimension on the test set in <ref type="table" target="#tab_4">Table 4</ref>. 3.332 ? 0.001 32 layers per level 3.283 ? 0.002 3.282 ? 0.001</p><p>The flow with the resampled base distribution outperforms the baseline when using 8 or 16 layers per level, while performing about equal with 32 layers. The difference is larger for smaller models, i.e. those where fewer layers are used, since models with many layers are already rather expressive. Using a more expressive base distribution also increases the model size and the  training time, but this amounts only to 0.4-1.5% and 5-15%, respectively, versus a roughly linear increase with the number of layers. Hence, this can be a desirable trade-off, depending on the use case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Boltzmann generators</head><p>An important application of normalizing flows is the approximation of Boltzmann distributions. Given the atom coordinates x of a molecule, the likelihood of finding it in this state, i.e. the Boltzmann distribution, is proportional to e ?u(x) , where u denotes the energy of the system, which can be obtained through physical modeling. Usually, samples are drawn from this distribution through molecular dynamics (MD) simulations <ref type="bibr" target="#b25">(Leimkuhler and Matthews, 2015)</ref>. However, the sampling process can be greatly accelerated by approximating the Boltzmann distribution with a normalizing flow, called a Boltzmann generator, and then sampling from the flow model <ref type="bibr" target="#b28">(No? et al., 2019)</ref>.</p><p>Here, we approximate the Boltzmann distribution of the 22 atom Alanine dipeptide, which has been used as a benchmark system in the machine learning literature <ref type="bibr" target="#b41">(Wu et al., 2020;</ref><ref type="bibr" target="#b5">Campbell et al., 2021;</ref><ref type="bibr" target="#b23">K?hler et al., 2021)</ref>. We use the coordinate transformation introduced by No? et al. <ref type="formula" target="#formula_0">(2019)</ref>, see also Appendix G.1, which incorporates the translational and rotational symmetry and reduces the number of dimensions from 66 to 60. Both ML learning and training using the KL divergence are used. For the former we generate a training dataset through a MD simulation over 10 7 steps each and keep every 10 th sample, resulting in datasets with 10 6 samples. With the same procedure we generate a test set to evaluate all trained models.</p><p>With ML learning we train real NVP models having 16 layers with a Gaussian, a mixture of 10 Gaussians, and a resampled base distribution. Furthermore, we train another real NVP model with a Gaussian base, but having 19 layers, which has roughly the same number of parameters as the real NVP model with the resampled base. More details of the architecture and the training procedure are listed in Appendix G.2. The marginal distribution of three dihedral angles are shown in <ref type="figure" target="#fig_4">Figure 4</ref>.</p><p>Although we tried various methods of initializing the mixture of Gaussians, training it jointly with the flow turns out to be unstable leading to a poor fit of the marginals, which is especially visible for ? 3 . Moreover, for two of the three angles, the 16-layered model with the Gaussian base distribution cannot represent the multimodal nature of the distribution accurately.</p><p>Increasing the number of layers to 19 improves the result, but even this model is clearly outperformed by the real NVP with a resampled base distribution. To compare the performance quantitatively, we computed the LL on the test set and estimated the KL divergence between the MD samples and the models of the marginals through histograms for all 60 dimensions and report the mean and median. All performance measures where averaged over 10 seeded runs and are shown in <ref type="table" target="#tab_5">Table 5</ref>. The real NVP model with the resampled base distribution outperforms all the baselines. The improved performance comes at the cost of increased training time, i.e. 49% and 26%, and sampling time, i.e. by a factor of 4 and 1.8, for the real NVP and the residual flow models with the resampled base distribution, when compared to their Gaussian counterparts. A further analysis of the Ramachandran plots of the models is done in Appendix G.3. There, we also do a comparison to stochastic normalizing flows <ref type="bibr" target="#b41">(Wu et al., 2020)</ref> and show the results of training residual flows with ML whereby the model with the resampled base distribution outperforms the baselines as well.</p><p>Moreover, we used the KL divergence to train real NVP models with Gaussian, mixture of Gaussians, and resampled base distributions as well, having the same architecture as the real NVP models with 16 layers in the experiments above. This is a challenging task since if samples from the model are too far away from the modes of the Boltzmann distribution, their gradients can be very high making training unstable. However, it is important for the application of Boltzmann generators since the necessity of creating a dataset through other expensive sampling procedures diminishes their ability to reduce the overall computational time needed for sampling. Details of the model architectures and the training procedure are given in Appendix G.2. Although it involves rejection sampling, training the flow models with the resampled base distribution only took 15% longer than the baseline models. As can be seen in <ref type="table" target="#tab_1">Table 15</ref>, the real NVP model with a resampled base outperforms those with a Gaussian and a mixture of Gaussians; however, they are still inferior to flows trained via ML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION AND RELATED WORK</head><p>The main challenge we tackle in this work, i.e. that normalizing flows struggle to model distributions with supports having a complicated topological structure due to their invertible nature, has been addressed in several articles. . Usually, when training layered models such as neural networks the activations of each layer need to be stored in the forward pass because they are needed for gradient computation in the backward pass. However, if the layers are invertible, the activations of the forward pass can be recomputed in the backward pass by applying the inverse of the layer to the activations of the previous layers. Thereby, models can be made basically infinitely deep with a fixed memory budget. Thirdly, exact evaluation of the likelihood is no longer possible. To train the models, a bound needs to be derived which is optimized instead of the actual likelihood. Our model does not make this sacrifice since only the base distribution is altered, but the transformation of the normalizing flow model is still invertible. On the other side, the base distribution itself cannot be evaluated exactly because its normalization constant is unknown. It can be estimated via Monte Carlo sampling, but its logarithm, appearing in the LL of the model, is biased. However, as discussed in Section 3.1 for large truncation parameter T we are only off by a constant so e.g. importance sampling could be done without a bias. Moreover, drawing samples from our model is less efficient as many samples from the proposal might get rejected before finally one is accepted and propagated through the flow.  <ref type="formula" target="#formula_0">(2021)</ref> explored normalizing flows with a multimodal base distribution, in their case a mixture of Gaussians. However, their intention was to model data with multiple classes, thereby performing classification and solving inverse problems. Our model allows to describe data with multiple classes as well through a conditional distribution, similar to the work of Dinh et al. <ref type="formula" target="#formula_0">(2017)</ref>; <ref type="bibr" target="#b20">Kingma and Dhariwal (2018)</ref>, but is also able to describe the complicated topological structure of the distribution of each class. <ref type="bibr" target="#b1">Bauer and Mnih (2019)</ref> used LARS successfully to create more expressive priors for VAEs, thereby boosting their performance. They demonstrated that the resampled prior can be learned jointly with the encoder and decoder by maximizing the evidence lower bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An autoregressive base distribution was introduced by</head><p>In contrast, we showed that a resampled base distribution can be jointly trained with a normalizing flow transformation using both the LL and the KL divergence as an objective. For the latter we derived an expression of the gradient with reduced variance inspired by the work of Grover et al. <ref type="formula" target="#formula_0">(2018)</ref>. Furthermore, <ref type="bibr" target="#b1">Bauer and Mnih (2019)</ref> reported that they tried to fully factorize their resampled prior, which would allow them to scale to higher dimensional problems, but they were not able not beat the baseline of a VAE with a factorized Gaussian prior. We were successful by not fully factorizing our resampled base distribution, but defining factors for groups of variables. Moreover, combining LARS with the multiscale architecture of <ref type="bibr">Dinh et al. (2017)</ref> and using a factorization similar to <ref type="bibr" target="#b26">(Ma et al., 2019)</ref> allowed us to scale up our base distribution even further. The largest base distribution in our work, used in Glow to model the CIFAR10 dataset, has 3072 dimensions, while the largest prior of <ref type="bibr" target="#b1">Bauer and Mnih (2019)</ref> only had 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we introduced a base distribution for normalizing flows based on learned rejection sampling. We derived how it can be trained jointly with the flow layers maximizing the expected LL or minimizing the KL divergence. This base distribution can assimilate the complex topological structure of a target and, thereby, overcome a structural weakness of normalizing flows. By applying our procedure to 2D distributions, tabular data, images, and Boltzmann distributions we demonstrated that resampling the base distribution can improve their performance qualitatively and quantitatively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material: Resampling Base Distributions of Normalizing Flows A LEARNING ALGORITHMS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Estimating the normalization constant</head><p>To stabilize training, we use the exponential moving average to estimate the value of the normalization constant <ref type="bibr" target="#b1">(Bauer and Mnih, 2019)</ref>. In practice, this means that if Z i is the current Monte Carlo estimate of the normalization constant, the exponential moving average Z i is computed by</p><formula xml:id="formula_12">Z 1 = Z 1 ,<label>(12)</label></formula><formula xml:id="formula_13">Z i = (1 ? ) Z i?1 + Z i for i &gt; 1,<label>(13)</label></formula><p>where is the decay parameter which we set to 0.05 throughout this article. However, the gradients are estimated only with the current Monte Carlo estimate Z i , because otherwise backpropagation through the entire history of Z i would be necessary, which would be computationally expensive and memory demanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Gradient estimators of the Kullback-Leibler divergence</head><p>We repeat Theorem 1 as stated in the main text and supplement its proof.</p><p>Theorem 1. Let p ? (z) be the base distribution of a normalizing flow, having parameters ?, and F ? be the respective invertible mapping, depending on its parameters ?, such that the density of the model is</p><formula xml:id="formula_14">log (p(x)) = log (p ? (z)) ? log |det J F ? (z)| ,<label>(14)</label></formula><p>with x = F ? (z). Then, the gradients of the KL divergence with respect to the parameters are given by</p><formula xml:id="formula_15">? ? KLD(?, ?) = Cov p ? (z) log (p ? (z)) ? log |det J F ? (z)| ? logp * (F ? (z)), ? ? log p ? (z) (15) ? ? KLD(?, ?) = ?E p ? (z) ? ? log |det J F ? (z)| + logp * (F ? (z))<label>(16)</label></formula><p>Proof. The KL divergence is defined as</p><formula xml:id="formula_16">KLD(?, ?) := E p(x) [log p(x)] ? E p(x) [log p * (x)] .<label>(17)</label></formula><p>By plugging in <ref type="formula" target="#formula_0">(14)</ref> into <ref type="formula" target="#formula_0">(17)</ref> we obtain</p><formula xml:id="formula_17">KLD(?, ?) = E p ? (z) [log p ? (z) ? log |det J F ? (z)| ? logp * (F ? (z))] .<label>(18)</label></formula><p>Computing the gradient of (18) with respect to ? is straight forward.</p><formula xml:id="formula_18">? ? KLD(?, ?) = ? ? E p ? (z) [log p ? (z) ? log |det J F ? (z)| ? logp * (F ? (z))] = E p ? (z) ? ? log p ? (z) ? log |det J F ? (z)| ? logp * (F ? (z)) = ?E p ? (z) ? ? log |det J F ? (z)| + logp * (F ? (z))<label>(19)</label></formula><p>To get the gradient with respect to ?, we decompose (18) into two parts and consider their gradients separately.</p><formula xml:id="formula_19">? ? E p ? (z) [log p ? (z)] = ? ? p ? (z) log p ? (z)dz = ? ? p ? (z) log p ? (z) dz = ? ? p ? (z) + log p ? (z)? ? p ? (z)dz = ? ? p ? (z)dz =1 + p ? (z) log p ? (z)? ? log p ? (z)dz = E p ? (z) [log p ? (z)? ? log p ? (z)]<label>(20)</label></formula><formula xml:id="formula_20">ld : = log |det J F ? (z)| + logp * (F ? (z)) ? ? E p ? (z) [ld ] = ? ? ld p ? (z)dz = ld ? ? p ? (z)dz = ld p ? (z)? ? log p ? (z)dz = E p ? (z) [ld ? ? log p ? (z)]<label>(21)</label></formula><p>Using these two expressions, we obtain</p><formula xml:id="formula_21">? ? KLD(?, ?) = E p ? (z) log p ? (z) ? log |det J F ? (z)| ? logp * (F ? (z)) ? ? log p ? (z) (22) = Cov p ? (z) p ? (z) ? log |det J F ? (z)| ? logp * (F ? (z)), ? ? log p ? (z) .<label>(23)</label></formula><p>When concluding (23) from <ref type="formula" target="#formula_1">(22)</ref> we used the well known identity</p><formula xml:id="formula_22">E p ? (z) [? ? log p ? (z)] = p ? (z)? ? log p ? (z)dz = p ? (z) p ? (z) ? ? p ? (z)dz = ? ? p ? (z)dz =1 = 0.<label>(24)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MULTISCALE ARCHITECTURE</head><p>As already mentioned in the main paper, Dinh et al. <ref type="formula" target="#formula_0">(2017)</ref> introduced the multiscale architecture for normalizing flows to deal with high dimensional data such as images. As sketched in <ref type="figure" target="#fig_7">Figure 5</ref>, initially, the entire input x is transformed by several flow layers. The result is split up into two parts, h</p><p>(1) 1 and h</p><p>(2) 1 . Dinh et al. (2017) did this by first squeezing the image, i.e. reducing the height and width of the image by a factor 2 and adding the surplus pixels as additional channels, and then splitting the resulting tensor along the channel dimension. h</p><p>(1) 1 is immediately factored out in the density, while h</p><p>(2) 2 is further transformed by F 2 . The process is the repeated until a desired depth is reached. The output of the last map, in <ref type="figure" target="#fig_7">Figure 5</ref> it is F 4 , is not split, but directly passed to its base distribution. The full density for a multiscale architecture with n levels is given by In order to get an impression of what the architecture of the neural network defining the acceptence probability a for LARS, we did an ablation experiment on the Power UCI dataset. We left the flow architecture of a real NVP model constant but changed the number of hidden layers and units of the neural network representing a.</p><formula xml:id="formula_23">p(x) = n i=1 |det (J Fi (h i?1 ))| p(z i ), where we set h 0 = x. x h (2) 1 h (1) 1 h (2) 2 h (1) 2 h (2) 3 h (1) 3 z 1 z 2 z 3 z 4 F 1 F 2 F 3 F 4</formula><p>The baseline model with a Gaussian base distribution achieved 0.330 ? 0.003 on the test set. When changing the number of hidden layers we used 512 hidden units and 3 hidden layers when changing the number hidden units.  We see that both the number of hidden layers and units is important. The LL increases as we are adding more with diminishing returns. However, note that especially inceasing the number of hidden units increases the parameter count as well as the computational cost; hence, an application specific trade-off needs to be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Tuning the rejection rate</head><p>As discussed in the main text, the rejection rate of LARS can be controlled through the truncation parameter T . It sets a limit on how often subsequent proposals can be rejection in order to generate one sample. However, <ref type="figure">Figure 6</ref>: LL on the test set and Z with respect to the hyperparameter ? Z introduced in (26).</p><p>it does not tell us something about the actual rejection rate determining the sampling speed, which might be lower. The number of expected samples per sample from the proposal ? is given by</p><formula xml:id="formula_24">E ? (z)[a(z)] = a(z)?(z)dz = Z,<label>(25)</label></formula><p>which is equivalent to the normalization constant Z. Hence, if we increase Z we can decrease the rejection rate. We can simply do so by including it in our optimization, e.g. when doing ML we can instead minimize the loss</p><formula xml:id="formula_25">L = ?E p * (x) [log p(x)] ? ? Z Z,<label>(26)</label></formula><p>where ? Z ? R + is a positive hyperparameter.</p><p>In order to test this procedure, we trained 30 real NVP models with a resampled base distribution with different values of ? Z on the UCI Power dataset. The neural network representing the acceptance probability a had 3 hidden layers with 512 hidden units and we set T = 20. In <ref type="figure">Figure 6</ref> we show the LL of the models on the test set as well as Z depending on the hyperparameter ? Z . We see that by increasing ? Z we can trade off performance in terms of LL with the expected number of LARS samples per sample from the proposal. When Z approaches one, i.e. nearly all samples from the proposal get accepted, the LL drops to the value achieved by the flow with a Gaussian base distribution being 0.330 ? 0.003, see <ref type="table" target="#tab_11">Table 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D 2D DISTRIBUTIONS</head><p>The densities of the distributions used as sample targets in Section 4.1 are given in <ref type="table" target="#tab_9">Table 8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Circle of Gaussians log</head><formula xml:id="formula_26">? ? 8 i=1 ? ? 9 2? 2 ? ? 2 e ? 9 ( z 1 ?2 sin ( 2? 8i )) 2 + ( z 1 ?2 cos ( 2? 8i )) 2 4?2 ? 2 ? ? ? ? Two Rings log 2 i=1 32 ? e ?32( z ?i?1) 2</formula><p>All models approximating a 2D distribution uses for each layer a fully connected network having 2 hidden layers with 32 hidden units each as parameter map or residual learning block, respectively. The mixture of Gaussian  base distributions are initialized by uniformly sampling the mean in the hypercube [?2.5, 2.5] D and setting the variances to 0.5 ? 1 D , where 1 D is the D-dimensional identity matrix.</p><p>The models are trained on a computer with 6 Intel i5-9400F CPUs and a Nvidia GeForce RTX 2070 graphics card. The Adam optimizer with a learning rate of 10 ?3 is used. Training is done for 2 ? 10 4 iterations with a batch size of 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E TABULAR DATA</head><p>In addition to the NSF models, we also trained real NVP models with a Gaussian, a mixture of Gaussians, and a resampled base distribution to the four UCI datasets. The results are shown in <ref type="table" target="#tab_11">Table 9</ref>.   In all experiments regarding the UCI datasets, we use dropout both in the neural networks defining the flow map and the acceptance probability function a of the resampled base distribution during training. Adamax is used as an optimizer <ref type="bibr" target="#b19">(Kingma and Ba, 2015)</ref>. The experiments are run on machines with 36 Intel Xeon Platinum 9242 CPUs and 128 GB RAM. Further details on the datasets, the flow architecture, and the training procdure are given in <ref type="table" target="#tab_1">Table 10 and Table 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F IMAGE GENERATION</head><p>The parameter maps of the Glow models are convolutional neural networks (CNNs) with 3 layers, the first and the last having a kernel size of 3 ? 3 and the middle layer of 1 ? 1. The number of channels of the middle layer is 512 and those of the other layers is determined by the respective input and output. This is the same architecture as used in <ref type="bibr" target="#b20">(Kingma and Dhariwal, 2018)</ref>.</p><p>To ensure that each factor of the base distribution has not more than 100 dimensions, we apply a squeeze operation to the feature map of the first level before passing it to the base distribution. Therefore, each channel has a maximum size of 8 ? 8 = 64. A CNN with 4 layers, having 32 channels and a kernel size of 3 ? 3 each and a fully connected output layer, is used as acceptance function at each level. The convolutions of this CNN are strided with a stride of 2 until the image size is 4 ? 4. The normalization constants are updated with 2048 samples per iteration and before evaluating our models we estimated them with 10 10 samples.</p><p>Each model is trained for 10 6 iterations with the Adam optimizer having a learning rate of 10 ?3 . The learning rate is warmed up linearly over 10 3 iterations and the batch size is 512. The models with 8, 16, and 32 layers per level are trained in a distributed fashion on 1, 2, and 4 Nvidia Quadro RTX 5000 graphics cards. We apply Polyak-Ruppert weight averaging <ref type="bibr" target="#b32">(Polyak, 1990;</ref><ref type="bibr" target="#b35">Ruppert, 1988)</ref> with an update rate of 10 ?3 , where the exponential moving average of the model weights is computed in order to improve the generalization performance on the test set <ref type="bibr" target="#b15">(Izmailov et al., 2018)</ref>. Here, the bond length b is the distance between atom A and B, the bond angle ? being the angle between the bonds between B and C as well as C and D, and the dihedral angle ? is the angle between the plans spanned by A, B, and C as well as B, C, and D. We use a combination of Cartesian and internal coordinates.</p><p>To simplify the approximation Boltzmann distributions of complex molecules, a coordinate transformation was introduced <ref type="bibr" target="#b28">(No? et al., 2019)</ref>. Some of the Cartesian coordinates are mapped to their respective internal coordinates, i.e. bond lengths, bond angles, and dihedral angles, which are illustrated in <ref type="figure" target="#fig_1">Figure 11</ref>. The internal coordinates are normalized, with mean and standard deviation calculated on the training dataset generated through MD, but a suitable experimental dataset could be used as well. To the remaining Cartesian coordinates principal component analysis is applied. Subsequently, the weights of all but the last six principal components are used as coordinates. Thereby, six degrees of freedom are eliminated, corresponding to the three translational and free rotational coordinates which leave the Boltzmann distribution invariant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Setup of the experiments</head><p>All real NVP models trained via ML have a neural network with 2 hidden layers and 64 hidden units as a parameter map at each coupling layer. Between the coupling layers, we apply a invertible linear transformation which is learned with the other parameters of the flow, similar to the invertible 1x1 convolutions introduced in <ref type="bibr" target="#b20">(Kingma and Dhariwal, 2018)</ref>. The acceptance function of the resampled base distribution is a fully connected neural network with 2 hidden layers having 256 hidden units each. At each iteration, the normalization constant Z is updated with 512 samples from the Gaussian proposal during training. Before evaluating our models, we estimated Z with 10 10 samples. The residual flow models have 8 layers each with each layer having 2 layer fully connected neural network with 64 hidden units and the resampled base distribution has 3 hidden layers with 512 hidden units. All models are trained for 5 ? 10 5 iterations with the Adam optimizer <ref type="bibr" target="#b19">(Kingma and Ba, 2015</ref>) and a batch size of 512. The learning rate is set to 10 ?3 and decreased to 10 ?4 after 2.5 ? 10 5 iterations. We also do Polyak-Ruppert weight averaging <ref type="bibr" target="#b32">(Polyak, 1990;</ref><ref type="bibr" target="#b35">Ruppert, 1988)</ref> with an update rate of 10 ?2 . Each model is trained and evaluated on a server with 16 Intel Xeon E5-2698 CPUs and a Nvidia GTX980 GPU.</p><p>The real NVP models trained by minimizing the KL divergence have the same architecture as those in the previous experiment. However, to improve the stability of the training process, the models are trained with double precision numbers on 32 Intel Xeon E5-2698 CPUs each. 10 5 iterations are done with the Adam optimizer with a learning rate of 10 ?4 , which is exponentially decayed every 2.5 ? 10 4 iterations by a factor of 0.5.</p><p>The KL divergences were computed by drawing 10 6 samples from the model and estimating the respective integrals with histograms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Further results</head><p>As additional performance metric to compare the models, we compute the Ramachandran plots, i.e. a 2D histogram of two dihedral angles. These plots are frequently used to analyse how proteins fold locally and are hence of high importance for many applications. Some Ramachandran plots are show in <ref type="figure" target="#fig_1">Figure 13</ref>. We also estimate the KL divergences of the ground truth Ramachandran plot obtained from the MD test set and the plots of the models by performing numerical integration with the histograms. The results are given in <ref type="table" target="#tab_1">Table 13</ref>, <ref type="table" target="#tab_1">Table 14, and Table 15</ref>.</p><p>We also evaluated the stochastic normalizing flow model trained by <ref type="bibr" target="#b41">Wu et al. (2020)</ref> through ML on our metrics. The median KL divergences of the marginals is 2.3 ? 10 ?3 while the mean is 2.6 ? 10 ?2 , which is almost an order of magnitude higher that the results of the models with a resampled base distribution. However, the stochastic normalizing flow models the Ramachandran plot very well, where the KL divergence is only 2.4 ? 10 ?1 . Note that these results have to be taken with a grain of salt, since <ref type="bibr" target="#b41">Wu et al. (2020)</ref> used an augmented normalizing flow with less layers than we did. We tried to include their stochastic layers into our models but found training to be very unstable in this setting.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Proceedings of the 25 th International Conference on Artificial Intelligence and Statistics (AISTATS) 2022, Valencia, Spain. PMLR: Volume 151. Copyright 2022 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the architectural limitation of normalizing flows. (a) depicts the multimodal target distribution, (b) the Gaussian base distribution used, and (c) the learned real NVP model. The model's support has one connected component with a density filament between the modes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of a feature map when processing an image in a machine learning model. The unit which is used for factorization in our resampled base distribution is shown in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of the real NVP densities as well as the learned resampled base distribution when approximating three 2D distributions with complex topological structure. The models are trained via ML learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Marginal distribution of three dihedral angles of Alanine dipeptide. The ground truth was determined with a MD simulation. The flow models are based on real NVP and were trained via ML.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc><ref type="bibr" target="#b9">Cornish et al. (2020)</ref> introduced a new set of variables for each flow layer, called contin-uous indices, which they used as additional input to the flow maps. Thereby, they relaxed the bijectivity of the transformation leading to a better model performance.<ref type="bibr" target="#b13">Huang et al. (2020)</ref> augmented the dataset by auxiliary dimensions before applying their normalizing flow model. Although the topological constraints are still present in the augmented space, the marginal distribution of interest can be arbitrary complex.<ref type="bibr" target="#b41">Wu et al. (2020);</ref><ref type="bibr" target="#b27">Nielsen et al. (2020)</ref> suggested adding sampling layers to the model. Hence, the topology of the support can be changed through the sampling process. Nielsen et al. (2020) also introduced surjective layers, which do not suffer from topological constraints and essentially combine VAEs with flow-based models. These approaches sacrifice the invertibility of the flow map, which has several disadvantages. First of all, the model is no longer a perfect autoencoder, i.e. the original datapoint cannot be fully recovered from its latent representation. Second, if the layers of the flow-based model are bijective, significant memory savings are possible(Gomez et al., 2017)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc><ref type="bibr" target="#b4">Bhattacharyya et al. (2020)</ref>. While they only considered image generation, their entire model, i.e. including the base distribution, is tractable in contrast to ours. However, the computational cost of their models scales with the square root of the number of pixels, while ours is constant.<ref type="bibr" target="#b14">Izmailov et al. (2020)</ref>; Ardizzone et al. (2020); Hagemann and Neumayer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Multiscale architecture with four levels as introduced in(Dinh et al., 2017). First, the entire input x is transformed by F 1 . The result is then split up into two parts of which one of them is factored out immediately and the other one is further processed by F 2 . This process is repeated a few times until the desired depth is reached. The input is drawn in blue, intermediate results are red, and the components of the final variable z are yellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :Figure 8 :Figure 9 :Figure 10 :</head><label>78910</label><figDesc>Visualization of the densities when approximating three 2D distributions with complex topological structure. Real NVP models with Gaussian and a resampled base distributions where trained using the KL divergence. Visualization of the learned base distributions of the real NVP flow models shown inFigure 3. Visualization of the residual flow densities when approximating three 2D distributions with complex topological structure. The models were trained using ML learning and the corresponding base distributions are shown inFigure 10. Visualization of the learned base distributions of the residual flow models shown inFigure 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Marginal distribution of three dihedral angles of Alanine dipeptide. The ground truth was determined with a MD simulation. The flow models are based on the residual flow architecture and were trained via ML learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>KL divergences of the target distribution and the flow models which are trained to approximate the three 2D distributions, shown inFigure 3, with ML learning. For each target distribution and flow architecture, the model with the lowest KL divergence is marked in bold.</figDesc><table><row><cell>Flow architecture</cell><cell cols="6">Real NVP Real NVP Real NVP Residual Residual Residual</cell></row><row><cell>Base distribution</cell><cell>Gaussian</cell><cell>Mixture</cell><cell cols="4">Resampled Gaussian Mixture Resampled</cell></row><row><cell>Dual moon</cell><cell>1.83</cell><cell>1.80</cell><cell>1.77</cell><cell>1.82</cell><cell>1.80</cell><cell>1.76</cell></row><row><cell cols="2">Circle of Gaussians 0.090</cell><cell>0.060</cell><cell>0.043</cell><cell>0.045</cell><cell>0.042</cell><cell>0.039</cell></row><row><cell>Two rings</cell><cell>10.7</cell><cell>10.6</cell><cell>10.4</cell><cell>11.7</cell><cell>10.8</cell><cell>10.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>LL on the test sets of the respective datasets of NSF, its CIF variant, and a NSF with a resampled base distribution (RBD). The values are averaged over 3 runs each and the standard error is given as a measure of uncertainty. The highest values within the confidence interval are marked in bold.</figDesc><table><row><cell>Method</cell><cell>Power</cell><cell>Gas</cell><cell>Hepmass</cell><cell>Miniboone</cell></row><row><cell>NSF</cell><cell cols="2">0.69 ? 0.00 13.01 ? 0.02</cell><cell>?14.30 ? 0.05</cell><cell>?10.68 ? 0.06</cell></row><row><cell>CIF-NSF</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>KL divergences of the target distribution and the models which were trained using the KL divergence, shown inFigure 7. For each target distribution, the real NVP model with the lower KL divergences is marked in bold.</figDesc><table><row><cell>Base distribution</cell><cell cols="2">Gaussian Resampled</cell></row><row><cell>Dual moon</cell><cell>1.844</cell><cell>1.839</cell></row><row><cell cols="2">Circle of Gaussians 0.167</cell><cell>0.122</cell></row><row><cell>Two rings</cell><cell>11.5</cell><cell>10.3</cell></row><row><cell>4.2 Tabular data</cell><cell></cell><cell></cell></row><row><cell cols="3">Next, we estimate the density of four tabular datasets</cell></row><row><cell cols="3">from the UCI Machine Learning Repository (Dheeru</cell></row><row><cell cols="3">and Taniskidou, 2022). We use the same preprocess-</cell></row><row><cell cols="3">ing and training, validation, and test splits as Papa-</cell></row><row><cell cols="3">makarios et al. (2017), which have been adopted by</cell></row><row><cell cols="3">others in the field (Durkan et al., 2019; Cornish et al.,</cell></row><row><cell cols="3">2020). For each dataset, we train a Neural Spline Flow</cell></row><row><cell cols="3">(NSF) (Durkan et al., 2019), its continuously indexed</cell></row><row><cell>(CIF) variant</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Bits per dimension on the test set of the Glow models with Gaussian and resampled base distribution trained on CIFAR-10. For each architecture, three seeded training runs were done, the reported bits per dimension values are averages over these runs and the standard error is given as an uncertainty estimate. For each number of layers, the lowest values within the confidence interval is marked in bold.</figDesc><table><row><cell>Base distribution</cell><cell>Gaussian</cell><cell>Resampled</cell></row><row><cell>8 layers per level</cell><cell>3.403 ? 0.002</cell><cell>3.399 ? 0.001</cell></row><row><cell cols="2">16 layers per level 3.339 ? 0.001</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Quantitative comparison of the real NVP models approximating the Boltzmann distribution of Alanine dipeptide trained via ML learning. The LL is evaluated on a test set obtained with a MD simulation. The KL divergences of the 60 marginals were computed and the mean and median of them are reported. All results are averages over 10 runs, the standard error is given, and highers LL as well as lowest KL divergences are marked in bold.</figDesc><table><row><cell>Base distribution</cell><cell>Gaussian</cell><cell>Mixture</cell><cell>Gaussian</cell><cell>Resampled</cell></row><row><cell>Number of layers</cell><cell>16</cell><cell>16</cell><cell>19</cell><cell>16</cell></row><row><cell>LL (?10 Mean KLD (?10 ?3 )</cell><cell>1.76 ? 0.08</cell><cell>8.23 ? 0.82</cell><cell>1.35 ? 0.03</cell><cell>1.12 ? 0.02</cell></row><row><cell cols="2">Median KLD (?10 ?4 ) 5.20 ? 0.10</cell><cell>43.5 ? 6.0</cell><cell>4.63 ? 0.08</cell><cell>4.36 ? 0.05</cell></row></table><note>2 ) 1.8096 ? 0.0002 1.8106 ? 0.0002 1.8109 ? 0.0001 1.8118 ? 0.0001</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>with Continuously Indexed Normalising Flows. In Proceedings of the 37th International Conference on Machine Learning. Dheeru, D. and Taniskidou, E. K. (2022). UCI machine learning repository. http://archive.ics. uci.edu/ml.</figDesc><table><row><cell>Dinh, L., Krueger, D., and Bengio, Y. (2015). NICE:</cell></row><row><cell>Non-linear Independent Components Estimation. In</cell></row><row><cell>3rd International Conference on Learning Represen-</cell></row><row><cell>tations, Workshop Track Proceedings.</cell></row><row><cell>Dinh, L., Sohl-Dickstein, J., and Bengio, S. (2017).</cell></row><row><cell>Density estimation using Real NVP. International</cell></row><row><cell>Conference on Learning Representations.</cell></row><row><cell>Durkan, C., Bekasov, A., Murray, I., and Papamakar-</cell></row><row><cell>ios, G. (2019). Neural Spline Flows. In Advances in</cell></row><row><cell>Neural Information Processing Systems, volume 32.</cell></row><row><cell>Gomez, A. N., Ren, M., Urtasun, R., and Grosse, R. B.</cell></row><row><cell>(2017). The Reversible Residual Network: Back-</cell></row><row><cell>propagation Without Storing Activations. In Ad-</cell></row><row><cell>vances in Neural Information Processing Systems,</cell></row><row><cell>volume 32.</cell></row><row><cell>Grci?, M., Grubi?i?, I., and?egvi?, S. (2021). Densely</cell></row><row><cell>connected normalizing flows. In Advances in Neural</cell></row><row><cell>Information Processing Systems 34.</cell></row><row><cell>Grover, A., Gummadi, R., Lazaro-Gredilla, M., Schu-</cell></row><row><cell>urmans, D., and Ermon, S. (2018). Variational</cell></row><row><cell>Rejection Sampling. In International Conference</cell></row><row><cell>on Artificial Intelligence and Statistics, volume 84,</cell></row><row><cell>pages 823-832.</cell></row><row><cell>Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin,</cell></row><row><cell>V., and Courville, A. (2017). Improved Training</cell></row><row><cell>of Wasserstein GANs. In Advances in Neural Infor-</cell></row><row><cell>mation Processing Systems, volume 30, pages 5767-</cell></row><row><cell>5777.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>LL of the test set for different number of hidden layers for a while leaving the number of hidden units constant at 512.</figDesc><table><row><cell cols="2">Hidden layers 1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9</cell></row><row><cell>LL</cell><cell cols="5">0.37 0.53 0.58 0.62 0.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>LL of the test set for different number of hidden layers for a while leaving the number of hidden layers constant at 3.</figDesc><table><row><cell cols="2">Hidden units 32</cell><cell>128 512 2048 8192</cell></row><row><cell>LL</cell><cell cols="2">0.39 0.45 0.53 0.61 0.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Logarithm of the unnormalized densities of the target distributions used in Section 4.1.</figDesc><table><row><cell></cell><cell></cell><cell cols="6">Unnormalized log density</cell></row><row><cell>Dual Moon</cell><cell>?</cell><cell>( z ? 1) 0.08</cell><cell>2</cell><cell>?</cell><cell>(|z 1 | ? 2) 0.18</cell><cell>2</cell><cell>+ log 1 + e ? 4z 1 0.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>LL of real NVP models with different base distributions on the test sets of the respective datasets. The values are averaged over 3 runs each and the standard error is given as a measure of uncertainty. The highest values within the confidence interval are marked in bold.</figDesc><table><row><cell cols="2">Base distribution Power</cell><cell>Gas</cell><cell>Hepmass</cell><cell>Miniboone</cell></row><row><cell>Gaussian</cell><cell>0.330 ? 0.003</cell><cell>10.1 ? 0.1</cell><cell>?19.5 ? 0.1</cell><cell>?11.65 ? 0.05</cell></row><row><cell>Mixture</cell><cell>0.341 ? 0.001</cell><cell>9.9 ? 0.2</cell><cell>?19.5 ? 0.1</cell><cell>?11.49 ? 0.04</cell></row><row><cell>Resampled</cell><cell cols="4">0.560 ? 0.006 12.8 ? 0.1 ?18.4 ? 0.1 ?11.48 ? 0.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Details about datasets from the UCI machine learning repository, the architecture of the NSF models as well as the resampled base distribution, and the training procedure.</figDesc><table><row><cell></cell><cell>Power</cell><cell>Gas</cell><cell cols="2">Hepmass Miniboone</cell></row><row><cell>Dimension</cell><cell>6</cell><cell>8</cell><cell>21</cell><cell>43</cell></row><row><cell>Train data points</cell><cell cols="3">1.6 ? 10 6 8.5 ? 10 5 3.2 ? 10 5</cell><cell>3.0 ? 10 4</cell></row><row><cell>Flow layers</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell cols="2">Hidden layers flow maps 2</cell><cell>2</cell><cell>2</cell><cell>1</cell></row><row><cell>Hidden units flow maps</cell><cell>256</cell><cell>128</cell><cell>256</cell><cell>64</cell></row><row><cell>Hidden layers a</cell><cell>7</cell><cell>9</cell><cell>4</cell><cell>2</cell></row><row><cell>Hidden units a</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>128</cell></row><row><cell cols="2">Truncation parameter T 100</cell><cell>50</cell><cell>40</cell><cell>40</cell></row><row><cell>Dropout rate</cell><cell>0</cell><cell>0.1</cell><cell>0.3</cell><cell>0.3</cell></row><row><cell>Batch size</cell><cell>512</cell><cell>512</cell><cell>256</cell><cell>64</cell></row><row><cell>Learning rate</cell><cell cols="3">3 ? 10 ?4 4 ? 10 ?4 4 ? 10 ?4</cell><cell>3 ? 10 ?4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Details about the architecture of the real NVP models used as well as the resampled base distribution, and the training procedure.</figDesc><table><row><cell></cell><cell>Power</cell><cell>Gas</cell><cell cols="2">Hepmass Miniboone</cell></row><row><cell>Flow layers</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>16</cell></row><row><cell cols="2">Hidden layers flow maps 2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>Hidden units flow maps</cell><cell>128</cell><cell>128</cell><cell>64</cell><cell>32</cell></row><row><cell>Hidden layers a</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell>Hidden units a</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>256</cell></row><row><cell cols="2">Truncation parameter T 100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Dropout rate</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.2</cell></row><row><cell>Batch size</cell><cell>512</cell><cell>512</cell><cell>256</cell><cell>128</cell></row><row><cell>Learning rate</cell><cell cols="3">5 ? 10 ?4 5 ? 10 ?4 3 ? 10 ?4</cell><cell>3 ? 10 ?4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Percentage increase in training time and model size when using a resampled instead of a Gaussian base distribution for the models trained in Section 4.3.Figure 11: Illustration of molecular coordinates. The state of the molecule can be described through the Cartesian coordinates, i.e. x, y, and z, of each of the four atoms A, B, C, and D. Alternatively, internal coordinates, i.e. bond lengths, bond angles, and dihedral angles, can be used.</figDesc><table><row><cell cols="4">Layers per level Training time Model size</cell></row><row><cell>8</cell><cell></cell><cell>4.7%</cell><cell>1.5%</cell></row><row><cell cols="2">16</cell><cell>15%</cell><cell>0.75%</cell></row><row><cell cols="2">32</cell><cell>9.1%</cell><cell>0.38%</cell></row><row><cell cols="2">G BOLTZMANN GENERATORS</cell><cell></cell><cell></cell></row><row><cell cols="2">G.1 Coordinate transformation</cell><cell></cell><cell></cell></row><row><cell></cell><cell>b</cell><cell>B</cell><cell>?</cell><cell>D</cell></row><row><cell></cell><cell>A</cell><cell>?</cell><cell>C</cell></row><row><cell>z</cell><cell>y</cell><cell></cell><cell></cell></row><row><cell></cell><cell>x</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>KL divergence of the Ramachandran plots of the MD simulation, serving as a ground truth, and real NVP models trained via ML learning. It was estimated based on a histogram computed from 10 6 samples. ? 0.73 10.8 ? 7.3 2.26 ? 0.27 3.00 ? 0.36</figDesc><table><row><cell cols="2">Base distribution Gaussian</cell><cell>Mixture</cell><cell>Gaussian</cell><cell>Resampled</cell></row><row><cell cols="2">Number of layers 16</cell><cell>16</cell><cell>19</cell><cell>16</cell></row><row><cell>KL divergence</cell><cell>4.79</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 :</head><label>14</label><figDesc>Quantitative comparison of the residual flow models approximating the Boltzmann distribution of Alanine dipeptide trained via ML learning. The LL is evaluated on a test set obtained with a MD simulation. The KL divergences of the 60 marginals were computed and the mean and median of them are reported. Moreover, the KL divergences of the Ramachandran plots are listed. All results are averages over 10 runs, the standard error is given, and highers LL as well as lowest KL divergences are marked in bold.</figDesc><table><row><cell>Base distribution</cell><cell>Gaussian</cell><cell>Mixture</cell><cell>Resampled</cell></row><row><cell>LL (?10 2 )</cell><cell cols="3">1.8048 ? 0.0002 1.8061 ? 0.0002 1.8144 ? 0.0002</cell></row><row><cell>Mean KLD marginals (?10 ?3 )</cell><cell>6.16 ? 0.17</cell><cell>31.5 ? 1.8</cell><cell>3.49 ? 0.15</cell></row><row><cell cols="2">Median KLD marginals (?10 ?4 ) 5.21 ? 0.12</cell><cell>14.2 ? 5.2</cell><cell>4.67 ? 0.05</cell></row><row><cell>KLD Ramachandran plot</cell><cell>8.1 ? 2.2</cell><cell>25.4 ? 10.2</cell><cell>4.4 ? 0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 15 :</head><label>15</label><figDesc>Quantitative comparison of the real NVP models approximating the Boltzmann distribution of Alanine dipeptide trained via the KL divergence. The LL is evaluated on a test set obtained with a MD simulation. The KL divergences of the 60 marginals were computed and the mean and median of them are reported. Moreover, the KL divergences of the Ramachandran plots are listed. All results are averages over 10 runs, the standard error is given, and highers LL as well as lowest KL divergences are marked in bold.</figDesc><table><row><cell>Base distribution</cell><cell>Gaussian</cell><cell>Mixture</cell><cell>Resampled</cell></row><row><cell>LL (?10 2 )</cell><cell cols="3">?2.78 ? 0.07 ?2.70 ? 0.04 ?1.84 ? 0.13</cell></row><row><cell>Mean KLD marginals (?10 ?1 )</cell><cell>2.91 ? 0.05</cell><cell>2.98 ? 0.02</cell><cell>2.84 ? 0.07</cell></row><row><cell cols="2">Median KLD marginals (?10 ?3 ) 4.75 ? 0.04</cell><cell>4.77 ? 0.03</cell><cell>4.66 ? 0.05</cell></row><row><cell>KLD Ramachandran plot</cell><cell>7.63 ? 0.18</cell><cell>16.6 ? 8.4</cell><cell>6.92 ? 0.37</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Matthias Bauer, Richard Turner, Andrew Campbell, Austin Tripp, and David Liu for the help-ful discussions. Jos? Miguel Hern?ndez-Lobato acknowledges support from a Turing AI Fellowship under grant EP/V023756/1. This work was supported by the German Federal Ministry of Education and Research (BMBF): T?bingen AI Center, FKZ: 01IS18039B; and by the Machine Learning Cluster of Excellence, EXC number 2064/1 -Project number 390727645.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Training Normalizing Flows with the Information Bottleneck for Competitive Generative Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ardizzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mackowiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>K?the</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Resampled Priors for Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Invertible Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding and Mitigating Exploding Inverses in Invertible Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 24th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>The 24th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1792" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Normalizing Flows With Multi-Scale Autoregressive Priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8415" to="8424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Gradient Based Strategy for Hamiltonian Monte Carlo Hyperparameter Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stimper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernandez-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1238" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Residual Flows for Invertible Generative Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">PixelSNAIL: An Improved Autoregressive Generative Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<title level="m">Generating Long Sequences with Sparse Transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Relaxing Bijectivity Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cornish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Caterini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deligiannidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stabilizing invertible neural networks using mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hagemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Neumayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">verse Problems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">85002</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Black-box alpha-divergence Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1511" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07101</idno>
		<title level="m">Augmented Normalizing Flows: Bridging the Gap Between Generative Flows and Latent Variable Models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-Supervised Learning with Normalizing Flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kirichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4615" to="4630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Averaging Weights Leads to Wider Optima and Better Generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training Generative Adversarial Networks with Limited Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12104" to="12114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Style-Based Generator Architecture for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Analyzing and Improving the Image Quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<title level="m">Glow: Generative Flow with Invertible 1x1 Convolutions. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00630</idno>
		<title level="m">Variational Diffusion Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Smooth Normalizing Flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kr?mer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>No?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">60</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leimkuhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Matthews</surname></persName>
		</author>
		<title level="m">Molecular Dynamics With Deterministic and Stochastic Numerical Methods. Number 39 in Interdisciplinary Applied Methematics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Macow: Masked convolutional generative flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">; H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>Wallach,</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Stimper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos? Miguel Hern?ndez-Lobato</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Winther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<title level="m">SurVAE Flows: Surjections to Bridge the Gap between VAEs and Flows</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>No?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="issue">6457</biblScope>
			<biblScope unit="page">365</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Normalizing Flows for Probabilistic Modeling and Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">57</biblScope>
			<biblScope unit="page" from="1" to="64" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Masked Autoregressive Flow for Density Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2338" to="2347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">New stochastic approximation type procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Avtomatica i Telemekhanika</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="98" to="107" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Variational Inference with Normalizing Flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A Taste of Topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Runde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Universitext</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Efficient estimators from a slowly converging robbins-monro process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ruppert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Family of Nonparametric Density Estimation Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Tabak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="164" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Density estimation by dual ascent of the loglikelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Tabak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vanden-Eijnden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="217" to="233" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Parallel WaveNet: Fast High-Fidelity Speech Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Targeted free energy estimation via learned mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wirnsberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abercrombie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Racani?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">144112</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stochastic normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Noe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5933" to="5944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">16 layers (b) Real NVP, Gaussian mixture base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nvp</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaussian</forename><surname>Base</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Gaussian base, 19 layers (d) Real NVP, Resampled base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nvp</forename><surname>Real</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Ground truth (MD simulation)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ramachandran plots of Alanine dipeptide. The flow models were trained via ML learning</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
