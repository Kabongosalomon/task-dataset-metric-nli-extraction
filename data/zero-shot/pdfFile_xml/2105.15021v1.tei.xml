<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Bi-Lexicalized PCFG Induction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences ? ILCC</orgName>
								<orgName type="institution" key="instit3">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences ? ILCC</orgName>
								<orgName type="institution" key="instit3">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences ? ILCC</orgName>
								<orgName type="institution" key="instit3">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Shanghai Engineering Research Center of Intelligent Vision and Imaging Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution" key="instit1">ShanghaiTech University</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences University of Chinese Academy of Sciences ? ILCC</orgName>
								<orgName type="institution" key="instit3">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Bi-Lexicalized PCFG Induction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural lexicalized PCFGs (L-PCFGs) <ref type="bibr" target="#b54">(Zhu et al., 2020)</ref> have been shown effective in grammar induction. However, to reduce computational complexity, they make a strong independence assumption on the generation of the child word and thus bilexical dependencies are ignored. In this paper, we propose an approach to parameterize L-PCFGs without making implausible independence assumptions. Our approach directly models bilexical dependencies and meanwhile reduces both learning and representation complexities of L-PCFGs. Experimental results on the English WSJ dataset confirm the effectiveness of our approach in improving both running speed and unsupervised parsing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Challenges of L-PCFG induction</head><p>The major difference between L-PCFGs from vanilla PCFGs is that they use word-annotated nonterminals, so the nonterminal number of L-PCFGs</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Probabilistic context-free grammars (PCFGs) has been an important probabilistic approach to syntactic analysis <ref type="bibr" target="#b32">(Lari and Young, 1990;</ref><ref type="bibr" target="#b19">Jelinek et al., 1992)</ref>. They assign a probability to each of the parses admitted by CFGs and rank them by the plausibility in such a way that the ambiguity of CFGs can be ameliorated. Still, due to the strong independence assumption of CFGs, vanilla PCFGs <ref type="bibr" target="#b2">(Charniak, 1996)</ref> are far from adequate for highly ambiguous text.</p><p>A common premise for tackling the issue is to incorporate lexical information and weaken the independence assumption. There have been many approaches proposed under the premise <ref type="bibr" target="#b34">(Magerman, 1995;</ref><ref type="bibr" target="#b5">Collins, 1997;</ref><ref type="bibr" target="#b25">Johnson, 1998;</ref><ref type="bibr" target="#b30">Klein and Manning, 2003)</ref>. Among them lexicalized PCFGs (L-PCFGs) are a relatively straightforward formalism <ref type="bibr" target="#b6">(Collins, 2003)</ref>. L-PCFGs extend PCFGs by associating a word, i.e., the lexical head, with each grammar symbol. They can thus exploit lexical Corresponding Author information to disambiguate parsing decisions and are much more expressive than vanilla PCFGs. However, they suffer from representation and inference complexities. For representation, the addition of lexical information greatly increases the number of parameters to be estimated and exacerbates the data sparsity problem during learning, so the expectation-maximisation (EM) based estimation of L-PCFGs has to rely on sophisticated smoothing techniques and factorizations <ref type="bibr" target="#b6">(Collins, 2003)</ref>. As for inference, the CYK algorithm for L-PCFGs has a Opl 5 |G|q complexity, where l is the sentence length and |G| is the grammar constant. Although <ref type="bibr" target="#b12">Eisner and Satta (1999)</ref> manage to reduce the complexity to Opl 4 |G|q, inference with L-PCFGs is still relatively slow, making them less popular nowadays.</p><p>Recently, <ref type="bibr" target="#b54">Zhu et al. (2020)</ref> combine the ideas of factorizing the binary rule probabilities <ref type="bibr" target="#b6">(Collins, 2003)</ref> and neural parameterization <ref type="bibr" target="#b27">(Kim et al., 2019)</ref> and propose neural L-PCFGs (NL-PCFGs), achieving good results in both unsupervised dependency and constituency parsing. Neural parameterization is the key to success, which facilitates informed smoothing <ref type="bibr" target="#b27">(Kim et al., 2019)</ref>, reduces the number of learnable parameters for large grammars <ref type="bibr" target="#b3">(Chiu and Rush, 2020;</ref><ref type="bibr" target="#b50">Yang et al., 2021)</ref> and facilitates advanced gradient-based optimization techniques instead of using the traditional EM algorithm <ref type="bibr" target="#b10">(Eisner, 2016)</ref>. However, <ref type="bibr" target="#b54">Zhu et al. (2020)</ref> oversimplify the binary rules to decrease the complexity of the inside/CYK algorithm in learning (i.e., estimating the marginal sentence loglikelihood) and inference. Specifically, they make a strong independence assumption on the generation of the child word such that it is only dependent on the nonterminal symbol. Bilexical dependencies, which have been shown useful in unsupervised dependency parsing <ref type="bibr" target="#b14">(Han et al., 2017;</ref><ref type="bibr" target="#b49">Yang et al., 2020)</ref>, are thus ignored.</p><p>To model bilexical dependencies and meanwhile reduce complexities, we draw inspiration from the canonical polyadic decomposition (CPD) <ref type="bibr" target="#b31">(Kolda and Bader, 2009</ref>) and propose a latent-variable based neural parameterization of L-PCFGs. <ref type="bibr" target="#b4">Cohen et al. (2013)</ref>; <ref type="bibr" target="#b50">Yang et al. (2021)</ref> have used CPD to decrease the complexities of PCFGs, and our work can be seen as an extension of their work to L-PCFGs. We further adopt the unfold-refold transformation technique <ref type="bibr" target="#b11">(Eisner and Blatz, 2007)</ref> to decrease complexities. By using this technique, we show that the time complexity of the inside algorithm implemented by <ref type="bibr" target="#b54">Zhu et al. (2020)</ref> can be improved from cubic to quadratic in the number of nonterminals m. The inside algorithm of our proposed method has a linear complexity in m after combining CPD and unfold-refold.</p><p>We evaluate our model on the benchmarking Wall Street Journey (WSJ) dataset. Our model surpasses the strong baseline NL-PCFG <ref type="bibr" target="#b54">(Zhu et al., 2020)</ref> by 2.9% mean F1 and 1.3% mean UUAS under CYK decoding. When using the Minimal Bayes-Risk (MBR) decoding, our model performs even better. We provide an efficient implementation of our proposed model at https://github.com/ sustcsonglin/TN-PCFG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Lexicalized CFGs</head><p>We first introduce the formalization of CFGs. A CFG is defined as a 5-tuple G " pS, N , P, ?, Rq where S is the start symbol, N is a finite set of nonterminal symbols, P is a finite set of preterminal symbols, 1 ? is a finite set of terminal symbols, and R is a set of rules in the following form:</p><formula xml:id="formula_0">S ? A A P N A ? BC, A P N , B, C P N Y P T ? w, T P P, w P ?</formula><p>N , P and ? are mutually disjoint. We will use 'nonterminals' to indicate N Y P when it is clear from the context. Lexicalized CFGs (L-CFGs) <ref type="bibr" target="#b6">(Collins, 2003)</ref> extend CFGs by associating a word with each of the 1 An alternative definition of CFGs does not distinguish nonterminals N (constituent labels) from preterminals P (partof-speech tags) and treats both as nonterminals. nonterminals:</p><formula xml:id="formula_1">S ? Arw p s A P N Arw p s ? Brw p sCrw q s, A P N ; B, C P N Y P Arw p s ? Crw q sBrw p s, A P N ; B, C P N Y P T rw p s ? w p , T P P</formula><p>where w p , w q P ? are the headwords of the constituents spanned by the associated grammar symbols, and p, q are the word positions in the sentence. We refer to A, a parent nonterminal annotated by the headword w p , as head-parent. In binary rules, we refer to a child nonterminal as head-child if it inherits the headword of the headparent (e.g., Brw p s) and as non-head-child otherwise (e.g., Crw q s). A head-child appears as either the left child or the right child. We denote the head direction by D P t?, ?u, where ? means head-child appears as the left child.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Grammar induction with lexicalized probabilistic CFGs</head><p>Lexicalized probabilistic CFGs (L-PCFGs) extend L-CFGs by assigning each production rule r " A ? ? a scalar ? r such that it forms a valid categorical probability distribution given the left hand side A. Note that preterminal rules always have a probability of 1 because they define a deterministic generating process. Grammar induction with L-PCFGs follows the same way of grammar induction with PCFGs. As with PCFGs, we maximize the log-likelihood of each observed sentence w " w 1 , . . . , w l :</p><formula xml:id="formula_2">log ppwq " log ? tPT G L pwq pptq ,<label>(1)</label></formula><p>where pptq " ? rPt ? r and T G L pwq consists of all possible lexicalized parse trees of the sentence w under an L-PCFG G L . We can compute the marginal ppwq of the sentence by using the inside algorithm in polynomial time. The core recursion of the inside algorithm is formalized in Equation 3. It recursively computes the probability s A,p i,j of a head-parent Arw p s spanning the substring w i , . . . , w j?1 (p P ri, j?1s). Term A1 and A2 in Equation 3 cover the cases of the head-child as the left child and the right child respectively.  <ref type="bibr" target="#b54">Zhu et al. (2020)</ref>: W q is independent with B, D, A, W p given C. (c) Our proposed parameterization. We slightly abuse the Bayesian network notation by grouping variables. In the standard notation, there would be arcs from the parent variables to each grouped variable as well as arcs between the grouped variables.</p><p>is up to |?| times the number of nonterminals in PCFGs. As the grammar size is largely determined by the number of binary rules and increases approximately in cubic of the nonterminal number, representing L-PCFGs has a high space complexity Opm 3 |?| 2 q (m is the nonterminal number). Specifically, it requires an order-6 probability tensor for binary rules with each dimension representing A, B, C, w p , w q , and head direction D, respectively. With so many rules, L-PCFGs are very prone to the data sparsity problem in rule probability estimation. <ref type="bibr" target="#b6">Collins (2003)</ref> suggests factorizing the binary rule probabilities according to specific independence assumptions, but his approach still relies on complicated smoothing techniques to be effective. The addition of lexical heads also scales up the computational complexity of the inside algorithm by a factor Opl 2 q and brings it up to Opl 5 m 3 q. Eisner and Satta (1999) point out that, by changing the order of summations in Term A1 (A2) of Equation 3, one can cache and reuse Term B1 (B2) in Equation 4 and reduce the computational complexity to Opl 4 m 2`l3 m 3 q. This is an example application of unfold-refold as noted by <ref type="bibr" target="#b11">Eisner and Blatz (2007)</ref>. However, the complexity is still cubic in m, making it expensive to increase the total number of nonterminals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Neural L-PCFGs</head><p>Zhu et al. (2020) apply neural parameterization to tackle the data sparsity issue and to reduce the total learnable parameters of L-PCFGs. Considering the head-child as the left child (similarly for the other case), they further factorize the binary rule probability as:</p><formula xml:id="formula_3">ppArw p s ? Brw p sCrw q sq " ppB, ?, C|A, w p qppw q |Cq .<label>(2)</label></formula><p>Bayesian networks representing the original probability and the factorization are illustrated in can be rewritten as Equation 5. <ref type="bibr" target="#b54">Zhu et al. (2020)</ref> implement the inside algorithm by caching Term C1-1 in Equation 6, resulting in a time complexity Opl 4 m 3`l3 mq, which is cubic in m. We note that, we can use unfold-refold to further cache Term C1-2 in Equation 6 and reduce the time complexity of the inside algorithm to Opl 4 m 2`l3 m`l 2 m 2 q, which is quadratic in m.</p><p>Although the factorization of Equation 2 reduces the space and time complexity of the inside algorithm of L-PCFG, it is based on the independence assumption that the generation of w q is independent of A, B, D and w p given the non-head-child C. This assumption can be violated in many scenarios and hence reduces the expressiveness of the grammar. For example, suppose C is Noun, then even if we know B is Verb, we still need to know D to determine if w q is an object or a subject of the verb, and then need to know the actual verb w p to pick a likely noun as w q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Factorization with latent variable</head><p>Our main goal is to find a parameterization that removes the implausible independence assumptions of <ref type="bibr" target="#b54">Zhu et al. (2020)</ref> while decreases the complexities of the original L-PCFGs.</p><p>To reduce the representation complexity, we draw inspiration from the canonical polyadic decomposition (CPD). CPD factorizes an n-th order tensor into n two-dimensional matrices. Each matrix consists of two dimensions: one dimension comes from the original n-th order tensor and the other dimension is shared by all the n matrices. The shared dimension can be marginalized to recover the original n-th order tensor. From a probabilistic perspective, the shared dimension can be regarded as a latent-variable. In the spirit of CPD, we introduce a latent-variable H to decompose the order-6 probability tensor ppB, C, D, w q |A, w p q. Instead of fully decomposing the tensor, we empirically find that binding some of the variables leads to better results. Our best factorization is as follows (also illustrated by a Bayesian network in <ref type="figure" target="#fig_0">Figure 1</ref> </p><formula xml:id="formula_4">s A,p i,j " j?1 ? k"p`1 j?1 ? q"k ? B,C s B,p i,k?s C,q k,j?p pArwps ? BrwpsCrwqsq loooooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooooon Term A1`p ? k"i`1 k?1 ? q"i ? B,C s B,q i,k?s C,p k,j?p pArwps ? BrwqsCrwpsq loooooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooooon Term A2 (3) " j?1 ? k"p`1 ? B s B,p i,k j?1 ? q"k ? C s C,q k,j?p pArwps ? BrwpsCrwqsq looooooooooooooooooooooooomooooooooooooooooooooooooon Term B1`p ? k"i`1 ? C s C,p k 2 ,j k?1 ? q"i ? B s B,q i,k?p pArwps ? BrwqsCrwpsq looooooooooooooooooooooooomooooooooooooooooooooooooon Term B2 (4) Term A1 " j?1 ? k"p`1 j?1 ? q"k ? B,C s B,p i,k?s C,q k,j?p pB, ?, C|A, wpq?ppwq|Cq looooooooooooooooomooooooooooooooooon factorization of ppArwps?BrwpsCrwq sq (5) " j?1 ? k"p`1 ? B s B,p i,k ? C ppB, ?, C|A, wpq j?1 ? q"k s C,q k,j?p pwq|Cq loooooooooomoooooooooon Term C1-1 looooooooooooooooooooooooomooooooooooooooooooooooooon Term C1-2 (6) Term A1 " j?1 ? k"p`1 j?1 ? q"k ? B,C s B,p i,k?s C,q k,j?? H ppH|A, wpqppB|HqppC, ? |Hqppwq|Hq loooooooooooooooooooooooooooomoooooooooooooooooooooooooooon factorization of ppArwps?BrwpsCrwq sq (7) " ? H ppH|A, wpq j?1 ? k"p`1 ? B s B,p i,k ppB|Hq loooooooomoooooooon Term D1-1 j?1 ? q"k ? C s C,q k,j ppC ? |Hqppwq|Hq looooooooooooooooooomooooooooooooooooooon Term D1-2 (8)</formula><formula xml:id="formula_5">(c)): ppB, C, W q , D|A, W p q " (9) ? H ppH|A, W p qppB|HqppC, D|HqppW q |Hq .</formula><p>According to d-separation <ref type="bibr" target="#b39">(Pearl, 1988)</ref>, when A and w p are given, B, C, w q , and D are interdependent due to the existence of H. In other words, our factorization does not make any independence assumption beyond the original binary rule. The domain size of H is analogous to the tensor rank in CPD and thus influences the expressiveness of our proposed model.</p><p>Based on our factorization approach, the binary rule probability is factorized as</p><formula xml:id="formula_6">ppArw p s ? Brw p sCrw q sq " (10) ? H ppH|A, w p qppB|HqppC ? |Hqppw q |Hq , and ppArw p s ? Brw q sCrw p sq " (11) ? H ppH|A, w p qppC|HqppB ? |Hqppw q |Hq .</formula><p>We also follow <ref type="bibr" target="#b54">Zhu et al. (2020)</ref> and factorize the start rule as follows. </p><formula xml:id="formula_7">ppS ? Arw p sq " ppA|Sqppw p |Aq .<label>(12</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choices of factorization:</head><p>If we follow the intuition of CPD, then we shall assume that B, C, D, and w q are all independent conditioned on H. However, properly relaxing this strong assumption by binding some variables could benefit our model. Though there are many different choices of binding the variables, some bindings can be easily ruled out. For instance, binding B and C inhibits us from caching Term D1-1 and Term D1-2 in Equation 7 and thus we cannot implement the inside algorithm efficiently; binding C and w q leads to a high computational complexity because we will have to compute a high-dimensional (m|?|) categorical distribution. In Section 6.3, we make an ablation study on the impact of different choices of factorizations.</p><p>Neural parameterizations: We follow <ref type="bibr" target="#b27">Kim et al. (2019)</ref> and <ref type="bibr" target="#b54">Zhu et al. (2020)</ref> and define the following neural parameterization: We conduct experiments on the Wall Street Journal (WSJ) corpus of the Penn Treebank <ref type="bibr" target="#b35">(Marcus et al., 1994)</ref>. We use the same preprocessing pipeline as in <ref type="bibr" target="#b27">Kim et al. (2019)</ref>. Specifically, punctuation is removed from all data splits and the top 10,000 frequent words in the training data are used as the vocabulary. For dependency grammar induction, we follow <ref type="bibr" target="#b54">(Zhu et al., 2020)</ref> to use the Stanford typed dependency representation <ref type="bibr" target="#b36">(de Marneffe and Manning, 2008)</ref>.</p><formula xml:id="formula_8">ppA|Sq " exppu J S f 1 pw A qq ? A 1 PN exppu J S f 1 pw A 1 qq , ppw|Aq " exppu J A f 2 pw w qq ? w 1 P? exppu J A f 2 pw w 1 qq , ppB|Hq " exppu J H w B q ? B 1 PN YP exppu J H w B 1 q , ppw|Hq " exppu J H f 2 pw w qq ? w 1 P? exppu J H f 2 pw w 1 qq , ppC ? |Hq " exppu J H w C? q ? C 1 PM exppu J H w C 1 q , ppC ? |Hq " exppu J H w C? q ? C 1 PM exppu J H w C 1 q , ppH|A, wq " exppu J H f 4 prw A ; w w sqq ? H 1 PH exppu J H 1 f 4 prw A ; w w sqq , where H " tH 1 , . . . , H d H u, M " pN Y Pq?t?<label>,</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyperparameters</head><p>We optimize our model using the Adam optimizer with ? 1 " 0.75, ? 2 " 0.999, and learning rate 0.001. All parameters are initialized with Xavier uniform initialization. We set the dimension of all embeddings to 256 and the ratio of the nonterminal number to the preterminal number to 1:2. Our best model uses 15 nonterminals, 30 preterminals, and d H " 300. We use grid search to tune the nonterminal number (from 5 to 30) and domain size d H of the latent H (from 50 to 500).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>We run each model four times with different random seeds and for ten epochs. We train our models on training sentences of length ? 40 with batch size 8 and test them on the whole testing set. For each run, we perform early stopping and select the best model according to the perplexity of the development set. We use two different parsing methods: the variant of CYK algorithm <ref type="bibr" target="#b12">(Eisner and Satta, 1999)</ref> and Minimum Bayes-Risk (MBR) decoding <ref type="bibr" target="#b48">(Smith and Eisner, 2006)</ref>. 2 For constituent grammar induction, we report the means and standard deviations of sentence-level F1 scores. 3 For dependency grammar induction, we report unlabeled directed attachment score (UDAS) and unlabeled undirected attachment score (UUAS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Main result</head><p>We present our main results in <ref type="table" target="#tab_4">Table 2</ref>. Our model is referred to as Neural Bi-Lexicalized PCFGs (NBL-PCFGs). We mainly compare our approach against recent PCFG-based models: neural PCFG (N-PCFG) and compound PCFG (C-PCFG) <ref type="bibr" target="#b27">(Kim et al., 2019)</ref>, tensor decomposition based neural PCFG (TN-PCFG) <ref type="bibr" target="#b50">(Yang et al., 2021)</ref> and neural L-PCFG (NL-PCFG) <ref type="bibr" target="#b54">(Zhu et al., 2020)</ref>. We report both official result of <ref type="bibr" target="#b54">Zhu et al. (2020)</ref> and our reimplementation. We do not use the compound trick <ref type="bibr" target="#b27">(Kim et al., 2019)</ref> in our implementations of lexicalized PCFGs because we empirically find that using it results in unstable training and does not necessarily bring performance improvements.</p><p>We draw three key observations: (1) Our model achieves the best F1 and UUAS scores under both CYK and MBR decoding. It is also comparable to the official NL-PCFG in the UDAS score.</p><p>(2) When we remove the compound parameterization from NL-PCFG, its F1 score drops slightly while its UDAS and UUAS scores drop dramatically. It implies that compound parameterization is the key to achieve excellent dependency grammar induction performance in NL-PCFG. (3) The MBR decoding outperforms CYK decoding.</p><p>Regarding UDAS, our model significantly outperforms NL-PCFGs in UDASs if compound parameterization is not used (37.1 vs. 23.8 with CYK decoding), showing that explicitly modeling bilexical relationship is helpful in dependency grammar induction. However, when compound parameterization is used, the UDAS of NL-PCFGs is greatly improved, slightly surpassing that of our model. We believe this is because compound parameterization greatly weakens the independence assumption of NL-PCFGs (i.e., the child word is dependent on C only) by leaking bilexical information via the global sentence embedding. On the other hand, NBL-PCFGs are already expressive enough and thus compound parameterization brings no further increase of their expressiveness but makes learning more difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>In the following experiments, we report results using MBR decoding by default. We also use d H " 300 by default unless otherwise specified.</p><p>6.1 Influence of the domain size of H d H (the domain size of H) influences the expressiveness of our model. <ref type="figure" target="#fig_2">Figure 2a</ref> illustrates perplexities and F1 scores with the increase of d H and a fixed nonterminal number of 10 (plots of UDAS and UUAS can be found in Appendix). We can see that when d H is small, the model has a high perplexity and a low F1 score, indicating the lim-  ited expressiveness of NBL-PCFGs. When d H is larger than 300, the perplexity becomes plateaued and the F1 score starts to decrease possibly because of overfitting. <ref type="figure" target="#fig_2">Figure 2b</ref> illustrates perplexities and F1 scores with the increase of the nonterminal number and fixed d H " 300 (plots of UDAS and UUAS can be found in Appendix). We observe that increasing the nonterminal number has only a minor influence on NBL-PCFGs. We speculate that it is because the number of word-annotated nonterminals (m|?|) is already sufficiently large even if m is small. On the other hand, the nonterminal number has a big influence on NL-PCFGs. This is most likely because NL-PCFGs make the independence assumption that the generation of w q is solely determined by the non-head-child C and thus require more nonterminals so that C has the capacity of conveying information from A, B, D and w p . Using more nonterminals (? 30) seems to be helpful for NL-  PCFGs, but would be computationally too expensive due to the quadratically increased complexity in the number of nonterminals. <ref type="table" target="#tab_6">Table 3</ref> presents the results of our models with the following bindings:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Influence of nonterminal number</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Influence of different variable bindings</head><p>? D-alone: D is generated alone.</p><p>? D-w q : D is generated with w q .</p><p>? D-B: D is generated with head-child B.</p><p>? D-C: D is generated with non-head-child C.</p><p>Clearly, binding D and C (the default setting for NBL-PCFG) results in the lowest perplexity and the highest F1 score. Binding D and w q has a surprisingly good performance in unsupervised dependency parsing.</p><p>We find that how to bind the head direction has a huge impact on the unsupervised parsing performance and we give the following intuition. Usually given a headword and its type, the children generated in each direction would be different. So, D is intuitively more related to w q and C than to B. On the other hand, B is dependent more on the headword instead. In <ref type="table" target="#tab_6">Table 3</ref> we can see that (D-B) has a lower UDAS score than (D-C) and (D-w q ), which is consistent with this intuition. Notably, in <ref type="bibr" target="#b54">Zhu et al. (2020)</ref>, their Factorization III has a significantly lower UDAS than the default model (35.5 vs. 25.9), and the only difference is whether the generation of C is dependent on the head direction. This is also consistent with our intuition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Qualitative analysis</head><p>We analyze the parsing performance of different PCFG extensions by breaking down their recall numbers by constituent labels (see <ref type="table" target="#tab_8">Table 4</ref>). NPs and VPs cover most of the gold constituents in WSJ test set. TN-PCFGs have the best performance in predicting NPs and NBL-PCFGs have better performance in predicting other labels on average.</p><p>We further analyze the quality of our induced trees. Our model prefers to predict left-headed constituents (i.e., constituents headed by the leftmost word). VPs are usually left-headed in English, so our model has a much higher recall on VPs and correctly predicts their headwords. SBARs often start with which and that and PPs often start with prepositions such as of and for. Our model often relies on these words to predict the correct constituents and hence erroneously predicts these words as the headwords, which hurts the dependency accuracy. For NPs, we find our model often makes mistakes in predicting adjective-noun phrases. For example, the correct parse of a rough market is (a (rough market)), but our model predicts ((a rough) market) instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion on dependency annotation schemes</head><p>What should be regarded as the headwords is still debatable in linguistics, especially for those around function words <ref type="bibr" target="#b55">(Zwicky, 1993)</ref>. For example, in phrase the company, some linguists argue that the should be the headword <ref type="bibr" target="#b0">(Abney, 1972)</ref>. These disagreements are reflected in the dependency annotation schemes. Researchers have found that different dependency annotation schemes result in very different evaluation scores of unsupervised dependency parsing <ref type="bibr" target="#b37">(Noji, 2016;</ref><ref type="bibr" target="#b45">Shen et al., 2020)</ref>.</p><p>In our experiments, we use the Stanford Dependencies annotation scheme in order to compare with NL-PCFGs. Stanford Dependencies prefers to select content words as headwords. However, as we discussed in previous sections, our model prefers to select function words (e.g., of, which, for) as headwords for SBARs or PPs.This explains why our model can outperform all the baselines on constituency parsing but not on dependency parsing (as judged by Stanford Dependencies) at the same time. <ref type="table" target="#tab_6">Table 3</ref> shows that there is a trade-off between the F1 score and UDAS, which suggests that adapting our model to Stanford Dependencies would hurt its ability to identify constituents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Speed comparison</head><p>In practice, the forward and backward pass of the inside algorithm consumes the majority of the running time in training a N(B)L-PCFG. The existing implementation by <ref type="bibr" target="#b54">Zhu et al. (2020)</ref>  <ref type="bibr">4</ref> does not employ efficient parallization and has a cubic time    <ref type="bibr" target="#b50">Yang et al. (2021)</ref> complexity in the number of nonterminals. We provide an efficient reimplementation (we follow <ref type="bibr" target="#b52">Zhang et al. (2020)</ref> to batchify) of the inside algorithm based on Equation 6. We refer to an implementation which caches Term C1-1 as re-impl-1 and refer to an implementation which caches Term C1-2 as re-impl-2. We measure the time based on a single forward and backward pass of the inside algorithm with batch size 1 on a single Titan V GPU. <ref type="figure" target="#fig_3">Figure 3a</ref> illustrates the time with the increase of the sentence length and a fixed nonterminal number of 10. The original implementation of NL-PCFG by <ref type="bibr" target="#b54">Zhu et al. (2020)</ref> takes much more time when sentences are long. For example, when sentence length is 40, it needs 6.80s, while our fast implementation takes 0.43s and our NBL-PCFG takes only 0.30s. <ref type="figure" target="#fig_3">Figure  3b</ref> illustrates the time with the increase of the non-terminal number m and a fixed sentence length of 30. The original implementation runs out of 12GB memory when m " 30. re-impl-2 is faster than re-impl-1 when increasing m as it has a better time complexity in m (quadratic for re-impl-2, cubic for re-impl-1). Our NBL-PCFGs have a linear complexity in m, and as we can see in the figure, our NBL-PCFGs are much faster when m is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Related Work</head><p>Unsupervised parsing has a long history but has regained great attention in recent years. In unsupervised dependency parsing, most methods are based on Dependency Model with Valence (DMV) <ref type="bibr" target="#b28">(Klein and Manning, 2004)</ref>. Neurally parameterized DMVs have obtained state-of-the-art performance <ref type="bibr" target="#b20">(Jiang et al., 2016;</ref><ref type="bibr" target="#b14">Han et al., 2017</ref><ref type="bibr" target="#b15">Han et al., , 2019</ref><ref type="bibr" target="#b49">Yang et al., 2020)</ref>. However, they rely on gold POS tags and sophisticated initializations (e.g. K&amp;M initialization or initialization with the parsing result of another unsupervised model).  propose a left-corner parsing-based DMV model to limit the stack depth of center-embedding, which is insensitive to initialization but needs gold POS tags. <ref type="bibr" target="#b16">He et al. (2018)</ref> propose a latent-variable based DMV model, which does not need gold POS tags but requires good initialization and high-quality induced POS tags. See  for a survey of unsupervised dependency parsing. Compared to these methods, our method does not require gold/induced POS tags or sophisticated initializations, though its performance lags behind some of these previous methods.</p><p>Recent unsupervised constituency parsers can be roughly categorized into the following groups:</p><p>(1) PCFG-based methods. Depth-bounded PCFGs <ref type="bibr">(Jin et al., 2018a,b)</ref> limit the stack depth of centerembedding. Neurally parameterized PCFGs <ref type="bibr" target="#b23">(Jin et al., 2019;</ref><ref type="bibr" target="#b27">Kim et al., 2019;</ref><ref type="bibr" target="#b54">Zhu et al., 2020;</ref><ref type="bibr" target="#b50">Yang et al., 2021)</ref> use neural networks to produce grammar rule probabilities. (2) Deep Inside-Outside Recursive Auto-encoder (DIORA) based methods <ref type="bibr" target="#b8">(Drozdov et al., 2019a</ref><ref type="bibr">(Drozdov et al., ,b, 2020</ref><ref type="bibr" target="#b18">Hong et al., 2020;</ref><ref type="bibr">Sahay et al., 2021)</ref>. They use neural networks to mimic the inside-outside algorithm and they are trained with masked language model objectives.</p><p>(3) Syntactic distance-based methods <ref type="bibr" target="#b42">(Shen et al., 2018</ref><ref type="bibr" target="#b43">(Shen et al., , 2019</ref><ref type="bibr" target="#b45">(Shen et al., , 2020</ref>. They encode hidden syntactic trees into syntactic distances and inject them into language models. (4) Probing based methods . They extract phrase-structure trees based on the attention distributions of large pre-trained language models. In addition to these methods, <ref type="bibr" target="#b1">Cao et al. (2020)</ref> use constituency tests and <ref type="bibr" target="#b47">Shi et al. (2021)</ref> make use of naturally-occurring bracketings such as hyperlinks on webpages to train parsers. Multimodal information such as images <ref type="bibr" target="#b46">(Shi et al., 2019;</ref><ref type="bibr" target="#b53">Zhao and Titov, 2020;</ref><ref type="bibr" target="#b24">Jin and Schuler, 2020)</ref> and videos <ref type="bibr" target="#b51">(Zhang et al., 2021)</ref> have also been exploited for unsupervised constituency parsing.</p><p>We are only aware of a few previous studies in unsupervised joint dependency and constituency parsing. <ref type="bibr" target="#b28">Klein and Manning (2004)</ref> propose a joint DMV and CCM <ref type="bibr" target="#b29">(Klein and Manning, 2002)</ref> model. <ref type="bibr" target="#b45">Shen et al. (2020)</ref> propose a transformer-based method, in which they define syntactic distances to guild attentions of transformers. <ref type="bibr" target="#b54">Zhu et al. (2020)</ref> propose neural L-PCFGs for unsupervised joint parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>We have presented a new formalism of lexicalized PCFGs. Our formalism relies on the canonical polyadic decomposition to factorize the probability tensor of binary rules. The factorization reduces the space and time complexity of lexicalized PCFGs while keeping the independence assumptions encoded in the original binary rules intact. We further parameterize our model by using neural networks and present an efficient implementation of our model. On the English WSJ test data, our model achieves the lowest perplexity, outperforms all the existing extensions of PCFGs in constituency grammar induction, and is comparable to strong baselines in dependency grammar induction.  <ref type="figure">Figure 4</ref> illustrates the change of UUAS and UDAS with the increase of d H . We find similar tendencies compared to the change of F1 scores and perplexities with the increase of d H . d H " 300 performs best. <ref type="figure">Figure 5</ref> illustrates the change of UUAS and UDAS when increasing the number of nonterminals. We can see that NL-PCFGs benefit from using more nonterminals while NBL-PCFGs have a better performance when the number of nonterminals is relatively small.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) The original parameterization of L-PCFGs. (b) The parameterization of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1</head><label>1</label><figDesc>(a) and (b). With the factorized binary rule probability in Equation 2, Term A1 in Equation 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The change of F1 scores, perplexities with the change of |H| and nonterminal number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Total time in performing the inside algorithm and automatic differentiation with different sentence lengths and nonterminal numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Influence of d H on UUAS and UDAS. Influence of the number of nonterminals on UUAS and UDAS.h i pxq " g i,1 pg i,2 pW i xqq g i,j pyq " ReLU pV i,j ReLU pU i,j yqq`y f prx, ysq " h 4 pReLUpWrx; ysq`yq B Influence of the domain size of H and the number of nonterminals</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Recursive formulas of the inside algorithm for Eisner and Satta (1999) (Equation 4), Zhu et al. (2020) (Equation 5-6), and our formalism (Equation 7-8), respectively. s A,p i,j indicates the probability of a head nonterminal Arw p s spanning the substring w i , . . . , w j?1 , where p is the position of the headword in the sentence.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Opl 4 d H`l 2 md H q (d H is the domain size of the latent variable H), which is linear in m.</figDesc><table><row><cell>)</cell></row><row><cell>Computational complexity: Considering the</cell></row><row><cell>head-child as the left child (similarly for the other</cell></row><row><cell>case), we apply Equation 10 in Term A1 of Equa-</cell></row><row><cell>tion 3 and obtain Equation 7. Rearranging the</cell></row></table><note>summations in Equation 7 gives Equation 8, where Term D1-1 and D1-2 can be cached and reused, which also uses the unfold-refold technique. The fi- nal time complexity of the inside computation with our factorization approach is</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>?u, u and w are nonterminal embeddings and word embeddings respectively, and f 1 p?q, f 2 p?q, f 3 p?q, f 4 p?q are neural networks with residual layers<ref type="bibr" target="#b17">(He et al., 2016)</ref> (Full parameterization is shown in Appendix.).</figDesc><table><row><cell>4 Experimental setup</cell></row><row><cell>4.1 Dataset</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>3?2 .1 23.8?1 .1 47.4?1 .0 NL-PCFG : 57.4?1 .4 25.3?1 .3 47.2?0 .7 NBL-PCFG ? 58.2?1 .5 37.1?2 .8 54.6?1 .3 NBL-PCFG : 60.4?1 .6 39.1?2 .8 56.1?1 .3</figDesc><table><row><cell>Model</cell><cell></cell><cell>WSJ</cell><cell></cell></row><row><cell></cell><cell>F1</cell><cell>UDAS</cell><cell>UUAS</cell></row><row><cell></cell><cell cols="2">Official results</cell><cell></cell></row><row><cell>N-PCFG ?</cell><cell>50.8</cell><cell></cell><cell></cell></row><row><cell>C-PCFG ?</cell><cell>55.2</cell><cell></cell><cell></cell></row><row><cell>NL-PCFG ?</cell><cell>55.3</cell><cell>39.7</cell><cell>53.3</cell></row><row><cell>TN-PCFG :</cell><cell>57.7</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Our results</cell><cell></cell></row><row><cell>NL-PCFG ?</cell><cell cols="2">53.For reference</cell><cell></cell></row><row><cell>S-DIORA</cell><cell>57.6</cell><cell></cell><cell></cell></row><row><cell cols="2">StructFormer 54.0</cell><cell>46.2</cell><cell>61.6</cell></row><row><cell cols="2">Oracle Trees 84.3</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Unlabeled sentence-level F1 scores, unlabeled directed attachment scores and unlabeled undirected attachment scores on the WSJ test data.</figDesc><table><row><cell>(2020)</cell></row></table><note>: indicates using MBR decoding.? indicates using CYK decod- ing. Recall that the official result of Zhu et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Binding the head direction D with different variables.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Recall on six frequent constituent labels and perplexities of the WSJ test data.</figDesc><table /><note>: means that the re- sults are reported by</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In MBR decoding, we use automatic differentiation<ref type="bibr" target="#b10">(Eisner, 2016;</ref><ref type="bibr" target="#b40">Rush, 2020)</ref> to estimate the marginals of spans and arcs, and then use the CYK and Eisner algorithms for constituency and dependency parsing, respectively.3 Following<ref type="bibr" target="#b27">Kim et al. (2019)</ref>, we remove all trivial spans (single-word spans and sentence-level spans). Sentence-level means that we compute F1 for each sentence and then average over all sentences.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/neulab/neural-lpcfg</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their constructive comments. This work was supported by the National Natural Science Foundation of China (61976139).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Full Parameterization</head><p>We give the full parameterizations of the following probability distributions.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The english noun phrase in its sentential aspect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised parsing via constituency tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.389</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4798" to="4808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Tree-bank grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scaling hidden Markov language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.103</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1341" to="1349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Approximate PCFG parsing using tensor decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Satta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="487" to="496" />
		</imprint>
	</monogr>
	<note>Georgia</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Three generative, lexicalised models for statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.3115/976909.979620</idno>
	</analytic>
	<monogr>
		<title level="m">35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Head-driven statistical models for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.1162/089120103322753356</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="637" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised parsing with S-DIORA: Single tree encoding for deep inside-outside recursive autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Drozdov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subendhu</forename><surname>Rongali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Pei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">O</forename><surname>&amp;apos;gorman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.392</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4832" to="4845" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised latent tree induction with deep inside-outside recursive auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Drozdov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1116</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1129" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised latent tree induction with deep inside-outside recursive auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Drozdov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1116</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1129" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inside-outside and forwardbackward algorithms are just backprop (tutorial paper)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-5901</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Structured Prediction for NLP</title>
		<meeting>the Workshop on Structured Prediction for NLP<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Program transformations for optimization of parsing algorithms and other weighted logic programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of FG 2006: The 11th Conference on Formal Grammar</title>
		<meeting>FG 2006: The 11th Conference on Formal Grammar</meeting>
		<imprint>
			<publisher>CSLI Publications</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="45" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient parsing for bilexical context-free grammars and head automaton grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
		<idno type="DOI">10.3115/1034678.1034748</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 37th Annual Meeting of the Association for Computational Linguistics<address><addrLine>College Park, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey of unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2522" to="2533" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dependency grammar induction with neural lexicalization and big training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1176</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1683" to="1688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enhancing unsupervised generative dependency parser with contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1526</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5315" to="5325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised learning of syntactic structure with invertible neural projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1160</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1292" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep inside-outside recursive autoencoder with all-span objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruyue</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3610" to="3615" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Basic methods of probabilistic context free grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech Recognition and Understanding</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1073</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="763" to="771" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Depthbounding is effective: Improvements and evaluation of unsupervised PCFG induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1292</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2721" to="2731" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised grammar induction with depth-bounded PCFG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00016</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="211" to="224" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised learning of PCFGs with normalizing flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1234</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2442" to="2452" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grounded PCFG induction with images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</title>
		<meeting>the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing<address><addrLine>Suzhou, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="396" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PCFG models of linguistic tree representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="613" to="632" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Are pre-trained language models aware of phrases? simple but strong baselines for grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Edmiston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanggoo</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="OpenRe-view.net" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020, Addis Ababa</title>
		<meeting><address><addrLine>Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Compound probabilistic context-free grammars for grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1228</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2369" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Corpusbased induction of syntactic structure: Models of dependency and constituency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/1218955.1219016</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A generative constituent-context model for improved grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073106</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/1075096.1075150</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
		<idno type="DOI">10.1137/07070111X</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The estimation of stochastic context-free grammars using the insideoutside algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Lari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer speech &amp; language</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="56" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Heads-up! unsupervised constituency parsing via self-attention heads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinald</forename><surname>Kim Amplayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</title>
		<meeting>the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing<address><addrLine>Suzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="409" to="424" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Statistical decision-tree models for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Magerman</surname></persName>
		</author>
		<idno type="DOI">10.3115/981658.981695</idno>
	</analytic>
	<monogr>
		<title level="m">33rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="276" to="283" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The Penn Treebank: Annotating predicate argument structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Macintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Britta</forename><surname>Schasberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology: Proceedings of a Workshop</title>
		<meeting><address><addrLine>Plainsboro, New Jersey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-03-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Stanford typed dependencies representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation</title>
		<meeting><address><addrLine>Manchester, UK. Col</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Left-corner methods for syntactic modeling with universal structural constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
		</author>
		<idno>abs/1608.00293</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Using left-corner parsing to encode universal structural constraints in grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Torch-struct: Deep structured prediction library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-demos.38</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="335" to="342" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atul</forename><surname>Sahay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Nasery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Rishabh Iyer. 2021. Rule augmented unsupervised constituency parsing</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural language modeling by jointly learning syntax and lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. Open-Review.net</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Ordered memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyedarian</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1910.13466</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Structformer: Joint unsupervised induction of dependency and constituency structure from masked language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visually grounded neural syntax acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1180</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1842" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning syntax from naturally-occurring bracketings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianze</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Ozanirsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Malioutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2941" to="2949" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Minimum risk annealing for training log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions</title>
		<meeting>the COLING/ACL 2006 Main Conference Poster Sessions<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="787" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Second-order unsupervised neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3911" to="3924" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">PCFGs can do better: Inducing probabilistic contextfree grammars with many symbols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1487" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Video-aided unsupervised grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1513" to="1524" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fast and accurate neural CRF constituency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/560</idno>
		<ptr target="ijcai.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4046" to="4053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Visually grounded compound PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.354</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4369" to="4379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The return of lexical dependencies: Neural lexicalized PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00337</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="647" to="661" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Heads in grammatical theory: Heads, bases and functors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zwicky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
