<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FLEX: Extrinsic Parameters-free Multi-view 3D Human Motion Reconstruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Gordon</surname></persName>
							<email>briangordon@mail.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sigal</forename><surname>Raab</surname></persName>
							<email>sigalraab@tauex.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Azov</surname></persName>
							<email>guyazov@mail.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FLEX: Extrinsic Parameters-free Multi-view 3D Human Motion Reconstruction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Motion reconstruction</term>
					<term>Character animation</term>
					<term>Pose estima- tion</term>
					<term>Camera parameters</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The increasing availability of video recordings made by multiple cameras has offered new means for mitigating occlusion and depth ambiguities in pose and motion reconstruction methods. Yet, multi-view algorithms strongly depend on camera parameters, particularly on relative transformations between the cameras. Such a dependency becomes a hurdle once shifting to dynamic capture in uncontrolled settings. We introduce FLEX (Free muLti-view rEconstruXion), an end-to-end extrinsic parameter-free multi-view model. FLEX is extrinsic parameter-free (dubbed ep-free) in the sense that it does not require extrinsic camera parameters. Our key idea is that the 3D angles between skeletal parts, as well as bone lengths, are invariant to the camera position. Hence, learning 3D rotations and bone lengths rather than locations allows for predicting common values for all camera views. Our network takes multiple video streams, learns fused deep features through a novel multi-view fusion layer, and reconstructs a single consistent skeleton with temporally coherent joint rotations. We demonstrate quantitative and qualitative results on three public data sets, and on multi-person synthetic video streams captured by dynamic cameras. We compare our model to state-of-the-art methods that are not ep-free and show that in the absence of camera parameters, we outperform them by a large margin while obtaining comparable results when camera parameters are available. Code, trained models, and other materials are available on https://briang13.github.io/FLEX.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human motion reconstruction is the task of associating a skeleton with temporally coherent joint locations and rotations. Acquiring accurate human motion in a controlled setting, using motion capture systems with adequate sensors is a tedious and expensive procedure that cannot be applied for capturing spontaneous activities, such as sporting events. Motion reconstruction from RGB cameras is low-cost and non-intrusive, but is an uncontrolled setup. Thus, while being simple, it has technical challenges that are worsened by occlusion and depth ambiguity. Using multiple cameras may alleviate these difficulties as different views may compensate for occlusion and be used for mutual consistency.   <ref type="figure">Fig. 1</ref>: Results on the KTH Multi-view Football II dataset <ref type="bibr" target="#b37">[34]</ref>, in occluded and blurry scenes with dynamic cameras. Recently, there has been a significant progress in using deep learning for pose and motion reconstruction <ref type="bibr" target="#b59">[55,</ref><ref type="bibr" target="#b47">43,</ref><ref type="bibr" target="#b68">64,</ref><ref type="bibr" target="#b71">67,</ref><ref type="bibr" target="#b61">57,</ref><ref type="bibr" target="#b41">37,</ref><ref type="bibr" target="#b56">52]</ref>. Most of these methods work in a monocular setting, but a growing number of works learn a multi-view setting <ref type="bibr" target="#b33">[30,</ref><ref type="bibr" target="#b79">75,</ref><ref type="bibr" target="#b63">59,</ref><ref type="bibr" target="#b28">25,</ref><ref type="bibr">20,</ref><ref type="bibr" target="#b65">61]</ref>. However, these approaches depend on the relative position between the cameras, derived from extrinsic camera parameters, and assume they are given. In the lack of extrinsic parameters, several works estimate them <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b42">38]</ref>, but at the cost of innate inaccuracy of estimated values. While camera parameters are often given in multi-view datasets, they are rarely given in dynamic capture environments. We refer to cameras as dynamic if they occasionally move during video capture, such that their extrinsic parameters and their inter-camera relative positions are not fixed. An example of such a camera is the SkyCam [69], commonly used in sports events.</p><p>This work introduces an extrinsic parameter-free (dubbed ep-free) multi-view motion reconstruction method, whose setting is illustrated in the inset to the right. Our method builds upon a new conceptual observation that uses the well-known joint rotations and bone lengths, to free us from the burdening dependency on extrinsic camera parameters. Our approach relies on a key insight that joint rotations and bone lengths are identical for all views. That is, the 3D angle between skeletal parts is invariant to the camera position. We train a neural network to predict 3D joint angles and bone lengths without using the extrinsic camera parameters, neither in training nor in test time. Predicting motion rather than locations is not a novel idea by itself. The innovation of our work is in the way we use motion to bypass the need for camera parameters. The input from multiple cameras is integrated by a novel fusion layer that implicitly promotes joints detected by some cameras and demotes joints detected by others, hence mitigating occlusion and depth ambiguities.</p><p>Our model, named FLEX, is an end-to-end deep convolutional network. Its input is multi-view 2D joints that are either given or extracted using a 2D pose estimation technique. FLEX employs multi-view blocks with cross-view attention on top of a monocular baseline <ref type="bibr" target="#b71">[67]</ref>, and uses temporal information over a video of arbitrary length, thus obtaining temporal consistency.</p><p>We evaluate FLEX qualitatively and quantitatively using the Human3.6M <ref type="bibr" target="#b32">[29,</ref><ref type="bibr" target="#b10">9]</ref>, the KTH Multi-view Football II <ref type="bibr" target="#b37">[34]</ref> and the Ski-Pose PTZ-Camera <ref type="bibr" target="#b66">[62]</ref> datasets. <ref type="figure">Figure 1</ref> demonstrates qualitative results, and more are depicted in Section 4 and in the appendix. FLEX is also applied on synthetic videos. We have generated these videos using Mixamo <ref type="bibr" target="#b0">[1]</ref> and Blender <ref type="bibr" target="#b20">[18]</ref>, to mitigate the lack of a multi-person video dataset that is captured by dynamic cameras, and created them such that they contain severe inter-person occlusions.</p><p>We compare performance with state-of-the-art methods that are not ep-free and show comparable results. To simulate an ep-free setting, we perturb groundtruth camera parameters or use works that estimate them. We show that in an ep-free setting, our model outperforms state-of-the-art by a large margin.</p><p>Our main contributions are twofold: (i) a network that reconstructs motion and pose in a multi-view setting with unknown extrinsic camera parameters, and (ii) a novel fusion layer with a multi-view convolutional layer combined with a multi-head attention mechanism over a number of views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Pose Estimation using a Single View Pose estimation receives significant interest in computer vision. Before the deep era, it was approached using heuristics such as physical priors <ref type="bibr" target="#b67">[63]</ref>. The emergence of deep learning and large datasets <ref type="bibr" target="#b32">[29,</ref><ref type="bibr" target="#b19">17,</ref><ref type="bibr" target="#b54">50,</ref><ref type="bibr" target="#b37">34]</ref>, have led to significant advances. Pose estimation methods can generally be divided into two groups. The first infers 3D locations directly from images or videos <ref type="bibr" target="#b44">[40,</ref><ref type="bibr" target="#b60">56,</ref><ref type="bibr" target="#b92">87,</ref><ref type="bibr" target="#b77">73,</ref><ref type="bibr" target="#b74">71,</ref><ref type="bibr" target="#b15">13,</ref><ref type="bibr" target="#b27">24]</ref>. The second, aka lifting, applies two steps: (i) estimating 2D poses and (ii) lifting them to 3D space <ref type="bibr" target="#b55">[51,</ref><ref type="bibr" target="#b61">57,</ref><ref type="bibr" target="#b25">22,</ref><ref type="bibr" target="#b73">70,</ref><ref type="bibr" target="#b47">43,</ref><ref type="bibr" target="#b29">26,</ref><ref type="bibr" target="#b70">66]</ref>. The first group benefits from directly using images, which are more descriptive compared to 2D joint locations. The second gains from using intermediate supervision. Recently, transformers and convolutional graph based methods were shown to improve performance <ref type="bibr" target="#b49">[45,</ref><ref type="bibr" target="#b46">42,</ref><ref type="bibr" target="#b48">44,</ref><ref type="bibr" target="#b45">41,</ref><ref type="bibr" target="#b87">82,</ref><ref type="bibr" target="#b30">27,</ref><ref type="bibr" target="#b58">54]</ref>.</p><p>Pose Estimation using Multiple Views The growing availability of synchronized video streams taken by multiple cameras has contributed to the emergence of multi-view algorithms. Such algorithms exploit the diversity in camera views to predict more accurate 3D poses. All works described below predict pose and many of them analyze each frame individually. On the other hand, our model, FLEX, reconstructs motion and exploits temporal information.</p><p>Most works in the multi-view setting rely on lifting from 2D to 3D space. Early works <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b7">7]</ref> estimate the input 2D pose from single images, while later works [20, <ref type="bibr" target="#b63">59,</ref><ref type="bibr" target="#b33">30,</ref><ref type="bibr" target="#b28">25,</ref><ref type="bibr" target="#b42">38,</ref><ref type="bibr" target="#b11">10,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b65">61,</ref><ref type="bibr" target="#b34">31,</ref><ref type="bibr" target="#b17">15]</ref> obtain the 2D pose by running a CNN over 2D poses given in multiple views; resulting in an increase in 2D pose prediction accuracy. After estimating the 2D poses, most works apply heuristics such as triangulation or pictorial structure model (PSM). FLEX is one of the few works <ref type="bibr" target="#b33">[30,</ref><ref type="bibr" target="#b79">75]</ref> that present an end-to-end model.</p><p>Several methods use multi-view data to improve the 2D pose estimation. Some use the camera parameters to find the matching epipolar lines such that features gathered from several cameras are aggregated <ref type="bibr" target="#b63">[59,</ref><ref type="bibr" target="#b28">25]</ref>. Chen et al . <ref type="bibr" target="#b11">[10]</ref> learn a geometric representation in latent space with an encoder-decoder.</p><p>Several works <ref type="bibr" target="#b42">[38,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b85">80,</ref><ref type="bibr" target="#b78">74]</ref> use self-supervision, hence need no 3D groundtruth. Their main idea is to project the predicted 3D joints (using real or esti-mated camera parameters) and expect consistency with 2D input joints. Recent techniques <ref type="bibr" target="#b31">[28,</ref><ref type="bibr" target="#b90">85]</ref> exploit more sensors, such as IMU, during data capturing.</p><p>Current state-of-the-art results are attained by Iskakov et al . <ref type="bibr" target="#b33">[30]</ref>, Tu et al . <ref type="bibr" target="#b79">[75]</ref> and Reddy et al . <ref type="bibr" target="#b64">[60]</ref>. They use end-to-end networks, and present a volumetric approach, where 2D features are un-projected from individual views to a common 3D space, using camera parameters. Sun et al . <ref type="bibr" target="#b73">[70]</ref> show that synthetic generation of additional views helps produce more accurate lifting.</p><p>At inference time, some of the aforementioned works expect monocular inputs <ref type="bibr" target="#b73">[70,</ref><ref type="bibr" target="#b28">25,</ref><ref type="bibr" target="#b11">10,</ref><ref type="bibr" target="#b18">16]</ref> and some, including FLEX, get multi-view inputs <ref type="bibr" target="#b33">[30,</ref><ref type="bibr" target="#b79">75,</ref><ref type="bibr" target="#b63">59]</ref>. The advantage of the first is the use of monocular data that is more common, and of the second is better results on multi-view settings.</p><p>Epipolar Transformers <ref type="bibr" target="#b28">[25]</ref> attend to spatial locations on an epipolar line in a single view and query it using one joint in a query view. A concurrent work, TransFusion <ref type="bibr" target="#b52">[48]</ref>, applies a transformer on inter and intra-view features.</p><p>In the absence of camera parameters, most of the methods cannot be used. Some estimate rotation assuming the translation is given <ref type="bibr" target="#b42">[38,</ref><ref type="bibr" target="#b1">2]</ref> or engage an extra effort to estimate the camera parameters <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b85">80,</ref><ref type="bibr" target="#b13">11,</ref><ref type="bibr" target="#b80">76,</ref><ref type="bibr" target="#b75">72]</ref>. Such an effort is not required by FLEX as it uses no camera parameters whatsoever.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rotation and Motion Reconstruction</head><p>Pose estimation may suffice for many applications; however, pose alone does not fully describe the motion and the rotations associated with the joints. Rotation reconstruction relates to the prediction of joint rotation angles, while motion reconstruction requires the prediction of bone lengths associated with them. Many works explore the task of 3D shape recovery <ref type="bibr" target="#b50">[46,</ref><ref type="bibr" target="#b35">32,</ref><ref type="bibr" target="#b43">39,</ref><ref type="bibr" target="#b41">37,</ref><ref type="bibr" target="#b40">36,</ref><ref type="bibr" target="#b89">84,</ref><ref type="bibr" target="#b36">33,</ref><ref type="bibr" target="#b26">23,</ref><ref type="bibr" target="#b51">47,</ref><ref type="bibr" target="#b16">14]</ref>, focusing on human mesh prediction along with joint rotations. Most of them do not guarantee temporal coherence, e.g., bone length may vary across time frames.</p><p>Other works <ref type="bibr" target="#b62">[58,</ref><ref type="bibr" target="#b53">49]</ref> focus on motion generation. Given a series of human motions, they predict future motions, using various techniques such as temporal supervision and graph convolutional networks (GCN). Similar to us, human motion reconstruction methods <ref type="bibr" target="#b91">[86,</ref><ref type="bibr" target="#b56">52,</ref><ref type="bibr" target="#b71">67,</ref><ref type="bibr" target="#b57">53]</ref> focus on the temporal coherence of the body, where the bone lengths are fixed over time and rotations are smooth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Extrinsic Parameter-free multi-view model</head><p>The premise of our work is that 3D joint rotations and bone lengths are view-independent values. For example, the 3D angle between, say, the thigh and the shin, as well as the length of these bones, are fixed, no matter which camera transformation is used. On the other hand, joint locations differ for each camera transformation, as seen in <ref type="figure" target="#fig_1">Figure 2</ref>. Our key idea is to directly predict joint 3D angles and bone lengths without using the extrinsic camera parameters, during both training and test time. Extrinsic parameters correspond to the rotation and translation (aka transformation) from 3D real world axes into 3D camera axes. A formal definition of the camera parameters can be found in Appendix F.</p><p>Our method takes multi-view sequences of 2D poses and estimates the motion of the observed human. The 2D poses are either given or extracted using a prediction technique. Having multi-view data compensates for the inherent inaccuracy of 2D pose estimation algorithms. Many methods estimate view-dependent 2D joint positions and then lift them to 3D by transforming them into a shared space. Such transformations require acquaintance of the relative position (rotation and translation) between the cameras, which is derived from the extrinsic camera parameters. Our model directly predicts 3D rotations and bone lengths, which are agnostic to camera transformation. The predicted values are shared by all views, so there is no need for extrinsic parameters information.</p><p>Pose estimation methods may mitigate the lack of extrinsic parameters by estimating them <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b42">38]</ref>. Yet, this has two drawbacks: (i) most approaches perform the estimation in a prepossessing step that breaks the end-to-end computation, and (ii) the estimated parameters are never exact and typically lead to a performance drop, as we show in Section 4.</p><p>Our architecture leverages Shi et al . <ref type="bibr" target="#b71">[67]</ref> and is illustrated in high-level terms in <ref type="figure" target="#fig_2">Figure 3</ref>. FLEX is an end-to-end network that maps 2D joint positions, extracted from multiple synchronized input videos, into two separate components: (i) a sequence of 3D joint rotations, global root positions and foot contact labels (upper branch in the figure); this sequence is skeleton-independent and varies per frame; and (ii) a single, symmetric, 3D skeleton, represented by its bone lengths (lower branch in the figure). We can combine these two components into a complete description of a motion and use it for 3D animation tasks without further processing or inverse kinematics (IK).</p><p>In addition to being free of extrinsic parameters, our model does not use intrinsic parameters at all, at the cost of an up-to-scale global skeleton position. While FLEX removes the need for extrinsics, it uses the common weak perspective assumption <ref type="bibr" target="#b40">[36]</ref> for intrinsics; in particular for mitigating the lack of focal length. Indeed, some works seek to mitigate the lack of intrinsic parameters <ref type="bibr" target="#b72">[68,</ref><ref type="bibr" target="#b26">23,</ref><ref type="bibr" target="#b40">36]</ref> whereas this is not the focus of our work. In Section 4 we show that using a customary weak perspective we attain an accurate global position.</p><p>The terms motion, pose, reconstruction and estimation are used in various contexts in the literature. To avoid confusion, we define motion as one set of bone lengths associated with temporally coherent 3D joint rotations, and pose as a temporal sequence of 3D joint locations. We use the term reconstruction rather than estimation, as the latter often describes 2D spatial motion. A weakly related term, pose tracking, associates poses to identities in a multi-person setting.</p><p>Motion data, and in particular rotations rather than positions, are required in animation platforms and game engines. FLEX directly outputs a kinematic skeleton, which is a complete, commonly used, motion representation. On the other hand, methods that predict joint positions, rely on IK to associate a skeleton with joint rotations. IK is slow, non-unique, and prone to temporal inconsistencies and unnatural postures. Moreover, methods that only predict pose cannot guarantee the consistency of bone lengths across frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>We start with a high-level description of the architecture (see <ref type="figure" target="#fig_2">Figure 3</ref>). The inputs are K synchronized video streams of T frames each. For each video <ref type="figure" target="#fig_2">Fig. 3</ref>: FLEX takes multi-view temporal sequences of 2D poses and their confidence values. It uses two encoders, E Q and E S , to extracts per-frame 3D rotations and foot contact labels, per-view and per-frame 3D root transformations, and one static skeleton. A discriminator D monitors the temporal differences of rotation angles, and a forward kinematic layer, F K, combines encoders' outputs into 3D joint locations. These outputs depict one human, transformed into the axis systems of K cameras, to be compared with K sets of ground-truth values. stream, we obtain 2D joints, which are either the ground-truth of a dataset or the output of a 2D pose estimation algorithm. Our network is agnostic to the way those 2D joints were obtained. In addition, each estimated joint is associated with a confidence value. The confidence value plays an important role in balancing between visible and occluded joints.</p><p>Our model takes input from all views, aggregates it, and streams it into two independent fusion layers F S and F Q , followed by encoders E S and E Q , respectively. The two fusion layers differ in some architectural details, but share the same concept. Both aggregate data of all views and frames and fuse it to exploit characteristics that recur in views and/or frames. Each fusion layer outputs view-agnostic features that represent the target human.</p><p>The fusion layers consist of two innovative elements, a multi-view convolutional layer and a cross-view attention mechanism, which encodes information from all views. Our use of attention is unique, as typically attention in other works is applied mostly over pixels <ref type="bibr" target="#b38">[35]</ref> and sometimes over time <ref type="bibr" target="#b49">[45,</ref><ref type="bibr" target="#b46">42,</ref><ref type="bibr" target="#b48">44]</ref>. Attention over views is a novel approach, which we find only in concurrent works for other tasks, assessing human shape <ref type="bibr" target="#b93">[88]</ref> and rigid objects <ref type="bibr" target="#b86">[81]</ref>. The fusion layers are described in detail in the supplementary material.</p><p>The encoder E S predicts the length of each bone. As the same human is analyzed along all frames and views, the output is a single set of bone lengths.</p><p>The encoder E Q predicts joint rotations, global root positions, and foot contact labels. Since 3D joint rotations and foot contact labels are identical to all views, E Q predicts a single set of rotations and contact labels per frame, shared by all views. One exception is the root (pelvis) joint, whose rotation angle and position depend on the camera view and not on the human itself. Thus, for the root joint we predict the rotation angle and position for each frame and view. Root rotation and position are relative to the camera; hence visualizing the reconstructed object depicts the filmed person from the camera view, as expected.</p><p>Notice that root rotation and position carry the knowledge of the relative transformation between the cameras. This insight suggests that our algorithm has the potential to output additional valuable information, e.g., cameras relative location to each other (left to future work).</p><p>At train time, the output of both encoders is combined in K identical forward kinematic (F K) layers. Each F K layer computes the estimated 3D joint positions related to one view, which in turn are compared to the ground-truth for loss computation. In addition, temporal differences of the rotations extracted out of E Q , are fed to a discriminator D <ref type="bibr" target="#b35">[32]</ref>, so they get near the manifold of true rotations in an adversarial way.</p><p>Formally, let L denote the number of bones, T the temporal length of the sequence, J the number of joints, Q the size of the rotations representation vector, and K the number of cameras. Let P s,q,r ? R T ?3J?K denote K temporal sequences of 3D joint positions generated by a skeleton s ? R L with joint rotations q ? R T ?Q?(J?1) , and global root position and rotation r ? R T ?(3+Q)?K . Note that q is related to all joints except for the root joint. The rotation of the root joint, as well as its position, are related to r.</p><p>Our approach expects an input V s,q,r ? R T ?3J?K denoting K temporal sequences of 2D joints and a confidence value per joint, related to a skeleton s, joint rotations q, and global root position and rotation r. Each input V is fed into our deep neural network, which in turn predictsq ? R T ?Q?(J?1) , that captures the dynamic, rotational information of the motion,s ? R L , that describes a single, consistent, skeleton,r ? R T ?(3+Q)?K that estimates the global position and rotation of the root along time and along views, andf ? {0, 1} T ?2 that predicts whether each of the two feet touches the ground in each frame:</p><formula xml:id="formula_0">s = E S (F S (V s,q,r )),q,r,f = E Q (F Q (V s,q,r )).<label>(1)</label></formula><p>These attributes can be then combined via forward kinematics to estimate K global 3D pose sequences,Ps ,q,r ? R T ?3J?K , specified by joint positions:</p><p>Ps ,q,r = F K(s,q,r).</p><p>We employ five loss functions. Our losses are inspired by Shi et al . <ref type="bibr" target="#b71">[67]</ref> and are enhanced to encompass the multitude of views.</p><p>Joint Position Loss (the main loss) L P ensures that joints in the extracted positions are in their correct 3D positions:</p><formula xml:id="formula_2">L P = E Ps,q,r?P ?F K(s,q,r pos0 ) ? P s,q,rpos 0 ? 2 ,<label>(3)</label></formula><p>where P s,q,r ? R T ?3J?K denotes a 3D motion sequence, P represents the distribution of 3D motion sequences in our dataset, andr pos0 , r pos0 stand for global position and rotation of the predicted and given root respectively, where the location is set to (0, 0, 0), but the rotation is unchanged.</p><p>Skeleton Loss L S stimulates the skeleton branch of the network, F S and E S , to correctly extract the skeleton s:</p><formula xml:id="formula_3">L S = E Ps,q,r?P ?E S (F S (V s,q,r )) ? s? 2 .<label>(4)</label></formula><p>Adversarial Rotation Loss Our network learns to output rotations with natural velocity distribution using adversarial training. To achieve this, instead of focusing on rotation absolute values, like Kanazawa et al . <ref type="bibr" target="#b35">[32]</ref> we focus on the temporal differences of joint rotations. We create a discriminator D j for each joint. Note that the loss involving D j? =0 takes the rotation values fromq while the loss involving D 0 takes the rotation values fromr. It reads as</p><formula xml:id="formula_4">L Q GAN j? =0 = E q?Q ?D j (? t q j )? 2 + E Ps,q,r?P ?1 ? D j (? t E Q (F Q ((V s,q,r )) qj ? 2 L Q GAN j=0,k = E q?Q ?D j (? t q j )? 2 (5) + E Ps,q,r?P ?1 ? D j (? t E Q (F Q ((V s,q,r )) rrot k ? 2 ,</formula><p>where Q stands for the distribution of natural joint angles in the dataset, E Q (F Q (?)) qj denotes the predicted rotations of the jth joint, E Q (F Q (?)) r rot k represents the predicted rotation of the pelvis joint relative to camera k, and ? t denotes temporal differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Root Position Loss</head><p>We estimate the depth parameter, Z r ? R T ?K , by minimizing:</p><formula xml:id="formula_5">L R = E Ps,q,r?P ?E Q (F Q (V s,q,r )) rpos z ? Z r ? 2 ,<label>(6)</label></formula><p>where Z r is the depth of the ground-truth root, and E Q (F Q (?)) rpos z is the depth of the predicted root. Note that Z r consists of values for all views and all frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Foot Contact Loss</head><p>We predict whether each foot contacts the ground in each frame and train the network via</p><formula xml:id="formula_6">L F = E Ps,q,r?P ?E Q (F Q (V s,q,r )) f ? f ? 2 ,<label>(7)</label></formula><p>where E Q (F Q ((?)) f denotes the predicted foot contact label part (f ? {0, 1} T ?2 ). We encourage the velocity of foot positions to be zero during contact frames, by</p><formula xml:id="formula_7">L FC = E Ps,q,r?P ? ? ?f i j ? t F K(s,q,r) fi ? 2 ? ? ,<label>(8)</label></formula><p>where F K(?, ?, ?) fi ? R T ?3 and f i denote the positions and the contact labels of one of the feet joints (i ? left, right), and j sums the components for all axes. Altogether, we obtain a total loss of:</p><formula xml:id="formula_8">L = L P + ? S L S ? Q ? ? j? =0 L Q GAN j + j=0,k L Q GAN j,k ? ? (9) + ? R L R + ? F L P F + ? FC L P F C .</formula><p>In most experiments we use ? S = 0.1, ? Q = 1, ? R = 1.3, ? F = 0.5 and ? FC = 0.5.</p><p>In the appendix we provide more implementation details, such as the description of each architectural block; in particular the novel fusion layers F S and F Q . We discuss the advantages of early vs. middle and late fusion, and describe how we improve skeleton topology comparing to our single-view baseline. We also provide a detailed description of the datasets, a discussion of 2D pose estimators, and a description of the ground-truth we use. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and evaluation</head><p>We present quantitative results on the Human3.6M <ref type="bibr" target="#b32">[29,</ref><ref type="bibr" target="#b10">9]</ref> and Ski-Pose PTZ-Camera <ref type="bibr" target="#b66">[62]</ref> datasets. We present qualitative results on the Human3.6M, KTH Multi-view Football II <ref type="bibr" target="#b37">[34]</ref> and Ski-Pose PTZ-Camera <ref type="bibr" target="#b66">[62]</ref> datasets, and on synthetic videos captured by dynamic cameras. Detailed description of these datasets can be found in the supplementary material.</p><p>Quantitative results We show quantitative results using the Mean Per Joint Position Error (MPJPE) <ref type="bibr" target="#b32">[29,</ref><ref type="bibr" target="#b10">9]</ref>, and report standard protocol #1 MPJPE (that is, error relative to the pelvis), in millimeters. <ref type="table" target="#tab_1">Table 1</ref> presents a quantitative comparison of the MPJPE metric on the Human3.6M <ref type="bibr" target="#b32">[29]</ref> dataset. We present monocular methods, followed by multiview ones that are split into ones that are acquainted with camera parameters and ones that are not. We show that in the absence of camera parameters, our model outperforms state-of-the-art methods by a large margin, and that even when camera parameters are available, FLEX is among the top methods. Note that these achievements are although FLEX aims at a slightly different task, which is motion reconstruction rather than pose estimation.</p><p>Being the only ep-free algorithm, we have no methods to compare to directly. However, algorithms can mitigate the lack of extrinsic camera parameters by estimating them. In the following comparisons, we show that when extrinsic parameters are not given, using estimated ones induces larger prediction errors, due to the innate inaccuracy of predicted values. On the other hand, FLEX is not affected by the lack of extrinsic parameters, since it does not use them whatsoever. We compare FLEX with two models:</p><p>(1) There are two methods that do not use given camera parameters <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b42">38]</ref>. They are not ep-free since they use estimated camera parameters, but we can still use them in settings where camera parameters are not given. Only one of them <ref type="bibr" target="#b18">[16]</ref> publishes MPJPE protocol #1 results, and we significantly outperform it (See <ref type="table" target="#tab_1">Table 1</ref>). This gap is mostly because of the inaccuracy of parameter prediction and partially because their model is semi-supervised.</p><p>(2) For comparing with the best available method, we have chosen the current state-of-the-art multi-view algorithm of Iskakov et al . <ref type="bibr" target="#b33">[30]</ref> (TesseTrack <ref type="bibr" target="#b64">[60]</ref> is marginally better, but it does not provide code). Since their algorithm is not epfree, we imitate parameter estimation by running a controlled perturbation of the camera parameters. We re-train their method with distorted data to simulate an environment where camera distortion parameters are unknown. In addition, we perturb the extrinsic parameters by Gaussian noise with an extremely small standard deviation of 3% of each parameter's value. That is, for a parameter p, we samplep ? N (p, (0.03p) 2 ) and usep as the input extrinsic parameter. We show that increasing the standard deviation from 3% to 4% yields a significant increase in the error, reflecting the sensitivity of non ep-free methods to inaccuracy in camera parameters. To obtain an equivalent environment, we compare FLEX to the method of Iskakov et al . <ref type="bibr" target="#b33">[30]</ref> after using their own 2D pose estimation. The lower part of <ref type="table" target="#tab_1">Table 1</ref> shows that FLEX outperforms the non ep-free state-of-theart, even when perturbation percentage is extremely small. Their results in that lower part are grayed out, to emphasize that we simulate an unrealistic setting. Next, we show that a 3% perturbation, rather than estimation, is fairer toward the compared method, as estimation induces larger inaccuracy. We estimate the extrinsic camera parameters with two leading frameworks, COLMAP <ref type="bibr" target="#b69">[65]</ref> and OpenCV-SFM <ref type="bibr" target="#b6">[6]</ref>, and obtain errors of 5.5% and 8.6%, respectively. The error is the mean value of |p?p| p for all extrinsic values p and their estimationp. Moreover, the estimation process involves friction: OpenCV-SFM strongly depends on an initial guess, and COLMAP requires that each pair of cameras observes partially overlapping images, a limiting factor that prevents its usage in settings where the cameras face each other. In addition to the comprehensive comparison on the Human3.6 dataset, in <ref type="table" target="#tab_2">Table 2</ref> we show a quantitative comparison on the Ski-Pose PTZ-Camera <ref type="bibr" target="#b66">[62]</ref> dataset, for methods that are trained when camera parameters are not given. These methods are comparable in settings that lack extrinsic parameters because they estimate them. However, since they still use (estimated) parameters, they are not ep-free. FLEX leads the table with a large gap. This gap is mostly because parameter  Method Acc. Err. ? VIBE <ref type="bibr" target="#b41">[37]</ref> 18.3 MEVA <ref type="bibr" target="#b51">[47]</ref> 15.3 HMMR <ref type="bibr" target="#b36">[33]</ref> 9.1 TCMR <ref type="bibr" target="#b16">[14]</ref> 5.3 Iskakov <ref type="bibr" target="#b33">[30]</ref> 3.9 Shi <ref type="bibr" target="#b71">[67]</ref> 3.6(?) / 2.0(?) FLEX 1.6(?) / 0.9(?) estimation induces an inevitable inaccuracy, and partially because the compared models are self/semi-supervised. A known strength of predicting rotation angles rather than locations, is the smoothness of predicted motion. In <ref type="table">Table 3</ref> we show that FLEX's smoothness result outperforms others by a large margin. Following Kanazawa et al . <ref type="bibr" target="#b36">[33]</ref>, we measure smoothness using the acceleration error of each joint.</p><p>Qualitative results In the following figures we show rigs, that is, bone structure from reconstructed animation videos, selecting challenging scenes. Videos of the reconstructed motions are available on our project page, presenting the smoothness of motion and the naturalness of rotations. <ref type="figure">Figures 1, 4 and 5</ref> show scenes from the KTH Multi-view Football II <ref type="bibr" target="#b37">[34]</ref>, the Human3.6M <ref type="bibr" target="#b32">[29,</ref><ref type="bibr" target="#b10">9]</ref> and the Ski-Pose PTZ-Camera <ref type="bibr" target="#b66">[62]</ref> datasets, respectively. Each row depicts three views of one time frame. To the right of each image, we place a reconstructed rig, which is sometimes zoomed in for better visualization. Notice the occluded and blurry scenes in the football figure <ref type="bibr" target="#b0">(1)</ref>. The KTH Football dataset is filmed using dynamic (moving) cameras, a setting where extrinsic parameters are rarely given, thus disqualifying methods that require camera parameters. Our algorithm is agnostic to the lack of camera parameters and attains good qualitative results.</p><p>In <ref type="figure" target="#fig_4">Figure 6</ref> we show qualitative results of FLEX, compared to current non ep-free multi-view state-of-the-art <ref type="bibr" target="#b33">[30]</ref>, and to our monocular baseline <ref type="bibr" target="#b71">[67]</ref>. Note that the method in <ref type="bibr" target="#b33">[30]</ref> produces unnatural poses such as a huge leg in the first row and a backward-bent elbow in the last row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-person captured by dynamic cameras</head><p>We evaluate our algorithm on a setting with dynamic cameras, with multi-person scenes introducing severe inter-person occlusions. Recall that the term dynamic refers to moving cameras that occasionally change their location and rotation. There are several multiview datasets. Most of them are not fully dynamic: Human3.6M <ref type="bibr" target="#b32">[29,</ref><ref type="bibr" target="#b10">9]</ref>, CMU Panoptic <ref type="bibr" target="#b19">[17]</ref> and TUM Shelf &amp; Campus <ref type="bibr" target="#b2">[3]</ref> contain static scenes only, while Tagging <ref type="bibr" target="#b84">[79]</ref> and Ski-Pose PTZ-Camera <ref type="bibr" target="#b66">[62]</ref> contain rotating cameras whose locations are fixed. KTH <ref type="bibr" target="#b37">[34]</ref> is fully dynamic, but it is too blurry and does not provide ground-truth for all subjects. Despite its limitations, we use the KTH   dataset for qualitative analysis, but we cannot use it for thorough research. To mitigate the lack of a dynamic dataset, we generate synthetic videos using animated characters downloaded from Mixamo <ref type="bibr" target="#b0">[1]</ref>, an online dataset of character animation. Then, we generate video sequences of two interacting characters using Blender <ref type="bibr" target="#b20">[18]</ref>, which is a 3D creation suite. The newly created data is available on our project page. Our "synthetic studio" is illustrated at the appendix, where two interacting figures are video-filmed by multiple dynamic cameras. Using Blender, we obtain a rendered video stream from the view angle of each synthetic camera. Recall that the input to our algorithm is 2D joint locations, hence it is agnostic to the video appearance, and to whether the input image is real or synthetic.</p><p>The 2D backbone we use over the rendered video sequences is Alphapose <ref type="bibr" target="#b24">[21]</ref>, a state-of-the-art multi-person 2D pose estimator. Once obtaining the 2D joint locations, we use a na?ve heuristic, which is not part of the suggested algorithm, to associate each detected person with its ID: for each frame, we associate the detected 2D pose with the one that is geometrically closest to it in the previous frame. In <ref type="figure">Figure 8</ref> we depict qualitative results of two boxers. We emphasize several viewpoints where the 2D estimator attains large errors. Yet, FLEX com- <ref type="figure">Fig. 8</ref>: Results on multi-person synthetic videos. In the zoomed-in circular images we depict 2D pose estimations, which are erroneous due to occlusion. A matching circle in the center rectangular image shows that our method reconstructs correct 3D motion although it takes inaccurate 2D joints for input. pensates for these errors by fusing multi-view information. In the appendix we show additional characters and the predicted 2D pose for all the viewpoints.</p><p>Global position In <ref type="figure" target="#fig_5">Figure 7</ref> we draw the global position of the scaled predicted root joint along time. Ground-truth is depicted using a thin black curve, and our prediction is an overlay on top of it, changing from light to dark as time progresses. The start and the end of each trajectory are signaled by the letters S and E, respectively. Depicted motions are evaluated on the test set of Human3.6M, on the motions of walking, talking on the phone, and eating. Note that our predictions almost completely overlap the ground-truth curve. Recall we use weak perspective to bypass dependency on intrinsic parameters, resulting in up-to-scale global position accuracy. Quantitatively, our MPJPE on the H36M validation set is 118mm, outperforming Iskakov et al . <ref type="bibr" target="#b33">[30]</ref> (perturbed by 3%) that attain 123mm. The other ep-free work <ref type="bibr" target="#b18">[16]</ref> does not solve global locations.</p><p>Ablation study We evaluate the impact of different settings on the performance of FLEX using various ablation tests. <ref type="table" target="#tab_4">Table 4</ref> compares different multiview fusion architectures. Note that using attention rather than convolution yields a 2mm improvement. The performance degrades with the transformer encoder due to its large number of parameters, which require more data for training than what is available in our case. <ref type="table" target="#tab_5">Table 5</ref> measures MPJPE on Human3.6M in several studies. <ref type="table" target="#tab_5">Table 5</ref>(a) studies a varying number of views, where the 2D pose is once given and once estimated. It confirms that a larger number of views induces more accurate results.</p><p>Note that the gap between the two columns decreases once the number of views increases. It shows that using several views compensates for the inaccuracy of estimated 2D poses. <ref type="table" target="#tab_5">Table 5</ref>(b) compares 2D pose estimation backbones, and justifies our use of Iskakov et al . <ref type="bibr" target="#b33">[30]</ref>. Finally, in <ref type="table" target="#tab_5">Table 5</ref>(c) we explore two variations, both with ground-truth 2D inputs. The first variation runs FLEX as a monocular method (K=1) and averages the monocular predictions. The second changes the fusion layers, F S and F Q , to use late fusion instead of an early one. We conclude that the configuration used by FLEX is better than both variations.</p><p>Generalization We exhibit generalization by training on one dataset and evaluating on a different, more challenging one. The train dataset is Human3.6M, and the evaluation ones are the KTH Football dataset, and the synthetic videos. For quantitative measurement, we train our model on two of the four cameras of the Human3.6M dataset. We test it using the other two cameras, on which the model has not been trained. We repeat this process for all possible camera pairs and obtain an average MPJPE of 148mm. Note that this error is not large compared to the human body size, and indeed we attain pleasing visual results as shown in the inset on the right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and limitations</head><p>We have presented FLEX, a multi-view method for motion reconstruction. It relies on a key understanding that 3D rotation angles and bone lengths are invariant to camera view, and their direct reconstruction spares the need for camera parameters. On a technical viewpoint, we presented a novel fusion layer with a multi-view convolutional layer and a multi-head attention mechanism that attends views.</p><p>One limitation of our approach is the dependency on 3D joint location ground-truth, and in particular, the requirement that it is given at the axis system of the train cameras. Another limitation is the dependency on the 2D backbone quality, and on the accuracy value associated with each joint. Lastly, being ep-free, the output 3D joint positions are only relative to the camera, lacking the transformation with respect to a global axis system.</p><p>In summary, FLEX is unique in fusing multi-view information to reconstruct motion and pose in dynamic photography environments. It is unaffected by settings in which the relative rotations between the cameras are unknown, and can maintain a high level of accuracy regardless. FLEX offers a simpler setting, where the correspondence and compatibility among the different views are rather lean, and thus more resilient to input errors and innate inaccuracies.</p><p>Shahaf Goren for contributing to FLEX's video clip. This work was supported in part by the Israel Science Foundation (grants no. 2366/16 and 2492/20).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Code</head><p>Our code, together with usage instructions, is available on our project page 1 . The reader is encouraged to run the code and witness the reproducibility of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional visualizations</head><p>On our project page 1 , the reader can find attached video files. The reader is encouraged to browse the video files in full-screen size. Here is their description: Note that we present only results that use input obtained by 2D estimation (as opposed to ground truth). Thus, our input is affected by occlusion and blur. Yet, we are able to mitigate the noisy input by exploiting multi-view data, in an ep-free fashion.</p><p>In <ref type="figure">Figure 9</ref> we show how our algorithm is able to grasp fine details. The player's left hand cannot be seen in the center view and is blurred in the left views. Yet, our model accurately reconstructs it.</p><p>In <ref type="figure" target="#fig_4">Figures 16 and 17</ref>, we show additional results on the Human3.6M and KTH Football multi-view II datasets. Each row depicts three views of one time frame. To the right of each image we place a reconstructed rig. <ref type="figure" target="#fig_1">Figures 18 to 20</ref> are enlarged versions of the figures shown in the main paper. <ref type="figure">Figure 10</ref> shows our recording setup for creation of synthetic data. Note the depicted cameras, that dynamically move in the scene. <ref type="figure">Fig. 9</ref>: Our algorithm is able to grasp fine details. The player's left hand cannot be seen in the center view and is blurred in the left views. Yet, our model accurately reconstructs it.</p><p>In <ref type="figure" target="#fig_7">Figure 11</ref> we depict qualitative results for a scene with two macarena dancers. We emphasize several viewpoints where the 2D backbone attains large errors. Yet, FLEX is able to compensate for these errors by fusing multi-view information. <ref type="figure" target="#fig_1">Figures 21 and 22</ref> depict 2D joint locations estimated by the Al-phaPose <ref type="bibr" target="#b24">[21]</ref> backbone. A close look at these figures shows that many of the estimated locations are inaccurate, e.g., a hand of one subject is confused with the hand of the other subject. Even though the number of 2D errors is large, our algorithm is able to reconstruct the characters correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Skeleton</head><p>To better reconstruct the motion in a given video stream, we modify the skeleton connectivity used in our baseline <ref type="bibr" target="#b71">[67]</ref>  <ref type="figure" target="#fig_1">(Figure 12</ref>). The root and neck joints of the baseline skeleton are both rigidly attached to the three bones neighboring each of them. This rigid connectivity constrains the skeleton, e.g., a motion where each shoulder moves forward and the neck moves to the right is impossible. In order to remove this constraint we add joints that overlap the root and the neck, hence enabling the neighboring bones to move independently of each other.</p><p>The new skeleton connectivity better matches the Human3.6M rotation angles ground-truth, thus, it better matches the way the dataset motions were captured. The new skeleton improves the mean per joint position error (MPJPE) both in the multi-view setting and the monocular case. The improvements are by ?4mm and ?6mm for monocular and four cameras setting, respectively. <ref type="figure">Fig. 10</ref>: Our "synthetic studio" created using Blender <ref type="bibr" target="#b20">[18]</ref> software with Mixamo <ref type="bibr" target="#b0">[1]</ref> 3D characters. Two interacting characters are captured by multiple dynamic cameras and rendered into multiple video streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Architecture details</head><p>The architectural blocks in our implementation are the multi-view feature fusion layers F S and F Q , the two encoders, E S and E Q , a forward kinematics layer F K and a discriminator D. Our discriminator D is a linear component that contains two convolution layers and one fully connected layer. We base it on Kanazawa et al . <ref type="bibr" target="#b35">[32]</ref>). The F K layer is based on Villegas et al . <ref type="bibr">[78]</ref>.</p><p>There are two novel building blocks contained in the new fusion layers, F S and F Q . The first is a multi-view convolutional layer; that is, a convolution that is aware of features stemming from multiple views as well as multiple frames. This convolutional layer is used in F Q only. The second is a multi-head attention layer, used in both F S and F Q . A standard attention mechanism looks at the data as a sequence of embeddings. In our mechanism, the views form the sequence, and the channels form the embeddings. Our attention layer is based on the LiftFormer <ref type="bibr" target="#b49">[45]</ref>. While the LiftFormer attends to time, we attend to views. The embedding size is 1024, and we use 64 heads (see <ref type="table">Table 6</ref>), so for the Query, Key and Value (each), we have 64 learned linear filters of size K ? 16, where K is the number of views and 16 is the result of 1024/64.</p><p>A key architectural choice in our fusion layers, F S and F Q , is at which stage to fuse the input views. In <ref type="figure" target="#fig_2">Figure 13</ref> we depict the conceptual idea of fusing. Each fusing scheme has its own advantages and disadvantages. Following the insight that early convolutional layers yield coarse features and late ones yield semantic features, we observe early fusion as generating all features (coarse and semantic) when a network is aware to all input branches, and observe middle fusion as first generating coarse features that are distinct for each branch, and only then fuse the coarse features together to generate common semantic features. When applying late fusion, the network creates distinct coarse and semantic features for each branch and only then fuses them together. During the development of our model, we have experimented with different fusion schemes, and found out that for our setting the early fusion works best. <ref type="figure" target="#fig_10">Figure 14</ref> depicts diagrams of the multi-view fusion layers and the encoders. The input to both fusion layers is V s,q,r ? R T ?3J?K (described in Section 3.1. In order to make the diagram more intuitive, we sketch V as K temporal sequences. Each temporal sequence is a 2D tensor, where channels are formed by the joints. The fusion layer first streams these temporal sequences through an expansion layer, increasing their channel size. Next, our fusion layer concatenates the expanded data and obtains a 3D tensor, on which it applies multi-view convolutional filters. These filters consider the data from all views. At the next stage we apply a multi-head attention layer that attends to views. Our network uses the attention layer output to create a 2D tensor representing one 'fused' view. The features are then passed to the encoder. The encoder block E Q consists of three parallel 1D convolutional layers of different kernel sizes, followed by a final  additional 1D convolution. The encoder E S starts with an adaptive max pooling to collapse the time dimension and then runs a final 1D convolution. After each convolution block, we apply batch normalization, a leaky rectified linear unit and dropout. Finally, we run a shrinking filter to decrease the number of channels to the desired output size. <ref type="table">Table 6</ref> describes the weight parameters of each layer.</p><p>In <ref type="figure" target="#fig_11">Figure 15</ref> we zoom into the attention block (item (d) in <ref type="figure" target="#fig_10">Figure 14</ref>). Each slice of one temporal frame is separately forwarded through this block. Such a slice contains features from all the views. Within the attention block, we concatenate an additional view, which we call the fusion view. This additional view is a learned token <ref type="bibr" target="#b21">[19]</ref>, in which the attention mechanism combines significant information from all views. Our model attends to all views, including the added one. After exiting the attention block we omit the data related to the given views and keep the learned fusion view only. This fusion view is then concatenated with the outputs of the other temporal frames.   <ref type="table">Table 6</ref>: FLEX structure. J denotes the number of joints, K the number of views, and L denotes the number of limbs. k and s denote kernel width and the stride, respectively. ? denotes parallel convolutions while ? denotes sequential ones. very challenging. We adjust the skeleton topology of the KTH dataset to match the topology of Human3.6M in the following way. KTH extracts 14 joints (tophead, mid-head, shoulders, hips, knees, feet, elbows, and hands). We create root (pelvis) and neck joints by averaging the hips and the shoulders respectively and then create a spine joint by averaging the root and the neck. Then we draw bones according to the Human3.6M skeleton topology. The raw data can be accessed and downloaded from the dataset webpage 2 . This dataset may only be used for academic research and not for commercial use.</p><p>Ski-Pose PTZ-Camera <ref type="bibr" target="#b66">[62]</ref> is a 6 cameras' multi-view pant-tilt-zoom-camera (PTZ) dataset. It features competitive alpine skiers performing giant slalom runs. It holds 20K images, with a single skier in each. The cameras can rotate, but their locations are fixed. This dataset provides labels for the skiers' 3D locations in each frame, their projected 2D locations, and per-frame calibration of the PTZ cameras. In the dataset webpage 3 there are more descriptions of the dataset as well as download instructions.</p><p>Our synthetic videos are generated using Mixamo <ref type="bibr" target="#b0">[1]</ref> and Blender <ref type="bibr" target="#b20">[18]</ref>. We maintain two scenes with two interacting subjects in each. One scene illustrates a boxing arena, and one shows Macarena dancers. We create as many cameras as we wish, with full control on each camera's dynamic motion trajectory. Each synthetic camera outputs a video of the scene, taken from its view. To evaluate FLEX on these videos, we use a network that has been trained on the Hu-man3.6M dataset, introducing satisfactory generalization abilities of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Input data</head><p>The input to our network is 2D joint locations per frame, accompanied by a confidence value. We train our network with several variations of input data.</p><p>Ground truth 2D pose Obviously, training with ground truth input data yields the best possible results. We use the 2D labeling provided by the Hu-man3.6M dataset.</p><p>Estimated 2D pose To simulate dynamic capture environments, where 2D labels are not available, we use several state-of-the-art 2D pose estimators as 2D backbones. In our ablation studies we demonstrate the dependency on a good estimator. The estimators that we use are OpenPose <ref type="bibr" target="#b9">[8]</ref>, CPN <ref type="bibr" target="#b14">[12]</ref>, and the one used by Iskakov et al . <ref type="bibr" target="#b33">[30]</ref> (who base their 2D estimation on the "simple baselines" architecture <ref type="bibr" target="#b88">[83]</ref>). OpenPose and Iskakov et al . provide confidence values that we add to the network input. CPN does not provide these values, hence we assign identical confidence values for all joints when using it. While OpenPose and CPN are dedicated 2D pose estimators, Iskakov et al .'s 2D estimator is part of a 3D pose estimator. To extract the 2D pose we retrain their model using its given code and save intermediate values. The 2D pose estimation computed by Iskakov et al . <ref type="bibr" target="#b33">[30]</ref> uses camera distortion parameters. In addition to being free of extrinsic camera parameters, we are strict about not using the intrinsic ones as well (see Section 3 in the main paper); hence, we retrain Iskakov et al . <ref type="bibr" target="#b33">[30]</ref> without those parameters.</p><p>Skeleton topology may vary between the aforementioned 3D datasets and 2D poses predicted by backbone algorithms. To mitigate this, we make adjustments to the predicted 2D joints. Openpose <ref type="bibr" target="#b9">[8]</ref> extract 16 joints (root, neck, mid-head, top-head, shoulders, hips, knees, feet, elbows, and hands). These joints exist in the aforementioned datasets as well. In addition, a spine joint, which exists only in the 3D datasets, is artificially added (calculated as the 2D spatial average between the root and the neck joint). For the CPN <ref type="bibr" target="#b14">[12]</ref> 2D prediction, we simply use the values computed by Pavllo et al . <ref type="bibr" target="#b61">[57]</ref> and provided in their project page, which already possess the requested topology. Lastly, Iskakov et al . <ref type="bibr" target="#b33">[30]</ref> predict the exact joints required by the aforementioned 3D datasets.</p><p>At inference time, when videos from the wild are used, we use a network that was trained using an estimated 2D pose and make sure that during inference, the exact 2D backbone that was used for training, is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Ground truth</head><p>During train time we use 3D joint location ground truth per view, plus rotation ground truth for the discriminator. In contrast to location ground truth, rotation ground truth is required only once, no matter how many views we have. During test time we need none of the above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Camera Parameters</head><p>We next formulate the notion of camera parameters. Consider a pinhole camera model. Such a model possesses two types of parameters, extrinsic and intrinsic. Extrinsic parameters correspond to -A rotation matrix R: a matrix of size 3 ? 3 characterizing the rotation from 3D real world axes into 3D camera axes. -A translation vector T : a vector of size 3 representing the translational offset of the camera in the 3D scene.</p><p>Intrinsic parameters, stored in a 3 ? 3 matrix K, are specific to a camera. K consists of the focal length f x , f y , the camera optical center c x , c y and a skew coefficient s k :</p><formula xml:id="formula_9">K = ? ? f x s k c x 0 f y c y 0 0 1 ? ? .<label>(10)</label></formula><p>We denote the mapping from 3D world coordinates into a 2D image plane by a 3 ? 4 matrix P . P is sometimes called camera matrix or projection matrix. To calculate P , both camera extrinsic and intrinsic parameters are used:    </p><formula xml:id="formula_10">P = K ? R | T .<label>(11)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>equal contribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>3D locations vary across axis systems while 3D rotation angles and bone lengths remain identical.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 :</head><label>3</label><figDesc>Smoothness, measured by acceleration error (mm/s 2 ), on Human3.6M. (?): 2D pose from [30]. (?): ground-truth 2D poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Our results on videos from the Human3.6M dataset. Our results on videos from the Ski-Pose PTZ-Camera dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Qualitative comparison of our work vs. non epfree state-of-the-art (Iskakov et al . [30]) and vs. our single-view baseline (Shi et al . [67]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Global root position.Groundtruth is in thin black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>-</head><label></label><figDesc>A clip describing our work: clip.mp4 -Video files showing our results on the Human3.6M dataset: Human36M*.mp4 -Video files showing our results on the KTH multi-view Football II dataset: KTH football.mp4 -Video files comparing MotioNet (single-view) and Iskakov et al . [30] results versus ours: MotioNet comparison.mp4 and Iskakov comparison.mp4, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 :</head><label>11</label><figDesc>Our results on multi-person synthetic videos, picturing two Macarena dancers. Some of the 2D joints, used as input to our method, are severely inaccurate. However, our method is able to reconstruct correct 3D motion.In the following examples, let white dancer and orange dancer denote the dancer wearing a white and an orange shirt respectively. Several 2D based skeleton error examples are depicted in the zoomed-in circular insets: (a) Wrong pose estimation of the left arm of the orange dancer; (b) The right arm of the orange dancer is occluded hence detected erroneously; (c) The nose tip of the orange dancer is erroneously detected as the nose tip of the white dancer; (d) Erroneous 2D pose estimation of the white dancer's right hand.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 :</head><label>12</label><figDesc>Skeletal connectivity changes, demonstrated on the neck joint. Left: original connectivity, where shoulders and head are rigidly connected, yielding poor reconstruction. Right: new connectivity, with extra degrees of freedom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 :</head><label>13</label><figDesc>Fusion schemes. Left: early fusion; Middle: late fusion; Right: middle fusion. Yellow and blue blocks symbolize features from distinct input branches, and green blocks represent fused data. Each block stands for a tensor of features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 :</head><label>14</label><figDesc>Architecture in detail. The upper and lower parts are the rotations and bones branches, respectively. (a) Channel-wise expansion layer; (b) View concatenation; (c) Multi-view convolutional filters; (d) Cross-view attention layer (detailed in Figure 15); (e) Single-view convolutional filters; (f) Channel-wise shrinkage layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 15 :</head><label>15</label><figDesc>Cross-view attention layer (Figure 14(d)) in detail. Our attention mechanism processes each frame separately, attending the multi-view features and fusing them to a single output per frame. (a) Process each temporal frame independently. Add a learned token [19] that forms a fusion view ; (b) Linear layer; (c) Split the channels to H attention heads; (d) Multi-Head attention [77]; (e) Concatenate the attention heads; (f) Drop features from the original views. Collect fusion view features from all the frames in the temporal sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 20 :</head><label>20</label><figDesc>Enlarged results on the KTH Football II dataset (from main paper).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 21 :</head><label>21</label><figDesc>2D joint locations estimated on a multi-person synthetic video of boxers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 22 :</head><label>22</label><figDesc>2D joint locations estimated on a multi-person synthetic video of dancers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2105.01937v4 [cs.CV] 21 Oct 2022</figDesc><table><row><cell>Camera ?</cell><cell>Camera ??</cell><cell>Camera ???</cell></row><row><cell>Frame A</cell><cell></cell><cell></cell></row><row><cell>Frame B</cell><cell></cell><cell></cell></row><row><cell>Frame C</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Protocol #1 MPJPE error on Human3.6M. Legend: ( * ) is a non epfree algorithm. In case parameters are not given, we imitate their computation by perturbing the GT params by an unrealistically small perturbation amount; ( ?) exploit temporal information; (+) extra training data. In blue -best result when camera parameters are not given, in bold -best result per method group.Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Mean Reddy et al . [60]( ?) 38.4 46.2 44.3 43.2 44.8 48.3 52.9 36.7 45.3 54.5 63.4 44.Hu et al . [27]( ?) 35.5 41.3 36.6 39.1 42.4 49.0 39.9 37.0 51.9 63.3 40.9 41.4 40.3 29.8 28.9 41.1 Cheng et al . [13] ( ?) 36.2 38.1 42.7 35.9 38.2 45.7 36.8 42.0 45.9 51.3 41.8 41.5 43.8 33.1 28.6 40.1 Multi-view methods, extrinsic camera parameters are given Qiu et al . [59] (+) 24.0 26.7 23.2 24.3 24.8 22.8 24.1 28.6 32.1 26.9 31.0 25.Iskakov et al . [30] 19.9 20.0 18.9 18.5 20.5 19.4 18.4 22.1 22.5 28.7 21.2 20.8 19.7 Reddy et al . [60]( ?) 17.5 19.6 17.2 18.3 18.2 17.7 18.0 18.0 20.5 20.3 19.4 17.2 18.9 19.0 17.8 18.7 Multi-view methods, extrinsic camera parameters are not given Chu and Pan [16]( ?) 49.1 63.6 48.6 56.0 57.4 69.6 50.4 62.0 75.4 77.4 57.2 53.5 57.7</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Monocular methods</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Shi et al . [67]( ?)</cell><cell>47.3 53.1 50.3 53.9 53.5 52.8 52.0 55.4 64.2 54.8 66.8 55.0 50.3</cell><cell>59.1</cell><cell>50.3</cell><cell>54.6</cell></row><row><cell>Llopart [45]( ?)</cell><cell>42.2 44.5 42.6 43.0 46.9 53.9 42.5 41.7 55.2 62.3 44.9 42.9 45.3</cell><cell>31.8</cell><cell>31.8</cell><cell>44.8</cell></row><row><cell></cell><cell>4 41.9</cell><cell>46.2</cell><cell>39.9</cell><cell>44.6</cell></row><row><cell>Li et al . [41]( ?)</cell><cell>39.9 43.4 40.0 40.9 46.4 50.6 42.1 39.8 55.8 61.6 44.9 43.3 44.9</cell><cell>29.9</cell><cell>30.3</cell><cell>43.6</cell></row><row><cell cols="2">Tome et al . [74] (+) 43.3 49.6 42.0 48.8 51.1 64.3 40.3 43.3 66.0 95.2 50.2 52.2 51.1</cell><cell>43.9</cell><cell>45.3</cell><cell>52.8</cell></row><row><cell>Kadkhodamohammadi and Padoy [31]</cell><cell>39.4 46.9 41.0 42.7 53.6 54.8 41.4 50.0 59.9 78.8 49.8 46.2 51.1</cell><cell>40.5</cell><cell>41.0</cell><cell>49.1</cell></row><row><cell>He et al . [25]</cell><cell>25.7 27.7 23.7 24.8 26.9 31.4 24.9 26.5 28.8 31.7 28.2 26.4 23.6</cell><cell>28.3</cell><cell>23.5</cell><cell>26.9</cell></row><row><cell></cell><cell>6 25.0</cell><cell>28.0</cell><cell>24.4</cell><cell>26.2</cell></row><row><cell>Ma et al . [48]( ?)</cell><cell>24.4 26.4 23.4 21.1 25.2 23.2 24.7 33.8 29.8 26.4 26.8 24.2 23.2</cell><cell>26.1</cell><cell>23.3</cell><cell>25.8</cell></row><row><cell></cell><cell></cell><cell>22.1</cell><cell>20.2</cell><cell>20.8</cell></row><row><cell></cell><cell></cell><cell>37.6</cell><cell>38.1</cell><cell>56.9</cell></row><row><cell>Iskakov et al . [30]( * ) param. perturb by 4%</cell><cell>30.2 37.2 32.7 33.2 38.8 43.7 29.7 43.0 49.4 67.6 38.0 33.1 42.1</cell><cell>27.2</cell><cell>29.3</cell><cell>38.4</cell></row><row><cell></cell><cell>2</cell><cell>26.2</cell><cell>28.4</cell><cell>33.1</cell></row></table><note>Iskakov et al . [30]( * ) param. perturb by 3% 27.6 30.3 29.0 29.4 33.1 36.5 27.4 34.8 39.1 54.0 34.4 30.7 36.Ours( ?) 22.0 23.6 24.9 26.7 30.6 35.7 25.1 32.9 29.5 32.5 32.6 26.5 34.7 26.0 27.7 30.2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>MPJPE on the Ski-PTZ dataset, measured for methods trained when extrinsic parameters are not given. ( ?) is self/weakly-supervised.</figDesc><table><row><cell>Method</cell><cell>MPJPE</cell></row><row><cell cols="2">CanonPose [80] ( ?) 128.1</cell></row><row><cell>Chen et al . [11] ( ?)</cell><cell>99.4</cell></row><row><cell>Ours</cell><cell>65.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Attention impact. TE: Transformer Encoder. MHA: Multi-head Attention. l: no. of stacked layers. h: no. of attention heads.</figDesc><table><row><cell cols="2">Method MPJPE</cell></row><row><cell>Conv. layer</cell><cell>31.9</cell></row><row><cell>TE -1l, 64h</cell><cell>30.9</cell></row><row><cell>TE -2l, 64h</cell><cell>37.8</cell></row><row><cell>MHA -128h</cell><cell>30.5</cell></row><row><cell>MHA -64h</cell><cell>30.2</cell></row><row><cell>MHA -32h</cell><cell>30.6</cell></row><row><cell>MHA -16h</cell><cell>30.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation studies: The impact of (a) Number of views; (b) 2D backbone, and (c) Fusion method (refer to the sup. mat. for details regarding fusion).</figDesc><table><row><cell></cell><cell>(a)</cell><cell></cell><cell cols="2">(b)</cell><cell>(c)</cell><cell></cell></row><row><cell>#Views</cell><cell cols="2">2D backbone GT [30]</cell><cell>2D backbone</cell><cell>MPJPE</cell><cell>Method Averaged K views</cell><cell>MPJPE 36.4</cell></row><row><cell>1</cell><cell>47.7</cell><cell>56.3</cell><cell>[8]</cell><cell>38.6</cell><cell>Late fusion</cell><cell>31.0</cell></row><row><cell>2</cell><cell>33.9</cell><cell>41.4</cell><cell>[12]</cell><cell>31.7</cell><cell>FLEX</cell><cell>22.9</cell></row><row><cell>3</cell><cell>26.3</cell><cell>34.6</cell><cell>[30]</cell><cell>30.2</cell><cell></cell><cell></cell></row><row><cell>4</cell><cell cols="2">22.9 30.2</cell><cell>GT</cell><cell>22.9</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Fig. 16: Additional results on videos from the Human3.6M dataset.Fig. 17: Additional results on videos from the KTH Multi-view Football II dataset.Fig. 18: Enlarged results on the Ski-Pose PTZ-Camera dataset (from main paper).Fig. 19: Enlarged results on the Human3.6M dataset (from main paper).</figDesc><table><row><cell></cell><cell>Camera ?</cell><cell>Camera ??</cell><cell>Camera ???</cell></row><row><cell>Frame A</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame B</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame C</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Camera ?</cell><cell>Camera ??</cell><cell>Camera ???</cell></row><row><cell>Frame A</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame B</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame C</cell><cell>Camera ?</cell><cell>Camera ??</cell><cell>Camera ???</cell></row><row><cell>Frame A</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Camera ?</cell><cell>Camera ??</cell><cell>Camera ???</cell></row><row><cell>Frame B Frame A</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame C Frame B</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame C</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Project page: https://briang13.github.io/FLEX</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://vision.imar.ro/human3.6m/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.csc.kth.se/cvap/cvg/?page=footballdataset2 3 https://www.epfl.ch/labs/cvlab/data/ski-poseptz-dataset/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This research would not have been possible without the exceptional support of Mingyi Shi. We are grateful to Kfir Aberman and Yuval Alaluf for reviewing earlier versions of the manuscript, and to Yuval Alaluf and</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This appendix provides further details for the main paper and is constructed in the following way. Appendix A describes how to access our model's code, and Appendix B provides additional media, namely figures that have not been included in the main paper due to page limitation, and a description of video files that are available at our project page 1 . In Appendix C we describe an improvement in the skeleton structure, and in Appendix D we detail the internals of our architecture. Appendix E thoroughly describes the datasets and various data aspects of our work, and finally, Appendix F presents technical details related to camera parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Train and Evaluation</head><p>We train our model on the Human3.6M dataset <ref type="bibr" target="#b32">[29,</ref><ref type="bibr" target="#b10">9]</ref>. We evaluate FLEX on the Human3.6M and the KTH Multi-view Football II <ref type="bibr" target="#b37">[34]</ref> datasets, and on synthetic multi-person video streams captured by dynamic cameras.</p><p>Human3.6M <ref type="bibr" target="#b32">[29,</ref><ref type="bibr" target="#b10">9]</ref> is a dataset of 3.6 Million accurate 3D Human poses, acquired by recording the performance of 5 female and 6 male subjects, under 4 different viewpoints. This dataset holds a diverse set of motions and poses encountered as part of 17 typical human activities such as talking on the phone, walking, and eating. As recommended on the Human3.6M dataset page, we use subjects S1, S5, S6, S7, and S8 for training and subjects S9 and S11 for testing. This dataset grants licenses free of charge that are limited to academic use only. More information and access to raw data are provided on the dataset webpage 1 .</p><p>KTH Multi-view Football II <ref type="bibr" target="#b37">[34]</ref> is a dataset of video streams from three synchronized cameras with 800-time frames per camera. The streams depict two different players (in separate streams), where each player has two sequences in varying levels of scene complexity. This dataset is unique in the sense that the cameras are dynamic, hence the approximation of camera extrinsic parameters is</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="http://www.mixamo.com" />
	</analytic>
	<monogr>
		<title level="j">Adobe Systems Inc.: Mixamo</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Motion capture from pan-tilt cameras with unknown orientation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sp?rri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="308" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2014.216</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2014.216" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d pictorial structures revisited: Multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2015.2509986</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2015.2509986" />
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1929" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A study of parts-based object class detection using complete graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bergtholdt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kappes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schn?rr</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-009-0209-1</idno>
		<ptr target="https://doi.org/10.1007/s11263-009-0209-1" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="93" to="117" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The OpenCV Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dr. Dobb&apos;s Journal of Software Tools</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d pictorial structures for multiple view articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings / CVPR, IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>/ CVPR, IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="3618" to="3625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2013.464</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2013.464" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Openpose: Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition. CVPR &apos;18</title>
		<meeting>the 2018 IEEE Conference on Computer Vision and Pattern Recognition. CVPR &apos;18<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="172" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Weakly-supervised discovery of geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10887" to="10896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deductive learning for weakly-supervised 3d human pose estimation via uncalibrated cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1089" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d human pose estimation using spatiotemporal networks with explicit occlusion training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10631" to="10638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond static features for temporally consistent 3d human pose and shape from a video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Part-aware measurement for robust multi-view multi-human 3d pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="1472" to="1481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised 3d human pose estimation by jointly considering temporal and multiview information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">W</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.3045794</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2020.3045794" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="226974" to="226981" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Cmu graphics lab motion capture database</title>
		<ptr target="http://mocap.cs.cmu.edu" />
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Blender Foundation, Stichting Blender Foundation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">O</forename><surname>Community</surname></persName>
		</author>
		<ptr target="http://www.blender.org" />
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
	<note>Blender -a 3D modelling and rendering package</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<idno type="DOI">10.48550/ARXIV.1810.04805</idno>
		<ptr target="https://arxiv.org/abs/1810" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast and robust multi-person 3d pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7792" to="7801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="2334" to="2343" />
			<pubPlace>Washington, DC, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepcap: Monocular human performance capture using weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5052" to="5063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2d features and intermediate 3d representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10905" to="10914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">Epipolar transformers. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7776" to="7785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)<address><addrLine>Berlin/Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Conditional Directed Graph Convolution for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Wong</surname></persName>
		</author>
		<idno type="DOI">10.1145/3474085.3475219</idno>
		<ptr target="https://doi.org/10.1145/3474085.3475219" />
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="602" to="611" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepfuse: An imu-aware network for real-time 3d human pose estimation from multi-view image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">https:/doi.ieeecomputersociety.org/10.1109/WACV45572.2020.9093526</idno>
		<ptr target="https://doi.ieeecomputersociety.org/10.1109/WACV45572.2020.9093526" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2020-03" />
			<biblScope unit="page" from="418" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7717" to="7726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A generalizable approach for multi-view 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00744</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00744" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-view body part recognition with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.27.48</idno>
		<ptr target="https://doi.org/10.5244/C.27.48" />
	</analytic>
	<monogr>
		<title level="m">BMVC 2013 -Electronic Proceedings of the British Machine Vision Conference</title>
		<meeting><address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Transformers in vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ACM Computing Surveys</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3505244</idno>
		<ptr target="https://doi.org/10.1145/3505244" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Beyond weak perspective for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kissos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Meir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kliger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="541" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5253" to="5263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-16808-1_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-16808-123" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Exploiting temporal contexts with strided transformer for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1954" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improving 3d human pose estimation via 3d part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1004" to="1013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Asari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5064" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Liftformer: 3d human pose estimation using attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Llopart</surname></persName>
		</author>
		<idno>abs/2009.00348</idno>
		<ptr target="https://arxiv.org/abs/2009.00348" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="248" />
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">3d human motion estimation via motion compression and refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Golestaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Transfusion: Cross-view fusion with transformer for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning trajectory dependencies for human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<meeting><address><addrLine>Berlin/Heidelberg; Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="601" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Xnect: Real-time multi-person 3d motion capture with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="82" to="83" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Video motion capture from the part confidence maps of multi-camera images by spatiotemporal filtering using the human skeletal model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ikegami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Takano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakamura</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS.2018.8593867</idno>
		<ptr target="https://doi.org/10.1109/IROS.2018.8593867" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4226" to="4231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Human mesh recovery from multiple shots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="1485" to="1495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Quaternet: A quaternion-based recurrent model for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4341" to="4350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Tessetrack: End-to-end learnable multi-person articulated 3d pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pischulini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eledath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15185" to="15195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multiview images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sp?rri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8437" to="8446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multiview images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sp?rri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8437" to="8446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">3d human pose estimation: A review of the literature and analysis of covariates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2016.09.002</idno>
		<ptr target="https://doi.org/10.1016/j.cviu.2016.09.002" />
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Metric-scale truncation-robust heatmaps for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>S?r?ndi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
	<note>FG 2020</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Improving robustness and accuracy via relative information encoding in 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Motionet: 3d human motion reconstruction from monocular video with skeleton consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aristidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Neural monocular 3d human motion capture with physical awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="DOI">10.1145/3450626.3459825</idno>
		<ptr target="http://www.skycam.tv" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2021-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Multi-view pose generator based on deep learning for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Human pose as calibration pattern: 3d human pose estimation with multiple unsynchronized and uncalibrated cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kimata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1856" to="18567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPRW.2018.00230</idno>
		<ptr target="https://doi.org/10.1109/CVPRW.2018.00230" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<title level="m">Structured prediction of 3d human pose with deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Rethinking pose in 3d: Multi-stage refinement and recovery for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="474" to="483" />
			<pubPlace>Washington, DC, USA</pubPlace>
		</imprint>
	</monogr>
	<note>international conference on 3D vision (3DV)</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Voxelpose: Towards multi-camera 3d human pose estimation in wild environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Metapose: Fast 3d pose from multiple views without 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="6759" to="6770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title/>
		<idno type="DOI">10.48550/ARXIV.1706.03762</idno>
		<ptr target="https://arxiv.org/abs/1706" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Neural kinematic networks for unsupervised motion retargetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="8639" to="8648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Selfsupervised multi-view person association and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="2794" to="2808" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Canonpose: Selfsupervised monocular 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Multi-view 3d reconstruction with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salcudean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE International Conference on Computer Vision</title>
		<meeting>eeding of the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Motion guided 3d pose estimation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)<address><addrLine>Berlin/Heidelberg; Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Skeleton transformer networks: 3d human pose and skinned mesh from single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshiyasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ayusawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Murai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="485" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Fusing wearable imus with multi-view images for human pose estimation: A geometric approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="186" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Reconstructing nba players</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="177" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Data-driven 3d reconstruction of dressed humans from sparse views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wuhrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
