<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporally-Aware Feature Pooling for Action Spotting in Soccer Broadcasts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
							<email>silvio.giancola@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporally-Aware Feature Pooling for Action Spotting in Soccer Broadcasts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Toward the goal of automatic production for sports broadcasts, a paramount task consists in understanding the high-level semantic information of the game in play. For instance, recognizing and localizing the main actions of the game would allow producers to adapt and automatize the broadcast production, focusing on the important details of the game and maximizing the spectator engagement. In this paper, we focus our analysis on action spotting in soccer broadcast, which consists in temporally localizing the main actions in a soccer game. To that end, we propose a novel feature pooling method based on NetVLAD, dubbed NetVLAD++, that embeds temporally-aware knowledge. Different from previous pooling methods that consider the temporal context as a single set to pool from, we split the context before and after an action occurs. We argue that considering the contextual information around the action spot as a single entity leads to a sub-optimal learning for the pooling module. With NetVLAD++, we disentangle the context from the past and future frames and learn specific vocabularies of semantics for each subsets, avoiding to blend and blur such vocabulary in time. Injecting such prior knowledge creates more informative pooling modules and more discriminative pooled features, leading into a better understanding of the actions. We train and evaluate our methodology on the recent large-scale dataset SoccerNet-v2, reaching 53.4% Average-mAP for action spotting, a +12.7% improvement w.r.t the current state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The volume of sports TV broadcast available worldwide increased at a fast pace over the last years. The amount of hours of sports TV broadcasted in the United States from 2002 to 2017 has grown &gt;4? <ref type="bibr" target="#b10">[11]</ref>, with similar trends in European countries <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">14]</ref>. Consequently, the market size for the sports media rights is booming <ref type="bibr" target="#b11">[12]</ref>, revealing a ludicrous market estimated to worth &gt;25B USD by 2023 in US alone. Yet, creating broadcast contents still requires a tremendous manual effort from the producers who could heavily benefit from automated processes.</p><p>Autonomous broadcast production requires an understanding of the sports it focuses on. In particular, it needs to be aware of where to look at, e.g. focusing the camera on spatial areas of the field, but also when to look at, e.g. focusing on a given actions of interest temporally anchored in the broadcast. While most literature focuses on where to look at, less effort were put on when to look at. As an example, player or ball detection and tracking algorithms reach excellent performances <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4]</ref> and are commonly used as priors to identify where to look at, by simply regressing the extrinsic parameters of the cameras to center those objects <ref type="bibr" target="#b36">[36]</ref>. Yet, the level of semantics involved in this task is rather low and does not require higher understanding of the game. On the other hand, identifying when to look at requires understanding higher semantics level, closer to the game, focusing on abstract action concepts rather than well defined actor's or object's priors. To that end, we believe that understanding actions rather than actors is a more challenging task, yet to explore in sports videos.</p><p>In this work, we propose to tackle the task of action spotting, i.e. localizing well-defined actions in time, an-chored with single timestamps along a video. Such task is commonly solved by looking at a temporal context around a given timestamp and regressing the actionness in time. The class-aware actionness is then reduced to a singularly anchored spot using non maximum suppression techniques <ref type="bibr" target="#b18">[19]</ref>. Previous works on that realm consider temporal pooling techniques <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b41">41]</ref>, spatio-temporal encoding <ref type="bibr" target="#b32">[32]</ref>, multi-tower temporal CNN <ref type="bibr" target="#b42">[42]</ref> or context-aware regression modules <ref type="bibr" target="#b6">[7]</ref>. Inspired by those related works, we propose a temporally-aware pooling technique that push forward the previous state-of-the-art. In particular, we developed a pooling module that consider the near past and future context around the action, independently. Our novel temporal module, dubbed NetVLAD++, is based on two NetVLAD pooling layer across the frames before and after the action occurs, respectively. Such temporal awareness brings a significant boost in the performances in the SoccerNet-v2 benchmark, leading into state-of-the-art performances for action spotting.</p><p>Contributions. We summarize them as follow: (i) We introduce NetVLAD++, a novel pooling module for action spotting that learns a temporally-aware vocabulary for past and future temporal context. (ii) We implement a more efficient architecture for action spotting, in term of memory and computational complexity, leading to state-of-the-art performances for action spotting on SoccerNet-v2. (iii) We propose a comprehensive ablation that points out the contributions of each architectural block design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Computer Vision in Soccer. The literature in soccerrelated computer vision mainly focuses on low-level understanding of a soccer broadcast <ref type="bibr" target="#b29">[29]</ref>, e.g. localizing a field and its lines <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref>, detecting players <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b44">44]</ref>, their motion <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">28]</ref>, their pose <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b46">46]</ref>, their team <ref type="bibr" target="#b23">[24]</ref>, the ball <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b35">35]</ref> or a pass feasibility <ref type="bibr" target="#b33">[33]</ref>. Understanding framewise information is useful to enhance the visual experience of sports viewers <ref type="bibr" target="#b30">[30]</ref> and to gather player statistics <ref type="bibr" target="#b36">[36]</ref>, but it falls short of higher-level game understanding needed for automatic editing purposes. With the appearance of large scale datasets such as SoccerNet <ref type="bibr" target="#b18">[19]</ref>, Yu et al. <ref type="bibr" target="#b45">[45]</ref> and SoccerDB <ref type="bibr" target="#b26">[26]</ref>, higher level tasks started to appear. SoccerNet <ref type="bibr" target="#b18">[19]</ref> introduced the task of action spotting, i.e. localizing every action with its timestamp in a large corpus of TV broadcasts. They introduced a dataset of 500 games from the European leagues, annotated with 6637 actions of goals, cards and substitutions. Yu et al. <ref type="bibr" target="#b45">[45]</ref> released a novel dataset of 222 broadcast videos of 45 min each. They introduced interesting annotations of camera shots, players position, events and stories, yet do not provide any task nor baseline on how to use those annotations. SoccerDB <ref type="bibr" target="#b26">[26]</ref> merged a subset of 270 games from SoccerNet with 76 soc-cer games from the Chinese Super League. They proposed several tasks, ranging from object detection, action recognition, temporal action localization and replay segmentation. Lastly, SoccerNet-v2 <ref type="bibr" target="#b9">[10]</ref> extended SoccerNet <ref type="bibr" target="#b18">[19]</ref> with more than 300k extra annotations and propose novel tasks that would support the automatic production of soccer broadcast. In particular, SoccerNet-v2 <ref type="bibr" target="#b9">[10]</ref> extended the task of action spotting to 17 classes to understand the finegrained details of a soccer game. They also introduced two novel tasks: camera shot segmentation for broadcast editing purposes and replay grounding for highlight and summarization purposes. In this work, we leverage the finegrained annotations from SoccerNet-v2 <ref type="bibr" target="#b9">[10]</ref> and compete in the task of action spotting.</p><p>Action spotting. Action spotting was introduced in Soc-cerNet <ref type="bibr" target="#b18">[19]</ref> and defined as the localization of a instantaneous event anchored with a single timestamp, namely an action, in contrast with activities, defined with a start and an end <ref type="bibr" target="#b21">[22]</ref>. It draws similarities with the concept of action completion <ref type="bibr" target="#b20">[21]</ref> where an action is defined with a single anchor in time, but serve a different purpose of predicting the future completion of that action. Giancola et al. <ref type="bibr" target="#b18">[19]</ref> introduced a first baseline on SoccerNet <ref type="bibr" target="#b18">[19]</ref> based on different pooling techniques. Yet, their code is hardly optimized, leading into an strong under-estimation of the pooling performances for action spotting. Vanderplaetse et al. <ref type="bibr" target="#b41">[41]</ref> later improved that baseline by merging visual and audio features in a multi-modal pooling approach. Rongved et al. <ref type="bibr" target="#b32">[32]</ref> trained a 3D ResNet encoder <ref type="bibr" target="#b39">[39]</ref> directly from the video frames. Although the performances were far from the baseline, mostly due to the difficulty of training an encoder from scratch, the technical prowess lied in training end-to-end for action spotting with 16 V100 GPU combining 512GB of memory. Vats et al. <ref type="bibr" target="#b42">[42]</ref> leveraged a multi-tower CNN to process information at various temporal scales to account for the uncertainty of the action locations. Cioppa et al. <ref type="bibr" target="#b6">[7]</ref> proposed a method based on a context-aware loss function that model the temporal context surrounding the actions. They propose an alternative approach that predicts multiple spots from each chunk of video, by regressing multiple temporal offsets for the actions. Most recently, Tomei et al. <ref type="bibr" target="#b37">[37]</ref> introduced a regression and masking approach with RMS-Net, inspired by common detection pipeline <ref type="bibr" target="#b31">[31]</ref> and self-supervised pre-training <ref type="bibr" target="#b15">[16]</ref>. It is worth noting that they reach impressive performance gains by fine tuning the last ResNET block of the video frame encoder. In our work, we improve the original temporal pooling mechanism proposed in SoccerNet <ref type="bibr" target="#b18">[19]</ref> by introducing temporally-aware bag-ofwords pooling modules. Unlike Tomei et al. <ref type="bibr" target="#b37">[37]</ref>, we refrain from fine-tuning the pre-extracted ResNET frame features for a fair comparison with the related work, but simply allow for a learnable projection (similar to PCA) to reduce the feature dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we first recall the definition of NetVLAD and propose a more computationally efficient implementation (3.1), we present our novel temporally-aware NetVLAD pooling module learning the past and future temporal context independently (3.2) and its implementation in a more comprehensive pipeline for action spotting (3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Recall on NetVLAD</head><p>NetVLAD <ref type="bibr" target="#b1">[2]</ref> is a differentiable pooling technique inspired by VLAD <ref type="bibr" target="#b25">[25]</ref>. In particular, VLAD learns clusters of features descriptors and defines an aggregation of feature as the average displacement of each features with respect to the center of its closer cluster. NetVLAD generalizes VLAD by (i) softening the assignment for fulldifferentiable capability, and (ii) disentangling the definition of the cluster and the assignment of the samples. VLAD. Formally, given a set of N D-dimensional features {x i } i=1..N as input, a set of K clusters centers {c k } k=1..K with same dimension D as VLAD parameters, the output of the VLAD descriptor V is defined by:</p><formula xml:id="formula_0">V (j, k) = N i=1 a k (x i )(x i (j) ? c k (j))<label>(1)</label></formula><p>where x i (j) and c k (j) are respectively the j-th dimensions of the i-th descriptor and k-th cluster center. a k (x i ) denotes the hard assignment of the sample x i from its closer center, i.e. a k (x i ) = 1 if c k is the closest center of x i , 0 otherwise. The matrix V is then L2-normalized at the cluster level, flatten into a vector of length D ? K and further L2normalized globally. NetVLAD. The VLAD module is non-differentiable due to the hard assignment a k (x i ) of the samples</p><formula xml:id="formula_1">{x i } N i=1 to the clusters {c k } K i=1</formula><p>. Those hard-assignment creates discontinuities in the feature space between the clusters, impeding gradients to flow properly. To circumvent this issue,</p><formula xml:id="formula_2">NetVLAD [2] introduces a soft-assignment? k (x i ) for the samples {x i } N i=1</formula><p>, based on their distance to each cluster center. Formally:</p><formula xml:id="formula_3">a k (x i ) = e ?? xi?c k 2 K k =1 e ?? xi?c k 2<label>(2)</label></formula><p>a k (x i ) ranges between 0 and 1, with the highest value assigned to the closest center. ? is a temperature parameter that controls the softness of the assignment, a high value for ? (e.g. ? ? ? +?) would lead to a hard assignment like in VLAD. Furthermore, by expanding the squares and noticing that e ?? xi 2 will cancel between the numerator and the denominator, we can interpret Equation <ref type="formula" target="#formula_3">(2)</ref> as the softmax of a convolutional layer for the input features parameterized by w k = 2?c k and b k = ?? c k 2 . Formally:?</p><formula xml:id="formula_4">k (x i ) = e w T k xi+b k k e w T k xi+b k<label>(3)</label></formula><p>Finally, by plugging the soft-assignment from (3) into the VLAD formulation in (1), the NetVLAD features are defined as in Equation <ref type="formula">(4)</ref>, later L2-normalized per cluster, flattened and further L2-normalized in its entirety.</p><formula xml:id="formula_5">V (j, k) = N i=1 e w T k xi+b k k e w T k xi+b k (x i (j) ? c k (j)) (4)</formula><p>Note that the original VLAD optimizes solely the cluster centers c k , while NetVLAD optimizes for {w k }, {b k } and {c k } independently, dropping the constraint of w k = 2?c l and b k = ?? c k 2 . These constraints were similarly dropped in <ref type="bibr" target="#b2">[3]</ref>, arguing for further freedom in the training process. Efficient implementation. Implementing NetVLAD with libraries such as Tensorflow or Pytorch could lead to several memory challenges in mini-batch training. In particular, the formulation in (4) would lead to a 4-dimensional tensor, in particular due to the residuals (x i (j) ? c k (j)), defined with a batch size (B), a set size (N ), a number of clusters (K) and a features dimension (D). With small considerations in Equation <ref type="formula">(4)</ref>, in particular splitting the residual in the two operands like in Equation <ref type="formula" target="#formula_6">(5)</ref>, leads to a difference of two 3D tensors only, reducing the memory footprint as well as the computational complexity. Empirically, we experienced a ?5? speed up in computation (backward and forward) and a similar reduction for the memory footprint.</p><formula xml:id="formula_6">V (j, k) = N i=1? k (x i )(x i (j) ? c k (j)) = N i=1? k (x i )x i (j) ? N i=1? k (x i ) c k (j)<label>(5)</label></formula><p>Pooling for Action Spotting. We follow a similar architecture structure proposed in SoccerNet <ref type="bibr" target="#b18">[19]</ref>. In particular, we learn to classify whether specific actions occurs within a temporal window. For inference, we densely slide the temporal window along the video to produce a class-aware actionness, on top of which we apply a non-maximum suppression (NMS). The frame features are pre-computed and pre-reduced in dimension with PCA, then pooled along the sliding window to predict the actionness of the central frame. Yet, SoccerNet <ref type="bibr" target="#b18">[19]</ref> does not optimize the dimensionality reduction for the end-task and the pooling method is not aware of the temporal order of the frame features, nor consider past and future context independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">NetVLAD++: Temporally-aware pooling</head><p>We propose a temporally-aware pooling module dubbed NetVLAD++ as our primary contribution. The VLAD and NetVLAD pooling methods are permutation invariant, as a consequence, do not consider the order of the frames, but only aggregates the features as a set. In the particular case of action spotting, the frames features from the videos are temporally ordered in time, and can be categorized between past and future context.</p><p>As noted by Cioppa et al. <ref type="bibr" target="#b6">[7]</ref>, the amount of context embedded before and after an action occurs is different, yet complementary. In addition, we argue that different actions might share similar vocabulary either before or after those actions occur, but usually not both. As an example, the semantic information contained before a "goal" occurs and before a "shot on/off target" occurs are similar, representing a lower level semantic concept of a player shooting on a target and a goalkeeper trying to catch that ball. Yet, those two action classes depict different contextual semantics after it occurs, with the presence of cheering (for "goal") or frustration (for simple "shot") in the players. Following a similar logic, the spotting of a "penalty" would benefit more from the knowledge of what happened before that penalty was shot, as the follow-up cheering would look similar to any other goal. Without loss of generality, it appears that the amount of information to pool among the features before and after an action occurs might contain different low-level semantics, helping identifying specific fine-grained actions.</p><p>To that end, we propose a novel temporally-aware pooling module, dubbed NetVLAD++, as depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. In particular, we learn 2 different NetVLAD pooling modules for the frame features from before and after an action occurs. We define the past context as the frame feature with a temporal offset in [?T b , 0[ and the future context as the frame feature with a temporal offset in [0, T a ]. Each pooling module aggregates different clusters of information from the 2 subsets of features, using K a and K b clusters, respectively for the after and before subsets. Formally:</p><formula xml:id="formula_7">V = (V b , V a )<label>(6)</label></formula><p>with an aggregation of V b and V a that represent the NetVLAD pooled features for the sample before and after the action occurs, parameterized with K b clusters for the past context and K a clusters for the future context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Architecture for Action Spotting</head><p>We integrated our novel pooling module into a larger architecture depicted in <ref type="figure" target="#fig_1">Figure 2</ref>, follows a similar structure presented in SoccerNet <ref type="bibr" target="#b18">[19]</ref>. In particular, it is based on a pre-trained frame feature encoder, a dimensionality reduction, a pooling module from a temporally sliding window and a per-frame classifier that depicts a class-aware actionness. The action spotting is then performed using a non-maximum suppression (NMS). The main difference with SoccerNet <ref type="bibr" target="#b18">[19]</ref> are twofold: an end-to-end learnable dimensionality reduction layer different from PCA and a temporally-aware pooling module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video encoding.</head><p>We use the features extracted by SoccerNet-v2, based on ResNet-152 <ref type="bibr" target="#b19">[20]</ref> pre-trained on Im-ageNet <ref type="bibr" target="#b14">[15]</ref>. The weights are frozen and the frame features are pre-extracted at 2fps with a resolution of 224x224, scaled down in height then cropped on the sides for the width. The features correspond to the activation of the last layer of the ResNet-152 architecture, after the max pooling across the 2D feature map and before the classification layer, resulting in features of dimension 2048. We consider those features as input for the remaining of the architecture.</p><p>Dimensionality reduction. The dimension of the features are reduced from 2048 to 512, following SoccerNet <ref type="bibr" target="#b18">[19]</ref> that learned a PCA reduction to that dimension. We argue that a linear layer would learn a better linear combination of the frame features, by removing the orthogonality constraint introduced by PCA. We refer to the experiments to appreciate the boost in performances. Moreover, learning a PCA reduction is feasible offline only, hence not practical for online training as it require the feature to be pre-extracted.</p><p>Temporally-aware pooling. We consider window chunks of time T s along the video. The temporally contiguous set of features are split equally before and after the center of the window and pooled accordingly. We normalize the features along the feature dimension and apply the 2 NetVLAD module for each subset of features. The 2 output NetVLAD features are concatenated along the feature dimension, leading into a feature of dimension (K b + K a ) ? D. <ref type="table">Table 1</ref>. State-of-the-art comparison. We report the results of NetVLAD++ for action spotting (Average-mAP %) on SoccerNet-v2 <ref type="bibr" target="#b9">[10]</ref>. We report the performances of our best model over 5 runs and detail its performances for each action class.</p><p>SoccerNet-v2 Video Chunk Classification. In training, we consider nonoverlapping window chunks with a sliding window of stride T . We build a classifier on top of the pooled feature, composed of a single neural layer with sigmoid activation and dropout. Since multiple actions can occur in the same temporal window, we consider a multi-label classification approach. A video chunk is labeled with all classes that appear on the chunk with a multi-label one-hot encoding. Similar to SoccerNet <ref type="bibr" target="#b18">[19]</ref>, we optimize for a multi-label binary cross-entropy loss as defined in Equation <ref type="formula" target="#formula_8">(7)</ref>.</p><formula xml:id="formula_8">L = 1 N N i=1 y i log (x n ) + (1 ? y i ) log (1 ? x n )<label>(7)</label></formula><p>Inference. We run the sliding window of time T along unseen videos with a temporal stride of 1 to report the classaware actionness scores in time. Similar to <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref>, we use a Non Maximum Suppression (NMS) module to reduce positive spots closer than a given temporal threshold T N M S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Architectural design. We set our temporally-aware NetVLAD pooling to have as many parameters and similar complexity as a traditional NetVLAD pooling. We refrain on adapting the size of the vocabulary for the context before and after the action occurs, nor share the clusters between the 2 pooling layers. As a result, we not only enforce K = K a + K b , but also set K a = K b = K/2. Similarly, we set T a = T b = T /2 and consider the same amount of temporal context from before and after the actions. We reached highest performances by setting a temporal window T = 15s and K = 64 clusters. Finally, we suppress duplicate spottings around the highest confidence score with a NMS considering a centered window of T N M S = 30s. Training details. We use the Adam <ref type="bibr" target="#b27">[27]</ref> optimizer with default ? parameters from PyTorch and a starting learning rate of 10 ?3 that we decay from a factor of 10 after the validation loss does not improve for 10 consecutive epochs. We stop the training once the learning rate decays below 10 ?8 . Typically, a training converges in ?100 epochs corresponding to ?2h on a GTX1080Ti with a memory footprint of ?1GB. Note that such footprint does not account for the extraction of the ResNet-152 frame features, that were preextracted. The code is available at https://soccer-net.org/.</p><p>Dataset and metrics. We train our novel architecture with the learnable dimensionality reduction and the temporallyaware pooling module on the SoccerNet dataset using the recent annotations with 17 classes from SoccerNet-v2 <ref type="bibr" target="#b9">[10]</ref> and the recommended train/val/test split (300/100/100 games). We consider the action spotting Average-mAP introduced by SoccerNet <ref type="bibr" target="#b18">[19]</ref>, that considers the average precision (AP) for the spotting results per class within a given tolerance ?, averaged per class (mAP). The mAP are further averaged over tolerances ranging from 5s to 60s using a step size of 5s as per common practice <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">37</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Main Results</head><p>The main performances of our action spotting architecture based on NetVLAD++ are compared in <ref type="table">Table 1</ref> with the current state-of-the-art for action spotting on SoccerNet-v2. For our method, we report the best model over 5 runs, as per common practice in video understanding <ref type="bibr" target="#b0">[1]</ref>, yet report a standard deviation contained within 0.2%. The main metric Average-mAP exhibits a boost of 12.7% w.r.t the previous state-of-the-art method CALF <ref type="bibr" target="#b6">[7]</ref>. The improvement is consistent across 16 over the 17 classes of actions, where only the class Goal displayed worst performances. All the methods reported in <ref type="table">Table 1</ref> leverage ResNet-152 features extracted at 2fps (in addition of VGGish audio features for AudioVid <ref type="bibr" target="#b41">[41]</ref>). Each of those baselines have different ways to deal with the frame features to solve for action spotting. Ablation for the size of the NMS window NetVLAD (SNv1) NetVLAD (SNv2) NetVLAD (PCA) NetVLAD (lin.layer) NetVLAD++ (PCA) NetVLAD++ (lin.layer) <ref type="figure">Figure 3</ref>. Ablation on the size of the NMS window. We report the performances of the 4 models presented in <ref type="table" target="#tab_1">Table 2</ref>, with different size for the NMS window. Each entry point is averaged over 5 runs overlaid with max and min performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Ablation Study</head><p>Our improvement originates from 3 main differences w.r.t NetVLAD: (i) an optimized NMS head to extract spotting results, (ii) a linear layer for the frame features vs. a PCA reduction and (iii) a temporally-aware pooling module. We ablate each component in <ref type="table" target="#tab_1">Table 2</ref> with performances averaged over 5 runs. <ref type="figure">Figure 3</ref> ablates the window size for the NMS and illustrates in transparency the variation between best and worst performances (yet contained).</p><p>Optimal setup for NetVLAD. First, we optimized the hyper-parameters for NetVLAD-like pooling methods. In particular, we identified 2 components that boost further the performances reported in SoccerNet-v2 <ref type="bibr" target="#b9">[10]</ref>: the NMS head and the size of the sliding window T . Soccer-Net <ref type="bibr" target="#b18">[19]</ref> only considered spotting predictions with confidence scores higher than 0.5, depicting a lower-bound estimation of the performances (31.4%), as shown in <ref type="figure">Figure 3</ref> (black) and <ref type="table" target="#tab_1">Table 2</ref> (NetVLAD). Considering all action spots without the threshold constraint on the confidence score leads to 42.0% Avg-mAP (+10.6%). Yet, further finetuning the size of the NMS window leads to 47.1% Avg-mAP (+5.1%), as shown in <ref type="figure">Figure 3</ref> (grey) and <ref type="table" target="#tab_1">Table 2</ref> (+NMS*). Finally, optimizing the size of the window T from 20s to 15s leads to an Avg-mAP of 48.4% (+1.3%), as shown in <ref type="figure">Figure 3</ref> (purple) and <ref type="table" target="#tab_1">Table 2</ref> (+NMS*/T*).  <ref type="figure">(Figure 3 (red)</ref>). Ablation per class. <ref type="table" target="#tab_1">Table 2</ref> further depicts the performances per class. The linear layer mostly improves the performances for the visible instances of actions w.r.t the PCA dimensionality reduction, yet displays a drop for the unshown instances. We argue the unshown actions are diverse hence challenging to learn from. In contrast, the PCA reduction is generic and unsupervised, hence does not not suffer from the challenging input data. Regarding the temporal-awareness, the improvement is consistent regardless of the action visibility. Most classes appears to provide optimal results with NetVLAD++. Yet, Clearances, Kick-Off, Yellow and Red cards appear not to benefit from the temporal-awareness. We hypothesize that that those actions are already discriminative enough without it. Similarly, Substitution and Kick-Off do not benefit from the linear projection. Here, we believe the visual cue for those actions are global enough to not require a trainable projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Clusters and Temporal Windows</head><p>We further investigate the choice of hyper-parameters for NetVLAD++, in particular the size of the temporal windows T <ref type="table" target="#tab_1">(Table 2</ref>) and the number of cluster K <ref type="figure" target="#fig_2">(Figure 4</ref>).</p><p>For the temporal windows, T = 15s appears to be optimal for the Average-mAP, yet specific action classes could benefit from different temporal boundaries. We believe that considering a different temporal window per class could lead to optimal results, yet impractical in our architecture that consider a single window to pool features from. As for the number of clusters, the more the better, yet the performances appears to saturate after 64 clusters. In fact, K define the size of the vocabulary to cluster the pooled features but the vocabulary can only improve up to a certain extent. Furthermore, note that an increase in the number of cluster irrevocably leads into an increase in the number of parameters. The classifier process the NetVLAD features of dimension K * D in a fully connected layer parameterized with (K * D + 1) weight and biases per class. Practically, we refrain on using a large vocabulary K as it leads to large number of parameters and chose K = 64 in the design of our architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">More Video Encoders</head><p>Most related works consider ResNet-152 for the video feature encoder <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref>. SoccerNet <ref type="bibr" target="#b18">[19]</ref> provides alternative I3D <ref type="bibr" target="#b5">[6]</ref> and C3D <ref type="bibr" target="#b38">[38]</ref> video features, yet showed worst performances <ref type="bibr" target="#b18">[19]</ref>. In <ref type="table" target="#tab_2">Table 3</ref>, we show that our temporally-aware pooling NetVLAD++ transfers well to I3D and C3D, boosting NetVLAD with 6.6% and 2.5%, respectfully. More recent video encoders such as R3D <ref type="bibr" target="#b43">[43]</ref> and R(2+1)D <ref type="bibr" target="#b40">[40]</ref> could lead to higher performances, but due to the computational complexity of pre-extracting frame features, we leave that for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">More Temporally-Aware Pooling Modules</head><p>We further transfer our temporally-aware pooling to further pooling modules. In particular, we implement temporally-aware Max, Average and NetRVLAD pooling modules as reported in <ref type="table" target="#tab_3">Table 4</ref>. We refrained in optimizing the performances for each module, and only highlight the relative improvement brought by the temporal awareness.</p><p>We developed MaxPool++ and AvgPool++ based on MaxPool and AvgPool with an extra temporal awareness. We considered the PCA-reduced ResNet features as the low number of parameters for those models (9234 parameters each) impeded a stable learning on top of higher dimensionality features. Note that MaxPool++ and AvgPool++ concatenates the past and future context (twice the dimensionality) which inevitably leads to a similar increase in parameters (18450 parameters each). MaxPool++ and AvgPool++ displayed impressive boosts in performances (+7.9% and +8.1% resp.) now flirting with performances similar to previous baselines proposed in SoccerNet-v2 <ref type="bibr" target="#b9">[10]</ref>, yet leveraging ?20? less parameters for the spotting head.</p><p>Following previous experiments on residual-less NetVLAD <ref type="bibr" target="#b18">[19]</ref>, we developed NetRVLAD++ on top of NetRVLAD, which drops the cluster parameters c k (j) in (4), leading to slightly less parameters to learn. We build NetRVLAD++ on top of full ResNet feature with our learnable feature projection. The relative improvement here is similar (+3.1%), yet the performances are not on par (?2.2%) with NetVLAD++, highlighting the importance of the NetVLAD residuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Temporal-awareness vs. CALF <ref type="bibr" target="#b6">[7]</ref> vs. RMS-Net <ref type="bibr" target="#b37">[37]</ref>. NetVLAD++ is not the first approach that considers temporal semantic regions around the action to spot. CALF <ref type="bibr" target="#b6">[7]</ref> defines a high-level semantic context from different temporal regions far distant, just before and just after an action occurs. They introduce a hand-crafted loss function that weights the contextual information. Still, they leverage the same features for the context before and after the actions occurs. In contrast, we drop the far distant semantic context in NetVLAD++ and learn specific features from different vocabulary (NetVLAD clusters) for the past and future temporal context. RMS-Net <ref type="bibr" target="#b37">[37]</ref> propose a similar contextual approach borrowed from the NLP literature that masks out part of the temporal context. In particular, they drop the past information during training, expecting the model to focus exclusively on the future frames. In contrast, we learn both past and future temporal context independently on NetVLAD++, and merge both learned context. More temporal regions. We considered extending the temporal region beyond the close past and future contexts, following insights from CALF <ref type="bibr" target="#b6">[7]</ref> that considered far distant temporal segments. Our experiments with far before and far after temporal contexts did not lead to any improvement for the learning of the pooling module and inevitably increases the number of hyper-parameters defining those temporal regions. We believe the temporal context before and after are discriminative enough, while the far distant equivalent are more blurry in time with the close context. Also, each action class might consider different temporal context for the far distant, which would lead to more confusion for the learnable pooling layers. We believe a global video feature or a better temporal aggregation of the features across the complete video could lead to a better temporal understanding and would take care of the far distant temporal context. Spotting Regression. Both CALF <ref type="bibr" target="#b6">[7]</ref> and RMS-Net <ref type="bibr" target="#b37">[37]</ref> learn to regress action spots. We decided not to regress the actions spot but rather rely on a dense sliding windows with an NMS to discard non-optimal action spots. A dense sliding window inevitably leads to slower inference, yet NetVLAD++ takes &lt;1 second to infer a complete 90min soccer game from pre-extracted features. SoccerNet-v2 Challenge. We tested our approach on the segregated challenge set of SoccerNet-v2. For the competition, we trained on the train+val sets, validated on the test set and inferred on the challenge set, that we submitted on the evaluation server. At submission time <ref type="table" target="#tab_4">(Table 5)</ref>, we reached SOTA performances with 52.54% Avg-mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we proposed a temporally-aware learnable pooling module for the task of action spotting on soccer videos. We first showed that NetVLAD can further be optimized on SoccerNet-v2. We further improve the poolingbased action spotting architecture by learning a linear projection that reduce the features dimension and split the past and future features to pool, leading to state-of-the-art performances on the SoccerNet-v2 benchmark. We show a complete ablation and transfer capability of our contribution to any pooling layer and input features, paving the road for more temporally-aware learning in video. We believe future works should focus on integrating local frame features from low-level semantics (player, ball, field, etc...) and consider complete videos rather than temporally-bounded clips as input. Future works should learn to accumulate knowledge in time or based on attention models in order to reach higher-level of understanding in soccer broadcasts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>NetVLAD (top) vs. NetVLAD++ (bottom) pooling modules for action spotting. Our temporally-aware NetVLAD++ pooling module learns specific vocabularies for the past and future semantics around the action to spot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Action spotting architecture based on our novel temporally aware pooling module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Ablation for the Vocabulary Size. The spotting performances are plotted in blue, averaged over 5 runs. Light blue illustrate the range of Average-mAP (min/max). Red indicated the number of parameters. More clusters increase the vocabulary but also the number of parameters, with a saturation after K = 64. NetVLAD++. The learnable linear layer exhibits in Figure 3 (yellow) and Table 2 (w/o tmp-aware) a +1.8% boost w.r.t the best optimized NetVLAD (50.2% Avg-mAP). Similarly, the temporally-aware pooling exhibit in Figure 3 (blue) and Table 2 (w/o lin.layer) a +2.3% boost w.r.t the best optimized NetVLAD (50.7% Avg-mAP). Our final NetVLAD++ displays an Avg-mAP of 53.4%, a boost of 4.9% w.r.t to the best optimized NetVLAD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>MaxPool [19] 18.6 21.5 15.0 38.7 34.7 26.8 17.9 14.9 14.0 13.1 26.5 40.0 30.3 11.8 2.6 13.5 24.2 6.2 0.0 0.9 NetVLAD [19] 31.4 34.3 23.3 47.4 42.4 32.0 16.7 32.7 21.3 19.7 55.1 51.7 45.7 33.2 14.6 33.6 54.9 32.3 0.0 0.0</figDesc><table><row><cell></cell><cell>visible</cell><cell>unshown</cell><cell>Ball out</cell><cell>Throw-in</cell><cell>Foul</cell><cell>Ind. free-kick</cell><cell>Clearance</cell><cell>Shots on tar.</cell><cell>Shots off tar.</cell><cell>Corner</cell><cell>Substitution</cell><cell>Kick-off</cell><cell>Yellow card</cell><cell>Offside</cell><cell>Dir. free-kick</cell><cell>Goal</cell><cell>Penalty</cell><cell>Yel.?Red</cell><cell>Red card</cell></row><row><cell cols="20">AudioVid [41] 39.9 43.0 23.3 54.3 50.0 55.5 22.7 46.7 26.5 21.4 66.0 54.0 52.9 35.2 24.3 46.7 69.7 52.1 0.0 0.0</cell></row><row><cell>CALF [7]</cell><cell cols="19">40.7 42.1 29.0 63.9 56.4 53.0 41.5 51.6 26.6 27.3 71.8 47.3 37.2 41.7 25.7 43.5 72.2 30.6 0.7 0.7</cell></row><row><cell>NetVLAD++</cell><cell cols="19">53.4 59.4 34.8 70.3 69.0 64.2 44.4 57.0 39.3 41.0 79.7 68.7 62.1 56.7 39.3 57.8 71.6 79.3 3.7 4.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation Studies. (Top) Main Contributions: We highlight the improvement of each component of our novel [NetVLAD++] module and architecture on top of [NetVLAD], with optimal NMS parameters [NMS*] and optimal window size T [NMS*/T*]. We highlight the contribution of the learnable linear layer [w/o lin.layer] and the temporally-aware feature pooling [w/o tmp-aware]. All performances are averaged over 5 runs. (Bottom) Temporal context: We report the Average-mAP for temporal window size T ranging from 5 to 30, averaged over 5 runs. Best performances per class are reported in bold. T = 15s appears to be optimal. 52.8 28.6 66.9 68.6 46.5 36.1 50.2 34.9 41.1 81.2 58.9 54.7 51.7 9.5 58.6 45.2 64.3 12.5 1.2 T = 10s 50.7 57.1 34.5 70.2 70.1 61.5 42.8 53.7 37.3 39.3 81.9 66.6 59.1 55.1 26.8 58.3 63.1 68.6 5.8 1.7 T = 15s 53.3 59.1 35.1 70.2 68.9 64.1 45.2 56.6 38.2 40.4 79.8 68.9 61.1 56.1 38.0 58.2 71.6 79.1 5.5 3.5</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>SoccerNet-v2</cell><cell>visible</cell><cell>unshown</cell><cell>Ball out</cell><cell cols="2">Throw-in</cell><cell>Foul</cell><cell>Ind. free-kick</cell><cell>Clearance</cell><cell>Shots on tar.</cell><cell>Shots off tar.</cell><cell>Corner</cell><cell>Substitution</cell><cell>Kick-off</cell><cell>Yellow card</cell><cell>Offside</cell><cell>Dir. free-kick</cell><cell>Goal</cell><cell>Penalty</cell><cell>Yel.?Red</cell><cell>Red card</cell></row><row><cell cols="2">Ablation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Main Contributions</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">NetVLAD</cell><cell></cell><cell cols="18">31.4 34.2 23.5 46.9 41.2 31.3 17.4 34.2 18.5 19.1 55.6 50.9 46.7 31.4 17.8 34.2 54.5 33.9 0.0 0.0</cell></row><row><cell cols="2">+ NMS*</cell><cell></cell><cell cols="18">47.1 52.3 34.8 60.2 57.5 52.8 38.8 54.5 36.2 36.0 72.4 66.7 63.4 49.6 33.0 50.6 66.3 55.0 2.4 4.5</cell></row><row><cell cols="2">+ NMS*/T*</cell><cell></cell><cell cols="18">48.4 54.2 32.5 62.8 60.0 53.8 38.8 56.2 36.6 37.7 76.7 67.2 62.5 51.8 30.8 51.2 67.0 60.2 3.8 5.2</cell></row><row><cell cols="2">NetVLAD++</cell><cell></cell><cell cols="18">53.3 59.1 35.1 70.2 68.9 64.1 45.2 56.6 38.2 40.4 79.8 68.9 61.1 56.1 38.0 58.2 71.6 79.1 5.5 3.5</cell></row><row><cell cols="21">w/o tmp-aware 50.2 56.6 32.2 64.2 61.3 54.4 39.4 56.6 37.3 39.6 77.6 66.1 60.7 56.4 32.7 55.6 66.4 64.3 4.5 16.9</cell></row><row><cell cols="2">w/o lin.layer</cell><cell></cell><cell cols="18">50.7 55.8 37.3 68.2 65.3 62.4 43.4 56.0 37.1 38.3 78.9 70.3 59.6 50.0 35.3 55.2 70.2 67.7 1.7 1.5</cell></row><row><cell cols="2">Window Size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Temporal context</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="21">T = 05s 46.0 T = 20s 53.0 58.3 35.1 67.5 66.1 62.2 44.5 56.4 38.5 39.5 77.2 68.9 59.1 54.8 39.0 57.0 73.4 78.6 10.3 7.1</cell></row><row><cell cols="2">T = 25s</cell><cell></cell><cell cols="18">50.7 55.6 34.9 64.2 62.8 59.4 45.0 55.4 38.9 37.3 71.0 65.2 59.6 54.5 39.1 54.0 70.9 63.7 16.5 4.6</cell></row><row><cell cols="2">T = 30s</cell><cell></cell><cell cols="18">49.4 53.9 35.1 60.1 57.5 54.7 43.2 51.6 38.3 35.9 65.7 62.1 59.3 55.2 39.4 53.8 70.8 75.5 9.4 7.5</cell></row><row><cell>Average-mAP (Action Spotting)</cell><cell>25 30 35 40 45 50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>20</cell><cell>30</cell><cell cols="4">40 Size of the NMS window (s) 50 60</cell><cell>70</cell><cell>80</cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>More video encoder: Spotting performance using I3D, C3D and ResNET-152 video encoders, averaged over 5 runs (means ? std). NetVLAD++ with linear layer for dimensionality reduction results in best performances for all encoders.</figDesc><table><row><cell>Pooling</cell><cell cols="3">NetVLAD NetVLAD++ NetVLAD++</cell></row><row><cell>Encoder</cell><cell>(PCA)</cell><cell>(PCA)</cell><cell>(lin layer)</cell></row><row><cell>I3D</cell><cell>34.9 ? 0.3</cell><cell>38.1 ? 0.1</cell><cell>41.5 ? 0.1</cell></row><row><cell>C3D</cell><cell>46.1 ? 0.3</cell><cell>47.2 ? 0.2</cell><cell>48.6 ? 0.8</cell></row><row><cell>ResNet</cell><cell>48.4 ? 0.2</cell><cell>50.7 ? 0.2</cell><cell>53.3 ? 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>More pooling modules. Spotting performances using Max, Avg and NetRVLAD pooling modules. All temporallyaware pooling method outperforms the original pooling.</figDesc><table><row><cell>Pooling</cell><cell>Original</cell><cell>Tmp.-Aware</cell></row><row><cell>MaxPooling (PCA)</cell><cell cols="2">23.7 ? 0.4 31.6 ? 0.7</cell></row><row><cell>AvgPooling (PCA)</cell><cell cols="2">32.5 ? 0.1 40.6 ? 0.2</cell></row><row><cell cols="3">NetRVLAD (lin.layer) 48.0 ? 0.2 50.9 ? 0.3</cell></row><row><cell>NetVLAD (lin.layer)</cell><cell cols="2">50.2 ? 0.6 53.3 ? 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>SoccerNet-v2 Challenge. Our NetVLAD++ approach reach best performances on the SoccerNet</figDesc><table><row><cell>Method</cell><cell cols="3">Avg-mAP Visible Unshown</cell></row><row><cell>NetVLAD [19]</cell><cell>30.74</cell><cell>32.99</cell><cell>23.27</cell></row><row><cell>CALF [7]</cell><cell>42.22</cell><cell>43.51</cell><cell>37.91</cell></row><row><cell>RMS-Net [37]</cell><cell>49.66</cell><cell>53.11</cell><cell>38.92</cell></row><row><cell>NetVLAD++</cell><cell>52.54</cell><cell>57.12</cell><cell>46.15</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tsp: Temporally-sensitive pretraining of video encoders for localization tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11479,2020.5</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5297" to="5307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">All about VLAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1578" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-person 3d pose estimation and tracking in sports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Bridgeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Yves</forename><surname>Guillemaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-Person 3D Pose Estimation and Tracking in Sports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Bridgeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Yves</forename><surname>Guillemaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2487" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A context-aware loss function for action spotting in soccer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Cioppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Deli?ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rikke</forename><surname>Gade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="13126" to="13136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ARTHuS: Adaptive Real-Time Human Segmentation in Sports Through Online Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Cioppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Deli?ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Istasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><forename type="middle">De</forename><surname>Vleeschouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2505" to="2514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A bottom-up approach based on semantics for the interpretation of the main camera stream in soccer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Cioppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Deli?ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1846" to="1855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Deli?ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Cioppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meisam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><forename type="middle">V</forename><surname>Seikavandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Dueholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Nasrollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Droogenbroeck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13367</idno>
		<title level="m">Soccernet-v2: A dataset and benchmarks for holistic understanding of broadcast soccer videos</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Length of sports tv broadcast hours in the united states from</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deloitte</surname></persName>
		</author>
		<ptr target="https://www.statista.com/statistics/290110/length-sports-tv-programming-available-usa/.1" />
	</analytic>
	<monogr>
		<title level="m">Statista -The Statistics Portal</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Retrieved</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sports media rights market size in north america from 2006 to 2023 (in billion u.s. dollars)*. In Statista -The Statistics Portal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deloitte</surname></persName>
		</author>
		<ptr target="https://www.statista.com/statistics/194225/sports-media-rights-revenue-in-north-america/.1" />
		<imprint>
			<date type="published" when="2021-01-11" />
		</imprint>
	</monogr>
	<note>Retrieved</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Volume of sports programs on free television in france between</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deloitte</surname></persName>
		</author>
		<ptr target="https://www.statista.com/statistics/1016830/sports-television-hourly-volume-france/" />
	</analytic>
	<monogr>
		<title level="m">Statista -The Statistics Portal</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Retrieved</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deloitte</surname></persName>
		</author>
		<ptr target="https://www.statista.com/statistics/1025759/hourly-volume-sports-pay-tv-france/.1" />
		<title level="m">Statista -The Statistics Portal</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Retrieved</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust camera calibration for sport videos using court models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Farin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Krabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Peter De With</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Effelsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Storage and Retrieval Methods and Applications for Multimedia</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="80" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What will happen next? Forecasting player moves in sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3362" to="3371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohieddine</forename><surname>Amine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarek</forename><surname>Dghaily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Detecting the moment of completion: temporal models for localising action completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farnoosh</forename><surname>Heidarivincheh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Mirmehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02310</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sports field localization via deep structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namdar</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="4012" to="4020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Istasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><forename type="middle">De</forename><surname>Vleeschouwer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Associative Embedding for Team Discrimination</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2477" to="2486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Soccerdb: A large-scale database for comprehensive video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leilei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changliang</forename><surname>Xu</surname></persName>
		</author>
		<idno>Octo- ber 2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Multimedia Content Analysis in Sports</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A survey on player tracking in soccer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Manafifard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Ebadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid Abrishami</forename><surname>Moghaddam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="19" to="46" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Computer Vision in Sports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Soccer on your tabletop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="4738" to="4747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time detection of events in soccer videos using 3D convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Nerg?rd</forename><surname>Olav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">A</forename><surname>Rongved</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vajira</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thambawita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>H?kon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evi</forename><surname>Stensland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dag</forename><surname>Zouganeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?l</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Halvorsen</surname></persName>
		</author>
		<idno>press. 2</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Multimedia (ISM)</title>
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Using player&apos;s body-orientation to model pass feasibility in soccer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adri? Arbu?s Sang?esa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3875" to="3884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generation of Ball Possession Statistics in Soccer Using Minimum-Cost Flow Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amlan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipti Prasad</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2515" to="2523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Soccer: Who has the ball? Generating visual analytics and player statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajkumar</forename><surname>Theagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bir</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1830" to="1838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Computer vision for sports: Current applications and research topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rikke</forename><surname>Gade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Rms-net: Regression and masking for soccer event spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Tomei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Bronzin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07624</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved soccer action spotting using both audio and video streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastien</forename><surname>Vanderplaetse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Event detection in coarsely annotated sports videos via parallel multi-receptive field 1d convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanav</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnaz</forename><surname>Fani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Clausi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zelek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="882" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5783" to="5792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust player detection and tracking in broadcast soccer video based on enhanced particle filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="81" to="94" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Comprehensive dataset of broadcast soccer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiping</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zikai</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengyou</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Na</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Refining joint locations for human pose tracking in sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Einfalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Lienhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2524" to="2532" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
