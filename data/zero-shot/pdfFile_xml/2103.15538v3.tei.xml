<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SUTD-TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Systems Technology and Design</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Huang</surname></persName>
							<email>hehuang@mymail.sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Information Systems Technology and Design</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
							<email>junliu@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Information Systems Technology and Design</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SUTD-TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traffic event cognition and reasoning in videos is an important task that has a wide range of applications in intelligent transportation, assisted driving, and autonomous vehicles. In this paper, we create a novel dataset, SUTD-TrafficQA (Traffic Question Answering), which takes the form of video QA based on the collected 10,080 in-the-wild videos and annotated 62,535 QA pairs, for benchmarking the cognitive capability of causal inference and event understanding models in complex traffic scenarios. Specifically, we propose 6 challenging reasoning tasks corresponding to various traffic scenarios, so as to evaluate the reasoning capability over different kinds of complex yet practical traffic events. Moreover, we propose Eclipse, a novel Efficient glimpse network via dynamic inference, in order to achieve computation-efficient and reliable video reasoning. The experiments show that our method achieves superior performance while reducing the computation cost significantly. The project page: https://github.com/SUTDCV/ SUTD-TrafficQA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Intelligent transportation <ref type="bibr" target="#b64">[65]</ref> has been receiving increasing attention recently, and for the applications, such as assisted driving, violation detection, and congestion forecasting, accurate and efficient cognition and reasoning over the traffic events captured by video cameras is extremely important. As shown by previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref>, well-designed datasets are often crucial for the development, adaptation and evaluation of different data-driven approaches. This indicates the significance of creating comprehensive and challenging benchmarks for video causal reasoning and cognitive development of models, that explore the underlying causal structures of various traffic events. To this end, we introduce a novel dataset, SUTD-* Corresponding Author.</p><p>TrafficQA (Traffic Question Answering), to facilitate the research of causal reasoning in complex traffic scenarios.</p><p>In our dataset, to help develop models for addressing several major and concerning issues in intelligent transportation, we design 6 challenging reasoning tasks, which require exploring the complex causal structures within the inference process of the traffic events. As shown in <ref type="figure">Figure  1</ref>, these tasks correspond to various traffic scenarios involving both road-agents and surroundings, and the models are required to forecast future events, infer past situations, explain accident causes, provide preventive advice, and so on.</p><p>To present these reasoning tasks, video question answering <ref type="bibr" target="#b66">[67]</ref> is a natural and effective choice, and is used for our dataset construction, since to accurately answer the given questions, the models need to acquire strong capabilities of performing various levels of logical reasoning and spatiotemporal cognition for the events.</p><p>Besides providing the challenging and useful reasoning tasks, we adopt a combination scheme of online collection and offline capturing to collect videos, such that the data in our benchmark covers various traffic events, diversified road-agents and surroundings, and different capturing perspectives in the wild. With the provided various tasks and diverse videos, our dataset shall be able to serve as a comprehensive benchmark for video reasoning of traffic events.</p><p>In some application scenarios, (e.g., assisted driving), the computational resource and energy budget can be constrained. Thus both the inference accuracy and the computation efficiency are important for video event reasoning in these scenarios. Existing video QA methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31]</ref> mainly focus on strengthening the reasoning accuracy without emphasizing much efficiency, and most of these works apply fixed computation pipelines to answer different questions, while ignoring to conduct adaptive and efficient computation resource allocation based on the logic structure behind reasoning over video events.</p><p>In this paper, to achieve reliable and efficient video reasoning, we propose Eclipse, an Efficient glimpse network. Specifically, considering there is often large redundancy</p><p>The blue truck hit the while sedan from the back.</p><p>The white sedan crashed into the blue truck.</p><p>The blue truck braked hard suddenly.</p><p>The white sedan lost its control.</p><p>( Showing model the video from 00:26 onwards only. )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reverse Reasoning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: Will the white sedan crash into the highway's safety barrier?</head><p>Yes, very likely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No, very unlikely.</head><p>There is no white sedan at the scene.</p><p>There is no safety barrier at the scene. Yes, the road is not congested at the first place, and the accident is not related to the density of the vehicles on the road.</p><p>No, there is no accident.</p><p>No, fewer vehicles would have provided enough space to safely avoid the accident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Counterfactual Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: What is the type of the road?</head><p>Expressway Alley Boulevard Avenue Basic Understanding <ref type="figure">Figure 1</ref>. An example of our SUTD-TrafficQA dataset showing that a white sedan had missed the highway exit. Hence it chose to change the lane illegally for driving towards the exit. To avoid collision, the blue truck had to brake suddenly, and then an accident occurred. Six reasoning tasks are designed, covering a broad range of inference problems from basic understanding to complex reasoning and attribution analysis. To accurately answer these questions, the models need to explore the causal, logic, and spatio-temporal structures of the video event.</p><p>among video frames, via dynamic inference, our network adaptively determines where to skip and glimpse at each step, and what computation granularity needs to be allocated for the glimpsed frame. Such a dynamic reasoning scheme avoids feature extraction for the irrelevant segments in the video, and hence significantly reduces the overall computation cost towards reliable and efficient reasoning. It is noteworthy that both the determination of selecting a glimpse frame and the decision of computation granularity for each glimpse are essentially discrete operations, which are not trivial to optimize. To handle this issue, an effective joint Gumbel-Softmax mechanism is also introduced in this paper, which makes our Eclipse framework fully differentiable and end-to-end trainable.</p><p>To the best of our knowledge, this is the first work that simultaneously performs adaptive frame localization and feature granularity determination in a novel dynamic reasoning process for reliable and efficient causal reasoning and video QA. A joint Gumbel-Softmax operation is also introduced in this work to optimize the two decisions jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Intelligent Transportation. With the rapid development of deep learning techniques <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">50]</ref>, data-driven intelligent transportation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b4">5]</ref> has emerged as a prominent research topic. Lou et al. <ref type="bibr" target="#b33">[34]</ref> presented a dataset together with an adversarial learning model for vehicle reidentification. Different from existing intelligent transportation datasets and methods, in this paper, we investigate the problem of causal reasoning with video QA over various traffic scenarios. A new benchmark, SUTD-TrafficQA, together with a novel model, Eclipse, is proposed for this challenging task.</p><p>Video QA Datasets. Recently, there emerges a great interest in visual reasoning and question answering in videos <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b25">26]</ref>, and several video QA datasets <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b61">62]</ref> have been developed. Among them, MovieQA <ref type="bibr" target="#b47">[48]</ref> and TVQA <ref type="bibr" target="#b29">[30]</ref> present the movie and TVshow videos respectively with human-generated questions. More recently, CLEVRER <ref type="bibr" target="#b56">[57]</ref> focuses on collision event reasoning among several simple visual objects in a controlled environment using fully synthetic videos. Differently, our SUTD-TrafficQA focuses on reasoning over the complex traffic scenarios in the wild, where 6 challenging tasks for traffic event reasoning are introduced based on the diverse real-world traffic videos.</p><p>Video QA Methods. Extensive studies have been conducted for video QA <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b65">66]</ref>. Yu et al. <ref type="bibr" target="#b60">[61]</ref> employed LSTM to encode videos and QA pairs, and adopted an attention mechanism <ref type="bibr" target="#b57">[58]</ref>. Jang et al. <ref type="bibr" target="#b11">[12]</ref> used LSTMs with a different attention scheme to capture spatio-temporal patterns in videos. Different from existing video QA methods, our Eclipse model investigates the direction of learning an effective glimpse policy for adaptive reasoning to achieve reliable reasoning with computation efficiency.</p><p>Computation-Efficient Models. Recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b13">14]</ref> have pointed out the need of <ref type="table">Table 1</ref>. Comparison among SUTD-TrafficQA and some other video QA datasets. Providing challenging traffic-scenario reasoning tasks with real-world videos and human-generated QA pairs, our dataset shall serve as a comprehensive and challenging benchmark for video reasoning over traffic events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Synthetic Videos Real-World Videos CLEVRER <ref type="bibr" target="#b56">[57]</ref> MovieQA <ref type="bibr" target="#b47">[48]</ref> MSRVTT-QA <ref type="bibr" target="#b54">[55]</ref> TGIF-QA <ref type="bibr" target="#b11">[12]</ref> TVQA <ref type="bibr" target="#b29">[30]</ref> MarioQA <ref type="bibr" target="#b36">[37]</ref> Social-IQ <ref type="bibr" target="#b12">[13]</ref> SUTD-TrafficQA (Ours) Basic Understanding</p><p>Attribution improving the computation-efficiency when designing deep models, and different strategies, including filter pruning <ref type="bibr" target="#b31">[32]</ref>, weight sparsification <ref type="bibr" target="#b46">[47]</ref>, vector quantization <ref type="bibr" target="#b0">[1]</ref>, and dynamic routing <ref type="bibr" target="#b51">[52]</ref>, etc., have been proposed. In this paper, we propose an efficient model, Eclipse, the first model that performs dynamic inference and adaptive computation adjustment for video QA, which leverages an effective glimpse policy with the guidance of text and visual context information for both glimpse frame selection and computation granularity determination.</p><formula xml:id="formula_0">? ? Event Forecasting ? ? ? ? ? ? Reverse Reasoning ? ? ? ? ? ? ? Counterfactual Inference ? ? ? ? ? ? Introspection ? ? ? ? ? ? ? QA</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SUTD-TrafficQA Dataset</head><p>Our dataset contains 62,535 QA pairs and 10,080 videos of traffic scenes. Below we first propose 6 challenging traffic-related reasoning tasks, and then introduce the QA collection process and the dataset statistics. Basic understanding. This task evaluates the ability of the models in perceiving and understanding traffic scenarios at the basic level, which consists of multiple sub-tasks including feature-query (e.g., vehicle type, road situation, and environment description), event-query (e.g., accident existence, pedestrian action analysis, and events temporal relation), event classification (e.g., accident type), and counting (e.g., road-agent number). Event forecasting. This task requires a model to infer future events based on observed videos, and the forecasting questions query about the outcome of the current situation. Reverse reasoning. This task is to ask about the events that have happened before the start of a video segment. Counterfactual inference. This task queries the consequent outcomes of certain hypothesis (e.g., what if the blue sedan had not accelerated?). The hypothetical conditions do not occur in the video, so the model needs to reason about the imagined events under the designated condition.</p><p>Introspection. This task is to test if models are able to provide preventive advice (e.g., what could the pedestrian have done to avoid the collision with the car?). The candidate answers list actions that could have been taken to avoid traffic accidents or congestion.</p><p>Attribution. This task seeks the explanation about the causes of traffic events (e.g., what are the reasons of the rear-end crash?), so as to check if models are able to infer the underlying factors leading to the event.</p><p>We define all the above reasoning tasks as multiplechoice questions without limiting the number of candidate answers for each question. The number of candidate answers varies from 2 to 12 for different questions. We then sample among the candidate answers to balance the dataset and limit the occurrence of the same correct answers within each task to minimize language biases. As summarized in <ref type="table">Table 1</ref>, by introducing these challenging tasks, our dataset complements existing datasets, and facilitates the exploration of video QA in complex traffic scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">QA Collection</head><p>Videos. We collected videos by using a combination of online harvesting and offline capturing, to cover various real-world traffic scenarios. As for online video collection, a variety of video sharing platforms, based in different countries, are used to increase the diversity, including but not limited to YouTube, LiveLeak, Twitter, and Bilibili. More than nine thousand videos were thus collected from these online sources. As for offline video capturing, a set of videos were captured via handheld cameras by volunteers, while another set were fetched from car-mounted video recorders. These two sets of offline videos were then examined and trimmed into around one thousand video clips.</p><p>After combing the online videos and offline captured ones, we obtain a total of 10, 080 videos containing diversities in various aspects, including: a) different weathers (sunny/rainy/windy/snowy); b) different time (daytime/night); c) diverse road situations (congested/sparse, urban/rural roads); d) various traffic events (accidents, vehicle turning, pedestrian behaviors, traffic lights, etc.); e) different video perspectives (surveillance camera perspective/carmounted video perspective/hand-held camera perspective); and f) various clip lengths (from 1 to 70 seconds).</p><p>QA pairs. As the first step, the 6 tasks were explained to annotators. To ensure they fully understand the design principle of the tasks, multiple example questions were prepared for them to identify which task the questions belong to. Afterwards, each annotator was presented with batches of video folders, where each folder contains 100 clips that were randomly selected from the full video set. Annotators were asked to create at least 3 questions of the reasoning tasks for each video. We did not impose constraints on question formats to encourage annotators to keep their QA pairs diversified. Specifically, we encourage them to rephrase similar questions or candidate answers to push the models to learn underlying semantics of QA pairs rather than superficial language correlation. To ensure the quality of QA pairs, we cross-checked QA annotations on a weekly basis. In addition, we kept monitoring the distribution of different tasks in QA collection to maintain the balance and diversity of our dataset.  In this part, we present the statistics of our SUTD-TrafficQA dataset. <ref type="figure" target="#fig_2">Figure 2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dataset Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Eclipse Network</head><p>To deal with video reasoning, a common solution is to watch the full video and analyze the whole event information carefully. In this manner, generally, a fixed computation architecture <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b9">10]</ref> can be applied over the whole video for tackling each question. However, using a fixed network architecture for handling the video QA task is often computation-heavy and energy-consuming <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b42">43]</ref>, because the video sequence used for reasoning can be very long and contain plenty of frames for processing.</p><p>Recalling that, as humans, to analyze events in a video, we may not be patient enough to scrutinize the frames in the whole video, instead, we may adopt an adaptive information "foraging" strategy <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b6">7]</ref>. Concretely, we may use a "dynamic" inference manner to skip forth and back over the sequence to progressively infer and select some useful frames based on the task. Moreover, for the picked frames, we may examine a few of them very carefully while glancing over others. Such a dynamic and adaptive perception habit <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b6">7]</ref> frees us from watching the whole video thoroughly, and often enables fast yet still very accurate video reasoning at a very small frame usage.</p><p>Motivated by this, we aim to explore the direction of efficient and dynamic reasoning in complex traffic scenarios. Thus, we propose an Efficient glimpse (Eclipse) network for video QA, as illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>. Instead of using a fixed computation architecture over each video and question, our network learns to dynamically skip to and select a useful video frame at each inference step. Moreover, our network adaptively decides the feature computation granularity (i.e., coarse or fine) of the selected frame. To perform such a process, at each inference step, our network takes advantage of the guidance information including the QA pair, the currently selected frame and the historical cues for dynamic reasoning.</p><p>Specifically, as shown in <ref type="figure" target="#fig_4">Figure 3</ref>, in our network, to provide QA information for the Interaction Module, the QA Bank stores the representation of the QA pairs. At each inference step, to assist the dynamic reasoning of selecting the frame and the corresponding feature granularity, the Interaction Module leverages the QA information, the currently selected frame and the information from historically observed frames to derive an expressive representation, which then serves as the input of the dynamic reasoning process performed by the downstream modules, including the Prediction Module for outputting the reasoning result, and Glimpse-Determination Module for dynamically determining which frame to be observed at next step. Besides, the Exit-Policy Module also uses this representation to adaptively decide whether we can exit the reasoning process at current inference step. Via such a dynamic and recurring reasoning process, our network can derive reliable answer predictions with notable computation efficiency w.r.t. both the frame usage and feature computation granularity. We elaborate the network modules in detail below.</p><p>QA Bank. To provide the QA information as the guidance for dynamic reasoning, our QA Bank encodes the representation of the question via a bi-directional LSTM. By concatenating hidden states of the BiLSTM, the question representation can be denoted as H q ? R nq?2d , where n q is the number of words in the question, d denotes the dimension of LSTM hidden state. Similarly, we encode all the candidate answers as {H ai } N i=1 , where H ai ? R na i ?2d , N is the number of candidate answers, and n ai is the number of words in the answer a i . Interaction Module. To assist the subsequent dynamic reasoning with rich guidance information, we design an Interaction Module to fuse different kinds of available inputs. This module, consisting of a context-query sub-module and an interaction LSTM, recurrently interacts with a small number of frames selected from the full video sequence containing T frames. As shown in <ref type="figure" target="#fig_4">Figure 3</ref>, at each inference step t, this module fuses the QA information (H q and {H ai }), the currently selected frame (I t ), and the historical cues (h t?1 ) to produce an expressive representation (h t ) that can be used for dynamic reasoning.</p><p>More formally, at the t th inference step, to first fuse the textual QA information (H q and {H ai }) with the currently selected visual frame feature (I t ), we use a context-query sub-module to perform such a fusion process. We implement this sub-module by following the context-matching module of <ref type="bibr" target="#b29">[30]</ref>, which can effectively fuse the visual feature sequence with the textual sequence to produce a combined representation. Since the original method in <ref type="bibr" target="#b29">[30]</ref> takes a sequence of visual features as an input, and we have only the selected frame feature I t here, we treat I t as a visual feature sequence containing a single element. Hence the fusion process in this sub-module can be formulated as:</p><formula xml:id="formula_1">v i t = [I t ; F It,q ; F It,ai ; I t F It,q ; I t F It,ai ] (1)</formula><p>where represents element-wise product. F It,q and F It,ai are obtained by computing the similarity between the visual frame feature (I t ) and the textual features (H q and H ai ). More details of this process are referred to <ref type="bibr" target="#b29">[30]</ref> and also our supplementary. For simplicity, we use v t to represent the concatenated {v i t } N i=1 , as shown in <ref type="figure" target="#fig_4">Figure 3</ref>. The output v t , that incorporates the information of currently selected frame and the QA embedding, can be fed into the following interaction LSTM for more information interaction, as introduced below.</p><p>Besides fusing the QA information and the currently selected frame (I t ), to guide the dynamic reasoning at current inference step, we also incorporate the historical cues from past inference steps. Thus we design an interaction LSTM that takes the v t as the new input to interact with the histor-ical cues h t?1 as:</p><formula xml:id="formula_2">c t , h t = LST M (v t , c t?1 , h t?1 ; ? LST M )<label>(2)</label></formula><p>where ? LST M are parameters of LSTM. The generated hidden state h t encodes rich information of all available inputs and historical cues, and thus it can serve as an expressive representation to be fed into the downstream modules for dynamic reasoning as follows. Prediction Module. This module is used to generate the reasoning result (i.e. the probability distribution over candidate answers) at the current inference step t. This module computes the reasoning result as: p t = f p (h t ; ? p ), where p t ? R N represents the probability scores for all candidate answers. f p can be implemented with one fully-connected (FC) layer followed by a Sof tmax classifier.</p><p>Glimpse-Determination Module. At each inference step t, conditioned on h t , this module performs dynamic reasoning by making two decisions simultaneously. The first decision is to select which frame to be observed at next step, and the second is to decide whether to compute finegrained features or coarse features for this selected frame. Corresponding to these two decisions, we design the following two branches within this module.</p><p>The skip-policy branch selects the frame that we need to skip to at next inference step via the following process:</p><formula xml:id="formula_3">s t = f s (h t ; ? s ),</formula><p>where the output s t indicates the decision of the next frame location. Note that our network can skip forth and back over the entire video sequence, which is conceptually similar to the human reasoning process where we need to not only jump forward to find future informative frames but also go back to examine past information.</p><p>Besides determining next frame, this module also has a granularity-policy branch that adaptively decides the feature computation granularity for the next selected frame, formulated as: g t = f g (h t ; ? g ). The output g t , denotes the decision of feature granularity. In our implementation, we provide two kinds of feature granularity, namely, coarse features computed by a lightweight CNN; fine-grained features computed by a more representative yet computation-heavier CNN, to be chosen from. In the Glimpse-Determination module, both f s and f g are implemented with a FC layer.</p><p>Exit-Policy Module. To estimate when we can exit the reasoning process to achieve adaptive inference, we design the Exit-Policy Module. At each inference step t, this module decides if we can exit the reasoning process at current step based on the guidance information (h t ) as: e t = f e (h t ; ? e ), where the output e t denotes the confidence score of terminating the reasoning at current step. By training the exit-policy, our network can achieve adaptive inference, such that only a small and flexible number of frames are selected and computed on a per-video basis to derive reliable reasoning result.</p><p>Optimization. To optimize the above modules in our network, we introduce several loss functions. Specifically, at each inference step t, a cross-entropy loss L t pred is used to train the classifier of the Prediction Module:</p><formula xml:id="formula_4">L t pred = ? N n=1 y n log(p n t ) (3)</formula><p>where y is the ground-truth one-hot label vector for the candidate answers and p n t is the predicted score for the n th answer. As for Glimpse-Determination Module, to push the skip-policy branch to select a useful frame at each inference step, a simple yet effective loss is used:</p><formula xml:id="formula_5">L t incre = ?(m t ? m t?1 )<label>(4)</label></formula><p>where m t = p gt t ? max{p c t | c = gt} is the margin between the predicted probability of the correct answer (indexed by gt) and the largest probability of other candidate answers. We can simply infer that a larger m t indicates a more confident and accurate reasoning. Therefore, we use m t ? m t?1 to encourage the margin to keep growing over the inference steps, which indicates that at each step, we aim to select a useful frame to benefit our dynamic reasoning, considering that the confidence of our network for the correct answer increases when seeing the selected frames.</p><p>Meanwhile, to further save computation cost and prevent the granularity-policy branch from constantly using the computation-heavy fine features, we penalize the feature granularity policy when the fine feature is computed at each inference step t as follows:</p><formula xml:id="formula_6">L t f eat = g t<label>(5)</label></formula><p>where g t = 1 represents that the policy chooses to extract computation-heavy fine features for next step, while g t = 0 means it switches to extract computation-cheap coarse features for next step. To optimize our whole Glimpse-Determination Module, we incorporate the above two loss functions, L t incre and L t f eat , into a combined loss:</p><formula xml:id="formula_7">L t glimpse = L t incre + L t f eat<label>(6)</label></formula><p>Last but not least, we need to train the Exit-Policy to make a reliable decision if we can exit the reasoning process at current inference step. However, there are no groundtruth labels providing feedback on when our network can exit reasoning. Therefore, we leverage m t to generate dynamic labels to train Exit-Policy. Recalling that m t is the probability margin between the correct answer and the largest one of other candidate answers at each step, and m t is optimized to keep increasing under the constraint of Eqn. 4. Therefore, the gap between m t at different inference steps can be used to estimate the information gain of seeing more frames in our dynamic reasoning process. Given the pre-defined largest reasoning step T , the gap between m t (at the current step) and m T (of the final step) can estimate the value of remaining information gain by continuing reasoning till the end. Thus the gap can be used to determine whether our network can stop inference at t th step. When m t is very close to m T , this means the information gain by observing more frames is small, and thus we can exit reasoning in advance to reduce computation without incurring decrease in prediction accuracy.</p><p>In particular, at each step t, if m T ? m t &lt; ?(m T ? m 1 ), which means m t is close to m T , i.e., the estimated remaining information gain is small enough, we set the label, y t exit , as 1, indicating our model can exit reasoning at the t th inference step. Otherwise, the label is set to 0, representing we need to seek more frames. Here ? &gt; 0 controls how close m t should be to m T when the network exits inference. Conditioned on the estimated labels, y t exit , training Exit-Policy can be seen as a binary classification problem. Thus we train this module by minimizing a binary crossentropy loss:</p><p>L t exit = ?[y t exit log(e t ) + (1 ? y t exit ) log(1 ? e t )] (7) By combining Eqns <ref type="formula">(3)</ref>, <ref type="bibr" target="#b5">(6)</ref>, and <ref type="formula">(7)</ref>, the total loss function for each step t can be formulated as:</p><formula xml:id="formula_8">L t = L t pred + L t exit + ? * L t glimpse<label>(8)</label></formula><p>where ? is the weight of the combined loss function for optimizing the Glimpse-Determination Module. In our experiments, we compute the sum:</p><p>T t=1 L t from all inference steps as the final optimization objective.</p><p>Note that Eqn. (8) cannot be optimized directly with gradient descent, since the involved decisions of selecting frames and determining feature granularity in our dynamic reasoning are discrete, and sampling from discrete distribution makes the network non-differentiable. To address this issue, we introduce an effective joint Gumbel-Softmax operation.</p><p>Joint Gumbel Softmax. The original Gumbel-Softmax Sampling <ref type="bibr" target="#b8">[9]</ref> is an effective way to transform the original non-differentiable sample from a discrete distribution, to a differentiable decision from a corresponding Gumbel-Softmax distribution. In our task, to sample from the aforementioned two discrete distributions (namely, selecting frames and determining granularity) simultaneously, we here design an effective joint Gumbel-Softmax operation.</p><p>In particular, in the Glimpse-Determination Module, at each step t, we first derive the logits z ? R T * 2 by feeding the hidden state h t into a fully-connected layer. Then we use Sof tmax to obtain a categorical distribution ? t from z: ? t = p i,j | p i,j = exp(zi,j ) T c=1 2 k=1 exp(z c,k ) . With the Gumbel-Max trick <ref type="bibr" target="#b8">[9]</ref>, the discrete sample from the categorical distribution ? t can be defined as follows:</p><formula xml:id="formula_9">l t = arg max i?{1,...,T },j?{1,2} (log p i,j + g i,j )<label>(9)</label></formula><p>where g i,j = ? log(? log(u i,j )) denotes the Gumbel noise, and u i,j is the i.i.d. samples drawn from U nif orm(0, 1). We can further relax the non-differentiable operation argmax with sof tmax to facilitate gradient-based optimization:</p><formula xml:id="formula_10">l t = P i,j | P i,j = exp ((log p i,j + g i,j )/? ) T c=1 2 k=1 exp ((log p c,k + g c,k )/? ) , f or i ? {1, ..., T }, j ? {1, 2}<label>(10)</label></formula><p>where ? is the temperature parameter, which controls the smoothness of the sampling mechanism. When ? ? 0, the sampling approximates the argmax operation in Eqn. 9. The output l t incorporates the output of two decisions: the first dimension of l t denotes the decision of selecting the frame (i.e, the output s t of the skip-policy branch) and the second dimension of l t denotes the decision of the feature granularity of the selected frame (i.e, the output g t of the granularity-policy branch).</p><p>By using the outputs (s t and g t ) of the joint Gumbel-Softmax operation, our network manages to dynamically select the frame for next inference step and specify the feature granularity for the selected frame at each step. Therefore, by introducing the joint Gumbel-Softmax, our network can learn the two discrete policy decisions jointly in a fully differentiable way.</p><p>Training and testing. During training, we optimize our network within a fixed number of steps, which means the exit-policy is trained together with other modules but the exit decisions are not used. However, at the testing phase, if the exit-policy decides to stop reasoning at the t th inference step, we exit the model and use the current prediction result, p t , as the final reasoning result. In such a manner, our model achieves dynamic causal reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Given that the number of candidate answers for each question is not fixed in our dataset, we evaluate the performance of our network using binary and multi-choice setups. In binary case (denoted as Setting-1/2), the input to the model is a question with an answer, and the model needs to predict the correctness of this answer. In multi-choice setup (denoted as Setting-1/4), models are expected to select the correct answer from 4 candidate answers (i.e, 3 of them are incorrect). These two experiment setups can be treated as binary and four-class classification problems.</p><p>Implementation Details. We compute features from the penultimate layer of a pretrained ResNet-101 model <ref type="bibr" target="#b19">[20]</ref> as the fine-grained frame feature, and a pretrained Mo-bileNetv2 <ref type="bibr" target="#b41">[42]</ref> is used as the lightweight CNN to extract coarse features. In the QA Bank, we use Glove <ref type="bibr" target="#b37">[38]</ref> to embed QA text, and then use a BiLSTM with 150-Dimension hidden states to encode the textual sequence. As for the Interaction LSTM, the dimension of hidden states is 300. We implement the framework using Pytorch and adopts Adam <ref type="bibr" target="#b26">[27]</ref> with a learning rate of 3e-4 and a weight-decay of 1e-5. The ? in the Exit-Policy is set to 0.1 and ? is set to 0.01 in the loss function. We follow <ref type="bibr" target="#b8">[9]</ref> and set the initial temperature ? to 5, and gradually anneal it with an exponential decay factor of -0.045 in every epoch. According to evaluation statistics, our network shows a very fast inference speed of 16ms per testing video on a Nvidia RTX 2080Ti GPU.</p><p>We compare our network with the following baselines. Text-only models. These models only relying on text information without visual input, are relatively weak baselines used to assess language biases in our SUTD-TrafficQA. Qtype (random) randomly selects an answer from the answer space. QE-LSTM uses Glove <ref type="bibr" target="#b37">[38]</ref> to embed the input question and then encode it with LSTM <ref type="bibr" target="#b20">[21]</ref>. The final LSTM hidden state is passed to a MLP for predicting the correct answer. Different from QE-LSTM using questions only, QA-LSTM uses LSTM to encode both question embedding and answer embedding, and the final hidden states are used for predicting the answer.</p><p>Text+video models. We evaluate the following models that require both video and text inputs. VIS+LSTM <ref type="bibr" target="#b38">[39]</ref> uses LSTM to encode image representation and textual features. Since the original method takes a single image as input, we adapt this method by averaging features of all sampled frames in a video as the visual input. Avgpooling uses each frame with the encoded QA features to compute a prediction for each frame. We then perform mean pooling over all the frame predictions to obtain the final result. CNN+LSTM uses two LSTMs to encode both the video sequence and the QA text respectively. The two final hidden states are concatenated to predict the correct answer. I3D+LSTM uses I3D network <ref type="bibr" target="#b3">[4]</ref> to extract video motion features, and then fuse with QA textual features encoded by LSTM to compute the model prediction. TVQA <ref type="bibr" target="#b29">[30]</ref> is a multi-stream network to fuse input features from different modalities to answer the question. HCRN [10] adopts a Hierarchical Conditional Relation Networks to model sophisticated structure for reasoning over videos. BERT-VQA <ref type="bibr" target="#b55">[56]</ref> uses BERT <ref type="bibr" target="#b5">[6]</ref> to encode the visual and language information jointly to predict the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results and Analysis</head><p>The results in <ref type="table">Table 2</ref> demonstrate the minimal language biases in SUTD-TrafficQA, as the text-only baselines perform almost the same as the random choice. In contrast, the models using video input achieve obviously higher accuracy than text-only baselines. This demonstrates that to solve the reasoning tasks in our dataset, the model needs to associate visual content with linguistic cues to infer correct</p><formula xml:id="formula_11">Fine Coarse Coarse Coarse Fine ? ? ? ? ? ? ? ? ? ? ? ?</formula><p>Q: Why did the white sedan run away?</p><p>The white sedan caused the collision accident and then run away to escape the punishment. <ref type="table">(Eclipse prediction)</ref> The black sedan intended to hit the white sedan.</p><p>The white sedan was hit by the black sedan and then run away to catch the black sedan. The white sedan violated the traffic lights and then run away to escape the police.    . For a fair computation-efficiency comparison, we pick models requiring video input to compute GFLOPs per video, as the visual feature extraction consumes much computation budget, and the metric of GFLOPs is independent of hardware configurations. The results show our Eclipse achieves state-of-the-art reasoning accuracy with significantly improved computation efficiency. This verifies that compared to conventional video QA methods, through dynamic causal reasoning, our model effectively exploits the spatio-temporal and logical structure of video events to infer correct answers with much smaller frame usage and efficient feature computation. In addition, three volunteers who did not see the videos and questions before, were invited to pick correct answers, and we use the average prediction accuracy as Human performance. The discrepancy between neural networks and the human performance demonstrates the challenging nature of our dataset and the necessity of further research in video reasoning area. Ablation Study. As shown in <ref type="table">Table 3</ref>, compared with the model variant of using fine features only, by adopting the granularity-policy to adaptively decide feature granularity for the selected frame at each step, our network achieves nearly the same accuracy yet using much lower computation cost. Our network also achieves obviously higher reasoning accuracy than the method of using coarse features only. These results show the effectiveness of our granularity-policy by choosing fine features for most useful frames and coarse features for less important frames. Furthermore, we remove the skip-policy and simply uniformly sample frames at each step. As shown in <ref type="table" target="#tab_2">Table 4</ref>, our Eclipse performs the best yet at small computation cost. This shows that our skip-policy effectively reduces the computation cost by selecting useful frames for dynamic reasoning. Moreover, we present the frame location distributions for the first three inference steps of our network in <ref type="figure" target="#fig_6">Figure  5</ref>. As shown, our network selects frames dynamically for different videos as we expect. We also investigate the exitpolicy module by comparing it with the method of reasoning until the final inference step. The result in <ref type="table" target="#tab_3">Table 5</ref> shows that our model achieves best accuracy-to-computation ratio via adaptive inference. In <ref type="figure" target="#fig_5">Figure 4</ref>, we further present a qualitative example from our dataset to show how Eclipse performs dynamic and efficient reasoning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We create a new video QA dataset, SUTD-TrafficQA, focusing on video reasoning over traffic events. In our dataset, we introduce 6 reasoning tasks requiring various levels of causal reasoning. Besides, we propose the Eclipse network for video QA. By learning the dynamic glimpse policy and adaptive exit policy, our network achieves superior performance with significant computation efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>Showing model the video up to 00:22 only. ) Event Forecasting Q: Would the accident still happen if there were fewer vehicles on the road?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Distribution of question lengths (b) Distribution of question types based on beginning words (c) Distribution of question types based on reasoning tasks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Statistics of SUTD-TrafficQA. More in supplementary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) demonstrates the distribution of question length measured by number of words. The average length of questions is 8.6 words. Figure 2 (b)shows the various question types categorized by their beginning words, which implies the diversity of questions in our dataset.Figure 2(c) presents the split of questions in terms of reasoning tasks. Our dataset covers a broad range of traffic-related reasoning tasks requiring various levels of spatio-temporal understanding and causal reasoning in videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Architecture of Eclipse for dynamic causal reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>A qualitative example. The numbers above the selected frames show the order of the sequence selected by our network. It shows that our model selects informative frames dynamically and allocates large computation budget using fine features to most relevant frames for causal reasoning. More examples in supplementary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Distributions of frame location (Left to right:step1, 2, 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Results on SUTD-TrafficQA dataset. Results of removing granularity-policy in Eclipse.</figDesc><table><row><cell>Models</cell><cell></cell><cell>Setting-1/4</cell><cell>Setting-1/2</cell><cell>GFLOPs</cell></row><row><cell cols="2">Q-type (random)</cell><cell>25.00</cell><cell>50.00</cell><cell>-</cell></row><row><cell>QE-LSTM</cell><cell></cell><cell>25.21</cell><cell>50.45</cell><cell>-</cell></row><row><cell>QA-LSTM</cell><cell></cell><cell>26.65</cell><cell>51.02</cell><cell>-</cell></row><row><cell cols="2">Avgpooling</cell><cell>30.45</cell><cell>57.50</cell><cell>252.69</cell></row><row><cell cols="2">CNN+LSTM</cell><cell>30.78</cell><cell>57.64</cell><cell>252.95</cell></row><row><cell cols="2">I3D+LSTM</cell><cell>33.21</cell><cell>54.67</cell><cell>108.72</cell></row><row><cell cols="2">VIS+LSTM [39]</cell><cell>29.91</cell><cell>54.25</cell><cell>252.80</cell></row><row><cell cols="2">BERT-VQA [56]</cell><cell>33.68</cell><cell>63.50</cell><cell>266.77</cell></row><row><cell>TVQA [30]</cell><cell></cell><cell>35.16</cell><cell>63.15</cell><cell>252.11</cell></row><row><cell>HCRN [10]</cell><cell></cell><cell>36.49</cell><cell>63.79</cell><cell>2051.04</cell></row><row><cell>Eclipse</cell><cell></cell><cell>37.05</cell><cell>64.77</cell><cell>28.14</cell></row><row><cell>Human</cell><cell></cell><cell>95.43</cell><cell>96.78</cell><cell>-</cell></row><row><cell cols="5">Models Coarse Features Only Fine Features Only Eclipse (dynamic granularity)</cell></row><row><cell>Accuracy</cell><cell>34.35</cell><cell>37.16</cell><cell></cell><cell>37.05</cell></row><row><cell>GFLOPs</cell><cell>10.61</cell><cell>133.75</cell><cell></cell><cell>28.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Results of removing skip-policy in Eclipse. Uniform-n means uniformly sampling n frames from the video for reasoning.</figDesc><table><row><cell>Models</cell><cell>Uniform-10</cell><cell>Uniform-20</cell><cell cols="2">Uniform-40 Eclipse (skip-policy)</cell></row><row><cell>Accuracy</cell><cell>34.16</cell><cell>35.49</cell><cell>36.48</cell><cell>37.05</cell></row><row><cell>GFLOPs</cell><cell>32.17</cell><cell>51.19</cell><cell>68.41</cell><cell>28.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Results of removing exit-policy in Eclipse. Final-StepInference refers to that we remove the exit-policy and infer until the final step.</figDesc><table><row><cell>Models</cell><cell>Final-Step Inference</cell><cell>Eclipse (exit-policy)</cell></row><row><cell>Accuracy</cell><cell>37.11</cell><cell>37.05</cell></row><row><cell>GFLOPs</cell><cell>36.92</cell><cell>28.14</cell></row><row><cell>answers</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We would like to thank Yutian Lin, Renhang Liu, Yingjie Qiao, Xun Long Ng, Tran Nguyen Bao Long, Koh Kai Ting and Christabel Dorothy for their help in dataset collection and running baseline models. This work is supported by SUTD Projects PIE-SGP-Al2020-02 and SRG-ISTD-2020-153.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Soft-to-hard vector quantization for end-to-end learning compressible representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Cavigelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1141" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scaling to very very large corpora for natural language disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, ACL &apos;01</title>
		<meeting>the 39th Annual Meeting on Association for Computational Linguistics, ACL &apos;01<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="26" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient video classification using fewer frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shweta</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukundhan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aniket Bera, and Dinesh Manocha. Forecasting trajectory and behavior of road-agents using spectral clustering in graph-lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srujan</forename><surname>Panuganti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trisha</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4882" to="4890" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Skim reading by satisficing: evidence from eye tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen J</forename><surname>Duggan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Payne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1141" to="1150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Heterogeneous memory enhanced multimodal attention model for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eric</surname></persName>
		</author>
		<title level="m">Categorical reparameterization with gumbelsoftmax. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical conditional relation networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Minh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Beyond rnns: Positional self-attention with coattention for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tgif-qa: Toward spatio-temporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Social-iq: A question answering benchmark for artificial social intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Amir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive computationally efficient network for monocular 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yw Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatially adaptive computation time for residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1039" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Skim reading: an adaptive strategy for reading on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Fitzsimmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Weal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drieghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Web Science</title>
		<meeting>the ACM Conference on Web Science</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="211" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Knowledge-based video question answering with unsupervised scene descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08751</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowit vqa: Answering knowledge-based questions about videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayu</forename><surname>Otani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Fourth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Norvig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="12" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compositional attention networks for machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arad</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">In defense of grid features for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modality shifting attention network for multi-modal video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minuk</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multimodal dual attention memory for video story question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Min</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Ho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepstory: Video story qa by deep embedded memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Min</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Oh</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Ho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2016" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scsampler: Sampling salient clips from video for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6232" to="6242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01696</idno>
		<title level="m">Tvqa: Localized, compositional video question answering</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">TVQA+: Spatio-temporal grounding for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8211" to="8225" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Focal visual-text attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6135" to="6143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Veri-wild: A large dataset and a new method for vehicle re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3235" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A dataset and exploration of models for understanding video data through fill-in-theblank question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6884" to="6893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Led3d: A lightweight and efficient deep approach to recognizing low-quality 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Marioqa: Answering questions by watching gameplay videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghwan</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Hongsuck</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilchae</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2953" to="2961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="120" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><forename type="middle">Etzioni</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Green Ai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10597</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Explore multi-step reasoning in video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia (ACM MM)</title>
		<meeting>the ACM International Conference on Multimedia (ACM MM)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Energy and policy considerations for deep learning in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02243</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Endto-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sparsifying neural network connections for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4856" to="4864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Movieqa: Understanding stories in movies through questionanswering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Video relationship reasoning using gated spatio-temporal energy graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Movie question answering: Remembering the textual cues for layered visual contents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youjiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09412</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Skipnet: Learning dynamic routing in convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="409" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Liteeval: A coarse-to-fine framework for resource efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Adaframe: Adaptive frame selection for fast video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1278" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bert representations for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayu</forename><surname>Otani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haruo</forename><surname>Takemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2020-03" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Clevrer: Collision events for video representation and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01442</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Traffic accident benchmark for causality recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tackgeun</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">End-to-end concept word detection for video captioning, retrieval, and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3165" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Activitynet-qa: A dataset for understanding complex web videos via question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9127" to="9134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Leveraging video descriptions to learn video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuo-Hao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tseng-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Hong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Data-driven intelligent transportation systems: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1624" to="1639" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multi-turn video question answering via hierarchical attention context reinforced networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3860" to="3872" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Uncovering the temporal context for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="409" to="421" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
