<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">iFlow: Numerically Invertible Flows for Efficient Lossless Compression via a Uniform Coder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ryder</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">iFlow: Numerically Invertible Flows for Efficient Lossless Compression via a Uniform Coder</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It was estimated that the world produced 59ZB (5.9 ? 10 13 GB) of data in 2020, resulting in the enormous costs of both data storage and transmission. Fortunately, recent advances in deep generative models have spearheaded a new class of socalled "neural compression" algorithms, which significantly outperform traditional codecs in terms of compression ratio. Unfortunately, the application of neural compression garners little commercial interest due to its limited bandwidth; therefore, developing highly efficient frameworks is of critical practical importance. In this paper, we discuss lossless compression using normalizing flows which have demonstrated a great capacity for achieving high compression ratios. As such, we introduce iFlow, a new method for achieving efficient lossless compression. We first propose Modular Scale Transform (MST) and a novel family of numerically invertible flow transformations based on MST. Then we introduce the Uniform Base Conversion System (UBCS), a fast uniform-distribution codec incorporated into iFlow, enabling efficient compression. iFlow achieves state-of-the-art compression ratios and is 5? quicker than other high-performance schemes. Furthermore, the techniques presented in this paper can be used to accelerate coding time for a broad class of flow-based algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The volume of data, measured in terms of IP traffic, is currently witnessing an exponential yearon-year growth <ref type="bibr" target="#b12">[13]</ref>. Consequently, the cost of transmitting and storing data is rapidly becoming prohibitive for service providers, such as cloud and streaming platforms. These challenges increasingly necessitate the need for the development of high-performance lossless compression codecs.</p><p>One promising solution to this problem has been the development of a new class of so-called "neural compression" algorithms <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b40">41]</ref>. These methods typically posit a deep probabilistic model of the data distribution, which, in combination with entropy coders, can be used to compress data with the minimal codelength bounded by the negative log-likelihood <ref type="bibr" target="#b28">[29]</ref>. However, despite reliably improved compression performance compared to traditional codecs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34]</ref>, meaningful commercial applications have been limited by impractically slow coding speed.</p><p>In this paper, we focus on developing approaches with deep probabilistic models based on normalizing flows <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b22">23]</ref>. A normalizing flow admits a learnable bijective mapping between input data and a latent variable representation. In this paradigm, inputs can be compressed by first transforming data to latent variables, with the resulting output encoded by a prior distribution. Compared to other classes of generative models, normalizing flows typically perform best in both tasks of probability density estimation (as compared with variational autoencoders <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b7">8]</ref>) and inference speed (as compared with autoregressive factorizations <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b21">22]</ref>). This suggests that compression with flows can jointly achieve high compression ratios along with fast coding times. Unfortunately, lossless compression requires discrete data for entropy coding, and the continuous bijections of normalizing flows would not guarantee discrete latent variables. As such, transformations would require discretization, resulting in a loss of information <ref type="bibr" target="#b40">[41,</ref><ref type="bibr">3]</ref>. To resolve this issue, Integer Discrete Flows (IDF) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr">4]</ref>  <ref type="figure" target="#fig_0">(Fig. 1, left)</ref> proposed an invertible mapping between discrete data and latent variables. Similarly, iVPF <ref type="bibr" target="#b40">[41]</ref> achieves a discrete-space bijection using volume-preserving flows. However, the above models must introduce constraints on flow layers to ensure a discretespace bijection, which limits the expressivity of the transformation. Local Bits-Back Coding (LBB) <ref type="bibr" target="#b16">[17]</ref>  <ref type="figure" target="#fig_0">(Fig. 1, middle)</ref> was the first approach to succeed in lossless coding with the flexible family of continuous flows. It resolves the information loss by coding numerical errors with the rANS coder <ref type="bibr" target="#b11">[12]</ref>. However, the coder is extraordinarily slow, making the method impractical.</p><p>In this paper, we introduce iFlow: a numerically invertible flow-based neural compression codec that bridges the gap between high-fidelity density estimators and practical coding speed. To achieve this we introduce two novelties: Modular Scale Transform (MST) and Uniform Base Conversion Systems (UBCS). MST presents a flexible family of bijections in discrete space, deriving an invertible class of flows suitable for lossless compression. UBCS, built on the uniform distribution, permits a highly efficient entropy coding compatible with MST. The main ideas are illustrated in <ref type="figure" target="#fig_0">Fig. 1(right)</ref>. We test our approach across a range of image datasets against both traditional and neural compression techniques. Experimentally we demonstrate that our method achieves state-of-the-art compression ratios, with coding time 5? quicker than that of the next-best scheme, LBB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Numerically Invertible Flows and Lossless Compression</head><p>Let f : X ? Z be our normalizing flow, which we build as a composition of layers such that</p><formula xml:id="formula_0">f = f L ? ... ? f 2 ? f 1 . Defining d-dimensional y 0 =</formula><p>x ? X and y L = z ? Z, the latents can be computed as y l = f l (y l?1 ), with p X (x) calculated according to</p><formula xml:id="formula_1">log p X (x) = log p Z (z) + L l=1 log | det J f l (y l?1 )|,<label>(1)</label></formula><p>where J f l (x) is the Jocabian matrix of the transformation f l at y l?1 . In this paper, we use log to denote base 2 logarithms. Training the flow should minimize the negative log-likelihood ? log p X (x).</p><p>The inverse flow is defined as</p><formula xml:id="formula_2">f ?1 = f ?1 1 ? ... ? f ?1 L , such that f ?1 (z) = x.</formula><p>There are many types of invertible bijections, f l . Popular choices include element-wise, autoregressive (e.g. coupling layers) <ref type="bibr" target="#b40">[41]</ref> and 1 ? 1 convolution transformations <ref type="bibr" target="#b24">[25]</ref>.</p><p>To perform lossless compression with normalizing flows, the input data should be transformed to latent variables, which are then encoded with a prior distribution. As data and encoded bits should be in binary format, x and z must be discrete, with a bijection between discrete inputs and outputs established. However, as discussed, this operation is usually intractable with popular classes of transformations used in continuous flows due to the numerical errors induced by discretization.</p><p>In this section, we introduce a novel algorithm derived from continuous flows that allows us to achieve an exact bijiective mapping between discrete x and discrete z. We present the general idea of a flow layer's numerical invertibility in discrete space in Sec. 2.1. We then introduce our flow layers in Sec. 2.2 and 2.3, with their application to compression discussed in Sec. 2.4 and 2.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Invertibility of Flows in Discrete Space</head><p>To discretize the data, we follow <ref type="bibr" target="#b40">[41]</ref> and adopt k-precision quantization to assign floating points to discretization bins such thatx</p><formula xml:id="formula_3">= 2 k ? x 2 k ,<label>(2)</label></formula><p>where ? is the floor function. The associated quantization error is bounded by |x ? x| &lt; 2 ?k . Any d-dimensional x can be quantized into bins with volume ? = 2 ?kd . The probability mass ofx can then be approximated by P (x) = p(x)?, such that the theoretical codelength is ? log(p(x)?).</p><p>Denote our proposed numerically invertible flow (iFlow) according tof =f L ? ... ?f 1 . The input and output should be subject to k-precision quantization, where each layer should be appropriately invertible such that? l?1 ?f ?1 l (f l (? l?1 )). Denoting? 0 = x andz =? L , it is further expected that z =f (x) and x ?f ?1 (z). We will show in the following subsections thatf can be derived from any continuous flow f in such a way that the error betweenf (x) and f (x) is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Numerically Invertible Element-Wise Flows</head><p>We begin by describing our approach for element-wise flows z = f (x), where f is a monotonic element-wise transformation. For notation simplicity, we assume that the flow contains just one layer. In most cases, f does not present a unique inverse in discrete space, such that</p><formula xml:id="formula_4">f ?1 (z) = x.</formula><p>In what follows, we introduce an approach to derive an invertible, discrete-space operation from any element-wise flow transformation. For simplicity, we denote the inputs and output pairs of a continuous flow as x and z = f (x); and we denote the k-precision quantized input-output pairs of iFlow asx andz =f (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Scale Flow</head><p>Algorithm 1 Modular Scale Transform (MST): Numerically Invertible Scale Flow f (x) = R/S ? x.</p><p>Forward MST:z =f (x). 1:x ? 2 k ?x; 2: Decode r d using U (0, R);? ? R ?x + r d ; 3:? ? ?/S , re ?? mod S; 4: Encode re using U (0, S); 5: returnz ??/2 k .</p><p>Inverse MST:x =f ?1 (z). 1:? ? 2 k ?z; 2: Decode re using U (0, S);? ? S ?? + re; 3:x ? ?/R , r d ?? mod R; 4: Encode r d using U (0, R); 5: returnx ?x/2 k .</p><p>We begin with the scale transformation defined as z = f (x) = a ? x, (a &gt; 0), from which we are able to build more complex flow layers. The input can be converted to an integer withx = 2 k ?x, and the latent variablez can be recovered from integer? byz =?/2 k . Inspired by MAT in iVPF <ref type="bibr" target="#b40">[41]</ref>, we first approximate a with a fractional such that a ? R S , where R, S ? N. Denote? = R ?x + r d where r d is sampled uniformly from {0, 1, ..., R ? 1}, we can obtain? with? = ?/S , and the remainder r e =? mod R can be encoded to eliminate the error. It is clear that (x, r d ) ?? ? (?, r e ) are bijections.</p><p>Let U (0, A) be the uniform distribution such that P (s) = 1 A , s ? {0, 1, ..., A ? 1}. The numerically invertible scale flowf is displayed in Alg. 1. As the modular operation is essential to Alg. 1, we name it the Modular Scale Transform (MST). Setting S and R = round(S ? a) to be large, we observe that the following propositions hold.</p><formula xml:id="formula_5">Proposition 1. [41] |z ? f (x)| &lt; O(S ?1 , 2 ?k ). Proposition 2. The codelength of MST is L f (x,z) = log S ? log R ? ? log |a| = ? log |f (x)|.</formula><p>Proposition 1 establishes that the error is small if S and k are large. Proposition 2 demonstrates that the codelength is almost exactly the log-Jocabian of f . The above properties and the correctness of the algorithm are discussed in the Appendix. Further note that, with the exception of the usual encoding and decoding processes, MST's operations can be parallelized, resulting in minimal additional overhead. In contrast, MAT's operation in iVPF <ref type="bibr" target="#b40">[41]</ref> only deals with volume-preserving affine transform and is performed sequentially along dimensions, limiting its usage and efficiency. For simplicity, we assume the non-linear f is monotonically increasing. For a monotonically decreasing f , we simply invert the sign: ?f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">General Element-wise Flow</head><p>MST in Alg. 1 works with linear transformations, and is incompatible with non-linear functions. Motivated by linear interpolation, we can approximate the non-linear flow with a piecewise linear function. In general, consider inputx to be within an intervalx</p><formula xml:id="formula_6">? [x l ,x h ) 1 andz l = f (x l ),z h = f (x h ). The linear interpolation of f is then f inp (x) =z h ?z l x h ?x l (x ?x l ) +z l .<label>(3)</label></formula><p>It follows thatz can be derived from MST with inputx?x l followed by addingz l . Furthermore,z and x can be recovered with the corresponding inverse transformation. To preserve monotonicity,z must be within intervalz ? [z l ,z h ). Denoting the invertible linear flow derived from f inp asf inp , it must hold thatf inp (x l ) ?z l ,f inp (x h ?2 ?k ) ?z h ?2 ?k . As we use MST in Alg. 1, we observe that when</p><formula xml:id="formula_7">z h ?z l</formula><p>x h ?x l is approximated with R/S, the minimum possible value off inp (x l ) isz l (when r d = 0 in Line 2) and the maximum possible value off inp (</p><formula xml:id="formula_8">x h ?2 ?k ) is (R?2 k (x h ?x l ?2 ?k )+R?1)/(S?2 k ) +z l (when r d = R ? 1 in Line 2)</formula><p>. Given large S, the largest possible value of R should be</p><formula xml:id="formula_9">R = (2 k ? (z h ?z l ) ? 1) ? S + 1 2 k ? (x h ?x l ) .<label>(4)</label></formula><p>Another difficulty is in determining the correct interpolation interval [x l ,x h ). One simple solution is to split the input domain into uniform intervals with length 2 ?h such thatx l = (2 h ?x)/2 h ,x h = x l + 2 ?h . However, for the inverse computation givenz, it is not easy to recover the interpolation interval as</p><formula xml:id="formula_10">f ?1 (z) may not be within [x l ,x h ). Instead, asz ? [z h ,z l ), the interval [x l ,x h ) can be obtained via a binary search in whichx l is obtained withz ? [f (x l ), f (x h )).</formula><p>Another approach is to split the co-domain into uniform intervals such thatz l = (2 h ?z) </p><formula xml:id="formula_11">/2 h ,z h =z l + 2 ?h , with x l = f ?1 (z l ),x h = f ?1 (z h</formula><formula xml:id="formula_12">(x,z) = ? log(R/S) ? ? logz h ?z l x h ?x l ? ? log |f (x)|.</formula><p>It is possible to arrive at a similar conclusion in Proposition 1 where the corresponding error is |z ? f (x)| &lt; O(S ?1 , 2 ?k , 2 ?2h ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Practical Numerically Invertible Flow Layers</head><p>Whilst one can use many types of complex transformations to build richly expressive flow-based models, to ensure the existence and uniqueness of the inverse computation, these layers are generally constructed with element-wise transformations. In this subsection, we demonstrate the invertibility of discretized analogs to some of the most widely used flow layers using the operators as described in Sec. 2.2. Such flows include autoregressive flows <ref type="bibr" target="#b19">[20]</ref> (including coupling flows <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>) and 1 ? 1 convolutional flows <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Autoregressive and Coupling Flows</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Numerically Invertible Autoregressive Flow</head><p>Forward:z =f (x).</p><p>1: for i = m, ..., 1 do 2:zi ?fi(xi,x&lt;i) with Alg. 2 (forward); 3: end for</p><formula xml:id="formula_13">4: returnz = [z1, ...,zm] .</formula><p>Inverse:</p><formula xml:id="formula_14">x =f ?1 (z). 1: for i = 1, ..., m do 2:xi ?f ?1 i (zi,x&lt;i) with Alg. 2 (inverse); 3: end for 4: returnz = [z1, ...,zm] .</formula><p>Supposing that the inputs and outputs, x and z, are split into m parts</p><formula xml:id="formula_15">x = [x 1 , ..., x m ] , z = [z 1 , ..., z m ] , the autoregressive flow z = f (x) can be represented as z i = f i (x i ; x &lt;i ), where f i (?; x &lt;i ) is the element-wise flow (discussed in Sec. 2.2), conditioned on x &lt;i . Now letf i (?; x &lt;i )</formula><p>denote the invertible element-wise flow transformation as discussed in Alg. 1-2. Alg. 3 then illustrates the details of an invertible autoregressive flowf .</p><p>Propositions 1 and 2 hold in our discretized autoregressive transformation. In fact, the log-determinant of Jacobian is given by</p><formula xml:id="formula_16">log | det J f (x)| = m i=1 log | det J fi (x i ; x &lt;i )|, and the expected codelength is simply L f (x,z) = m i=1 L f (x i ,z i ) ? ? m i=1 log | det J fi (x i ;x &lt;i )| = ? log | det J f (x)|.</formula><p>When m = 2 and f 1 (x 1 ) = x 1 , the autoregressive flow is reduced to a coupling flow, which is widely used in flow-based models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>. It is therefore trivially clear that the coupling flow is additionally compatible with Alg. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">1 ? 1 Convolutional Flow</head><p>1 ? 1 convolutional layers can be viewed as a matrix multiplication along a channel dimension <ref type="bibr" target="#b24">[25]</ref>. Letx,z ? R c be inputs and outputs along channels, and W ? R c?c the weights of our network. The objective is to obtainz =f (x) where f (x) = Wx.</p><p>We use the ideas of iVPF <ref type="bibr" target="#b40">[41]</ref> to achieve a numerically invertible 1 ? 1 convolutional transformation. In particular, we begin by performing an LU decomposition such that W = PL?U. It then follows that the 1 ? 1 convolution is performed with successive matrix multiplications with U, ?, L and P. In iVPF, the authors extensively discussed matrix multiplications with factors U, L and P <ref type="bibr" target="#b40">[41]</ref>. Meanwhile, one can view the matrix multiplication with ? as a scale transform, such that f (x) = ? x, where is an element-wise multiplication and ? are the diagonal elements of ?. MST in Alg. 1 can then be applied. In such a case, it is clear that Proposition 1 and 2 hold for a 1 ? 1 convolutional flow. For Proposition 2, it is observed that</p><formula xml:id="formula_17">L f (x,z) ? ?sum(log ?) = ? log | det ?| = ? log | det W| = ? log | det J f (x)|.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Building Numerically Invertible Flows</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our flow model is constructed as a composition of layers</head><formula xml:id="formula_18">f = f L ? ... ? f 2 ? f 1 ,</formula><p>where each layer is a transformation of the type discussed in Sec. 2.2 and 2.3. Let us represent the resulting flow asf =f L ? ... ?f 1 , wheref l is a discretized transformation derived from the corresponding continuous f . It is clear that the quantized inputx(=? 0 ) and latentz(=? L ) establish a bijection with successive transformations between discrete inputs and outputs. For the forward pass,z is computed with? l =f (? l?1 ), l = 1, 2, ..., L; for the inverse pass,x is recovered with the inverse</p><formula xml:id="formula_19">flowf ?1 =f ?1 1 ? ... ?f ?1 L such that? l?1 = f ?1 l (? l ).</formula><p>For our resultant flow model, we can draw similar conclusions as in Propositions 1 and 2. Firstly, the error ofz =f (x) and</p><formula xml:id="formula_20">z = f (x) is small, bounded by |z ? z| &lt; O(LS ?1 , L2 ?k , L2 ?2h ). Secondly, the codelength is approximately L f (x,z) = L l=1 L f l (? l?1 ,? l ) ? ? L l=1 log | det J f l (? l?1 )|.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Lossless Compression with Flows via Bits-back Dequantization</head><p>Armed with our flow modelf , performing lossless compression is straight-forward. For the encoding process, the latent is generated according toz =f (x). We then encodez with probability p Z (z)?.</p><p>For the decoding process,z is decoded with</p><formula xml:id="formula_21">p Z (z)?, andx is recovered withf ?1 (z). The expected codelength is approximately ? log(p X (x)?) such that L(x) ? ? log(p Z (z)?) ? L l=1 log | det J f l (? l?1 )| ? ? log(p X (x)?).<label>(5)</label></formula><p>However, we note that if k is large, ? log ? = kd and the codelengths will also be large, resulting in a waste of bits. For what follows, we adopt the bits-back trick in LBB <ref type="bibr" target="#b16">[17]</ref> to reduce the codelength. In particular, consider coding with input data</p><formula xml:id="formula_22">x ? ? Z d , where a k-precision noise vector? ? [0, 1) d is decoded with q(?|x ? )? and added to input data such thatx = x ? +?.</formula><p>In this way,x is then encoded with our flowf . For the decoding process, x ? is recovered by applying the inverse transformation. We name this coding process Bits-back Dequantization, which is summarized in Alg. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 4 Lossless Compression with iFlow.</head><p>Encode</p><formula xml:id="formula_23">x ? . 1: Decode? using q(?|x ? )?; 2:z ?f (x ? +?); 3: Encodez using pZ (z)?. Decode. 1: Decodez using pZ (z)?; 2:x ?f ?1 (z), x ? ? x ; 3: Encode? =x ? x ? using q(?|x ? )?; 4: return x ? . In practice, q(u|x ? ) is constructed with a flow model such that u = g( ; x ? ) where ? p( ).</formula><p>u is decoded by first decoding? with p(? )?, and then applying? =?(? ; x ? ). Thus decoding? involves ? log(q(?|x ? )?) bits. Overall, the expected codelength is exactly the dequantization lower bound <ref type="bibr" target="#b18">[19]</ref> such that</p><formula xml:id="formula_24">L(x ? ) ? ? q(?|x ? )?[log(q(?|x ? )?) ? log(p(x ? +?)?)] ? E q(?|x ? ) [log q(?|x ? ) ? log p(x ? +?)].<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Extensions</head><p>With novel modifications, flow models can be applied to the generation of various data types, obtaining superior performance <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b22">23]</ref>. These models can be used for improved lossless compression. In general, given input data</p><formula xml:id="formula_25">x ? , these models generate intermediate data v with q(v|x ? ), and the density of v is modelled with flow model z = f (v). For generation, when v is generated with the inverse flow, x is generated with p(x ? |v). It is clear that p(x) can be estimated with variational lower bound such that log p(x ? ) ? E q(v|x ? ) [log P (x ? |v) + log p(v) ? log q(v|x ? )].</formula><p>For lossless compression, x ? can be coded with bits-back coding, which is similar with Alg. 4. In the encoding process, we first decodev with q(v|x ? )? and then encode x ? with P (x ? |v) (similar with Line 1 in Alg. 4-Encode).</p><p>We then obtain the priorz =f (v) (Line 2), before finally encodingz with p Z (z)? (Line 3). In the decoding process,z is firstly decoded with p Z (z)? (similar to Line 1 in Alg. 4-Decode), and then recoveredv and decoded x ? with P (x ? |v) (Line 2). Finally, we encode usingv with q(v|x ? ) <ref type="bibr" target="#b37">[38]</ref>. The expected codelength is approximately</p><formula xml:id="formula_26">L(x ? ) ? E q(v|x ? ) [log q(v|x ? ) ? log P (x ? |v) ? log p(v)].<label>(7)</label></formula><p>We introduce a selection of recent, state-of-the-art flow-based models modified according to the above. Each model corresponds to a certain coding algorithm.</p><p>VFlow <ref type="bibr" target="#b6">[7]</ref>. VFlow expands the input data dimension with variational data augmentation to resolve the bottleneck problem in the flow model.</p><formula xml:id="formula_27">In VFlow, v = [x ? + u, r](u ? [0, 1) d ), where u ? q u (u|x ? ), r ? q r (r|x ? +u), is modelled with flows g u , g r such that u = g u ( u ; x ? ), r = g r ( r ; x ? + u) ( u , r are priors). Then we have q(v|x ? ) = q u (u|x ? )q r (r|x ? + u), P (x ? |v) = 1 (as v ? (x ? + u) ? x ? )</formula><p>. Thus for the encoding process,? u ,? r are decoded. To constructv we hav? u =? u (? u ; x ? ),r =? r (? r ; x ? +?); and thenv is encoded with iFlow. For the decoding process,v is decoded with the inverse iFlow, and then x ? ,?,r is recovered withv. Here?,r is encoded and x ? is the decoded output. As VFlow achieves better generation results compared with general flows, one would expect a better compression ratio with VFlow.</p><p>Categorical Normalizing Flow <ref type="bibr" target="#b27">[28]</ref>. Categorical Normalizing Flows (CNF) succeed in modelling categorical data such as text, graphs, etc. Given categorical data</p><formula xml:id="formula_28">x ? = [x 1 , ..., x n ], x i ? {1, 2, ..., C} n , v = [v 1 , ..., v n ] is represented with word embeddings such that q(v i |x i ) = q e (v i |?(x i ), ?(x i )), in which q e could be a Gaussian or logistic distribution. Then P (x i |v i ) = p(xi)q(vi|xi) C c=1p (c)q(vi|c) withp being the prior over categories. Thus q(v|x ? ) = n i=1 q(v i |x i ), P (x ? |v) = n i=1 P (x i |v i )</formula><p>, and x ? can be coded with Alg. 8 given q(v|x ? ), P (x ? |v) and the iFlow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Uniform Base Conversion Systems</head><formula xml:id="formula_29">Algorithm 5 Uniform Base Conversion Systems ENCODE s using U (0, R)(R &lt; 2 K ). Input: symbol s, state c, bit-stream bs.</formula><p>Output: new state c and bit-stream bs. The previous section demonstrates that coding with a uniform distribution is central to our algorithm. Note that the distribution varies in each coding process, thus dynamic entropy coder is expected. Compared to the Gaussian distribution used in LBB <ref type="bibr" target="#b16">[17]</ref>, a uniform distribution is simpler, yielding improved coding speed. As follows, we introduce our Uniform Base Conversion Systems (UBCS), which is easy to implement and the coding bandwidth is much greater than that of rANS <ref type="bibr" target="#b11">[12]</ref>.</p><formula xml:id="formula_30">1: c ? c ? R + s; 2: if c ? 2 M +K then 3: bs.push_back(c mod 2 K ); push K bits to bit-stream. 4: c ? c 2 K ; 5: end if 6: return c, bs. DECODE with U (0, R)(R &lt; 2 K ).</formula><p>UBCS is implemented based on a number-base conversion. The code state c is represented as an integer. For coding some symbol s ? {0, 1, ..., R ? 1} with a uniform distribution U (0, R), the new state c is obtained by converting an R-base digit s to an integer such that</p><formula xml:id="formula_31">c = E(c, s) = c ? R + s.<label>(8)</label></formula><p>For decoding with U (0, R), given state c , the symbol s and state c are recovered by converting the integer to an R-base digit such that</p><formula xml:id="formula_32">s = c mod R, c = D(c , s) = c R .<label>(9)</label></formula><p>We note, however, that c will become large when more symbols are encoded, and computing Eq. (8-9) with large c will be inefficient. Similar to rANS, we define a "normalized interval" which bounds the state c such that c ? [2 M , 2 K+M ) (K, M are some integers values) after coding each symbol. For the encoding process, if c ? 2 K+M , the lower K bits are written to disk, and the remaining bits are reserved such that c ? c 2 K . For the decoding process, if c &lt; 2 M ? R, the stored K bits should be read and appended to the state before decoding. In this way, the decoded state is contained within the interval [2 M , 2 K+M ). The initial state can be set such that c = 2 M . We illustrate this idea in Alg. 5.</p><p>The correctness of Alg. 5 is illustrated in the following theorem. P1 demonstrates that the symbols can be correctly decoded with a UBCS coder, with coding performed in a first-in-last-out (FILO) fashion. P2 shows that the codelength closely approximates the entropy of a uniform distribution, subject to a large M and K. The proof of the theorem is in the Appendix. Theorem 3. Consider coding symbols s 1 , ...s n with s i ? U (0, R i ), (i = 1, 2, ..., n, R i &lt; 2 K ) using Alg. 5, and then decode s n , s n?1 , ..., s 1 sequentially. Suppose (1) the initial state and bit-stream are c 0 = 2 M , bs 0 = empty respectively; (2) After coding s i , the state is c i and the bit-stream is bs i ;</p><p>(3) After decoding s i+1 , the state is c i and the bit-stream is bs i . We have</p><formula xml:id="formula_33">P1: s i = s i+1 , c i = c i , bs i = bs i for all i = 0, ..., n ? 1.</formula><p>P2: Denote by the codelength l i = log c i + len(bs i ) where len is the total number of bits in the bit-stream. Then l n ? l 0 &lt; In practice, we set K = 32 and M = 4. Compared to rANS <ref type="bibr" target="#b11">[12]</ref> and Arithmetric Coding (AC) <ref type="bibr" target="#b39">[40]</ref>, UBCS is of greater efficiency as it necessitates fewer operations (more discussions are shown in the Appendix). UBCS can achieve greater computational efficiency via instantiating multiple UBCS coders in parallel with multi-threading. <ref type="table" target="#tab_3">Table 1</ref> demonstrates that UBCS achieves coding bandwidths in excess of giga-symbol/s -speed significantly greater than rANS.</p><formula xml:id="formula_34">1 1?1/(ln 2?2 M ?K) n i=1 R i + 1 + (ln 2 ? 2 M ) ?1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we perform a number of experiments to establish the effectiveness of iFlow. We will investigate: (1) how closely the codelength matches the theoretical bound; (2) the efficiency of iFlow as compared with the LBB <ref type="bibr" target="#b16">[17]</ref> baseline; (3) the compression performance of iFlow on a series low and high-resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Flow Architectures and Datasets</head><p>We adopt two types of flow architectures for evaluation: Flow++ <ref type="bibr" target="#b15">[16]</ref> and iVPF <ref type="bibr" target="#b40">[41]</ref>. Flow++ is a state-of-the-art model using complex non-linear coupling layers and variational dequantizations <ref type="bibr" target="#b18">[19]</ref>. iVPF is derived from a volume-preserving flow in which numerically invertible discrete-space operations are introduced. The models are re-implemented or directly taken from the corresponding authors. Unless specified, we use h = 12, k = 28 and set large S -around 2 <ref type="bibr" target="#b15">16</ref> , which we analyse further in the Appendix. To reduce the auxiliary bits in the bits-back coding scheme, we partition the d-dimensional data into b splits and perform MST (in Alg. 1 and 2) for each split sequentially. In this case, the auxiliary bits can be reduced to 1/b in MST. We use b = 4 in this experiment.</p><p>Following the lossless compression community <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>, we perform evaluation using toy datasets CIFAR10, ImageNet32 and ImageNet64. Results for alternate methods are obtained via re-implementation or taken directly from the corresponding papers, where available. We further test the generalization capabilities of iFlow in which all toy datasets are compressed with a model trained on ImageNet32. For benchmarking our performance on high-resolution images, we evaluate iFlow using CLIC.mobile, CLIC.pro 2 and DIV2k <ref type="bibr" target="#b0">[1]</ref>. For this purpose, we adopt our ImageNet32/64 model for evaluation, and process an image in terms of 32 ? 32 or 64 ? 64 patches, respectively. The experiment is conducted with PyTorch framework with one Tesla P100 GPU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compression Performance</head><p>For our experiments, we use the evaluation protocols of codelength and compression bandwidth. The codelength is defined in terms of the average bits per dimension (bpd). For no compression, the bpd is assumed to be 8. The compression bandwidth evaluates coding efficiency, which we define in terms of symbols compressed per unit time. <ref type="table" target="#tab_2">Table 2</ref> demonstrates the compression results on CIFAR10. Note that we only report the encoding time (results on decoding time are similar, and are available in the Appendix). For iVPF, we use settings almost identical to the original paper <ref type="bibr" target="#b40">[41]</ref> such that k = 14.</p><p>Firstly, we observe that, when using both the Flow++ and iVPF architectures, iFlow achieves a bpd very close to theoretically minimal codelength. When using Flow++, iFlow achieves identical performance as that of LBB. For the iVPF architecture, iFlow outperforms the underlying iVPF as it avoids the need to store 16 bits for each data sample.</p><p>Secondly, the coding latency highlights the main advantage of iFlow: we achieve encoding 5? faster than that of LBB and over 1.5? that of iVPF. In fact, the use of UBCS only represents 20% of the total coding time for all symbols (4.8ms in Flow++ and 1.6ms in iVPF). In contrast, the rANS coder of LBB commands over 85% of the total coding latency, which is the principal cause of LBB's impracticality. Indeed, <ref type="table" target="#tab_3">Table 1</ref> demonstrates that our UBCS coder achieves a speed-up in excess of 50? that of rANS (which results in a coder latency of 4.8ms in iFlow vs. 99.8ms in LBB).</p><p>Lastly, compared with LBB, iFlow necessitates fewer auxiliary bits. In fact, LBB requires crica 2k + log ? bits per dimension (for ? = 2 ?k and small ? in <ref type="bibr" target="#b16">[17]</ref>). Meanwhile, iFlow requires approximately k + 1 b log S, and 1 b log S is usually small with large b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the State-of-the-Art</head><p>To further demonstrate the effectiveness of iFlow, we compare the compression performance on benchmarking datasets against a variety of neural compression techniques. These include, L3C <ref type="bibr" target="#b29">[30]</ref>, Bit-swap <ref type="bibr" target="#b25">[26]</ref>, HilLoc <ref type="bibr" target="#b36">[37]</ref>, and flow-based models IDF <ref type="bibr" target="#b17">[18]</ref>, IDF++ <ref type="bibr">[4]</ref>, iVPF <ref type="bibr" target="#b40">[41]</ref>, LBB <ref type="bibr" target="#b16">[17]</ref>. We additionally include a number of conventional methods, such as PNG <ref type="bibr" target="#b4">[5]</ref>, FLIF <ref type="bibr" target="#b35">[36]</ref> and JPEG-XL <ref type="bibr" target="#b1">[2]</ref>.</p><p>Experiments on Low Resolution Images. Compression results on our described selection of datasets are available in left three columns of <ref type="table" target="#tab_4">Table 3</ref>. Here we observe that iFlow obtains improved compression performance over all approaches with the exception of LBB on low-resolution images, for which we achieve identical results.</p><p>Generalization. The last four rows in <ref type="table" target="#tab_4">Table 3</ref>   <ref type="table" target="#tab_4">Table 3</ref> display the results, which is observed that iFlow outperforms all compression methods across all available benchmarks. Note that as we crop patches for compression, the compression bandwidth is the same as that in small images like CIFAR10, i.e., 5 times faster than LBB and 30% speedup compared with iVPF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Dynamic Entropy coders, such as Arithmetic Coding (AC) <ref type="bibr" target="#b39">[40]</ref> and Asymmetric Numerical Systems (ANS) <ref type="bibr" target="#b11">[12]</ref>, form the basis of lossless compression. However, the binary search protocol required at decode time and their comparatively high number of numerical operations make them both relatively time-consuming. The term dynamic means that the data symbols are in different distributions, in this case, efficient entropy coders like Huffman coding <ref type="bibr" target="#b20">[21]</ref> and tANS <ref type="bibr" target="#b11">[12]</ref> are incompatible. Our proposed UBCS is dynamic coder, which requires only two operations per symbol, producing an faster algorithm than AC and ANS.</p><p>In order to utilise entropy coders, one must estimate the data distribution. For this purpose, the wider community has employed a variety of density estimators. One of the most popular, autoregressive models <ref type="bibr" target="#b34">[35]</ref>, estimates the joint density with per-pixel autoregressive factorizations. Whilst commonly achieving state-of-the-art compression ratios, the sequential pixel-by-pixel nature of encoding and/or decoding makes them impractically time-consuming. Alternatively, variational autoencoders (VAEs) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37</ref>] maximise a lower bound on the marginal data likelihood (otherwise known as the ELBO).</p><p>With the bits-back coding framework <ref type="bibr" target="#b37">[38]</ref>, the theoretical codelength is exactly equal to the ELBO. However, in most cases, VAE formulations typically produce inferior compression ratios as there exists a gap between the ELBO and the true data likelihood.</p><p>As discussed, flow-based models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b10">11]</ref>, which admit exact likelihood computation, represent an alternative route for density estimation. IDF <ref type="bibr" target="#b17">[18]</ref> and IDF++ <ref type="bibr">[4]</ref> proposed the titular integer discrete flow to preserve the existence and uniqueness of an invertible mapping between discrete data and latent variables. In a similar vein, iVPF <ref type="bibr" target="#b40">[41]</ref> achieved a mapping with volume-preserving flows. Here the remainders of a division operation are stored as auxiliary states to eliminate the numerical error arising from discretizing latent variables. However, all of these models must introduce constraints on the underlying transform, limiting their representational power. LBB <ref type="bibr" target="#b16">[17]</ref> was the first flowbased lossless compression approach to admit a broad class of invertible flows based on continuous transforms. LBB established this family of flexible bijections by introducing local bits-back coding techniques to encode numerical errors. However, LBB typical requires the coding of many such errors and does so with the ANS scheme, posing obvious challenges to computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Discussions</head><p>In this paper, we have proposed iFlow, a numerically invertible flow-based model for achieving efficient lossless compression with state-of-the-art compression ratios. To achieve this, we have introduced the Modular Scale Transform and Uniform Base Conversion Systems, which jointly permit an efficient bijection between discrete data and latent variables. Experiments demonstrate that the codelength comes extremely close to the theoretically minimal value, with compression achieved much faster than the next-best high-performance scheme. Moreover, iFlow is able to achieve state-of-the-art compression ratios on real-word image benchmarks.</p><p>We additionally consider the potential for extending iFlow. That is, recent advances in normalizing flows have achieved improved generation performance across various data types <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23]</ref>. We have discussed the possible extension to incorporate these advancements in Sec. 2.6, and consider its application as future work. We further recognise that compression aproaches, of which iFlow is one, present significant data privacy issues. That is, the generative model used for compression may induce data leakage; therefore, the codec should be treated carefully as to observe data-privacy laws.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head><p>A.1 Correctness of MST (Alg. 1)</p><p>Asx,z are quantized to k-precision,x andx form a bijection, and so do? andz. Thus MST is correct if and only ifx and? are valid bijections. In particular, denotingx d = (S ?? + r e )/R , r d d = (S ?? + r e ) mod R, we will showx d ?x, r d d ? r d . In fact, according to forward MST,? = (R ?x + r d )/S , r e = (R ?x + r d ) mod S, thus S ?? + r e = R ?x + r d . Considering r e is first accurately decoded by inverse MST (in Line 2), it is clear thatx d = (S ??+r e )/R = (R?x+r d )/R =x, and r d d = (S ??+r e ) mod R = (R?x+r d ) mod R = r d . Thus the correctness of MST is proven.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Propositions 1-2 in MST (Alg. 1)</head><p>Firstly, as S ? a ? 0.5 ? R &lt; S ? a + 0.5, we have |a ? R S | ? 0.5 </p><formula xml:id="formula_35">S = O(S ?1 ). Secondly, asz = (R ? 2 k ?x + r d )/S /2 k where r d ? [0, R), we havez &lt; (R ? 2 k ?x + R)/(S ? 2 k ) = R/S?x+(R?1)/(S?2 k ) andz &gt; (R?2 k ?x/S?1)/2 k = R/S?x?2 ?k , thus |z?R/S?x| &lt; O(2 ?k ).</formula><formula xml:id="formula_36">If the interpolation interval [x l ,x h ) and [z l ,z h )(z l = f (x l ),z l = f (x h )</formula><p>) are identical in both the forward and inverse processes, f inp is additionally identical in both the forward and inverse processes. Consequently, an exact bijection with MST algorithm is trivially achieved. Thus we principally seek to show that the interpolation interval can be correctly determined. Before the proof, it must be emphasised that the interpolation interval should demonstrate the following properties:</p><p>1. The interpolation interval is counted and covers the domain/co-domain.x l ,x h must be within the discretized set such thatx l ,x h ? X inp (e.g. X inp = {2 ?h ? n, n ? Z});</p><p>2. The interpolation interval must be not intersected. There does not exist x such that x ? [x l ,x h ) and x ? X inp \ {x l }.</p><p>Firstly, we show that in the forward computation,z will always be within [z l ,z h ). In fact,z =</p><formula xml:id="formula_37">R?2 k (x?x l )+r d S /2 k +z l where r d ? [0, R). Asx ?x l ,z ? R?0+0 S /2 k +z l =z l . Asx ?x h ?2 ?k , z ? R?2 k (x h ?x l ?2 ?k )+R?1 S /2 k +z l , following from Eq. (4) it is clear thatz ?z h ?2 ?k . Therefore, it holds thatz ? [z l ,z h ).</formula><p>In fact, by performing the Taylor expansion atx, we have f (</p><formula xml:id="formula_38">x l ) = f (x) + (x l ?x) ? f (x) + (x l ?x) 2 2 ? f (? l ) and f (x h ) = f (x) + (x h ?x) ? f (x) + (x h ?x) 2 2 ? f (? h ), where ? h , ? l ? (x l ,x h ). Firstly, f (x h ) ? f (x l ) = (x h ?x l ) ? f (x) + [ (x h ?x) 2 2 ? f (? h ) ? (x l ?x) 2 2 ? f (? l )]. As f (x), f (x) are bounded,x h ?x l ? O(2 ?h ), and we have f (x h ) ? f (x l ) = (x h ?x l ) ? f (x) + O(2 ?2h ). As |z l ? f (x l )| &lt; 2 ?k , |z l ? f (x l )| &lt; 2 ?k , then |z h ?z l ? (x h ?x l ) ? f (x)| &lt; O(2 ?k , 2 ?2h ). Finally, |f (x) ?z h ?z l x h ?x l | &lt; O(2 ?h , 2 h?k ). Secondly, by denoting g(x) = f (x h )?f (x l ) x h ?x l (x?x l )+f (x l ) and replacing f (x l ), f (x h ) with its Taylor expansion, we have g(x) = f (x) + 1 2(x h ?x l ) (x h ?x)(x ?x l )[(x h ?x)f (? h ) + (x ?x l )f (? l )]. As f (x) is bounded, |g(x) ? f (x)| &lt; O(2 ?2h ). Moreover, it is clear that |f inp (x) ? g(x)| &lt; O(2 ?k )</formula><p>, and as such it finally holds that</p><formula xml:id="formula_39">|f inp (x) ? f (x)| &lt; O(2 ?2h , 2 ?k ). For Proposition 1, with MST, we havez ?z l = R?2 k ?(x?x l )+r d S /2 k where r d ? [0, R), thus |z?z l ? R S ?(x?x l )| &lt; O(2 ?k )</formula><p>. Moreover, with Eq. (4), it is easy to arrive at thatz h ?z l ?2 ?k (1?S ?1 ) </p><formula xml:id="formula_40">x h ?x l ? S ?1 &lt; R S ?z h ?z l ?2 ?k (1?S ?1 ) x h ?x l , and therefore | R S ?z h ?z l x h ?x l | &lt; O(S ?1 , 2 h?k ). Overall, |z ? f inp (x)| = |(z ?z l ? R S ? (x ?x l )) + (( R S ?z h ?z l x h ?x l ) ? (x ?x l ))| &lt; O(S ?1 , 2 ?k ), and finally |z ? f (x)| ? |z ? f inp (x)| + |f inp (x) ? f (x)| &lt; O(S ?1 , 2 ?k , 2 ?2h ). For Proposition 2, with | R S ?z h ?z l x h ?x l | &lt; O(S ?1 , 2 h?k ), it is clear that the expected codelength is L f (x,z) = ? log(R/S) = ? logz h ?z l x h ?x l + O(S ?1 , 2 h?k ) = ? log |f (x)| + O(S ?1 , 2 h?k , 2 ?h ). Overall, L f (x,z) ? ? log |f (x)| if S,</formula><formula xml:id="formula_41">= 2 M ? [2 M , 2 K+M ). When i = k and c k ? [2 M , 2 K+M ), denote c ? k+1 = c k ? R k+1 + s k+1 . It is therefore clear that c ? k+1 = [2 M ? R k+1 , 2 M +K ? R k+1 ). Note that R k+1 &lt; 2 K and therefore 2 M ? R k+1 &lt; 2 M +K . If c ? k+1 &lt; 2 K+M , c k+1 = c ? k+1 ? [2 M ? R k+1 , 2 M +K ), it follows that c ? k+1 ? 2 K+M , c k+1 = c ? k+1 /2 K ? [2 M , 2 K ? R k+1 ). Overall, c k+1 ? [2 M , 2 K+M ). Thus c i ? [2 M , 2 K+M ) for all i = 1, ..., n such that c i = c i?1 ? R i + s i ? [2 M ? R i , 2 K+M ), c i?1 ? R i + s i &lt; 2 K+M ci?1?Ri+si 2 K ? [2 M , 2 M ? R i ), c i?1 ? R i + s i ? 2 K+M<label>(10)</label></formula><p>We will now demonstrate that s i = s i+1 , c i = c i , bs i = bs i for all i = 0, ..., n ? 1. Denote</p><formula xml:id="formula_42">c ? i = c i?1 ? R i + s i . (i) Consider i = n ? 1.</formula><p>(a) If c n &lt; 2 M ? R n , the last K bits (denoted by r n ) will be popped from bs n and added to c n . In this case, according to Eq. (10), r n = c ? n mod 2 K must be encoded to form bs n . Thus in the decoding process, r n is popped from bs n , and therefore bs n?1 = bs n?1 , c n?1 = (2 K ?c n +r n )/R n = c ? n?1 /R n = (c n?1 ?R n +s n?1 )/R n = c n?1 , and s n?1 = (2 K ?c n +r n ) mod R n = (c n?1 ? R n + s n?1 ) mod R n = s n?1 . (b) If c n ? 2 M ? R, no bits are popped from bs n such that bs n?1 = bs n . In this case, according to Eq. (10), no bits are pushed to bs n?1 and therefore bs n?1 = bs n = bs n?1 . In the decoding process, c n?1 = c ? n?1 /R n = c n?1 , s n?1 = c ? n?1 mod R n = s n?1 . Overall, P1 holds for i = n ? 1.</p><p>(ii) If P1 holds for i = k, we will prove that P1 holds for i = k ? 1. (a) If c k &lt; 2 M ? R k , the last K bits will be popped from bs k and added to c k . In this case, in the encoding process, as c k = c k , according to Eq. (10), r k = c ? k mod 2 K must be encoded to form bs k to obtain c k . In the decoding process, as bs k = bs k , r k is popped from bs k in the decoding process, it is therefore seen that bs k?1 = bs k?1 . Finally we obtain c k?1 = (2 K ? c k + r k )/R k = (2 K ?c k +r k )/R k = c ? k?1 /R k = c k?1 , and s k?1 = (2 K ?c k +r k ) mod R k = (c k?1 ?R n +s k?1 ) mod R n = s k?1 . (b) If c k ? 2 M ? R, no bits are popped from bs k such that bs k?1 = bs k . In this case, in the encoding process, as c k = c k , according to Eq. (10), no bits are pushed to bs k?1 to obtain c k and therefore bs k?1 = bs k = bs k = bs k?1 . In the decoding process, we have</p><formula xml:id="formula_43">c k?1 = (c k?1 ? R k + s k?1 )/R k = c ? k?1 /R k = c k?1 , s k?1 = (c k?1 ? R k + s k?1 ) mod R k = s k?1 . Overall, P1 holds for i = k ? 1.</formula><p>From (i)(ii), it is concluded that P1 holds by proof of mathematical induction.</p><p>P2. Denote that the lower K bits of c i need to be push to bs i at i = m 1 , ..., m T (m t &lt; m t+1 , m t ? {1, ..., n ? 1} for all t = 1, ..., T ? 1). In other words, we have</p><formula xml:id="formula_44">c i = ci?1?Ri+si 2 K , i ? {m 1 , ..., m T } c i?1 ? R i + s i , otherwise<label>(11)</label></formula><p>Firstly, it is clear that len(bs mt+1 ) = len(bs mt )+K. Secondly, for any i ? {m t +1, ..., m t+1 ? 1}, as</p><formula xml:id="formula_45">c i = c i?1 ? R i + s i , s i ? [0, R i ), it is clear that c i?1 ? R i ? c i ? c i?1 ? R i + R i ? 1. Thus c mt ? mt+1?1 i=mt+1 R i ? c mt+1?1 ? (c mt + 1) ? mt+1?1 i=mt+1 R i ? 1, and therefore c mt ? mt+1 i=mt+1 R i 2 K ? c mt+1 ? (c mt + 1) ? mt+1 i=mt+1 R i ? 1 2 K .<label>(12)</label></formula><p>Note that the above inequality also holds for t = 0 in which m 0 = 0. With Eq.</p><formula xml:id="formula_46">(12), log c mt+1 ? log (cm t +1)? m t+1 i=m t +1 Ri?1 2 K &lt; log (cm t +1)? m t+1 i=m t +1 Ri 2 K = log c mt + mt+1 i=mt+1 log R i + log(1 + c ?1 mt ) ? K &lt; log c mt + mt+1 i=mt+1 log R i + (ln 2 ? c mt ) ?1 ? K. As c mt ? [2 M , 2 M ? R mt ), it holds that log c mt+1 + len(bs mt+1 ) &lt; log c mt + len(bs mt ) + mt+1 i=mi+1 log R i + (ln 2 ? 2 M ) ?1 (13)</formula><p>If m T = n, log c n +len(bs n ) = log c m T +len(bs m T ); otherwise, len(bs n ) = len(bs m T ) and log c n ? log(c m T + 1) +</p><formula xml:id="formula_47">n i=m T +1 R i &lt; log c m T + n i=m T +1 R i + (ln 2 ? 2 M ) ?1 .</formula><p>Overall, we finally obtain log c n + len(bs n ) &lt; log c 0 + len(bs 0 ) + n i=1 log R i + (T + 1) ? (ln 2 ? 2 M ) ?1 <ref type="bibr" target="#b13">(14)</ref> Note that as c n ? 2 M = c 0 , len(bs n ) = T K and l 0 = M = log c 0 + len(bs 0 ), it is clear that log c n + len(bs n ) &gt; T K + l 0 . With Eq. <ref type="formula" target="#formula_1">(14)</ref>, the codelength is finally computed as l n ? l 0 ? log c n + len(bs n ) ? l 0 + 1 <ref type="bibr" target="#b14">(15)</ref> which completes the proof.</p><formula xml:id="formula_48">&lt; 1 1 ? 1/(ln 2 ? 2 M ? K) n i=1 R i + 1 + (ln 2 ? 2 M ) ?1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of Alg. 2 in iFlow</head><p>The main difficulty is in determining the interpolation interval [x l ,x h ), [z l ,z h ) givenx orz. The main paper discusses two interpolation tricks: (1) interpolating uniform intervals in domain x and (2) interpolating uniform intervals in co-domain z.</p><p>Interpolating uniform intervals in x. This usually applies in the case that f is large (e.g. inverse sigmoid). The uniform interval in domain x is defined asx l = (2 h ?x)/2 h ,x h =x l + 2 ?h . The corresponding interval in the co-domain isz l = f (x l ),z h = f (x h ). For the forward pass, givenx, the interval can be obtained as above. For the inverse pass, we first compute x = f ?1 (z) and then computex m = round(2 h ? x )/2 h ,x l =x m ? 2 ?h ,x h =x m + 2 ?h . Finally, we havez {l,m,h} = f (x {l,m,h} ). Ifz &lt;z m , we set the intervalx l =x l ,x h =x m ; otherwis? x l =x m ,x h =x h . The method is summarized in Alg. 6.</p><p>The correctness of the algorithm is guaranteed provided that f is Lipchitz continuous and the numerical error between f ?1 (f (x)) and x is limited. Let |f ?1 (f (x)) ? x| &lt; for all x and The extension of iFlow for lossless compression is summarized in Alg. 8. Note that for Variational Dequantization Flow <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17]</ref> </p><formula xml:id="formula_49">(Alg. 4), v = x ? + u(u ? [0, 1) d ), q(v|x ? ) = q(u|x ? ), P (x ? |v) = 1.</formula><p>Thus the above coding procedure reduces to Alg. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Dynamic Uniform Entropy Coder in AC, ANS and UBCS</head><p>In this section we will demonstrate the effectiveness of UBCS compared with AC and ANS. Both AC and ANS use probability mass function (PMF) and cumulative distribution function (CDF) for encoding and inverse CDF for decoding. For ease of coding, the PMF and CDF are all mapped to integers in [0, m). For uniform distribution U (0, R) in which P (s) = 1/R, s ? {0, 1, ..., R ? 1}, the most simple way to compute PMF and CDF are</p><formula xml:id="formula_50">l s = PMF(s) = m/R , s &lt; R ? 1 m ? (R ? 1) ? m/R , s = R ? 1 , b s = CDF(s ? 1) = m/R ? s<label>(16)</label></formula><formula xml:id="formula_51">Given b ? [0, m), the output of inverse CDF s = CDF ?1 (b), should be exactly b ? [b s , b s + l s ).</formula><p>The general way to determine CDF ?1 is binary search. But for uniform distribution, we can directly obtain the inverse CDF such that</p><formula xml:id="formula_52">s = CDF ?1 (b) = min( b/l , R ? 1), l = m/R<label>(17)</label></formula><p>We summarize uniform entropy coder AC, rANS and UBCS as follows:</p><p>For AC:</p><p>?   ? Number of atom operations in decoding: one division with remainder.</p><p>Overall, UBCS uses the least number of atom operations, which conveys that UBCS performs the best. Moreover, with PMF and CDF in Eq. (16), the optimal entropy coder cannot be guaranteed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More Experiments</head><p>In this section we will demonstrate our performance attributes across more benchmarking datasets. All experiments are conducted in PyTorch framework on one NVIDIA Tesla P100 GPU and Intel(R) Xeon(R) CPU E5-2690 @ 2.60GHz CPU. The code for LBB and the Flow++ model is directly taken from the original paper under MIT license.  The detailed experiments is shown in <ref type="table" target="#tab_7">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Coding Efficiency of UBCS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Compression Performance</head><p>Detailed experimental results on ImageNet32 and ImageNet64 datasets are displayed in <ref type="table" target="#tab_8">Tables 5 and  6</ref>. Note that we further report the decoding time, which we observe is close to the model inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Hyper-parameters</head><p>As discussed in Sec. 2.4, the codelength will be affected by the choices of h, k and S. As S is set to a large value -and will minimally affect the codelength resulting from MST -we mainly discuss h and k. <ref type="table" target="#tab_10">Tables 7, 8 and 9</ref> illustrate the codelength and auxiliary bits (in bpd) for differing choices of h and k. It is clear that, for large k, the codelength decreases with a larger h. This is expected as larger h corresponds to a greater numerical precision of our linear interpolation. On the other hand, for a fixed h, the codelength becomes larger with a smaller k, as a smaller k corresponds to a greater quantization error. A smaller k may even lead to the failure of iFlow entirely -especially if k is close to h, which would result in the potential of a zero-valued R in Eq. (4) for sufficiently small |f (x)|.</p><p>On the other hand, the auxiliary bits are principally affected by k and not h. Therefore we note that a smaller k is preferred. To conclude, we can nonetheless achieve a near-optimal codelength with a considered choice of hyper-parameters. Thus we set k = 28 and h = 12 for the experiments.  bpd k 18 3.829?0.000 3.779?0.000 3.867?0.000 N/A N/A 20 3.823?0.000 3.753?0.000 3.760?0.000 3.863?0.000 N/A 22 3.821?0.000 3.746?0.000 3.733?0.000 3.755?0.000 3.861?0.000 24 3.821?0.000 3.744?0.000 3.727?0.000 3.729?0.000 3.754?0.000 26 3.821?0.000 3.744?0.000 3.725?0.000 3.722?0.000 3.727?0.000 28 3.821?0.000 3.744?0.000 3.725?0.000 3.720?0.000 3.721?0.000 30 3.821?0.000 3.744?0.000 3.725?0.000 3.720?0.000 3.719?0.000 32 3.821?0.000 3.744?0.000 3.725?0.000 3.720?0.000 3.719?0.000 auxiliary length k</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>35thFigure 1 :</head><label>1</label><figDesc>Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia. arXiv:2111.00965v1 [cs.LG] 1 Nov 2021 Illustration of IDF(++), iVPF, LBB and the proposed iFlow method. Flow layers with darker color denote higher expressive power. The top-right of each model illustration shows the key procedure of exact bijections betweenx andz. The compression ratio and bandwidth are listed below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Input: state c, bit-stream bs. Output: decoded s, new state c and bit-stream bs. 1: if c &lt; 2 M ? R then 2: c ? 2 K ? c+bs.pop_back(); get last K bits from bit-stream and pop them. 3: end if 4: s ? c mod R; 5: c ? c/R ; 6: return s, c, bs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Then the error betweenz and f (x) is |z ? a ?x| ? |z ? R/S ?x| + |(a ? R/S) ?x| = O(S ?1 , 2 ?k ); therefore, Proposition 1 holds. For Proposition 2, decoding from U (0, R) involves ? log R bits, and encoding U (0, S) involves log S bits. As |a ? R/S| &lt; O(S ?1 ) and f (x) = a, the codelength of MST is L f (x,z) = log S ? log R = ? log |a + O(S ?1 )| = ? log |f (x)| + O(S ?1 ) ? ? log |f (x)|. Thus Proposition 2 holds. A.3 Correctness of our Invertible Non-linear Flows (Alg. 2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>A. 5</head><label>5</label><figDesc>k, h are large and k h. Theorem 3 in UBCS (Alg. 5) P1. We begin by showing that c i ? [2 M , 2 K+M ) for all i = 0, ..., n with mathematical induction. In fact, when i = 0, c 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>?</head><label></label><figDesc>Initial state: number c; ? Encoding: set m = 2 K , get l s , b s with Eq. (16), update c = c/l s ? m + (c mod l s ) + b s ; ? Decoding: set m = 2 K , for encoded bits c , get b = c mod m, decode s = CDF ?1 (b) with Eq. (17), update c = l s ? c /m + (c mod m) ? b s ; ? Number of atom operations in encoding: two divisions, two multiplications 4 ; ? Number of atom operations in decoding: two divisions, one multiplication. Binary search may involve if Eq. (17) is not used. For UBCS: ? Initial state: number c; ? Encoding: update c = c ? R + s; ? Decoding: for encoded bits c , decode s = c mod R, update c = c /R ; ? Number of atom operations in encoding: one multiplication;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). In this case, determining [z l ,z h ) during the inverse computation is simple. While for the forward pass, [z l ,z h ) should be determined with a binary search such thatx ? [f ?1 (z l ), f ?1 (z h )). In practice, we have simpler tricks to determine the correct interval [x l ,x h ), [z l ,x h ) for both the forward and inverse computation, which can be found in the Appendix.</figDesc><table><row><cell>The general idea of the non-linear flow adaptations are summarized in Alg. 2. Note that Alg. 2</cell></row><row><cell>can be used in any element-wise flow including linear flow. In Alg. 2, Proposition 2 holds such</cell></row><row><cell>that L f</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Coding performance of iFlow, LBB and iVPF on CIFAR10 dataset. We use batch size 64.</figDesc><table><row><cell>flow</cell><cell>compression</cell><cell></cell><cell></cell><cell></cell><cell cols="2">encoding time (ms)</cell><cell cols="2">decoding time (ms)</cell></row><row><cell>arch.</cell><cell>technique</cell><cell>nll</cell><cell>bpd</cell><cell cols="2">aux. bits inference</cell><cell>coding</cell><cell>inference</cell><cell>coding</cell></row><row><cell>Flow++</cell><cell>LBB [17] iFlow (Ours)</cell><cell>3.116</cell><cell>3.118 3.118</cell><cell>39.86 34.28</cell><cell>16.2?0.3</cell><cell>116?1.0 21.0?0.5</cell><cell>32.4?0.2</cell><cell>112?1.5 37.7?0.5</cell></row><row><cell>iVPF</cell><cell>iVPF [41] iFlow (Ours)</cell><cell>3.195</cell><cell>3.201 3.196</cell><cell>6.00 7.00</cell><cell>5.5?0.1</cell><cell>11.4?0.2 7.1?0.2</cell><cell>5.2?0.1</cell><cell>13.5?0.3 9.7?0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Coding bandwidth (M symbol/s) of UBCS and rANS coder on different threads(thrd). We use the implementations in<ref type="bibr" target="#b16">[17]</ref> for evaluating rANS.</figDesc><table><row><cell></cell><cell># thrd</cell><cell>rANS</cell><cell>UBCS</cell></row><row><cell>Encoder</cell><cell>1 16</cell><cell cols="2">5.1?0.3 21.6?1.1 2075?353 380?5</cell></row><row><cell>Decoder</cell><cell>1 16</cell><cell>0.8?0.02 7.4?0.5</cell><cell>66.2?1.7 552?50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Compression performance in bpd on benchmarking datasets. ? denotes the generation performance in which the models are trained on ImageNet32 and tested on other datasets. ? denotes compression of high-resolution datasets with our ImageNet64-trained model.</figDesc><table><row><cell></cell><cell cols="4">ImageNet32 ImageNet64 CIFAR10 CLIC.mobile</cell><cell>CLIC.pro</cell><cell>DIV2K</cell></row><row><cell>PNG [5]</cell><cell>6.39</cell><cell>5.71</cell><cell>5.87</cell><cell>3.90</cell><cell>4.00</cell><cell>3.09</cell></row><row><cell>FLIF [36]</cell><cell>4.52</cell><cell>4.19</cell><cell>4.19</cell><cell>2.49</cell><cell>2.78</cell><cell>2.91</cell></row><row><cell>JPEG-XL [2]</cell><cell>6.39</cell><cell>5.74</cell><cell>5.89</cell><cell>2.36</cell><cell>2.63</cell><cell>2.79</cell></row><row><cell>L3C [30]</cell><cell>4.76</cell><cell>4.42</cell><cell>-</cell><cell>2.64</cell><cell>2.94</cell><cell>3.09</cell></row><row><cell>RC [31]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.54</cell><cell>2.93</cell><cell>3.08</cell></row><row><cell>Bit-Swap [26]</cell><cell>4.50</cell><cell>-</cell><cell>3.82</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>IDF [18]</cell><cell>4.18</cell><cell>3.90</cell><cell>3.34</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>IDF++ [4]</cell><cell>4.12</cell><cell>3.81</cell><cell>3.26</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>iVPF [41]</cell><cell>4.03</cell><cell>3.75</cell><cell>3.20</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LBB [17]</cell><cell>3.88</cell><cell>3.70</cell><cell>3.12</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>iFlow (Ours)</cell><cell>3.88</cell><cell>3.70</cell><cell>3.12</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HiLLoC [37]  ?</cell><cell>4.20</cell><cell>3.90</cell><cell>3.56</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>IDF [18]  ?</cell><cell>4.18</cell><cell>3.94</cell><cell>3.60</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>iVPF  ? [41]</cell><cell>4.03</cell><cell>3.79</cell><cell>3.49</cell><cell>2.47/2.39  ?</cell><cell cols="2">2.63/2.54  ? 2.77/2.68  ?</cell></row><row><cell>iFlow (Ours)  ?</cell><cell>3.88</cell><cell>3.65</cell><cell>3.36</cell><cell>2.26/2.26  ?</cell><cell cols="2">2.45/2.44  ? 2.60/2.57  ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>demonstrate the literature-standard test of generalization, in which ImageNet32 trained model are used for testing. From these results, it is clear that iFlow achieves the best generalization performance in this test. It is worth noting that we obtain an improved performance on ImageNet64 when using our ImageNet32-trained model.Experiments on High ResolutionImages. Finally, we test iFlow across a number of high-resolution image datasets. Here images are processed into non-overlapping 32 ? 32 and 64 ? 64 patches for our ImageNet32 and ImageNet64-trained models. The right three columns in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Initial state: interval [lo, hi); ? Encoding: get m = hi ? ho, get l s , b s with Eq. (16), update interval [lo , hi ) such that lo = lo + b s , hi = lo + l s , get c ? [lo , hi ) as the encoded bits; ? Decoding: for encoded bits c ? [lo, hi) and current interval [lo, hi), get m = hi ? lo, b = c ? lo, decode s = CDF ?1 (b) with Eq. (17), update interval [lo , hi ) such that lo = lo + b s , hi = lo + l s ; ? Number of atom operations in encoding: one division, one multiplication 3 ; ? Number of atom operations in decoding: two divisions, one multiplication. Binary search may involve if Eq. (17) is not used.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>More results on coding bandwidth (M symbol/s) of UBCS and rANS coder. We use the implementations in<ref type="bibr" target="#b16">[17]</ref> for evaluating rANS.</figDesc><table><row><cell></cell><cell># threads</cell><cell>rANS</cell><cell>UBCS</cell></row><row><cell></cell><cell>1</cell><cell>5.1?0.3</cell><cell>380?5</cell></row><row><cell>Encoder</cell><cell>4 8</cell><cell cols="2">10.8?1.9 15.9?1.4 1297?137 709?56</cell></row><row><cell></cell><cell>16</cell><cell cols="2">21.6?1.1 2075?353</cell></row><row><cell></cell><cell>1</cell><cell>0.80?0.02</cell><cell>66.2?1.7</cell></row><row><cell>Decoder</cell><cell>4 8</cell><cell>2.8?0.1 5.5?0.2</cell><cell>248?8 460?16</cell></row><row><cell></cell><cell>16</cell><cell>7.4?0.5</cell><cell>552?50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Detailed results on the coding performance of iFlow and LBB on ImageNet32. We use a batch size of 64.</figDesc><table><row><cell>flow</cell><cell>compression</cell><cell></cell><cell></cell><cell></cell><cell cols="2">encoding time (ms)</cell><cell cols="2">decoding time (ms)</cell></row><row><cell>arch.</cell><cell>technique</cell><cell>nll</cell><cell>bpd</cell><cell cols="2">aux. bits inference</cell><cell>coding</cell><cell>inference</cell><cell>coding</cell></row><row><cell>Flow++</cell><cell>LBB [17] iFlow (Ours)</cell><cell>3.871</cell><cell>3.875 3.873</cell><cell>45.96 34.40</cell><cell>58.7?0.1</cell><cell>176?2.8 66.6?0.3</cell><cell>83.2?0.4</cell><cell>172?4.7 95.3?0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Detailed results on the coding performance of iFlow and LBB on ImageNet64. We use a batch size of 64.</figDesc><table><row><cell>flow</cell><cell>compression</cell><cell></cell><cell></cell><cell></cell><cell cols="2">encoding time (ms)</cell><cell cols="2">decoding time (ms)</cell></row><row><cell>arch.</cell><cell>technique</cell><cell>nll</cell><cell>bpd</cell><cell cols="2">aux. bits inference</cell><cell>coding</cell><cell>inference</cell><cell>coding</cell></row><row><cell>Flow++</cell><cell>LBB [17] iFlow (Ours)</cell><cell>3.701</cell><cell>3.703 3.703</cell><cell>38.00 34.42</cell><cell>24.4?0.0</cell><cell>284?2.5 45.0?1.7</cell><cell>35.7?0.1</cell><cell>281?2.2 57.0?1.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Codelengths in terms of bpd and auxiliary length on different h and k on the CIFAR10 dataset. N/A denotes the failure of the compression procedure. The theoretical bpd (nll) is 3.116.</figDesc><table><row><cell></cell><cell></cell><cell>h</cell><cell></cell><cell></cell></row><row><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell></row><row><cell></cell><cell>bpd</cell><cell></cell><cell></cell><cell></cell></row><row><cell>k</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Codelengths in terms of bpd and auxiliary length on different h and k on a SUBSET of ImageNet64 dataset. N/A denotes the failure of the compression procedure. The theoretical bpd (nll) is 3.718.</figDesc><table><row><cell></cell><cell></cell><cell>h</cell><cell></cell><cell></cell></row><row><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">All intervals must partition the domain of f .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.compression.cc/challenge/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We omit add/sub operations as they are negligible compared with multiplication/division.4  The multiplication/division/mod with m = 2 K only involve bit operations. With the result of c/ls , c mod ls only involve one multiplication.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Secondly, we show that during the inverse computation, the interpolation intervals [x l ,x h ) and [z l ,z h ) are the same as that in the forward process. In other words, if the interpolation interval is</p><p>Overall,x d l &lt;x h andx d h &gt;x l , such that onl? x d l =x l ,x d h =x h satisfy this condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Propositions 1-2 in Invertible Non-linear Flows (Alg. 2)</head><p>In general, f (x), f (x) is bounded for all x. Consider f (x) in the small interval such thatx h ?x l ? 2 ?h orz h ?z l ? 2 ?h (k h). We first prove the following two propositions in [x l ,x h ):</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Jpeg xl next-generation image compression architecture and coding tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyrki</forename><surname>Alakuijala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Ruud Van Asseldonk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Boukortt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulia-Maria</forename><surname>Bruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Coms</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Firsching</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenii</forename><surname>Fischbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Kliuchnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Obryk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Digital Image Processing XLII</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11137</biblScope>
			<biblScope unit="page">111370</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09347</idno>
		<title level="m">Understanding and mitigating exploding inverses in invertible neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><forename type="middle">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dehghani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12459</idno>
		<title level="m">Casper Kaae S?nderby, and Tim Salimans. Idf++: Analyzing and improving integer discrete flows for lossless compression</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Png (portable network graphics) specification version 1.0. Network Working Group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Boutell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="1" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Lossless image compression through super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02872</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vflow: More expressive generative flows with variational data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biqi</forename><surname>Chenli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1660" to="1669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<idno type="arXiv">arXiv:2011.10650</idno>
		<title level="m">Very deep vaes generalize autoregressive models and can outperform them on images</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Smaller and faster data compression with zstandard. Facebook Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chip</forename><surname>Turner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Nice: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Asymmetric numeral systems: entropy coding combining speed of huffman coding with compression rate of arithmetic coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarek</forename><surname>Duda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2540</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cisco visual networking index: global mobile data traffic forecast update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gmdt Forecast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Update</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new algorithm for data compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Gage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">C Users Journal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Flow++: Improving flow-based generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00275</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compression with flows via local bits-back coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Lohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3879" to="3888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Integer discrete flows and lossless compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorn</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12134" to="12144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tomczak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.11235</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Learning discrete distributions by dequantization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural autoregressive flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2078" to="2087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A method for the construction of minimum-redundancy codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IRE</title>
		<meeting>the IRE</meeting>
		<imprint>
			<date type="published" when="1952" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1098" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distribution augmentation for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5006" to="5019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joun Yeop</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Soo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<title level="m">Probabilistic framework for normalizing flow on manifolds. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bit-swap: Recursive bits-back coding for lossless compression with hierarchical latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Friso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06845</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Durk</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01434</idno>
		<title level="m">Videoflow: A flow-based generative model for video</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Lippe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09790</idno>
		<title level="m">Categorical normalizing flows via continuous transformations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Information theory, inference and learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David Jc Mac</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Practical full resolution learned lossless image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10629" to="10638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning better lossless compression using lossy compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6638" to="6647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Waveglow: A flow-based generative network for speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3617" to="3621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Jpeg2000: Image compression fundamentals, standards and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Rabbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">286</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">PNG: the definitive guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Koman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Reilly &amp; Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05517</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Flif: Free lossless image format based on maniac compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Sneyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Wuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Townsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julius</forename><surname>Kunze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hilloc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09953</idno>
		<title level="m">Lossless image compression with hierarchical latent variable models</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Practical lossless compression with latent variables using bits back coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Townsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04866</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Arithmetic coding for data compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John G</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cleary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="520" to="540" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">ivpf: Numerical invertible volume preserving flow for efficient lossless compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16211</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Determinex L ,X H</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>z h givenz. 1: x = f ?1 (z)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">2:x m = round(2 h ? x )/2 h ,x l =x m ? 2 ?h ,x h = x m + 2 ?h</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">z {l,m,h} = f (x {l,m,h} )</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">We will show that Alg. 6 is correct if ? = 2 ?k ? + &lt; 2 ?h?1 . In fact, it is clear that |f ?1 (z l ) ?x l | ? |f ?1 (z l ) ? f ?1 (f (x l ))| + |x l ? f ?1 (f (x l ))| &lt; 2 ?k ? + = ?. Similarly, |f ?1 (z h ) ?x h | &lt; ?. As f ?1 is monotonically increasing andz ? (z l ,z h ), it , it corresponds tox &lt;x m . As x ? [x l ,x h ), it is clear thatx m ? (x l ,x h + ? + 2 ?h?1 ) = (x h ? 2 ?h ,x h + ? + 2 ?h?1 )</title>
	</analytic>
	<monogr>
		<title level="m">|f ?1 (x 1 ) ? f ?1 (x 2 )| &lt; ?|x 1 ? x 2 | for all x 1</title>
		<imprint/>
	</monogr>
	<note>Note that x m ,x h ? {2 ?h ? n, n ? Z} when ? &lt; 2 ?h?1 ; therefore, it must hold thatx m =x h . (ii) Whenz ? z m , it corresponds tox ?x m. It is clear thatx m ? (x l ???2 ?h?1 ,x h ) = (x l ???2 ?h?1 ,x l +2 ?h ) -thus it must hold thatx m =x l . In fact, as h k, ? is bounded and is rather small. such that</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Determinex L ,X H</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>z h givenx. 1: z = f (z)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">2:z m = round(2 h ? z )/2 h ,z l =z m ? 2 ?h ,z h = z m + 2 ?h</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<title level="m">h} = f ?1 (z {l,m,h} )</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<title level="m">4: ifx &lt;x m then 5:x l =x l ,x h =x m ,z l =z l ,z h =z m ; 6: else 7:x l =x m ,x h =x h ,z l =z m ,z h =z h 8: end if</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>returnx l ,x h ,z l ,z h</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">1:z l = (2 h ?z)/2 h ,z h =z l + 2 ?h ; 2:x l = f ?1 (x l ),x h = f ?1</title>
		<imprint/>
	</monogr>
	<note>z h</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">corresponding interval in the domain isx l = f ?1 (z l ),x h = f ?1 (z h ). For the inverse pass given z, the interval can be obtained as above. For the forward pass, we first compute z = f (x), and then computez m = round(2 h ? z )/2 h ,z l =z m ? 2 ?h ,z h =z m + 2 ?h</title>
		<imprint/>
	</monogr>
	<note>andx {l,m,h} = f ?1 (z {l,m,h}</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<title level="m">Ifx &lt;x m , we set the intervalz l =z l ,z h =z m ; ? . 1: Decodev using q(v|x ? )?; 2:z ?f</title>
		<imprint/>
	</monogr>
	<note>v</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Encodez using pZ (z)?; 4: Encode x ? using P (x ? |v)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Decode. 1: Decodez using pZ (z)?; 2:v ?f ?1 (z)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">3: Decode x ? using P (x ? |v)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<title level="m">Encodev using q(v|x ? )?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Codelengths in terms of bpd and auxiliary length on different h and k on a SUBSET of</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>ImageNet32 dataset. N/A denotes the failure of the compression procedure. The theoretical bpd (nll</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
