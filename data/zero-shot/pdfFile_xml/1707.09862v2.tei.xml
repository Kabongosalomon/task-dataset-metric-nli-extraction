<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Iterative Manifold Embedding Layer Learned by Incomplete Data for Large-scale Image Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Iterative Manifold Embedding Layer Learned by Incomplete Data for Large-scale Image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Iterative manifold embedding layer</term>
					<term>image re- trieval</term>
					<term>incomplete data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing manifold learning methods are not appropriate for image retrieval task, because most of them are unable to process query image and they have much additional computational cost especially for large scale database.</p><p>Therefore, we propose the iterative manifold embedding (IME) layer, of which the weights are learned off-line by unsupervised strategy, to explore the intrinsic manifolds by incomplete data. On the large scale database that contains 27000 images, IME layer is more than 120 times faster than other manifold learning methods to embed the original representations at query time.</p><p>We embed the original descriptors of database images which lie on manifold in a high dimensional space into manifold-based representations iteratively to generate the IME representations in off-line learning stage. According to the original descriptors and the IME representations of database images, we estimate the weights of IME layer by ridge regression. In on-line retrieval stage, we employ the IME layer to map the original representation of query image with ignorable time cost (2 milliseconds per image).</p><p>We experiment on five public standard datasets for image retrieval. The proposed IME layer significantly outperforms related dimension reduction methods and manifold learning methods. Without post-processing, Our IME layer achieves a boost in performance of state-of-the-art image retrieval methods with postprocessing on most datasets, and needs less computational cost. Code is available at https://github.com/XJhaoren/IME layer.</p><p>Index Terms-Iterative manifold embedding layer, image retrieval, incomplete data</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>O VER the past decades, image retrieval has received widespread attention. The representations are shown to be effective for image retrieval <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, which are derived by aggregating Scale-Invariant Feature Transform (SIFT) <ref type="bibr" target="#b15">[16]</ref> features. After that, image retrieval methods based on Convolutional Neural Network (CNN) <ref type="bibr" target="#b16">[17]</ref> achieve excellent performance <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. These methods represent a image as the description vector, and sort the Euclidean distances between the feature vectors of query and database images as the retrieval results.</p><p>Manifolds are the fundamental to perception <ref type="bibr" target="#b30">[31]</ref>. For example, to recognize the faces, the brain equates all images from the same manifold but distinguishes between images from different manifolds. In image collection, objects and landmarks are depicted in various conditions, such as different viewing angles and under various illumination. As a consequence, query and relevant images are often connected by a sequence of images, where consecutive images are similar. The descriptors of these images form a manifold in the descriptor <ref type="figure">Fig. 1</ref>. Image manifold. The image with gray border is the query. The Euclidean distances to query image are expressed by the distance between the positions of the query and reference images in <ref type="figure">Fig. 1</ref>. Contour lines correspond to the geodesic distance. The images with green and red border are positive samples and negative samples respectively. The wires show the neighbourhood relationships of images. As shown above, a negative sample is closer to query than some positive samples in Euclidean space. However, it is far from query image along the wires in manifold. The shortest path between the negative sample and query are shown by images with blue border. space. As shown in <ref type="figure">Fig. 1</ref>, some negative samples are far from query image through the paths in K-NN graph (geodesic distances) but are closer to query than some positive samples based on Euclidean distances.</p><p>But the data in image collection is incomplete in most situation. These images do not vary smoothly. The data sampled from intrinsic manifolds are too sparse. As a result, the manifolds reconstructed by these sparse data have some holes and are not continuous and smooth. The holes lead to the large calculation error of geodesic distance. Moreover, existing manifold learning methods <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> are not appropriate for image retrieval task, because most of them are unable to process query image and they have much additional computational cost especially for large scale database. Except for IsoMap <ref type="bibr" target="#b31">[32]</ref> and LLE <ref type="bibr" target="#b32">[33]</ref>, these manifold learning methods can not handle new image at query time. And the computational cost for query image is high for IsoMap and LLE due to the computation of k-NN of query image.</p><p>For the above problems, we propose the iterative manifold arXiv:1707.09862v2 [cs.CV] 3 Apr 2018 embedding (IME) layer to explore the intrinsic manifolds by incomplete data in this paper. The weights of the IME layer are learned off-line by unsupervised strategy. In the on-line query stage, our IME layer maps the features of query images into embedding space with very little or even ignorable additional computational complexity. The IME layer solves the problem of sample holes from two aspects. <ref type="bibr" target="#b0">(1)</ref> The points that share similar neighbours tend to be similar to each other <ref type="bibr" target="#b36">[37]</ref>. We employ this natural intuition to improve the topological instability of k-NN graph. The information of second-order proximity suppresses the interference of the sample holes. <ref type="bibr" target="#b1">(2)</ref> We utilize the Euclidean distances to correct the calculation error of geodesic distances. Based on the corrected geodesic distances, we embed the data into lowdimensional space and preserve the intrinsic geometry of the data. The above steps are repeated many times to construct the stable and rubout k-NN graph by incomplete data. To adapt our algorithm to image retrieval task, we simplify and approximate the IME by linear mapping, called IME layer in this paper. The IME layer is the integration and simplification version of IME, which reduces the computational cost and estimation error of geodesic distances for query images. The query image is embedded with very little or even ignorable additional computational cost by IME layer in the on-line retrieval stage. Working as the additional fully connected layer, the proposed IME layer can be directly connected to CNNs <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. For SIFT-based representations <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, IME layer can work as the transform matrix to map the vector representation into low-dimensional space and preserve the original neighbourhood relationships.</p><p>We conduct extensive experiments on five public standard image retrieval datasets, including landmarks and logos. Experiments results show that our proposed algorithm for manifold-based embedding significantly improves the performance of global representation vectors. The proposed IME layer achieves a significant boost in the performance of the related dimension reduction methods and manifold learning methods. Without reranking, our IME layer still outperforms the state-of-the-art methods based on search reranking in postprocessing step on most datasets. On a set of five thousand images with 2048 dimensions, IME layer is up to twentyseven times faster than other manifold learning methods <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> at query time. The computational time of IsoMap <ref type="bibr" target="#b31">[32]</ref> and LLE <ref type="bibr" target="#b32">[33]</ref> increases as the scale of database grows, while the cost of our IME layer does not change. On the large scale dataset that contains 27000 images, our IME layer is more than 120 times faster than other manifold learning methods. Therefore our IME layer is efficient and effective for large scale image retrieval.</p><p>The main contributions of this paper are summarized as follows:</p><p>? We propose a iterative manifold embedding (IME) approach, which explores the intrinsic manifold by incomplete data. To suppress the interference of the sample holes, we employ the second-order proximity and original Euclidean distances to correct the geodesic distances during the iteration process. By reconstructing the manifold of database images, our IME method reduces the dimen-sions of the original representation vectors and enhances the discrimination of the embedded representations. ? We propose the iterative manifold embedding (IME) layer to simplify and accelerate calculation of IME, which is the integration and simplification version of IME. The weights of IME layer are learned off-line according to original representations and embedded representations by ridge regression. With embedding time below 2 milliseconds, the trained iterative manifold embedding layer can be directly connected to CNNs <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> or independently work as the transform matrix to map the SIFT-based representations <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The paper is organized as follows. In Section II we discuss the previous work related to manifold learning and manifoldbased methods for image retrieval. Then, we illustrate the formulation of the proposed algorithm and derive the solution in detail in Section III. The experimental results are described in Section IV. Finally, Section V concludes the paper and Section VI introduces our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we review several previous related works from two aspects: manifold learning methods and manifoldbased image retrieval methods.</p><p>To the best of our knowledge, very few manifold learning methods can be directly applied to image retrieval. Instead, most manifold learning methods pay attention to dimension reduction and data visualizations. Some manifold-based methods are applied to image retrieval in the search reranking process. Our IME layer embeds the original representations in image representation process, based on the image manifold reconstructed by incomplete data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Manifold learning</head><p>Our work is related to the manifold learning and dimension reduction methods, such as IsoMap <ref type="bibr" target="#b31">[32]</ref>, LLE <ref type="bibr" target="#b32">[33]</ref>, Laplacian Eigenmap <ref type="bibr" target="#b33">[34]</ref>, SNE <ref type="bibr" target="#b34">[35]</ref>, t-SNE <ref type="bibr" target="#b35">[36]</ref>, LINE <ref type="bibr" target="#b36">[37]</ref> and LargeVis <ref type="bibr" target="#b37">[38]</ref>.</p><p>The most popular method for manifold learning may be the IsoMap <ref type="bibr" target="#b31">[32]</ref>, which preserves shortest graph path distance by MDS <ref type="bibr" target="#b39">[40]</ref> method. IsoMap first constructs the k-NN graph of data, and then it computes the shortest path distances between all pairs of points according to the k-NN graph. Finally, the distance vectors are embedded into a low dimensional space. By exploiting the local symmetries of linear reconstructions, LLE <ref type="bibr" target="#b32">[33]</ref> is able to learn the global structure of nonlinear manifolds. Laplacian Eigenmap <ref type="bibr" target="#b33">[34]</ref> constructs a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space by geometrically motivated algorithm. SNE <ref type="bibr" target="#b34">[35]</ref> minimizes the Kullback-Leibler divergences between the original and induced distributions to preserves neighbour identities as well as possible. After that, the Student t-distribution is used to solve the crowding problem in t-SNE <ref type="bibr" target="#b35">[36]</ref> instead of the Gaussian distribution in SNE <ref type="bibr" target="#b34">[35]</ref>. By exploiting the first-order proximity and the second-order proximity between the vertices, LINE <ref type="bibr" target="#b36">[37]</ref> designs the objective function that preserves both the local and global network structures. Instead of building a large number of trees to obtain a highly accurate k-NN graph, LargeVis <ref type="bibr" target="#b37">[38]</ref> uses neighbour exploring techniques to improve the accuracy of the graph.</p><p>These manifold learning methods can not be used directly for image retrieval except for IsoMap <ref type="bibr" target="#b31">[32]</ref> and LLE <ref type="bibr" target="#b32">[33]</ref>, because we can not get the embedded representation for query image. To map a new query image, IsoMap <ref type="bibr" target="#b31">[32]</ref> estimates the geodesic distances between query image and database images by constructed k-NN graph, and then reduces the dimensions of geodesic distances vector. LLE <ref type="bibr" target="#b32">[33]</ref> computes the k-NN of a query image and presents the query image by the weighted sum. The computational cost of the embedded representation for query image is high for IsoMap <ref type="bibr" target="#b31">[32]</ref> and LLE <ref type="bibr" target="#b32">[33]</ref> due to the computation of k-NN of query image. The sample holes seriously interfere with the stability of k-NN graph. Therefore IsoMap <ref type="bibr" target="#b31">[32]</ref> and LLE <ref type="bibr" target="#b32">[33]</ref> are not robust, especially for incomplete data. Our IME layer embeds the original representation of query image quickly, and is not sensitive to the interference of sample holes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Manifold-based image retrieval</head><p>Some manifold-based methods are applied to image retrieval in the context of convolutional features as the post-processing, e.g., query expansion <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> and diffusion on region manifold <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Manifold-based methods that leverage the information of image manifold in the search reranking process are introduced into image retrieval and achieve outstanding performance.</p><p>A number of the highly ranked results that satisfy strong spatial constraints from the original query are reissued as a new query in query expansion <ref type="bibr" target="#b40">[41]</ref> in search reranking process. The average query expansion (AQE) <ref type="bibr" target="#b40">[41]</ref> is now used as a standard post-processing of the image retrieval methods, due to its efficiency and significant performance boost. However, AQE only explores the first-order neighbourhood of query images. Recursive average query expansion <ref type="bibr" target="#b40">[41]</ref> further improve the results by explicitly crawling the image manifold, but it increases much cost of query time. Different with query expansion exploits the manifold of images at query time, diffusion <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b44">[45]</ref> constructs the neighborhood graph of the dataset off-line and uses this information at query time to search on the manifold. In the recent work <ref type="bibr" target="#b44">[45]</ref>, the diffusion on image manifold is used to compute the rerank scoring in the search reranking process.</p><p>Working as the post-processing, these manifold-based image retrieval methods need much additional computational cost of reranking at query time. Without post-processing, our IME layer still achieves better performance than these state-ofthe-art image retrieval methods on most datasets. And the additional computational cost of our IME layer is ignorable (less than 2 milliseconds) in on-line retrieval stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED APPROACH</head><p>The diagram of the proposed method is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. We learn the weights of iterative manifold embedding (IME) layer in the first off-line stage. Then, we embed the original representations into IME representations rapidly by the proposed IME layer in the on-line image retrieval stage. While constructing the k-NN graph, we exploit the information of second-order proximity to suppress the interference of sample holes. The Euclidean distances of original representations are utilized to correct the calculation error of geodesic distances in the manifold embedding step. In order to better reconstruct the intrinsic manifold by incomplete sampled data, we repeat the k-NN graph construction and manifold embedding steps many times. By adequate approximation and simplification, IME layer embeds the original representations into IME representations which preserve the intrinsic manifold of database images with ignorable additional time. IME layer is the integration version of IME, which reduces the estimation error of geodesic distances and the loss of discriminative information in dimension reduction by ridge regression.</p><p>In this section, we describe our novel IME layer for image retrieval in detail. Firstly, we show the formulation of proposed iterative embedding method which embeds the original representation according to the intrinsic manifold of images in Section III-A. Then in Section III-B the proposed embedding is equivalently implemented by the fully connected layer, which is called IME layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Iterative manifold embedding</head><p>The iterative manifold embedding (IME) algorithm has two cyclic steps, which are detailed in Algorithm. 1. In the first step, we construct the k-NN graph and calculate the geodesic distances, considering the information of second-order proximity. Then, the original representations are embedded into the manifold-based representations which preserve the corrected geodesic distances in the second step.</p><p>The first step constructs the k-NN graph G and calculates the geodesic distances D G of original representations ? X = [? X (1), ? X (2), ..., ? X (d)] (where ? X (i) is the n-dimensional original representation vector of image i and d is the database scale).</p><p>To construct the first-order k-NN graph G 1th , each point is only connected to its k nearest neighbours based on the Euclidean distances D X = {d X (i, j)} between pairs of images i, j in the input space X. The neighbourhood relations are coarsely represented as a weighted first-order graph G 1th over the data, with the edges of weight d X (i, j) between neighbouring images. The weights W G 1th of the edges in firstorder graph G 1th are defined as the following d ? d matrix:</p><formula xml:id="formula_0">WG 1th (i, j) = d X (i, j), i is connected to j 0, i is not connected to j<label>(1)</label></formula><p>Where W G 1th (i, j) = 0 indicates that the edge between images i and j is cut in first-order graph</p><formula xml:id="formula_1">W G 1th . W G 1th (i, j) = d X (i, j)</formula><p>indicates that image i is connected to j by the edge with path distance d X (i, j).</p><p>To suppress the interference of the sample holes, we exploit the second-order proximity in the constructed graph G 1th . Like the idea in LINE <ref type="bibr" target="#b36">[37]</ref>, we assumes that the vertices sharing many connections to other vertices are similar to each other. Flow chart of the image retrieval framework which is based on proposed IME layer. We learn the weights of iterative manifold embedding (IME) layer in the first off-line stage. Then, we use the IME layer to embed the original representations according to the intrinsic manifold rapidly in the on-line image retrieval stage. While constructing the k-NN graph, we exploit the information of second-order proximity to suppress the interference of sample holes. The Euclidean distances of original representations are utilized to correct the calculation error of geodesic distances in the manifold embedding step. In order to better reconstruct the intrinsic manifold by incomplete sampled data, we repeat the k-NN graph construction and manifold embedding steps many times. By adequate approximation and simplification, IME layer embeds the original representations into IME representations and preserves the intrinsic manifold of database images. Different with LINE <ref type="bibr" target="#b36">[37]</ref>, we use this natural intuition to construct the stable graph G in a simple but effective way. The weights W G of the edges in graph G are calculated by the weights W G 1th in first-order graph G 1th :</p><formula xml:id="formula_2">W G = W G 1th W G 1th<label>(2)</label></formula><p>Similar to W G 1th (i, j), W G (i, j) = 0 indicates that image i is not connected to j directly in second-order graph W G . The weight W G (i, j) = 0 indicates the distance between directly connected images i and j in second-order graph, which is more robust for sparse samples. In the condition that the samples are sparse, the k nearest neighbours based on the Euclidean distances D X are farfetched and unstable sometimes. The second-order graph utilizes the natural intuition that the vertices sharing connections to other vertices are similar to each other to ensure the stability of connection. We estimate the geodesic distances d G (i, j) between all pairs of images on the manifold by computing their shortest path distances d G (i, j) in the graph G. The shortest path is simply computed by Floyd-Warshall algorithm <ref type="bibr" target="#b45">[46]</ref>.</p><p>Although the information of second-order proximity is utilized to suppress the interference of sample holes, the geodesic distances d G (i, j) still have estimation error. To further reduce the error of reconstruction of image manifold, we correct the matrix of geodesic distances D G = {d G (i, j)} by Euclidean distances d X (i, j) in the second step. We embed the representations into a low-dimensional space that best preserves the manifold's estimated intrinsic geometry. The manifold-</p><formula xml:id="formula_3">based representations ? Y = [? Y (1), ? Y (2), ..., ? Y (d)] (where ? Y (i)</formula><p>is the manifold-based representation vector of image i) are computed by minimizing the cost function</p><formula xml:id="formula_4">min S(D G ) + ? S(D X ) ? ? Y T ? Y F 2 s.t. ? Y ? Y T = I<label>(3)</label></formula><p>where A F denotes the F -norm of matrix A and the parameter ? denotes the strength of the correction. S(D) is a conversion function that converts distances D to similarity. Given distance matrix D = {d(i, j)}, many conversion functions can be used such as quadratic function S(d(i, j)) = ? d(i,j) 2 2 and t-distribution S(d(i, j)) = 1 1+d(i,j) 2 in practice. The solution of Eq. 3 using quadratic function as conversion function S is equivalent to the global optimal solution of the cost function in IsoMap <ref type="bibr" target="#b31">[32]</ref>. We compare different conversion functions in Section IV. In order to retain more discriminatory information, we constrain the embedded representations ? Y to be orthonormal.</p><p>The global minimum of Eq. 3 is achieved by setting the representations ? Y to the top d eigenvectors of the similarity matrix Compute Euclidean distances D X of updated representations ? X ;</p><formula xml:id="formula_5">S COR = S(D G ) + ?S(D X )<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Construct first-order neighbour graph G 1th based on D X by Eq. 1;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Calculate the weights W G of the edges in graph G:</p><formula xml:id="formula_6">W G = W G 1th W G 1th ; 7:</formula><p>Compute geodesic distances D G by Floyd-Warshall algorithm <ref type="bibr" target="#b45">[46]</ref>; <ref type="bibr">8:</ref> 2. Map the representations ? X into embedding space that best preserves the manifolds estimated intrinsic geometry: <ref type="bibr">9:</ref> Correct the geodesic distances D G by Euclidean distances D X ;</p><p>10:</p><p>Compute d-dimensional representation ? Y by solving Eq. 3;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>? X = ? Y ; 12:</p><formula xml:id="formula_7">I + +; 13: end while 14: ? IM E = ? Y ;</formula><p>selection of m is a compromise between computational cost and accuracy.</p><p>The manifold-based representation ? Y (i) exploits the neighborhood relationships to represent the feature of a image. We only use the small amount of neighbours with high credibility to construct k-NN graph in this paper. The constructed k-NN graph in first step is sparse and has few negative neighbour pairs that represent different objects. But some important connection paths are cut off. The sample holes do harm to the reliability of graph G. The correction operations in Eq. 2 and Eq. 4 solve this problem by exploring the information of second-order proximity and Euclidean distances. The performance of the embedded representations is improved significantly in this way.</p><p>Although these strategies is straightforward, the remarkable performance is achieved with some iterations. We repeat above two steps to improve the stability of the graph G. In each iteration, the Euclidean distances D X are updated based on manifold-based representations ? Y . In Algorithm. 1, the iteration process is stated in detail. After a few iterations, we get the final m-dimensional iterative manifold embedding</p><formula xml:id="formula_8">representations ? IM E = [? IM E (1), ? IM E (2), ..., ? IM E (d)] (where ? IM E (i) is the IME representation vector of image i).</formula><p>We map the original representations into a low-dimensional space that preserves the geometry of the image manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Integration as IME layer</head><p>The iterative manifold embedding (IME) method proposed above embeds the original representations ? X into manifoldbased representations ? IM E . But if we directly apply IME for query image, the embedding leads to more feature extraction time of query images and more estimation error of geodesic distances. Similar to Isomap <ref type="bibr" target="#b31">[32]</ref>, the IME proposed in previous Section III-A needs to compute the shortest pathes from a query image to all database images and estimate the geodesic distances at query time. They are implemented in image retrieval by linking the query into the graph of geodesic distances of the training data. First the k nearest neighbors of query are found in the training data. Then, the shortest geodesic distances from query to each point in the training data are computed and transformed into similarity vector by conversion function. The similarity vector corrected by Euclidean distance is projected into the IME representation by the eigenvector matrix of training data finally. The additional computational cost is proportional to the database scale. In order to reduce the estimation error and computational cost, the IME method is equivalently implemented and simplified by the fully connected layer in this section, which is called IME layer in this paper.</p><p>The IME layer can be regarded as transform matrix which integrates the estimation of geodesic distances with dimension reduction. We learn the transform matrix according to the original representations ? X and the IME representations ? IM E of database images. We minimize the following objective function f (M ) to calculate the weights of IME layer as the transform matrix M ? n?m .</p><formula xml:id="formula_9">argmin M ? X T M ? ? IM E T 2 F + ? M 2 F<label>(5)</label></formula><p>Where ? X ? n?d and ? IM E ? m?d are original representations and IME representations of the d images on retrieval database respectively. n and m are the dimensions of original representation and IME representation respectively. M F denotes the F -norm of matrix M . The first term in the objective function is the transformation cost term for minimizing the difference between the representations computed by the IME algorithm and representations mapped by the IME layer. M F is a regularization term, and ? controls the weight of the regularization.</p><p>In this case, this is equivalent to a ridge regression problem and M has a closed form solution. Let gradient ?f (M ) ?M =0 to minimize the objective function f (M ).</p><formula xml:id="formula_10">?f (M ) ?M = (? X ? X T + ?I)M ? ? X ? IM E T = 0 (6)</formula><p>Then, this reduces to</p><formula xml:id="formula_11">M = (? X ? T X + ?I) ?1 ? X ? T IM E<label>(7)</label></formula><p>Where I is the identity matrix. Since n is the dimensions of original representation, which is low, solving this problem (which needs to be solved only once, at learning time) is extremely fast. IME layer is the integration and simplification version of IME. The computational complexity of IME layer is low at both learning and retrieval steps. IME layer simplifies the calculation processes and integrates the estimation of geodesic distances with dimension reduction by ridge regression. Through the integration of IME, we reduce the estimation error and the loss of discriminative information. In IME, the representation of query image is directly computed by the corrected geodesics distances projected by the eigenvector matrix of training data. Therefore the estimation error of geodesics distances affect the result significantly. The performance of IME also depend on the parameters of computing processes very much. The IME layer is the integration of IME, which diminishes the number of parameters and omits the computation of the shortest geodesic distances from query to each point in the training data. The cumulative computation error in intermediate step is avoided by integration. In practice, our integration strategy also can be applied to other manifold learning methods <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> for image retrieval task. For SIFT-based representations <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, IME layer can work as the transform matrix to map the vector representations into embedding space. It also can be directly connected to CNNs <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> as an additional fully connected layer for CNNbased representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head><p>This section presents the experimental setup and investigates the accuracy of our approaches for image retrieval on five public datasets. To evaluate the efficiency and effectiveness of our IME layer, we compare the IME layer with the related manifold learning methods and the state-of-the-art image retrieval methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We evaluate the performance of our IME layer on five standard datasets for image retrieval. Mean average precision (mAP) is used as the performance measure on all datasets.</p><p>Two are well-known image retrieval benchmarks: Ox-ford5k <ref type="bibr" target="#b46">[47]</ref> and Paris6k <ref type="bibr" target="#b47">[48]</ref>. Oxford5k contains 5062 images collected from Flickr by searching for particular Oxford landmarks. Paris6k dataset contains 6412 photographs from Flickr associated with Paris landmarks. 55 queries corresponding to 11 buildings are manually annotated. The performance is measured using mean average precision (mAP) over the 55 queries.</p><p>For large scale image retrieval, we experiment at Ox-ford105k and Paris106k datasets which add 100k distractor images from Flickr <ref type="bibr" target="#b46">[47]</ref>.</p><p>The fifth dataset is the recently introduced instance search dataset called INSTRE <ref type="bibr" target="#b48">[49]</ref>. It contains various everyday 3D or planar objects from buildings to logos with many variations such as different scales, rotations and occlusions. Some objects cover a small part of the image, making it a challenging dataset. It contains of 28543 images from 250 different object classes. In particular, 100 classes with images retrieved from on-line sources, 100 classes with images taken by the dataset creators, and 50 classes consisting of pairs from the second category. Different from the original protocol <ref type="bibr" target="#b48">[49]</ref> that uses all databases images as queries, we evaluate the performance in the same way as the recent works <ref type="bibr" target="#b44">[45]</ref>. The INSTRE dataset is randomly split into 1250 queries, 5 per class, and 27293 database images, while a bounding box defines the query region. The query and the database sets have no overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>Our IME layer can combine with both SIFT-based representations and CNN-based representations. For CNN-based representations, we employ the fine-tuned network for image retrieval <ref type="bibr" target="#b28">[29]</ref> to extract the representation vectors. This finetuned ResNet101 produces 2048 dimensional representations. We extract regions at 3 different scales as in R-MAC <ref type="bibr" target="#b22">[23]</ref>, and we additionally include the full image as a region. In this fashion, each image has 21 regions on average. The regional representations are aggregated and re-normalized to unit norm in order to construct the original representations, which is exactly as in R-MAC <ref type="bibr" target="#b22">[23]</ref>. For SIFT-based representations, we employ the triangulation embedding <ref type="bibr" target="#b12">[13]</ref> to aggregate the RootSIFT descriptors <ref type="bibr" target="#b49">[50]</ref>. In practice, we employ the 8064dimensional representations of which the vocabulary size is 64.</p><p>The weights of the correction term and regularization term are set as ? = 2.0 and ? = 1.0 respectively, throughout our experiments. Time measurements are reported with a 32-core Intel Xean 2.2GHz CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Impact of different components</head><p>In this section, we conduct a series of experiments on second-order proximity, similarity computation, parameters of IME , IME layer and various original representations.</p><p>Second-order proximity. To demonstrate the effectiveness of second-order proximity information, we compare the results of employing first-order graph G 1th and graph G respectively to compute the geodesic distances in <ref type="table">Table.</ref> I. The weights of the edges in graph G are calculated based on both first-order and second-order relationships of database images. Obviously, the performance of graph G is consistently better on all datasets. The results show that graph G is more reliable to calculate geodesic distance. The second-order neighbour relationship is effective to suppress the interference of manifold's sample holes. As a result, the constructed k-NN graph G is more stable and robust.  Similarity computation. We compare different conversion functions in <ref type="figure" target="#fig_2">Fig. 3</ref>, such as quadratic function S(d(i, j)) = ? d(i,j) 2 2 and t-distribution S(d(i, j)) = 1 1+d(i,j) 2 . These conversion operators are used in IsoMap <ref type="bibr" target="#b31">[32]</ref> and t-SNE <ref type="bibr" target="#b35">[36]</ref> respectively. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the performance versus different functions to convert distances into similarity. The t-distribution performs much better than quadratic function on all datasets. It enhances the discrimination of the moderate geodesic distances and suppresses the difference of large geodesic distances. Due to error accumulation, the calculation error of large geodesic distances is large. The t-distribution reduces the calculation error of similarity between the points that are far apart by suppressing the difference of large geodesic distances, while quadratic function increases the calculation error. Therefore, we employ a Student t-distribution with one degree of freedom (which is the same as Cauchy distribution) as the conversion function in our IME layer.</p><p>Parameters of IME. We evaluate the performance of different parameters of IME, such as the number of iterations Iter, dimensions of IME representation m, the number of neighbours k and the strength of correction ?. <ref type="figure">Fig. 4(a)</ref> shows the performance versus the number of iterations Iter. Our IME layer performs well just with a small number of iterations. Too many iterations lead to the overfitting of embedding. We set Iter=2 in the rest experiments in this paper due to the better performance and moderate training time.</p><p>The performance comparison of various dimensions of embedded representations is shown in <ref type="figure">Fig. 4(b)</ref>. We achieve 92.0 and 96.6 on Oxford5k and Paris6k datasets respectively when we employ the 2048-dimensional IME representation. The high-dimensional representation preserves more discriminative information, so performance of it is better than lowdimensional representation. As shown in <ref type="figure">Fig. 4(b)</ref> by dotted lines, the mAP of original representation are 83.9 and 93.8 on Oxford5k and Paris6k datasets respectively. Our method achieves better performance than original representation even if the dimensions of final IME representation is one sixteenth of original representation, 128 dimension. The results show that our low-dimensional IME representation still preserves the intrinsic manifold. The reconstructed manifold is effective for image retrieval task to search the similar images. <ref type="figure">Fig. 4(c,d)</ref> shows the performance versus the number of neighbours to construct the k-NN graph. k 1 and k 2 are the number of neighbours for first and second iterations respectively. The selection of parameter k i depends on the sparsity of datasets to some degree. The small k i is better suited for sparser dataset. With small k i , most of the neighbours are positive. With large k i , there are many negative neighbours in the graph especially for the sparse dataset. Therefore the constructed k-NN graph with too large k i is unstable for sparse dataset. The calculation error of geodesic distances computed according to the graph with too large k i is large. But the dense dataset is more adaptable. Even with large k i , the stability of neighbours in dense dataset are little influenced by noise due to more stable neighbours. There are more similar objects in Paris6k than Oxford6k. The images in Oxford5k are sparser. But the images in Paris6k are denser. Therefore Paris6k is almost unaffected by k i but Oxford5k degrades with k i .</p><p>We evaluate the effect of various correction weights ?, and then report the results in <ref type="figure">Fig. 4(e,f)</ref>. ? 1 and ? 2 are the strength of correction for first and second iterations respectively. The results show that the performance of our method does not heavily rely on the correction weights ?. We set the correction weights ? = 2.0 in all the other experiments. The estimation error of geodesic distance is larger in the first iteration. ? 1 has a more significant impact than ? 2 . If we do not correct the geodesic distances D G by Euclidean distances D X while iterative embedding (that is, ? i = 0, i = 1, 2), the results are 89.9 and 95.7 on Oxford5k and Paris6k datasets respectively as shown in <ref type="figure">Fig. 4</ref>(e,f) by dotted lines. datasets respectively. The mAPs of uncorrected geodesic distances (? i = 0, i = 1, 2) are lower than mAPs of corrected geodesic distances (? i = 2, i = 1, 2), 92.0 and 96.6 on Oxford5k and Paris6k datasets respectively. The results demonstrate that the correction is important for the geodesic distances computed by the incomplete data. IME layer. The proposed IME layer is the integration and simplification version of IME. The accuracy and average additional query time of PCA, IME, IME layer and other manifold learning methods are shown in <ref type="table" target="#tab_0">Table II</ref>. IME layer <ref type="figure">Fig. 4</ref>. Performance comparison of different parameters on three datasets with varying the number of iterations, dimensions of IME representation, number of neighbours and correction weights. The number of iterations are set as Iter=2 due to its better performance. Even if the dimension m is reduced to 128, the performance of IME representation is better than original representation. The mAP of original representation is shown in (b) by dotted lines. Our method is not heavily relied on the correction weights w, and we set w=2 in other experiments. achieves better performance on both mAP and time cost. The additional computation cost of IME layer is roughly identical to PCA. On Oxford5k and Paris6k datasets, our IME layer is more than twenty-seven times faster than IsoMap <ref type="bibr" target="#b31">[32]</ref> and LLE <ref type="bibr" target="#b32">[33]</ref>, the manifold learning methods that can be applied to image retrieval, and significantly outperforms them on mAP. The computational cost of IME layer is unrelated to the scale of database, while the cost of IsoMap <ref type="bibr" target="#b31">[32]</ref> and LLE <ref type="bibr" target="#b32">[33]</ref> is proportional to the number of database images. On a large scale dataset INATRE, our method is more than 120 times faster than them. The performance of IME layer is better than IME. Because the integration reduces the calculation error of the geodesics distances between query image and database images and the loss of discriminative information in dimension reduction. The results demonstrate that our IME layer is effective and efficient for image retrieval.</p><p>Various original representations. We do experiments on both SIFT descriptors and CNN features. <ref type="table">Table.</ref> III presents the results of the accuracy of SIFT-based and CNN-based representations with/without IME layer. The dimensions of final IME representation is reduced to 2048 in this experiment. The results demonstrate that our IME layer is effective for various features. The IME layer can be directly connected with a CNN as the trained fully connected layer. For other features, our IME layer can work as the transform matrix to map the aggregated representations into embedding space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with the state-of-the-art</head><p>We compare with the state-of-the-art approaches with global representation. <ref type="table" target="#tab_0">Table IV</ref> summarizes the results. Our IME layer significantly outperforms all the existing dimension reduction and manifold learning methods on all datasets. Without post-processing, our IME layer still outperforms the state-of-the-art image retrieval methods with reranking on most datasets.</p><p>In the first part of the table, we show results of the methods that employ global representations of images and do not perform any form of spatial verification or query expansion at query time. The 2048-dimensional R-MAC vectors <ref type="bibr" target="#b28">[29]</ref> in the first part are employed as the original CNN-based representations in this paper.</p><p>We compare our IME layer with related dimension reduction and manifold learning methods in the second part of the table. Except for IsoMap <ref type="bibr" target="#b31">[32]</ref> and LLE <ref type="bibr" target="#b32">[33]</ref>, other nonlinear manifold learning methods can not be directly applied to image retrieval. We consistently outperform them for various dimensions on all datasets. In one case (namely, on INSTRE), our method is more than 13 mAP points ahead of the best competitor <ref type="bibr" target="#b31">[32]</ref>. Our IME layer requires less computational cost compared with IsoMap <ref type="bibr" target="#b31">[32]</ref> and LLE <ref type="bibr" target="#b32">[33]</ref> in the online image retrieval stage, and same computational cost as PCA <ref type="bibr" target="#b50">[51]</ref> and ICA <ref type="bibr" target="#b51">[52]</ref>.</p><p>As shown in the third part of the table, we show the results of state-of-the-art methods that employ global representations and perform search reranking (e.g. , spatial verification <ref type="bibr" target="#b52">[53]</ref>, query expansion (QE) <ref type="bibr" target="#b40">[41]</ref> or diffusion <ref type="bibr" target="#b44">[45]</ref>) at query time. Without reranking, our method still outperforms the state-ofthe-art methods with post-processing on most datasets. These methods with post-processing <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b44">[45]</ref> contain image scoring and ranking steps more than once. It is worth noting that our IME layer method is much faster than these methods and has comparable performance.</p><p>For Oxford105k and Paris106k datasets, we use the incomplete data (the images in Oxford5k and Paris6k respectively) and 5000 noisy data from 100k distractor images on Flickr <ref type="bibr" target="#b46">[47]</ref> to learn the weights of IME layer in the off-line stage taking a few minutes. In comparison, diffusion <ref type="bibr" target="#b44">[45]</ref> employs all 100 thousand images to construct the manifold and takes many hours. Our IME layer has less than 2 milliseconds additional time per query in the on-line retrieval stage, while diffusion <ref type="bibr" target="#b44">[45]</ref> requires about 14 seconds. Employing sparse samples to learn the weights of IME layer, our IME layer still achieves good performance on large scale image retrieval. The results demonstrate that our IME layer is effective to reconstruct image manifold by incomplete data.</p><p>In <ref type="figure" target="#fig_3">Fig. 5</ref> we present some query examples using original representation and IME representation respectively. The images with gray border are cropped query images. The images with green border are positive retrieval results (ground-truth) and the images with red border are negative results. IME layer significantly improves the retrieval results by mapping the original representations into embedding space. Due to the reconstructed continuous manifold of images, the images that contain the same objects in different viewing angles and various illumination are closer in the embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper we propose a manifold learning method called iterative manifold embedding (IME) layer and demonstrate its efficiency and effectiveness for image retrieval. Through the unsupervised strategy, the weights of IME layer are learned by incomplete data.</p><p>Our IME layer introduces the manifold learning into image retrieval. We solve the sample holes problem on manifold learning, using the information of second-order proximity and the correction of geodesic distances by Euclidean distances. In order to reduce the additional computational cost and estimation error of geodesic distances at query time, we integrate the manifold-based embedding by the approximate linear mapping.</p><p>Experiments on five standard retrieval datasets demonstrate that our IME layer significantly outperforms related dimension reduction and manifold learning methods with equivalent or lower computational complexity. Without search reranking, our method still outperforms the state-of-the-art methods with search reranking on most datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. FUTURE WORK</head><p>The main limitation of our IME layer is that the off-line learning is time-consuming. Especially for large datasets, the cost of construction of k-NN graph and calculation of geodesic distances is large. We will try to speed up the off-line learning stage in the future work by parallel computing and other strategies. The IME layer is learned via a linear transform in this work. We will employ nonlinear transform and try to learn more layers end-to-end in the future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Flow chart of the image retrieval framework which is based on proposed IME layer. We learn the weights of iterative manifold embedding (IME) layer in the first off-line stage. Then, we use the IME layer to embed the original representations according to the intrinsic manifold rapidly in the on-line image retrieval stage. While constructing the k-NN graph, we exploit the information of second-order proximity to suppress the interference of sample holes. The Euclidean distances of original representations are utilized to correct the calculation error of geodesic distances in the manifold embedding step. In order to better reconstruct the intrinsic manifold by incomplete sampled data, we repeat the k-NN graph construction and manifold embedding steps many times. By adequate approximation and simplification, IME layer embeds the original representations into IME representations and preserves the intrinsic manifold of database images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 : 1 .</head><label>31</label><figDesc>Let ? p be the p-th largest eigenvalue of the matrix S COR , and v i p be the i-th component of the p-th eigenvector. Then, the p-th component of the m-dimensional manifold-based representation ? Y (i) equals to ? p v i p . The dimension m impacts the discrimination of the representation ? Y (i). The Algorithm 1 Iterative Manifold Embedding Input: Original representation ? X , number of iterations Iter. Output: Iterative manifold embedding representation ? IM E 1: I = 0; 2: while I &lt; Iter do Construct graph G and calculate the geodesic distances D G : 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Performance comparison of different methods to convert distances into similarity. T-distribution has better performance, because it reduces the calculation error of similarity between the points that are far apart and enhances the discrimination of moderate geodesic distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Sample retrieval results of original representation and IME representation. The images with gray border are query images. The images with green border are positive retrieval results (ground-truth) and the images with red border are negative results. Because the reconstructed manifold of images is continuous and smooth, the images that contain the same objects in different viewing angles and various illumination are closer in the embedding space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="4">COMPARISON BETWEEN FIRST-ORDER GRAPH AND</cell></row><row><cell></cell><cell></cell><cell cols="2">SECOND-ORDER GRAPH</cell><cell></cell><cell></cell></row><row><cell cols="6">Graph INSTRE Oxford5k Oxford105k Paris6k Paris106k</cell></row><row><cell>G 1th</cell><cell>80.7</cell><cell>89.5</cell><cell>85.9</cell><cell>92.5</cell><cell>85.6</cell></row><row><cell>G</cell><cell>82.4</cell><cell>92.0</cell><cell>87.2</cell><cell>96.6</cell><cell>93.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>COMPARISON WITH OTHER MANIFOLD LEARNING METHODS FOR IMAGE RETRIEVAL</figDesc><table><row><cell></cell><cell></cell><cell>mAP</cell><cell></cell><cell cols="3">Average additional query time (second)</cell></row><row><cell>Method</cell><cell cols="3">Oxford5k Paris6k INSTRE</cell><cell cols="2">Oxford5k Paris6k</cell><cell>INSTRE</cell></row><row><cell>PCA [51]</cell><cell>82.6</cell><cell>91.5</cell><cell>62.2</cell><cell>0.002</cell><cell>0.002</cell><cell>0.002</cell></row><row><cell>IsoMap [32]</cell><cell>77.9</cell><cell>91.8</cell><cell>68.6</cell><cell>0.378</cell><cell>0.403</cell><cell>3.483</cell></row><row><cell>LLE [33]</cell><cell>51.7</cell><cell>40.5</cell><cell>42.7</cell><cell>0.054</cell><cell>0.066</cell><cell>0.249</cell></row><row><cell>IME</cell><cell>83.5</cell><cell>93.4</cell><cell>75.9</cell><cell>0.907</cell><cell>0.937</cell><cell>7.659</cell></row><row><cell>IME layer</cell><cell>92.0</cell><cell>96.6</cell><cell>82.4</cell><cell>0.002</cell><cell>0.002</cell><cell>0.002</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>OF IME LAYER FOR VARIOUS FEATURES</figDesc><table><row><cell>Feature</cell><cell cols="2">Oxford5k Oxford105k</cell></row><row><cell>SIFT [13]</cell><cell>52.7</cell><cell>27.6</cell></row><row><cell>SIFT+IME layer</cell><cell>62.2</cell><cell>31.3</cell></row><row><cell>CNN [29]</cell><cell>83.9</cell><cell>80.8</cell></row><row><cell>CNN+IME layer</cell><cell>92.0</cell><cell>87.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>COMPARISON WITH THE STATE-OF-THE-ART METHODS.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Datasets</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="6">Dimensions INSTRE Oxford5k Oxford105k Paris6k Paris106k</cell></row><row><cell></cell><cell></cell><cell cols="2">Original representations</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CroW [24]</cell><cell>512</cell><cell>-</cell><cell>68.2</cell><cell>63.2</cell><cell>79.8</cell><cell>71.0</cell></row><row><cell>R-MAC [27]</cell><cell>512</cell><cell>47.7</cell><cell>77.7</cell><cell>70.1</cell><cell>84.1</cell><cell>76.8</cell></row><row><cell>R-MAC [29]</cell><cell>2048</cell><cell>62.6</cell><cell>83.9</cell><cell>80.8</cell><cell>93.8</cell><cell>89.9</cell></row><row><cell></cell><cell cols="4">Dimension reduction and manifold learning</cell><cell></cell><cell></cell></row><row><cell>PCA [51]</cell><cell>512</cell><cell>50.0</cell><cell>78.2</cell><cell>74.7</cell><cell>91.0</cell><cell>85.4</cell></row><row><cell>ICA [52]</cell><cell>512</cell><cell>50.3</cell><cell>77.5</cell><cell>73.7</cell><cell>90.8</cell><cell>85.2</cell></row><row><cell>IsoMap [32]</cell><cell>512</cell><cell>69.7</cell><cell>77.8</cell><cell>64.7</cell><cell>91.8</cell><cell>69.6</cell></row><row><cell>LLE [33]</cell><cell>512</cell><cell>60.2</cell><cell>64.0</cell><cell>47.6</cell><cell>50.7</cell><cell>21.7</cell></row><row><cell>IME layer</cell><cell>512</cell><cell>83.1</cell><cell>91.2</cell><cell>85.1</cell><cell>96.3</cell><cell>92.5</cell></row><row><cell>PCA [51]</cell><cell>1024</cell><cell>58.7</cell><cell>80.8</cell><cell>78.0</cell><cell>91.7</cell><cell>86.8</cell></row><row><cell>ICA [52]</cell><cell>1024</cell><cell>58.7</cell><cell>81.6</cell><cell>78.0</cell><cell>92.1</cell><cell>87.3</cell></row><row><cell>IsoMap [32]</cell><cell>1024</cell><cell>69.1</cell><cell>78.1</cell><cell>65.4</cell><cell>92.1</cell><cell>72.1</cell></row><row><cell>LLE [33]</cell><cell>1024</cell><cell>50.4</cell><cell>58.8</cell><cell>42.3</cell><cell>45.0</cell><cell>16.2</cell></row><row><cell>IME layer</cell><cell>1024</cell><cell>82.8</cell><cell>91.8</cell><cell>86.2</cell><cell>96.5</cell><cell>92.9</cell></row><row><cell>PCA [51]</cell><cell>2048</cell><cell>62.2</cell><cell>82.6</cell><cell>79.1</cell><cell>91.5</cell><cell>86.5</cell></row><row><cell>ICA [52]</cell><cell>2048</cell><cell>62.2</cell><cell>82.7</cell><cell>79.1</cell><cell>91.5</cell><cell>86.5</cell></row><row><cell>IsoMap [32]</cell><cell>2048</cell><cell>68.6</cell><cell>77.9</cell><cell>68.3</cell><cell>91.8</cell><cell>76.4</cell></row><row><cell>LLE [33]</cell><cell>2048</cell><cell>42.7</cell><cell>51.7</cell><cell>34.9</cell><cell>40.5</cell><cell>14.7</cell></row><row><cell>IME layer</cell><cell>2048</cell><cell>82.4</cell><cell>92.0</cell><cell>87.2</cell><cell>96.6</cell><cell>93.3</cell></row><row><cell></cell><cell></cell><cell cols="2">Search reranking</cell><cell></cell><cell></cell><cell></cell></row><row><cell>QE [41]</cell><cell>2048</cell><cell>70.5</cell><cell>89.6</cell><cell>88.3</cell><cell>95.3</cell><cell>92.7</cell></row><row><cell>SCSM [53]</cell><cell>2048</cell><cell>71.4</cell><cell>89.1</cell><cell>87.3</cell><cell>95.4</cell><cell>92.5</cell></row><row><cell>Diffusion [45]</cell><cell>2048</cell><cell>80.5</cell><cell>87.1</cell><cell>86.8</cell><cell>96.5</cell><cell>95.4</cell></row><row><cell>IME layer</cell><cell>2048</cell><cell>82.4</cell><cell>92.0</cell><cell>87.2</cell><cell>96.6</cell><cell>93.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was supported by the National Natural Science Foundation of China under Grant 61531019, Grant 61601462, and Grant 71621002. The authors would like to thank the Associate Editor and the anonymous reviewers for their contributions to improve the quality of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">1470</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving bag-of-features for large scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="316" to="336" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Accurate image search using the contextual dissimilarity measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harzallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="11" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual word ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1271" to="83" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised semantic feature discovery for image object retrieval and tag refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1079" to="1090" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Database saliency for fast image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="359" to="369" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Localityconstrained linear coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jgou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1704" to="1720" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A comprehensive study over vlad and product quantization in large-scale image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Spyromitros-Xioufis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Y</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1713" to="1728" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2010, European Conference on Computer Vision</title>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Triangulation embedding and democratic aggregation for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3310" to="3317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Faemb: a function approximation-based embedding method for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3556" to="3564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving large-scale image retrieval through robust aggregation of local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bober</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="392" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural codes for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="584" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual instance retrieval with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITE Transactions on Media Technology and Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="258" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Aggregating local deep features for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1269" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Particular object retrieval with integral max-pooling of cnn activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jgou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross-dimensional weighting for aggregated deep convolutional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mellina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="685" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Voronoi-based compact image descriptors: Efficient region-of-interest retrieval with vlad and deeplearning based descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chadha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andreopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5297" to="5307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cnn image retrieval learns from bow: Unsupervised fine-tuning with hard examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep image retrieval: Learning global representations for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="241" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">End-to-end learning of deep visual representations for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Partbased weighting aggregation of deep convolutional features for image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cunzhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chengzuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chunheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baihua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01247</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The manifold ways of perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2268" to="2269" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stochastic neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="833" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Visualization large-scale and high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.00370</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Kruskal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wish</surname></persName>
		</author>
		<title level="m">Multidimensional Scaling. BOOK ON DEMAND POD</title>
		<imprint>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Total recall: Automatic query expansion with a generative feature model for object retrieval,&quot; in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Contextual query expansion for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1104" to="1114" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Diffusion processes for retrieval revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1320" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Democratic diffusion aggregation for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient diffusion on region manifolds: Recovering small objects with compact cnn representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Algorithm 97: shortest path</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Floyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">345</biblScope>
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Instre: a new benchmark for instance-level object retrieval and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2911" to="2918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley interdisciplinary reviews: computational statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="433" to="459" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Independent component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyv?rinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spatially-constrained similarity measurefor large-scale object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1229" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
