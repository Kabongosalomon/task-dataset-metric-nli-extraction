<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Difficulty Curriculum for Generative Adversarial Networks (CuGAN)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petru</forename><surname>Soviany</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Ardei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Mathematics of the Romanian Academy</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University &quot;Politehnica&quot; of Bucharest</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image Difficulty Curriculum for Generative Adversarial Networks (CuGAN)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the significant advances in recent years, Generative Adversarial Networks (GANs) are still notoriously hard to train. In this paper, we propose three novel curriculum learning strategies for training GANs. All strategies are first based on ranking the training images by their difficulty scores, which are estimated by a state-of-the-art image difficulty predictor. Our first strategy is to divide images into gradually more difficult batches. Our second strategy introduces a novel curriculum loss function for the discriminator that takes into account the difficulty scores of the real images. Our third strategy is based on sampling from an evolving distribution, which favors the easier images during the initial training stages and gradually converges to a uniform distribution, in which samples are equally likely, regardless of difficulty. We compare our curriculum learning strategies with the classic training procedure on two tasks: image generation and image translation.</p><p>Our experiments indicate that all strategies provide faster convergence and superior results. For example, our best curriculum learning strategy applied on spectrally normalized GANs (SNGANs) fooled human annotators in thinking that generated CIFAR-like images are real in 25.0% of the presented cases, while the SNGANs trained using the classic procedure fooled the annotators in only 18.4% cases. Similarly, in image translation, the human annotators preferred the images produced by the Cycle-consistent GAN (CycleGAN) trained using curriculum learning in 40.5% cases and those produced by CycleGAN based on classic training in only 19.8% cases, 39.7% cases being labeled as ties.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b10">[11]</ref> represent a hot topic in computer vision, drawing the attention of many researchers who developed several improvements of the standard architecture <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43</ref>]. Yet, this kind of neural models are still very hard to train <ref type="bibr" target="#b26">[27]</ref>. In this paper, we study the hypothesis of improving the training process of GANs in terms of both accuracy and time, by employing curriculum learning <ref type="bibr" target="#b2">[3]</ref>. Curriculum learning is the process of training machine learning models by presenting the training examples in a meaningful order which gradually illustrates more complex concepts. Although many curriculum learning approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44]</ref> have been proposed for training deep neural networks, to our knowledge, there are only a few studies that apply curriculum learning to GANs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>In this paper, we propose three novel curriculum learning strategies that provide faster convergence during GANs training, as well as improved results. Our curriculum learning strategies are general enough to be applied to any GAN architecture, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. They rely on a stateof-the-art image difficulty predictor <ref type="bibr" target="#b16">[17]</ref>, which scores the (real) training images with respect to the difficulty of solving a visual search task. After receiving the image difficulty scores as input, we employ one of our curriculum learning strategies listed below:</p><p>? Divide the training images into m easy-to-hard batches and start training the GAN with the easy batch. The other batches are added into the training process, in increasing order of difficulty, after a certain number of iterations. ? Add another component to the discriminator loss function which makes the loss value proportional to the easiness (inverse difficulty) score of the images. The impact of this new component is gradually attenuated, until the easiness score has no more influence in the last training iterations. ? Change the discriminator loss function by including probabilities of sampling real images from a biased distribution that strongly favors easier images during the first training iterations. The probability distribution is continuously updated with each iteration, until it becomes uniform in the last training iterations. Our three curriculum learning strategies follow two important principles. First, we keep the easier images until the end of the training process, to prevent catastrophic forgetting <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref>. Second, we want all training examples to receive equal importance in the end (when training is finished), as we have no reason to favor the easy or the difficult images. However, during the initial stages of training, we emphasize easier images in order to achieve faster convergence and possibly a better local minimum. The real training images are passed to an image difficulty predictor which provides a difficulty score for each image. A curriculum learning strategy that takes into account the difficulty scores is employed to train the discriminator. Best viewed in color.</p><p>We perform image generation experiments using the spectrally normalized GAN (SNGAN) model <ref type="bibr" target="#b28">[29]</ref>, and image translation experiments using the Cycle-consistent GAN (CycleGAN) model <ref type="bibr" target="#b44">[45]</ref>. The goal of our experiments is to compare the standard training process, in which examples are presented in a random order, with the training process based on curriculum. The image generation results on CIFAR-10 <ref type="bibr" target="#b21">[22]</ref> indicate that all the proposed curriculum learning strategies improve the Inception Score (IS) <ref type="bibr" target="#b34">[35]</ref> and the Fr?chet Inception Distance (FID) <ref type="bibr" target="#b14">[15]</ref> over the state-of-the-art SNGAN model. Furthermore, we conducted several human annotations studies, to determine whether our generated or translated images are better than those produced by the baselines SNGAN and CycleGAN, respectively. Our best curriculum learning strategy fooled human annotators in thinking that generated CIFAR-like images are real in 25.0% of the presented cases (on average), while the SNGAN fooled the annotators in only 18.4% cases. This represents an absolute gain of 6.6% over SNGAN. We obtain significant improvements in image translation as well. For example, in the horse2zebra <ref type="bibr" target="#b44">[45]</ref> experiment, the human annotators opted for our method in 52.5% of the presented cases and for the baseline Cy-cleGAN in only 11.9% cases, 35.6% cases being labeled as draws. This represents an absolute gain of 40.6% over CycleGAN. We thus conclude that employing curriculum learning for training GANs is useful.</p><p>We organize the rest of this paper as follows. In Section 2, we present related works and how our approach is different. In Section 3, we describe our curriculum learning strategies for training GANs. We present the image generation and image translation experiments in Section 4. We draw our conclusion and discuss future work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Generative Adversarial Networks. Generative Adversarial Networks <ref type="bibr" target="#b10">[11]</ref> are composed of two neural networks, a generator and a discriminator, which are trained for generating new images, similar to those provided in a training set. Since 2014, many variations of GANs have been proposed in order to improve the quality of the generated samples <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref>. Mirza and Osindero <ref type="bibr" target="#b27">[28]</ref> introduced a conditional version of GANs, termed CGAN, which is based on feeding label information to both the generator and the discriminator. As CGAN, the Auxiliary Classifier GAN (AC-GAN) <ref type="bibr" target="#b30">[31]</ref> is a class conditional model, which in addition, leverages side information through an auxiliary decoder that is responsible for reconstructing class labels. Deep convolutional GANs (DC-GANs) <ref type="bibr" target="#b31">[32]</ref> include a set of constraints to the architectural topology of the classic GAN, to improve training stability. Wasserstein GANs (WGANs) <ref type="bibr" target="#b0">[1]</ref> use the Earth Mover distance instead of other popular metrics to provide easier training, while lowering the chances of entering mode collapse. Still, WGAN employs a weight clipping technique which can result in failure to converge and bad outputs. This problem is addressed in WGAN-GP (Wasserstein GAN with Gradient Penalty) <ref type="bibr" target="#b13">[14]</ref>, where weight clipping is replaced by gradient penalty, providing better performance on different architectures. SNGAN <ref type="bibr" target="#b28">[29]</ref> introduces spectral normalization, another normalization technique used to stabilize the training of the discriminator. Compared to the other regularization methods, spectral normalization provides better results and lower computational costs.</p><p>CycleGAN <ref type="bibr" target="#b44">[45]</ref> performs image translation without requiring paired images to learn the mapping. Instead, it learns the relevant features of two domains and how to translate between these domains. It uses cycle consistency, which encodes the idea that translating from one domain to another and back again should take you back to the same place. Choi et al. <ref type="bibr" target="#b5">[6]</ref> introduced StarGAN, a conditional solution that has the advantage of providing good results when translating between more than two domains, using a single discriminator and generator network.</p><p>Some studies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43]</ref> showed that providing additional information to a GAN model can result in performance improvements in a wide range of common generative tasks. Similar to these approaches, we use an external difficulty score, trying to constrain the order and the importance of the training samples, in order to imitate the easy-to-hard (curriculum) learning paradigm from humans. Curriculum learning. Bengio et al. <ref type="bibr" target="#b2">[3]</ref> studied easy-tohard strategies to train machine learning models, showing that machines can also benefit from learning by gradually adding more difficult examples. They introduced a general formulation of the easy-to-hard training strategies known as curriculum learning. In the past few years curriculum learning has been applied to semi-supervised image classification <ref type="bibr" target="#b9">[10]</ref>, language modelling <ref type="bibr" target="#b11">[12]</ref>, question answering <ref type="bibr" target="#b11">[12]</ref>, object detection <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref>, person reidentification <ref type="bibr" target="#b40">[41]</ref>, weakly supervised object detection <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref>. Other works proposed refined techniques for improving neural network training components, e.g. dropout <ref type="bibr" target="#b29">[30]</ref>, or training frameworks, e.g. teacher-student <ref type="bibr" target="#b18">[19]</ref>, using curriculum learning. Ionescu et al. <ref type="bibr" target="#b16">[17]</ref> considered an image difficulty predictor trained on image difficulty scores produced by human annotators. Similar to Ionescu et al. <ref type="bibr" target="#b16">[17]</ref>, we use an image difficulty predictor, but with a completely different purpose, that of training GANs. In addition, we explore several curriculum learning strategies that enable end-to-end training by defining new curriculum loss functions. Curriculum GANs. To our knowledge, there are a just few works that propose curriculum learning approaches for training GANs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>. Doan et al. <ref type="bibr" target="#b6">[7]</ref> introduced an adaptive curriculum learning strategy for training GANs, called acGAN, which uses multiple discriminators with different architectures of various depths. The authors proposed a reward function that uses an online multi-armed bandit algorithm. The reward function measures the progress made by the generator and uses it to update the weights of each discriminator, ensuring that the generator and the discriminators learn at the same pace. Different from the approach of Doan et al. <ref type="bibr" target="#b6">[7]</ref>, we consider the difficulty of the training samples and propose strategies to train GANs gradually, from the easy images to the hard ones. While the approach of Doan et al. <ref type="bibr" target="#b6">[7]</ref> uses multiple discriminators, increasing the training time, our approach does not require any additional training time.</p><p>Ghasedi et al. <ref type="bibr" target="#b8">[9]</ref> proposed ClusterGAN, an easy-todifficult approach for image clustering. ClusterGAN is an unsupervised model composed of three elements: a generator, a discriminator and a clustering network. The samples are introduced gradually in the training, from the easy ones to the hard ones. The values of the loss function are used as difficulty scores for the corresponding image samples. Their curriculum learning strategy leads to good results when training clustering networks with large depth. While Ghasedi et al. <ref type="bibr" target="#b8">[9]</ref> study the problem of clustering images, we apply curriculum learning in order to generate or translate images. Furthermore, we propose and study three curriculum learning strategies instead of a single one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries and notations</head><p>Generative Adversarial Networks <ref type="bibr" target="#b10">[11]</ref> are composed of two neural networks, the generator (G) and the discriminator (D), which are trained to compete against each other in an adversarial game. The generator learns to generate image samples from a Gaussian noise density p z , such that the generated (fake) images (from the learned density p g ) are difficult to distinguish from real images for the discriminator. Meanwhile, the discriminator is trained to differentiate between real images from a density p r and fake images from the density p g learned by G. The two networks, G and D, compete in a minimax game with the objective function V (G, D) defined as follows:</p><formula xml:id="formula_0">V (G, D) = E x?pr [l(D(x))] + E z?pz [l(?D(G(z)))], (1)</formula><p>where x is a real image sampled from the true data density p r , z is the random noise vector sampled from the density p z , and l is a loss function, e.g. cross-entropy <ref type="bibr" target="#b10">[11]</ref> or Hinge loss <ref type="bibr" target="#b28">[29]</ref>. The goal of the generator G is to minimize this error, while the goal of the discriminator D is to maximize it. Hence, during training, we aim to optimize the objective function as follows:</p><formula xml:id="formula_1">min G max D V (G, D).<label>(2)</label></formula><p>The two networks are alternatively trained until the generator learns the probability density function of the training data p r , i.e. until p g ? p r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Curriculum GANs based on image difficulty</head><p>While machines are commonly trained by presenting examples in a random order, humans learn new concepts by organizing them in a meaningful order which gradually illustrates higher complexity. To this end, Bengio et al. <ref type="bibr" target="#b2">[3]</ref> proposed curriculum learning for training machine learning models, specifically neural networks, which are influenced by the order in which the examples are presented during training. Since deep neural networks are models that essentially try to mimic the brain, it seems natural to also adopt curriculum learning from humans <ref type="bibr" target="#b36">[37]</ref>. We hypothesize that a curriculum learning strategy for training GANs can bring several benefits, e.g. faster convergence, improved stability and superior results. To demonstrate our hypothesis we explore three curriculum learning strategies that are generic enough to be applied to any GAN architecture. In order to learn in the increasing order of difficulty (from easy to hard), we first need to apply an image difficulty predictor on the training set of real images. This allows us to change the distribution of the real images p r in order to introduce curriculum when training the discriminator D. Since the generator G tries to learn a distribution p g that closely follows <ref type="bibr">Figure 2</ref>. From left to right, images in increasing order of difficulty selected from CIFAR-10 <ref type="bibr" target="#b21">[22]</ref>, apple2orange <ref type="bibr" target="#b44">[45]</ref> and horse2zebra <ref type="bibr" target="#b44">[45]</ref> data sets, respectively. Best viewed in color. p r , G is implicitly influenced by the curriculum learning strategy. Therefore, it is not necessary to apply the image difficulty predictor on the generated images, saving the additional training time. Moreover, the difficulty predictor needs to be applied only once on the real images, before starting to train the GANs. We next present the image difficulty predictor and our three curriculum learning strategies. Image difficulty prediction. Ionescu et al. <ref type="bibr" target="#b16">[17]</ref> defined image difficulty as the human response time for solving a visual search task, collecting corresponding difficulty scores for the PASCAL VOC 2012 data set <ref type="bibr" target="#b7">[8]</ref>. We follow the approach proposed in <ref type="bibr" target="#b16">[17]</ref> to build a state-of-the-art image difficulty predictor. The model is based on concatenating deep features extracted from two Convolutional Neural Networks (CNN), VGG-f <ref type="bibr" target="#b4">[5]</ref> and VGG-verydeep-16 <ref type="bibr" target="#b35">[36]</ref>, which are pre-trained on ImageNet <ref type="bibr" target="#b33">[34]</ref>. We remove the softmax layer of each CNN model and use the output of the penultimate fully-connected layer, resulting in a feature vector of 4096 components. We divide each image into 1?1, 2?2 and 3?3 bins in order to obtain a pyramid representation, which leads to performance improvements <ref type="bibr" target="#b16">[17]</ref>. We concatenate the feature vectors corresponding to each bin into a single vector corresponding to the entire image. We L 2 -normalize the concatenated feature vectors before training a ?-Support Vector Regression (?-SVR) <ref type="bibr" target="#b3">[4]</ref> model to regress to the ground-truth difficulty scores provided for PASCAL VOC 2012 <ref type="bibr" target="#b7">[8]</ref>. We use the learned predictor P as an image difficulty scoring function that provides difficulty scores on a continuous scale:</p><formula xml:id="formula_2">s i = P (x i ) ? min xj ?X {P (x j )} max xj ?X {P (x j )} ? 2 ? 1,<label>(3)</label></formula><p>where s i is the difficulty score for the image x i in a set of images X = {x 1 , x 2 , ..., x n }, where n = |X|. Eq. (3) maps the predicted difficulty scores for the set X to the interval [?1, 1]. Our predictor attains a Kendall's ? correlation coefficient of 0.471 on the same test set of <ref type="bibr" target="#b16">[17]</ref>. In <ref type="figure">Figure 2</ref>, we present images in increasing order of difficulty from the data sets considered in our experiments from Section 4.</p><p>Learning using image difficulty batches. Our first curriculum learning strategy is based on dividing the real images into m equally-sized batches indexed from 1 to m, of increasing difficulty, such that images in each batch i + 1 have higher difficulty scores than the images in the batch i, ?i ? {1, 2, ..., m?1}. Thus, the first batch contains the easiest images and the last batch contains the hardest ones. After dividing the images into batches of increasing difficulty, we start training the GANs using only images from the first batch. After a fixed number of iterations, we include images from the second batch into the training. This process continues until all m batches are included into the training. In this way, the generator learns a progressively complex density p g . Since images in the former batches can be learned faster (due to their easiness), we consider a smaller number of iterations during the early training stages. The number of iterations increases as more difficult batches are added.</p><p>Learning by weighting according to difficulty. Our second curriculum learning strategy is based on integrating the difficulty scores into the discriminator loss function, by weighting the real images according to their difficulty scores. In the first training iterations, we aim to provide higher weights to the easy images and lower weights to the difficult images. With each training iteration, the weights of both easy and difficult images gradually converge to a single value. The weights are computed using the following scoring function w P :</p><formula xml:id="formula_3">w P (x i , t) = 1 ? k ? s i ? e ???t ,<label>(4)</label></formula><p>where x i is an image from the set of real images X, s i is the image difficulty score as in Eq. <ref type="formula" target="#formula_2">(3)</ref>, t is the current training iteration index, ? is a parameter that controls how fast the scores converge to the value 1 and k is a parameter that controls the impact of the difficulty weights to the overall loss value. <ref type="figure">Figure 3</ref> illustrates the behavior of w P for various easiness scores in the interval [0, 2]. In the first iteration (when t = 0), the easiness scores are equal to 1 ? s, for k = 1. Note that in the last iterations all images have basically the same weight, regardless of their difficulty. By including the scoring function w P from Eq. (4) into the objective function V defined in Eq. (1), we obtain a novel objective (loss) function V (1) based on curriculum learning, defined as follows:</p><formula xml:id="formula_4">V (1) (G, D, P ) = E x?pr [l(D(x)) + w P (x, t)] + E z?pz [l(?D(G(z)))].<label>(5)</label></formula><p>We note that when t approaches infinity, the objective func- <ref type="figure">Figure 3</ref>. Easiness scores between 0 and 2 converge to 1 as the number of training iterations increases, by applying the scoring function defined in Eq. (4) with k = 1 and ? = 5 ? 10 ?5 . Each curve represents the evolution of the weight for a given image, which starts with the weight equal to its easiness score (1 ? s) at the first iteration and ends with a weight equal to 1, regardless of its initial easiness score. Best viewed in color.</p><p>tion V (1) converges asymptotically to V + 1, i.e.:</p><formula xml:id="formula_5">lim t?? V (1) (G, D, P ) = V (G, D) + 1.<label>(6)</label></formula><p>This can be immediately demonstrated by considering that:</p><formula xml:id="formula_6">lim t?? w P (x, t) = lim t?? 1 ? k ? s ? e ???t = 1.<label>(7)</label></formula><p>Learning by sampling according to difficulty. Our third curriculum learning strategy is based on changing the probability density function of the real images p r , by multiplying it with another probability density function that is proportional to w P defined in Eq. (4):</p><p>p r,w P = p r ? p w P ? w P (x, t).</p><p>By including the novel density p r,w P into the objective function V defined in Eq. (1), we effectively obtain a novel loss function V (2) based on curriculum learning, defined as follows:</p><formula xml:id="formula_8">V (2) (G, D, P ) = E x?pr,w P [l(D(x))] + E z?pz [l(?D(G(z)))].<label>(9)</label></formula><p>We use the weights w P (x i , t) to define a distribution over the training images. We then sample training images from this distribution during training. We define a discrete random variable R with possible values associated to indexes of images in the training set X, such that the probability P rob(R = i) of sampling an index for real image x i from X is equal to the weight w P (x i , t) divided by the sum of all weights, making all probabilities sum up to 1:</p><formula xml:id="formula_9">P rob(R = i) = w P (x i , t) xj ?X w P (x j , t)</formula><p>, ?i ? {1, ..., n}, <ref type="bibr" target="#b9">(10)</ref> where n = |X|. Consequently, easier images have a higher chance of being sampled in the first learning iterations. When k &gt; 1 in Eq. (4), we need to add the constant k ? 1 to each term in Eq. (10), i.e. we replace w P (x, t) with w P (x, t) + k ? 1, to obtain positive values. Towards the end of the training process, as w P converges asymptotically to 1 (see Eq. <ref type="formula" target="#formula_6">(7)</ref>), it becomes equally likely to sample an easy or a difficult image, i.e.:</p><formula xml:id="formula_10">lim w P ?1 P rob(R = i) = 1 n , ?i ? {1, . . . , n},<label>(11)</label></formula><p>where n = |X|. At the limit, p w P converges to a uniform density and p r,w P becomes equal to p r . Observation. V <ref type="bibr" target="#b1">(2)</ref> can be seen as a continuous version of our first curriculum learning approach, in which the probability of sampling a real image x from the set of training images X is given by a step function, where the number of steps is equal to the number of batches m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data sets</head><p>We perform image generation experiments on the CIFAR-10 data set <ref type="bibr" target="#b21">[22]</ref>. It consists of 50000 color train images of 32 ? 32 pixels, equally distributed into 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck. Our translation experiments include two of the data sets used in <ref type="bibr" target="#b44">[45]</ref>. Horse2zebra contains 939 horse images and 1177 zebra images downloaded from Im-ageNet <ref type="bibr" target="#b33">[34]</ref> using the keywords wild horse and zebra. Ap-ple2orange has 996 apple images and 1020 orange images from the same source, labeled with apple and navel orange. All images are 256 ? 256 pixels in size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baselines, evaluation and parameter choices</head><p>Baselines.</p><p>For the image generation experiments on CIFAR-10, we employ a state-of-the-art baseline, SNGAN <ref type="bibr" target="#b28">[29]</ref>, which is based on the Hinge loss. We consider SNGAN as the most relevant baseline, since we use it as starting point for our curriculum learning approaches. However, we include additional models from the recent literature, namely DCGAN <ref type="bibr" target="#b31">[32]</ref>, WGAN-GP <ref type="bibr" target="#b13">[14]</ref>, Parallel Optimal Transport GAN (POT-GAN) <ref type="bibr" target="#b1">[2]</ref> and Generative Latent Nearest Neighbors (GLANN) <ref type="bibr" target="#b15">[16]</ref>. We also include the results of the acGAN proposed by Doan et al. <ref type="bibr" target="#b6">[7]</ref>, which uses adaptive curriculum. Since our curriculum SNGAN-based models are unsupervised, we do not compare with class conditional (supervised) baselines. For the image translation experiments, we employ Cycle-GAN <ref type="bibr" target="#b44">[45]</ref> as baseline. Evaluation metrics. Evaluating the quality and realism of generated content as perceived by humans is not an easy task. At this moment, there is no universally agreed metric able to measure the outputs of GANs, each having its own shortcomings. To automatically quantify the performance, we use the Inception Score (IS) <ref type="bibr" target="#b34">[35]</ref> and the Fr?chet Inception Distance (FID) <ref type="bibr" target="#b14">[15]</ref>, which are computed over 10000 generated images (not used in the training process). The reported scores are averaged over 5 runs. A higher IS or a lower FID indicates higher performance. Along with the automatic metrics, we evaluate the results by asking humans to annotate images, in order to determine if they prefer the baseline GANs or our Curriculum-GANs (CuGANs). Implementation details. In the image generation experiments, we used the SNGAN implementation available at https://github.com/watsonyanghx/GAN_ Lib_Tensorflow, which can reproduce the results reported in <ref type="bibr" target="#b28">[29]</ref>. The model is based on ResNet. We trained the model for 80000 iterations using mini-batches of 64 samples. We observed that the Inception Score stabilizes much sooner (before 50000 iterations) using the Adam optimizer <ref type="bibr" target="#b19">[20]</ref>. The learning rate is 2 ? 10 ?4 . For the first curriculum learning approach, we split the training set in m = 3 batches, as Ionescu et al. <ref type="bibr" target="#b16">[17]</ref>: an easy batch, a medium batch, and a difficult batch. Each batch contains the same number of samples. For the second and the third curriculum learning approaches, we set ? = 5 ? 10 ?5 , which is chosen with respect to the total number of iterations (80000). We conducted preliminary experiments to tune the other parameters. For the first curriculum learning approach, we experimented with three different numbers of iterations to train on the easy batch (5000, 10000 and 15000), and another three numbers of iterations to train on the easy and medium batches together (15000, 20000 and 25000). We obtained slightly better results for training on the easy batch for 15000 iterations, and on both easy and medium batches for 25000 iterations. The rest of the iterations (40000) include all three batches in the training. For the second and the third curriculum learning approaches, we conducted experiments with k ? {1, 2, 4}. When we weight the training images with the corresponding difficulty scores (as in Eq. (5)), we obtain optimal results with k = 2. When we sample the training images according to the difficulty scores (as in Eq. (9)), we obtain optimal results with k = 4.</p><p>In the image translation experiments, we used the Cy-cleGAN implementation available at https://github. com/leehomyc/cyclegan-1. The model is trained for 25000 iterations, using a mini-batch size of 8 samples. As for SNGAN, we employ the Adam optimizer <ref type="bibr" target="#b19">[20]</ref> with a learning rate of 2?10 ?4 . The weight of the cycle consistency loss term in the full objective function is set to ? = 10, as in the original paper <ref type="bibr" target="#b44">[45]</ref>. We apply linear weight decay after the first 12500 iterations. We compare the baseline Cycle-GAN with the Curriculum-CycleGAN based on weighting the training images with the corresponding difficulty scores, since the weighting strategy provides the best FID score in the image generation experiments on CIFAR-10. We did not evaluate the other two curriculum learning approaches to avoiding tripling the human annotation time and costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Image generation results</head><p>Faster convergence. In <ref type="figure" target="#fig_1">Figure 4</ref>, we present the evolution of the Inception Scores for the standard SNGAN and three Method IS FID DCGAN <ref type="bibr" target="#b31">[32]</ref> 6.16 ? 0.07 71.07 ? 1.06 DCGAN (with ResNet) * <ref type="bibr" target="#b31">[32]</ref> 6.64 ? 0.14 -WGAN-GP * <ref type="bibr" target="#b28">[29]</ref> 6.68 ? 0.06 40.20 WGAN-GP (with ResNet) <ref type="bibr" target="#b13">[14]</ref> 7.86 ? 0.07 -GLANN <ref type="bibr" target="#b15">[16]</ref> -46.50 ? 0.20 POT-GAN <ref type="bibr" target="#b1">[2]</ref> 6.87 ? 0.04 32.50 acGAN <ref type="bibr" target="#b6">[7]</ref> 6.22 ? 0.04 49.81 ? 0.23 SNGAN * <ref type="bibr" target="#b28">[29]</ref> 8.  <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref> are compared with our SNGAN-based approaches, each employing a different curriculum learning strategy proposed in this paper. The results marked with an asterisk are taken from the SNGAN paper <ref type="bibr" target="#b28">[29]</ref>. The results of acGAN are based on the source code provided by Doan et al. <ref type="bibr" target="#b6">[7]</ref>. The best IS (higher is better) and the best FID (lower is better) scores are marked in bold.</p><p>other SNGAN versions, that are enhanced through one of our curriculum learning strategies, on CIFAR-10. We note that the performance of each model stabilizes after 50000 iterations. However, the curriculum-based models reach higher IS values, right from the first iterations. This indicates that the Curriculum-SNGANs converge faster than the standard SNGAN. For instance, the Curriculum-SNGAN based on sampling (corresponding to Eq. (9)) achieves about the same IS value as the baseline SNGAN, in only 20000 iterations instead of 65000 iterations. Superior results. In <ref type="table" target="#tab_0">Table 1</ref>, we compare our Curriculum-SNGANs with the standard SNGAN, as well as DC-GAN <ref type="bibr" target="#b31">[32]</ref>, WGAN-GP <ref type="bibr" target="#b13">[14]</ref>, GLANN <ref type="bibr" target="#b15">[16]</ref>, POT-GAN <ref type="bibr" target="#b1">[2]</ref> and acGAN <ref type="bibr" target="#b6">[7]</ref>, on CIFAR-10. First, we note that SNGAN achieves the best results among all baselines, confirming that SNGAN is indeed representative for the state-of-theart. We observe that all our curriculum learning strategies can further boost the performance of SNGAN. When we divide the images into easy-to-hard batches, we achieve an IS of 8.46. The best IS score (8.51) is obtained when we sample the train images according to difficulty. For an unsupervised model, we believe that an IS of 8.51 is noteworthy. Furthermore, our improvements in terms of FID are much higher than all baselines, even compared to the adaptive curriculum approach of Doan et al. <ref type="bibr" target="#b6">[7]</ref>. While easyto-hard batches and sampling provide better IS values, we observe that the Curriculum-SNGAN based on weighting according to difficulty (corresponding to Eq. (5)) achieves the best FID value <ref type="bibr">(14.41)</ref>. For this reason we choose this curriculum learning strategy for the human evaluation experiments.</p><p>We asked 10 human annotators to label images either as real or fake. We provided the same set of 600 images (presented in a random order) to each annotator. We randomly selected 200 real CIFAR images, 200 images generated by SNGAN and 200 images generated by Curriculum-SNGAN (weighting). The goal of the annotation study is to determine the percentage of generated images that fool  the annotators, using the real images as a control set (preventing evaluators from labeling every image as fake). In <ref type="table">Table 2</ref>, we report the number of images labeled as real by each annotator. We note that in 25.7% cases, the annotators labeled real CIFAR images as being fake. Nevertheless, the humans largely figured out what images are generated. The standard SNGAN fooled annotators in 18.4% cases, while the Curriculum-SNGAN fooled annotators in 25.0% cases. Interestingly, each and every human labeled more images generated by Curriculum-SNGAN as real than images generated by the baseline SNGAN. As illustrated in <ref type="figure" target="#fig_2">Figure 5</ref>, there are several images generated by Curriculum-SNGAN, which are labeled as real by 9 out of 10 annotators. The number of votes drops faster for the standard SNGAN approach. All results indicate that the images generated by Curriculum-SNGAN are superior to those generated by SNGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Image translation results</head><p>The image translation results are evaluated only by human annotators. There are four test sets of images <ref type="bibr" target="#b44">[45]</ref>, corresponding to the following translations: horse2zebra (120 images), zebra2horse (140 images), apple2orange (266 images) and orange2apple (248 images). We asked 6 hu- man annotators to choose between the images translated by CycleGAN and those translated by Curriculum-CycleGAN (weighting), without disclosing any information about the models. In each case, we also provided the original (source) image. Since the test images are fixed for both models, the random chance factor is eliminated. In <ref type="figure" target="#fig_3">Figure 6</ref>, we show several images translated by both models side by side. We notice that there are several image pairs in which all 6 annotators opted for Curriculum-CycleGAN. For horse2zebra, the baseline CycleGAN wins when our model produces brownish zebras. For apple2orange, annotators prefer the baseline when our model produces artifacts, but they prefer our model when it produces the right tone of orange.</p><p>In <ref type="table" target="#tab_2">Table 3</ref>, we present the average percentage of cases (computed on the 6 annotators) in which the annotators prefer either the CycleGAN output images or the Curriculum-CycleGAN output images, as well as the percentage of tied cases (images are labeled as equally good). We note that on three sets of images (horse2zebra, zebra2horse and ap-ple2orange), the annotators show significant preference for our Curriculum-CycleGAN based on weighting. Furthermore, in these three test sets, all humans prefer our model over the baseline CycleGAN (the individual percentages are not shown in <ref type="table" target="#tab_2">Table 3</ref> due to lack of space). For or-ange2apple, only 2 out of 6 annotators prefer our model, although our model has a higher average preference (35.1%) compared to the baseline (32.7%), as seen in <ref type="table" target="#tab_2">Table 3</ref>. All in all, the human annotators seem to prefer the curriculumbased approach in 20.7% more cases than the baseline Cy-cleGAN, confirming once more that the curriculum strategy is indeed useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we presented three curriculum learning strategies for training GANs. The empirical results indicate that our curriculum learning strategies achieve faster convergence during training, i.e. the number of training iterations can be reduced by a factor of three without affecting the quality of the generative results. Furthermore, using a similar number of training iterations, our curriculum learning strategies can boost the quality of the generative and translation results, surpassing all considered baselines <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b44">45]</ref> on CIFAR-10. Both automatic measures and human evaluators confirm our findings. While we conducted experiments on images of 32 ? 32 and 256 ? 256 of pixels in size, in future work, we aim to apply our curriculum learning strategies in order to generate larger images, containing a natural variety of object classes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Our GAN training pipeline based on curriculum learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Inception Scores (IS) of SNGAN (baseline) versus three Curriculum-SNGAN models based on various curriculum learning strategies (batches, sampling, weighting), on CIFAR-10. The scores are computed on generated images, not used in the training process. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Most voted and least voted images from the set of 600 images labeled by human annotators. Images on each row are selected from different subsets: real images, generated by SNGAN and generated by Curriculum-SNGAN with weighting. Best viewed in color. A B C D E F G H I J Avg. CIFAR-10 156 195 124 168 151 142 167 160 95 127 74.3% SNGAN [29] 36 111 26 18 30 34 40 38 24 11 18.4% Curriculum 48 123 27 27 39 60 67 58 28 23 25.0%Table 2. Number of images voted as real by 10 human annotators (identified by letters from A to J). The annotators were asked to label 600 images (200 real CIFAR-10 images, 200 images generated by SNGAN and another 200 images generated by Curriculum-SNGAN with weighting) as real or fake.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Side by side image pairs generated by CycleGAN (left image in each pair) and Curriculum-CycleGAN (right image in each pair) with the corresponding number of votes provided by 6 human annotators. When the sum of the number of votes in a pair is lower than 6, it means that the missing votes correspond to ties. Image pairs that received most votes in favor of CycleGAN are presented in the left-hand side of the figure, while image pairs that received most votes in favor of Curriculum-CycleGAN are presented in the right-hand side. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>22 ? 0.05 21.70 ? 0.21 Curriculum-SNGAN (batches) 8.46 ? 0.13 14.64 ? 0.31 Curriculum-SNGAN (weighting) 8.44 ? 0.11 14.41 ? 0.24 Curriculum-SNGAN (sampling) 8.51 ? 0.09 14.48 ? 0.26 Inception Scores (IS) and Fr?chet Inception Distances (FID) on CIFAR-10. Several unsupervised GAN models</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>CycleGAN 52.5% 37.4% 37.1% 35.1% 40.5% Ties 35.6% 48.7% 42.3% 32.2% 39.7%</figDesc><table><row><cell>Option</cell><cell>H?Z Z?H A?O O?A Avg.</cell></row><row><cell>CycleGAN [45]</cell><cell>11.9% 13.9% 20.6% 32.7% 19.8%</cell></row><row><cell>Curriculum-</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Average percentage of cases in which 6 human annotators consider images generated by CycleGAN as better, images generated by Curriculum-CycleGAN (weighting) as better, or both equally good. Evaluations are provided for 4 test sets of images: horse2zebra (H?Z), zebra2horse (Z?H), apple2orange (A?O), orange2apple (O?A). The overall average is also included.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Parallel Optimal Transport GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Avraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="4411" to="4420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Training ?-Support Vector Regression: Theory and Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1959" to="1977" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8789" to="8797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On-line Adaptative Curriculum Learning for GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Albuquerque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mazoure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Balanced Self-Paced Learning for Generative Adversarial Clustering Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ghasedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-modal curriculum learning for semisupervised image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automated curriculum learning for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Curriculum learning for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of FG</title>
		<meeting>FG</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="505" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Non-Adversarial Image Synthesis with Generative Latent Nearest Neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="5811" to="5819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How hard can it be? estimating the difficulty of visual search in an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2157" to="2166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mentor-Net: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiple Instance Curriculum Learning for Weakly Supervised Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">PacGAN: The power of two samples in Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1505" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Paul</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of Learning and Motivation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="165" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The numerics of GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1825" to="1835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional Generative Adversarial Nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Curriculum dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cavazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conditional Image Synthesis with Auxiliary Classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of ICML</title>
		<meeting>eeding of ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generative Adversarial Text to Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The developing infant creates a curriculum for statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Clerkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="325" to="336" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Frustratingly Easy Tradeoff Optimization between Single-Stage and Two-Stage Deep Object Detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Soviany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CEFRL Workshop of ECCV</title>
		<meeting>CEFRL Workshop of ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="366" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Optimizing the Trade-off between Single-Stage and Two-Stage Deep Object Detectors using Image Difficulty Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Soviany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SYNASC</title>
		<meeting>SYNASC</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="209" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">AdaGAN: Boosting Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">O</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Simon-Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5424" to="5433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">MANCS: A Multi-task Attentional Network with Curriculum Sampling for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Weakly-and Semi-supervised Faster R-CNN with Curriculum Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generative image modeling using style and structure adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="318" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Leveraging priorknowledge for weakly supervised object detection under a collaborative self-paced curriculum learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
