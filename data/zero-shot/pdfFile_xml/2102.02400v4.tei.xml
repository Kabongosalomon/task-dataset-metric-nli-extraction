<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Provably End-to-end Label-noise Learning without Anchor Points</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
						</author>
						<title level="a" type="main">Provably End-to-end Label-noise Learning without Anchor Points</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In label-noise learning, the transition matrix plays a key role in building statistically consistent classifiers. Existing consistent estimators for the transition matrix have been developed by exploiting anchor points. However, the anchorpoint assumption is not always satisfied in real scenarios. In this paper, we propose an end-toend framework for solving label-noise learning without anchor points, in which we simultaneously optimize two objectives: the cross entropy loss between the noisy label and the predicted probability by the neural network, and the volume of the simplex formed by the columns of the transition matrix. Our proposed framework can identify the transition matrix if the clean class-posterior probabilities are sufficiently scattered. This is by far the mildest assumption under which the transition matrix is provably identifiable and the learned classifier is statistically consistent. Experimental results on benchmark datasets demonstrate the effectiveness and robustness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The success of modern deep learning algorithms heavily relies on large-scale accurately annotated data <ref type="bibr" target="#b4">(Daniely &amp; Granot, 2019;</ref><ref type="bibr" target="#b14">Han et al., 2020b;</ref><ref type="bibr" target="#b41">Xia et al., 2020;</ref><ref type="bibr">Berthon et al., 2021)</ref>. However, it is often expensive or even infeasible to annotate large datasets. Therefore, cheap but less accurate annotating methods have been widely used <ref type="bibr" target="#b43">(Xiao et al., 2015;</ref><ref type="bibr" target="#b22">Li et al., 2017;</ref><ref type="bibr" target="#b13">Han et al., 2020a;</ref><ref type="bibr" target="#b49">Yu et al., 2020;</ref><ref type="bibr" target="#b53">Zhu et al., 2021a)</ref>. As a consequence, these alternatives inevitably introduce label noise. Training deep learning models on noisy data can significantly degenerate the test performance due to overfitting to the noisy labels 1 University of New South Wales 2 Trustworthy Machine Learning Lab, University of Sydney 3 Hong Kong Baptist University 4 RIKEN AIP 5 University of Tokyo. Correspondence to: Tongliang Liu &lt;tongliang.liu@sydney.edu.au&gt;.</p><p>Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). <ref type="bibr" target="#b0">(Arpit et al., 2017;</ref><ref type="bibr" target="#b50">Zhang et al., 2017;</ref><ref type="bibr" target="#b39">Xia et al., 2021;</ref><ref type="bibr" target="#b39">Wu et al., 2021)</ref>.</p><p>To mitigate the negative impacts of label noise, many methods have been developed and some of them are based on a loss correction procedure. In general, these methods are statistically consistent, i.e., these methods guarantee that the classifier learned from the noisy data approaches to the optimal classifier defined on the clean risk as the size of the noisy training set increases <ref type="bibr" target="#b24">(Liu &amp; Tao, 2016;</ref><ref type="bibr" target="#b35">Scott, 2015;</ref><ref type="bibr" target="#b29">Natarajan et al., 2013;</ref><ref type="bibr" target="#b10">Goldberger &amp; Ben-Reuven, 2017;</ref><ref type="bibr" target="#b32">Patrini et al., 2017;</ref><ref type="bibr" target="#b38">Thekumparampil et al., 2018)</ref>. The idea is that the clean class-posterior P (Y |X = x) := [P (Y = 1|X = x), . . . , P (Y = C|X = x)] can be inferred by utilizing the noisy class-posterior P (? |X = x) and the transition matrix T (x) where T ij (x) = P (? = i|Y = j, X = x), i.e., P (Y |X = x) = [T (x)] ?1 P (? |X = x). While those methods theoretically guarantee the statistical consistency, they all heavily rely on the success of estimating transition matrices.</p><p>Generally, the transition matrix is unidentifiable without additional assumptions <ref type="bibr" target="#b40">(Xia et al., 2019)</ref>. In the literature, methods have been developed to estimate the transition matrices under the so-called anchor-point assumption: it assumes the existence of anchor points, i.e., instances belonging to a specific class with probability one <ref type="bibr" target="#b24">(Liu &amp; Tao, 2016)</ref>. The assumption is reasonable in certain applications <ref type="bibr" target="#b24">(Liu &amp; Tao, 2016;</ref><ref type="bibr" target="#b32">Patrini et al., 2017)</ref>. However, the violation of the assumption in some cases could lead to a poorly learned transition matrix and a degenerated classifier <ref type="bibr" target="#b40">(Xia et al., 2019)</ref>. This motivates the development of algorithms without exploiting anchor points <ref type="bibr" target="#b40">(Xia et al., 2019;</ref><ref type="bibr" target="#b25">Liu &amp; Guo, 2020;</ref><ref type="bibr" target="#b44">Xu et al., 2019;</ref><ref type="bibr" target="#b54">Zhu et al., 2021b)</ref>. However, the performance is not theoretically guaranteed in these works.</p><p>Motivation. In this work, our interest lies in designing a consistent algorithm without anchor points, subject to class-dependent label noise, i.e., T (x) = T for any x in the feature space. Our algorithm is based on a geometric property of the label corruption process. Given an instance x, the noisy class-posterior probability P (? |X = x) := [P (? = 1|X = x), . . . , P (? = C|X = x)] can be thought of as a point in the C-dimensional space where C is the number of classes. Since we have P (? |X = arXiv:2102.02400v4 <ref type="bibr">[cs.</ref>LG] 21 Oct 2021</p><p>Provably End-to-end Label-noise Learning without Anchor Points x) = T P (Y |X = x) and C i=1 P (Y = i|X = x) = 1, P (? |X = x) is then a convex combination of the columns of T . This means that the simplex Sim{T } formed by the columns of T encloses P (? |X = x) for any x <ref type="bibr" target="#b2">(Boyd et al., 2004)</ref>. Thus, the problem of identifying the transition matrix can be treated as the problem of recovering Sim{T }. However, when no assumption has been made, the problem is ill-defined as Sim{T } is not identifiable, i.e., there exists an infinite number of simplexes enclosing P (? |X), and any of them can be regarded as the true simplex Sim{T }. It is apparent that under the anchor-point assumption, Sim{T } can be uniquely determined by exploiting anchor points whose noisy class-posterior probabilities are the vertices of Sim{T }. The goal is thus to identify the points which have the largest noisy class-posterior probabilities for each class <ref type="bibr" target="#b24">(Liu &amp; Tao, 2016;</ref><ref type="bibr" target="#b32">Patrini et al., 2017)</ref>. However, if there are no anchor points, the identified points would not be the vertices of Sim{T }. In this case, existing methods cannot consistently estimate the transition matrices. To recover T without anchor points, a key observation is that, among all simplexes enclosing P (? |X), Sim{T } is the one with minimum volume. See <ref type="figure">Figure 1</ref> for a geometric illustration. This observation motivates the development of our method which incorporates the minimum volume constraint of Sim{T } into label-noise learning.</p><p>To this end, we propose Volume Minimization Network (VolMinNet) to consistently estimate the transition matrix and build a statistically consistent classifier. Specifically, VolMinNet consists of a classification network h ? and a trainable transition matrixT . We simultaneously opti-mizeT and h ? with two objectives: i) the discrepancy betweenT h ? (x) and the noisy class-posterior distribution P (? |X = x), ii) The volume of the simplex formed by the columns ofT . The proposed framework is end-toend, and there is no need for identifying anchor points or pseudo anchor points (i.e., instances belonging to a specific class with probability close to one) <ref type="bibr" target="#b40">(Xia et al., 2019)</ref>. Since our proposed method does not rely on any specific data points, it yields better noise robustness compared with existing methods. With a so-called sufficiently scattered assumption where the clean class-posterior distribution is far from uniform, we theoretically prove thatT will converge to the true transition matrix T while h ? (x) converges to the clean class-posterior P (Y |X = x). We also prove that the anchor-point assumption is a special case of the sufficiently scattered assumption.</p><p>The rest of this paper is organized as follows. In Section 2, we set up the notations and review the background of labelnoise learning with anchor points. In Section 3, we introduce our proposed VolMinNet. In Section 4, we present the main theoretical results. In Section 5, we briefly introduce the related works in the literature. Experimental results on both synthetic and real-world datasets are provided in Sec- <ref type="figure">Figure 1</ref>. Geometric illustration of the problem of estimating the transition matrix without anchor points. The red triangle is the simplex of T with vertices denoted by T :;i. When there are no anchor points, the simplex found with existing methods (blue triangle) by using extreme-valued noisy class-posterior probabilities (see Eq. <ref type="formula" target="#formula_6">(4)</ref>) is not the true simplex (red triangle). It is obvious that among possible enclosing simplexes (black and red triangles), the true simplex Sim{T } has the minimum volume. tion 6. Finally, we conclude the paper in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Label-Noise Learning with Anchor Points</head><p>In this section, we review the background of label-noise learning. We follow common notational conventions in the literature of label-noise learning. v ? R n and V ? R n?m denote a real-valued n-dimensional vector and a real-valued n ? m matrix, respectively. Elements of a vector are denoted by a subscript (e.g., v j ), while rows and columns of a matrix are denoted by V i,: and V :,i respectively. The ith standard basis vector in R C is denoted by e i . We denote the all-ones vector by 1, and ? C?1 ? [0, 1] C is the C-dimensional simplex. In this work, we also make extensive use of convex analysis. Let a set V = {v 1 , . . . , v m }, and the convex cone of V is de-</p><formula xml:id="formula_0">noted by cone(V) = {v|v = m j=1 v j ? j , ? j ? 0, ?j}. Similarly, the convex hull of V is defined as conv(V) = {v|v = m j=1 v j ? j , ? j ? 0, m j=1 ? j = 1, ?j}.</formula><p>Specially, when {v 1 , . . . , v m } are affinely independent, conv(V) is also called a simplex which we denote it as Sim(V).</p><p>Let D be the underlying distribution generating a pair of random variables (X, Y ) ? X ? Y, where X ? R d is the feature space, Y = {1, 2, . . . , C} is the label space and C is the number of classes. In many real-world applications, samples drawn from D are unavailable. Before being observed, labels of these samples are contaminated with noise and we obtain a set of corrupted data {(x i ,? i )} n i=1 wher? y is the noisy label and we denote byD the distribution of the noisy random pair (X,? ) ? X ? Y.</p><p>Given an instance x sampled from X,? is derived from the random variable Y through a noise transition matrix T (x) ? [0, 1] C?C :</p><formula xml:id="formula_1">P (? |X = x) = T (x)P (Y |X = x),<label>(1)</label></formula><p>where P (Y |X = x) = [P (Y = 1|X = x), . . . , P (Y = C|X = x)] and P (? |X = x) = [P (? = 1|X = x), . . . , P (? = C|X = x)] are the clean class-posterior probability and the noisy class-posterior probability, respectively. The ij-th entry of the transition matrix, i.e., T ij (x) = P (? = i|Y = j, X = x), represents the probability that the instance x with the clean label Y = j will have a noisy label? = i. Generally, the transition matrix is non-identifiable without any additional assumption <ref type="bibr" target="#b40">(Xia et al., 2019)</ref>. For example, we can decompose the transition matrix with T (</p><formula xml:id="formula_2">x) = T 1 (x)T 2 (x). If we define P (Y |X = x) = T 2 (x)P (Y |X = x), then P (? |X = x) = T (x)P (Y |X = x)) = T 1 (x)P (Y |X = x)</formula><p>are both valid. Therefore, in this paper, we study the classdependent and instance-independent transition matrix on which the majority of existing methods focus <ref type="bibr" target="#b12">(Han et al., 2018b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b32">Patrini et al., 2017;</ref><ref type="bibr" target="#b31">Northcutt et al., 2017;</ref><ref type="bibr" target="#b29">Natarajan et al., 2013)</ref>. Formally, we have:</p><formula xml:id="formula_3">P (? |X = x) = T P (Y |X = x),<label>(2)</label></formula><p>where the transition matrix T is now independent of the instance x. In this work, we also assume that the true transition matrix T is diagonally dominant 1 . Specifically, the transition matrix T is diagonally dominant if for every column of T , the magnitude of the diagonal entry is larger than any non-diagonal entry, i.e., T ii &gt; T ji for any j = i. This assumption has been commonly used in the literature of label-noise learning <ref type="bibr" target="#b32">(Patrini et al., 2017;</ref><ref type="bibr" target="#b40">Xia et al., 2019;</ref><ref type="bibr" target="#b46">Yao et al., 2020b)</ref>.</p><p>As in Eq.</p><p>(2), the clean class-posterior probability P (Y |X) can be inferred by using the noisy class-posterior probability P (? |X) and the transition matrix T as P (Y |X) = T ?1 P (? |X). For this reason, the transition matrix has been widely exploited to build statistically consistent classifiers, i.e., the learned classifier will converge to the optimal classifier defined with clean risk. Specifically, the transition matrix has been used to modify loss functions to build risk-consistent estimators <ref type="bibr" target="#b10">(Goldberger &amp; Ben-Reuven, 2017;</ref><ref type="bibr" target="#b32">Patrini et al., 2017;</ref><ref type="bibr" target="#b47">Yu et al., 2018;</ref><ref type="bibr" target="#b40">Xia et al., 2019)</ref>, and has been used to correct hypotheses to build classifier-consistent algorithms <ref type="bibr" target="#b29">(Natarajan et al., 2013;</ref><ref type="bibr" target="#b35">Scott, 2015;</ref><ref type="bibr" target="#b32">Patrini et al., 2017)</ref>. Thus, the successes of these consistent algorithms rely on an accurately learned transition matrix.</p><p>In recent years, considerable efforts have been invested in designing algorithms for estimating the transition matrix. These algorithms rely on a so-called anchor-point assumption which requires that there exist anchor points for each class <ref type="bibr" target="#b24">(Liu &amp; Tao, 2016;</ref><ref type="bibr" target="#b40">Xia et al., 2019)</ref>. Definition 1 (anchor-point assumption). For each class j ? {1, 2, . . . , C}, there exists an instance</p><formula xml:id="formula_4">x j ? X such that P (Y = j|X = x j ) = 1.</formula><p>Under the anchor-point assumption, the task of estimating the transition matrix boils down to finding anchor points for each class. For example, given anchor points x j , we have</p><formula xml:id="formula_5">P (? = i | X = x j ) = C k=1 T ik P (Y = k | X = x j ) = T ij .</formula><p>(3) Namely, the transition matrix can be obtained with the noisy class-posterior probabilities of anchor points. Assuming that we can accurately model the noisy classposterior P (? |X) given a sufficient number of noisy data, anchor points can be easily found as follows <ref type="bibr" target="#b24">(Liu &amp; Tao, 2016;</ref><ref type="bibr" target="#b32">Patrini et al., 2017)</ref>:</p><formula xml:id="formula_6">x j = arg max x P (? = j|X = x).<label>(4)</label></formula><p>However, when the anchor-point assumption is not satisfied, points found with Eq. (4) are no longer anchor points. Hence, the above-mentioned method can not consistently estimate the transition matrix with Eq. (3), which will lead to a statistically inconsistent classifier. This motivates us to design a statistically classifier-consistent algorithm which can consistently estimate the transition matrix without anchor points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Volume Minimization Network</head><p>In this section, we propose a novel framework for labelnoise learning which we call the Volume Minimization Network (VolMinNet). The proposed framework is end-toend, and there is no need for identifying anchor points or a second stage for loss correction, resulting in better noise robustness than existing methods.</p><p>To learn the clean class-posterior P (Y |X), we define a transformation h ? : X ? ? C?1 where h ? is a differentiable function represented by a neural network with parameters ?. To estimate the transition matrix, we construct a trainable diagonally dominant column stochastic matrix T , i.e.,T ? [0, 1] C?C , C i=1 T ij = 1 and T ii &gt; T ji for any i = j. To learn the noisy class posterior distribution from the noisy data, with some abuse of notation, we define the composition ofT and h ? asT h ? : X ? ? C?1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intuitively, as explained in Section 1, ifT h ? models</head><p>Provably End-to-end Label-noise Learning without Anchor Points <ref type="figure">Figure 2</ref>. Overview of the proposed VolMinNet. The training in the proposed framework is carried out in an end-to-end manner with two objectives (red blocks) optimized simultaneously. P (? |X) perfectly while the simplex ofT has the minimum volume,T will converge to the true transition matrix T and h ? will converge to P (Y |X). This motivates us to propose the following criterion which corresponds to a constraint optimization problem:</p><formula xml:id="formula_7">min T ?T vol(T ) s.t.T h ? = P (? |X), (5) where T = {T ? [0, 1] C?C | C i=1T ij = 1,T ii &gt; T ji , ? i = j}</formula><p>is the set of diagonally dominant column stochastic matrices. vol(T ) denotes a measure that is related or proportional to the volume of the simplex formed by the columns ofT .</p><p>To solve criterion (5), we first note that the constraint T h ? = P (? |X) can be solved with expected risk minimization <ref type="bibr" target="#b32">(Patrini et al., 2017)</ref>. The risk is defined as</p><formula xml:id="formula_8">R(h ? ) = E (x,?)?D [ (T h ? (x),?)],</formula><p>where is a loss function and we use the cross-entropy loss throughout this paper. We can then re-write criterion (5) as a Lagrangian under the KKT condition <ref type="bibr" target="#b16">(Karush, 1939;</ref><ref type="bibr" target="#b18">Kuhn &amp; Tucker, 2014)</ref> to obtain:</p><formula xml:id="formula_9">L(?,T ) := vol(T ) + ? ? E (x,?)?D [ (T h ? (x),?)],<label>(6)</label></formula><p>where ? &gt; 0 is the KKT multiplier. In the literature, various functions for measuring the volume of the simplex have been investigated <ref type="bibr" target="#b7">(Fu et al., 2015;</ref><ref type="bibr" target="#b19">Li &amp; Bioucas-Dias, 2008;</ref><ref type="bibr" target="#b28">Miao &amp; Qi, 2007)</ref>. GivenT is a square matrix, a common choice is vol(T ) = det(T ), where det denotes the determinant. However, this function is numerically unstable for optimization and computationally hard to deal with. Hence, we adopt another popular alternative log det(T ). This function has been widely used in lowrank matrix recovery and non-negative matrix decomposition <ref type="bibr" target="#b5">(Fazel et al., 2003;</ref><ref type="bibr" target="#b23">Liu et al., 2012;</ref><ref type="bibr" target="#b8">Fu et al., 2016)</ref>. Besides, since we only have access to a set of noisy train</p><formula xml:id="formula_10">- ing examples {(x i ,? i )} n i=1 instead of the distributionD,</formula><p>we employ the empirical risk for training. Formally, we propose the following objective function:</p><formula xml:id="formula_11">L(?,T ) := 1 n n i=1? (T h ? (x i )),? i ) + ? ? log det(T ),<label>(7)</label></formula><p>where ? &gt; 0 is a regularization coefficient that balances distribution fidelity versus volume minimization.</p><p>The problem remains how to designT so that it is differentiable, diagonally dominant and column stochastic. Specifically, we first create a matrix A ? R C?C so that diagonal elements A ii = 1 for all i ? {1, 2, . . . , C}, and all other elements A ij = ?(w ij ) for all i = j where ? is the sigmoid function ?(w) = 1 1+e ?w and each w ij ? R is a realvalued variable which will be updated throughout training. Then we do the normalizationT ij = Aij C k=1 A kj so that the sum of each column ofT is equal to one. Since the sigmoid function returns a value in the range 0 to 1, we hav? T ii &gt;T ji for all i = j. With this specially designedT , we ensure thatT ? T, i.e.,T is a diagonally dominant and column stochastic matrix. In addition,T is differentiable because the sigmoid function and the normalization operation are differentiable.</p><p>With bothT and h ? being differentiable, the objective in Eq. <ref type="formula" target="#formula_11">(7)</ref> can be easily optimized with any standard gradientbased learning rule. This allows us to replace the two-stage loss correction procedure in existing works with an endto-end learning framework. See <ref type="figure">Figure 2</ref> for a less formal, more pedagogical explanation of our proposed learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Theoretical Results</head><p>In this section, we show that criterion (5) guarantees the consistency of the estimated transition matrix and the learned classifier under the sufficiently scattered assumption. We also show that the anchor-point assumption is a special case of the sufficiently assumption. To explain, we give a formal definition of the sufficiently scattered assumption:</p><p>Definition 2 (Sufficiently Scattered). The clean classposterior P (Y |X) is said to be sufficiently scattered if there exists a set</p><formula xml:id="formula_12">H = {x 1 , . . . , x m } ? X such that the matrix H = [P (Y |X = x 1 ), . . . , P (Y |X = x m )] sat- isfies (1) R ? cone{H}, where R = {v ? R C | v 1 ? ? C ? 1 v 2 }</formula><p>and cone{H} denotes the convex cone formed by columns of H.</p><p>(2) cone{H} cone{Q}, for any unitary matrix Q ? R C?C that is not a permutation matrix.</p><p>This assumption is evolved from previous works in nonnegative matrix decomposition <ref type="bibr" target="#b7">(Fu et al., 2015;</ref> with necessary modification. Intuitively, in the case of C = 3, R corresponds to a "ball" tangential to the triangle formed by a permutation matrix, e.g., I = [e 1 , e 2 , e 3 ]. cone{H} is the polytope inside this triangle. Columns of Q also form triangles which are rotated versions of the triangle defined by permutation matrices; facets of those triangles are also tangential to R. Condition (1) of the sufficiently scattered assumption requires that R is enclosed by cone{H}, i.e., R is a subset of cone{H}. Condition (2) ensures that given condition (1), cone{H} is enclosed by the triangle formed by a permutation matrix and not any other unitary matrix.</p><p>To understand the sufficiently scattered assumption and its relationship with the anchor-point assumption, we provide several examples in <ref type="figure">Figure 3</ref>. In <ref type="figure">Figure 3.(a)</ref>, we show a situation where both the anchor-point assumption and sufficiently scattered assumption are satisfied. The first observation is that if the anchor-point assumption is satisfied, then the sufficiently scattered assumption must hold, but not vice versa. Intuitively, if the anchorpoint assumption is satisfied, then there exists a matrix</p><formula xml:id="formula_13">H = [P (Y |X = x 1 ), . . . , P (Y |X = x C )] = I where x 1 , . .</formula><p>. , x C are anchor points for different classes and I is the identity matrix. From <ref type="figure">Figure 3.(a)</ref> , it is clear that R ? cone{I} = cone{H}, and cone{H} can only be enclosed by the convex cone of permutation matrices. This shows that the sufficiently scattered assumption is satisfied. However, from <ref type="figure">Figure 3.(b)</ref>, it is clear that the sufficiently scattered assumption is satisfied but not the anchor-point assumption. Formally, we show that:</p><p>Proposition 1. The anchor-point assumption is a sufficient but not necessary condition for the sufficiently scattered as-sumption when C &gt; 2.</p><p>The proof of Proposition 1 is included in the supplementary material. Proposition 1 implies that the anchor-point assumption is a special case of the sufficiently scattered assumption. This means that the proposed framework can also deal with the case where the anchor-point assumption holds. Under the sufficiently scattered assumption, we get our main result:</p><p>Theorem 1. Given sufficiently many noisy data, if P (Y |X) is sufficiently scattered, thenT = T and h ? (x) = P (Y |X = x) must hold, where (T , ? ) are optimal solutions of Eq. (5).</p><p>The proof of Theorem 1 can be found in the supplementary material. Intuitively, if P (Y |X) is sufficiently scattered, the noisy class-posterior P (? |X) will be sufficiently spread in the simplex formed by the columns of T . Then, finding the minimum-volume data-enclosing convex hull of P (? |X) recovers the ground-truth T and P (Y |X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Works</head><p>In this section, we review existing methods in label-noise learning. Based on the statistical consistency of the learned classifier, we divided exsisting methods for label-noise learning into two categories: heuristic algorithms and statistically consistent algorithms.</p><p>Methods in the first category focus on employing heuristics to reduce the side-effect of noisy labels. For example, many methods use a specially designed strategy to select reliable samples <ref type="bibr" target="#b48">(Yu et al., 2019;</ref><ref type="bibr" target="#b12">Han et al., 2018b;</ref><ref type="bibr" target="#b27">Malach &amp; Shalev-Shwartz, 2017;</ref><ref type="bibr" target="#b34">Ren et al., 2018;</ref><ref type="bibr" target="#b15">Jiang et al., 2018;</ref><ref type="bibr" target="#b45">Yao et al., 2020a)</ref> or correct labels <ref type="bibr" target="#b26">(Ma et al., 2018;</ref><ref type="bibr" target="#b17">Kremer et al., 2018;</ref><ref type="bibr" target="#b37">Tanaka et al., 2018;</ref><ref type="bibr" target="#b33">Reed et al., 2015)</ref>. Although those methods empirically work well, there is not any theoretical guarantee on the consistency of the learned classifiers from all these methods.</p><p>Statistically consistent algorithms are primarily developed based on a loss correction procedure <ref type="bibr" target="#b24">(Liu &amp; Tao, 2016;</ref><ref type="bibr" target="#b32">Patrini et al., 2017;</ref><ref type="bibr" target="#b52">Zhang &amp; Sabuncu, 2018)</ref>. For these methods, the noise transition matrix plays a key role in building consistent classifiers. For example, <ref type="bibr" target="#b32">Patrini et al.(2017)</ref> leveraged a two-stage training procedure of first estimating the noise transition matrix and then use it to modify the loss to ensure risk consistency. These works rely on anchor points or instances belonging to a specific class with probability one or approximately one. When there are no anchor points in datasets or data distributions, all the aforementioned methods cannot guarantee the statistical consistency. Another approach is to jointly learn the noise transition matrix and classifier. For instance, on top of the softmax layer of the classification network (Gold- <ref type="figure">Figure 3</ref>. Illustration of the anchor-point assumption and the sufficiently scattered assumption in the case of C = 3 by assuming that the viewer are facing the hyperplane 1 x = 1 from the positive orthant. The dots are class-posterior probabilities P (Y |X = x); the triangle is the non-negative orthant; the inner circle is R; the region encompassed by red lines is cone{H}. Clearly, the anchor-point assumption (a) is a special case of the sufficiently scattered assumption (b).</p><p>berger &amp; Ben-Reuven, 2017), a constrained linear layer or a nonlinear softmax layer is added to model the noise transition matrix <ref type="bibr" target="#b36">(Sukhbaatar et al., 2015)</ref>. <ref type="bibr">Zhang et al. (2021)</ref> concurrently propose a one-step method for the label-noise learning problem and a derivative-free method for estimating the transition matrix. Specifically, their method uses a total variation regularization term to prevent the overconfidence problem of the neural network, which leads to a more accurate noisy class-posterior. However, the anchor-point assumption is still needed for their method. Based on different motivations, assumptions and learning objectives, their method achieves different theoretical results compared with our proposed method. Learning with noisy labels are also closely related to learning with complementary labels where instead of noisy labels, only compelementary labels are given for training <ref type="bibr" target="#b47">(Yu et al., 2018;</ref><ref type="bibr" target="#b3">Chou et al., 2020;</ref><ref type="bibr" target="#b6">Feng et al., 2020)</ref>.</p><p>Recently, some methods exploiting semi-supervised learning techniques have been proposed to solve the label-noise learning problem like SELF <ref type="bibr" target="#b30">(Nguyen et al., 2019)</ref> and Di-videMix <ref type="bibr" target="#b21">(Li et al., 2019)</ref>. These methods are aggregations of multiple techniques such as augmentations and multiple networks. Noise robustness is significantly improved with these methods. However, these methods are sensitive to the choice of hyperparameters and changes in data and noise types would generate degenerated classifiers. In addition, the computational cost of these methods increases significantly compared with previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we verify the robustness of the proposed volume minimization network (VolMinNet) from two folds: the estimation error of the transition matrix and the classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We evaluate the proposed method on three syn-thetic noisy datasets, i.e., MNIST, CIFAR-10 and CIFAR-100 and one real-world noisy dataset, i.e., clothing1M. We leave out 10% of the training examples as the validation set. The three synthetic datasets contain clean data. We corrupted the training and validation sets manually according to transition matrices. Specifically, we conduct experiments with two commonly used types of noise: (1) Symmetry flipping <ref type="bibr" target="#b32">(Patrini et al., 2017)</ref>; (2) Pair flipping <ref type="bibr" target="#b12">(Han et al., 2018b)</ref>. We report both the classification accuracy on the test set and the estimation error between the estimated transition matrixT and the true transition matrix T . All experiments are repeated five times on all datasets. Following T-Revision <ref type="bibr" target="#b40">(Xia et al., 2019)</ref>, we also conducted experiments on datasets where possible anchor points are removed from the datasets. The details and more experimental results can be found in the Supplementary Material.</p><p>Clothing1M is a real-world noisy dataset which consists of 1M images with real-world noisy labels. Existing methods like Forward <ref type="bibr" target="#b32">(Patrini et al., 2017)</ref> and T-revision <ref type="bibr" target="#b40">(Xia et al., 2019)</ref> use the additional 50k clean training data to help initialize the transition matrix and validate on 14k clean validation data. Here we use another setting which is also commonly used in the literature <ref type="bibr" target="#b41">(Xia et al., 2020)</ref>. We only exploit the 1M data for both transition matrix estimation and classification training. Specifically, we leave out 10% of the noisy training examples as a noisy validation set for model selection. We think this setting is more natural considering that it does not require any clean data. All results of baseline methods are quoted from PTD <ref type="bibr" target="#b41">(Xia et al., 2020)</ref> as we have the same setting.</p><p>Network structure and optimization For a fair comparison, we implement all methods with default parameters by PyTorch on Tesla V100-SXM2. For MNIST, we use a LeNet-5 network. SGD is used to train the classification network h ? with batch size 128, momentum 0.9, weight decay 10 ?3 and a learning rate 10 ?2 . Adam with default parameters is used to train the transition matrixT . The algorithm is run for 60 epoch. For CIFAR10, we use a ResNet-18 network. SGD is used to train both the classification network h ? and the transition matrixT with batch size 128, momentum 0.9, weight decay 10 ?3 and an initial learning rate 10 ?2 . The algorithm is run for 150 epoch and the learning rate is divided by 10 after the 30th and 60th epoch. For CIFAR100, we use a ResNet-32 network. SGD is used to train the classification network h ? with batch size 128, momentum 0.9, weight decay 10 ?3 and an initial learning rate 10 ?2 . Adam with default parameters is used to train the transition matrixT . The algorithm is run for 150 epoch and the learning rate is divided by 10 after the 30th and 60th epoch. For CIFAR-10 and CIFAR-100, we perform data augmentation by horizontal random flips and 32 ? 32 random crops after padding 4 pixels on each side. For clothing1M, we use a ResNet-50 pre-trained on Ima-geNet. We only use the 1M noisy data to train and validate the network. For the optimization, SGD is used train both the classification network h ? and the transition matrixT with momentum 0.9, weight decay 10 ?3 , batch size 32, and run with learning rates 2 ? 10 ?3 and 2 ? 10 ?5 for 5 epochs each. For each epoch, we ensure the noisy labels for each class are balanced with undersampling. Throughout all experiments, we fixed ? = 10 ?4 and the trainable weights w ofT are initialized with ln 1 C?2 (roughly -2 for MNIST and CIFAR10, -4.5 for CIFAR100 and -2.5 for clothing1M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Transition Matrix Estimation</head><p>For evaluating the effectiveness of estimating the transition matrix, we compare the proposed method with the following methods: (1) T-estimator max <ref type="bibr" target="#b32">(Patrini et al., 2017)</ref>, which identify the extreme-valued noisy class-posterior probabilities from given samples to estimate the transition matrix.</p><p>(2) T-estimator 3% <ref type="bibr" target="#b32">(Patrini et al., 2017)</ref>, which takes a ?-percentile in place of the argmax of Equation 4.</p><p>(3) T-Revision <ref type="bibr" target="#b40">(Xia et al., 2019)</ref>, which introduces a slack variable to revise the noise transition matrix after initializing the transition matrix with T-estimator. (4) Dual T-estimator , which introduces an intermediate class to avoid directly estimating the noisy classposterior and factorizes the transition matrix into the product of two easy-to-estimate transition matrices.</p><p>To show that the proposed method is more robust in estimating the transition matrix, we plot the estimation error for the transition matrix, i.e., T ?T ? ?T 1 / T 1 . <ref type="figure">Figure</ref>  its superior robustness against label noise. For example, on CIFAR100 (Flip-0.45), our method achieves estimation error around 0.25, while baseline methods can only reach at around 0.75. These results show that our method establishes the new state of the art in estimating transition matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Classification accuracy Evaluation</head><p>We compare the classification accuracy of the proposed method with the following methods: (1) Decoupling <ref type="bibr" target="#b27">(Malach &amp; Shalev-Shwartz, 2017)</ref>. (2) MentorNet <ref type="bibr" target="#b15">(Jiang et al., 2018)</ref>. (3) Co-teaching <ref type="bibr" target="#b12">(Han et al., 2018b)</ref>. (4) Forward <ref type="bibr" target="#b32">(Patrini et al., 2017)</ref>. (5) T-Revision <ref type="bibr" target="#b40">(Xia et al., 2019</ref>). (7) DMI <ref type="bibr" target="#b44">(Xu et al., 2019)</ref>. (8) Dual T . Note that we did not compare the proposed method with some methods like SELF <ref type="bibr" target="#b30">(Nguyen et al., 2019)</ref> and Di-videMix <ref type="bibr" target="#b21">(Li et al., 2019)</ref>. This is because these methods are aggregations of semi-supervised learning techniques which have high computational complexity and are sensitive to the choice of hyperparameters. In this work, we are more focusing on solving the label noise learning without anchor points theoretically.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we present the classification accuracy by the proposed VolMinNet and baseline methods on synthetic noisy datasets. VolMinNet outperforms baseline methods on almost all settings of noise. This result is natural after we have shown that VolMinNet leads to smaller estimation error of the transition matrix compared with baseline methods. While the differences of accuracy among differ- Finally, we show the results on Clothing1M in <ref type="table">Table 2</ref>. As explained in the previous section, Forward and T-Revision exploited the 50k clean data and their noisy versions in 1M noisy data to help initialize the noise transition matrix, which is not practical in real-world settings. For a fair comparison, we report results by only using the 1M noisy data to train and validate the network. As shown in Table 2, our method outperforms previous transition matrix based methods and heuristic methods on the Clothing1M dataset. In addition, the performance on the Clothing1M dataset shows that the proposed method has certain robustness against instance-dependent noise as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion and Conclusion</head><p>In this paper, we considered the problem of label-noise learning without anchor points. We relax the anchor-point assumption with our proposed VolMinNet. The consistency of the estimated transition matrix and the learned classifier are theoretically proved under the sufficiently scattered assumption. Experimental results have demonstrated the robustness of the proposed VolMinNet. Future work should focus on improving the estimation of the noisy class posterior which we believe is the bottleneck of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Proof of Proposition 1</head><p>To prove proposition 1, we first show that the anchor-point assumption is a sufficient condition for the sufficiently scattered assumption. In other words, we need to show that if the anchor-point assumption is satisfied, then two conditions of the sufficiently scattered assumption must hold.</p><p>We start with condition (2) of the sufficiently scattered assumption. We need to show that if the anchorpoint assumption is hold, then there exists a set H = {x 1 , . . . , x m } ? X such that the matrix H = [P (Y |X = x 1 ), . . . , P (Y |X = x m )] satisfies that cone{H} cone{Q}, for any unitary matrix Q ? R C?C that is not a permutation matrix.</p><p>Since the anchor-point assumption is satisfied, then there exists a matrix H = [P (Y |X = x 1 ), ..., P (Y |X = x C )] where x 1 , ..., x C are anchor points for each class. From the definition of anchor points, we have P (Y |X = x i ) = e i . This implies that</p><formula xml:id="formula_14">H = [P (Y |X = x 1 ), ..., P (Y |X = x C )] = I,<label>(8)</label></formula><p>where I is the identity matrix. By the definition of the identity matrix I, it is clear that cone{H} = cone{I} cone{Q}, for any unitary matrix Q ? R C?C that is not a permutation matrix. This shows that condition (2) of the sufficiently scattered assumption is satisfied if the anchorpoint assumption is hold.</p><p>Next, we show that condition (1) will also be satisfied, i.e., the convex cone R ? cone{H}, where R = {v ? R C |v 1 ? ? C ? 1 v 2 }. By Eq. (8), condition (1) of Theorem 1 is equivalent to</p><formula xml:id="formula_15">R ? cone{I} = {u|u = C j=1 e j ? j , ? j ? 0, ?j}. (9)</formula><p>This means that all elements in R must be in the nonnegative orthant of R C , i.e., for all v ? R, v i ? 0 for all i ? {1, . . . , C}. Consider v ? R and letv be the normalized vector of v, by definition of R we have the following chain:</p><formula xml:id="formula_16">v 1 ? ? C ? 1 v 2 ,<label>(10a)</label></formula><formula xml:id="formula_17">v v 1 =v 1 ? ? C ? 1,<label>(10b)</label></formula><p>i?{1,2,...,C}v</p><formula xml:id="formula_18">i ? ? C ? 1.<label>(10c)</label></formula><p>To show v is non-negative is equivalent to prove thatv is non-negative, i.e., ?k ? {1, . . . , C},v k ? 0. Let u ? R C?1 be the vector which has same elements withv except that the kth elementv k is removed. Following Eq. 10, we have:v</p><formula xml:id="formula_19">k ? ? C ? 1 ? i?{1,2,...,C}\{k}v i ,<label>(11a)</label></formula><formula xml:id="formula_20">v k ? ? C ? 1 ? u 1.<label>(11b)</label></formula><p>By the Cauchy-Schwarz inequality, we get the following inequality:</p><formula xml:id="formula_21">|u 1| ? u 1 .<label>(12)</label></formula><p>Then by the definition of u and 1, we have u ? 1 and 1 = ? C ? 1. Combined this with Eq. 11 and Eq. 12, we get:v</p><formula xml:id="formula_22">k ? ? C ? 1 ? u 1 ? 0.<label>(13)</label></formula><p>This simply implies that v k ? 0 for all k ? {1, 2, . . . , C} and we have proved that the anchor-point assumption is a sufficient condition of the sufficiently scattered assumption.</p><p>We now prove that the anchor-point assumption is not a necessary condition for the sufficiently scattered assumption. Suppose P (Y |X) has the property that x 1 , x 2 , . . . x C / ? X which means that the anchor-point assumption is not satisfied. We also assume that there exist a set H = {x 1 , . . . , x m } ? X such that cone{H} covers the whole non-negative orthant except the area along each axis (area formed by noisy class-posterior of anchor points). Since these areas along each axis are not part of R when C &gt; 2, it is clear that condition (1) of the sufficiently scattered assumption is satisfied. Besides, by definition of H, there is no other unitary matrix which can cover cone{H} except permutation matrices. This shows that condition (2) of the sufficiently scattered assumption is also satisfied and the proof is completed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Proof of Theorem 1</head><p>The insights of our proof are from previous works in nonnegative matrix factorisation <ref type="bibr" target="#b7">(Fu et al., 2015)</ref>. To proceed, let us first introduce following classic lemmas in convex analysis: Lemma 1. If K 1 and K 2 are convex cones and K 1 ? K 2 , then, dual{K 2 } ? dual{K 1 }.</p><formula xml:id="formula_23">Lemma 2. If A is invertible, then dual(A) = cone(A ? ).</formula><p>Readers are referred to <ref type="bibr" target="#b2">Boyd et al.(2004)</ref> for details. Our purpose is to show that criterion (5) has unique solutions which are the ground-truth P (Y |X) and T . To this end, let us denote (T , h ? ) as a feasible solution of Criterion (5), i.e.,</p><formula xml:id="formula_24">T h ? = T P (Y |X) = P (? |X).<label>(14)</label></formula><p>As defined in sufficient scattered assumption, we have the matrix H = [P (Y |X = x 1 ), . . . , P (Y |X = x m )] defined on the set H = {x 1 , . . . , x m } ? X . Let H = [h ? (x 1 ), . . . , h ? (x m )], it follows that</p><formula xml:id="formula_25">T H = T H.<label>(15)</label></formula><p>Note that both T and T have full rank because they are diagonally dominant square matrices by definition. In addition, since the sufficiently scattered assumption is satisfied, rank(H) = C also holds <ref type="bibr" target="#b7">(Fu et al., 2015)</ref>. Therefore, there exists an invertible matrix A ? R C?C such that</p><formula xml:id="formula_26">T = T A ? ,<label>(16)</label></formula><p>where A ? = HH ? and H ? = H (H H ) ?1 is the pseudo-inverse of H .</p><p>Since 1 H = 1 and 1 H = 1 by definition, we get</p><formula xml:id="formula_27">1 A ? = 1 HH ? = 1 H ? = 1 H H ? = 1 .<label>(17)</label></formula><p>Let v ? cone{H}, which by definition takes the form v = Hu for some u ? 0. Using H = A ? H , v can be expressed as v = A ? ? where? = H u ? 0. This implies that v also lies in cone{A ? }, i.e. cone{H} ? cone{A ? }.</p><p>Recall Condition (1) of the sufficiently scattered assumption, i.e., R ? cone{H} where R</p><formula xml:id="formula_28">= {v ? R C |1 v ? ? C ? 1 v 2 }. It implies R ? cone{H} ? cone(A ? ).<label>(18)</label></formula><p>By applying Lemmas (1-2) to Eq. (18), we have</p><formula xml:id="formula_29">cone(A) ? dual{R},<label>(19)</label></formula><p>where dual{R} is the dual cone of R, which can be shown to be</p><formula xml:id="formula_30">dual{R} = {v ? R C | v 2 ? 1 v}.<label>(20)</label></formula><p>Then we have the following inequalities:</p><formula xml:id="formula_31">|det(A)| ? C i=1 A :,i 2 (21a) ? C i=1 1 A :,i (21b) ? ( C i=1 1 A :,i C ) C (21c) = ( 1 A1 C ) C = 1,<label>(21d)</label></formula><p>where <ref type="formula" target="#formula_1">(21a)</ref> is Hadamard's inequality; (21b) is by Eq. <ref type="formula" target="#formula_1">(19)</ref>; (21c) is by the arithmetic-geometric mean inequality; and (21d) is by Eq. (17).</p><p>Note that |det(A)| ?1 = |det(A ? )| and det(T ) = det(T A ? ) = det(T )|det(A)| ?1 from properties of the determinant, it follows from Eq. (21) that det(T ) ? det(T ). We also know that det(T ) ? det(T ) must hold from Criterion <ref type="formula">(5)</ref>, hence we have det(T ) = det(T )</p><p>By Hadamard's inequality, the equality in (21a) holds only if A is column-orthogonal, which is equivalent to that A ? is column-orthogonal. Considering condition (2) in the definition of sufficiently scattered and the property of A ? that cone{H} ? cone(A ? ), the only possible choices of column-orthogonal A ? are</p><formula xml:id="formula_33">A ? = ??<label>(23)</label></formula><p>where ? ? R C?C is any permutation matrix and ? ? R C?C is any diagonal matrix with non-zero diagonals. By Eq. (17), we must have ? = I. Subsequently, we are left with A ? = ?, or equivalently, T = ?T . Since T and T are both diagonal dominant, the only possible permutation matrix is I, which means T = T holds. By Eq. <ref type="formula" target="#formula_1">(14)</ref>, it follows that h ? = P (Y |X). Hence we conclude that (T , h ? ) = (T , P (Y |X)) is the unique optimal solution to criterion (5).</p><p>B. Experiments on datasets where possible anchor points are manually removed.</p><p>Following <ref type="bibr" target="#b40">Xia et al.(2019)</ref>, to show the importance of anchor points, we remove possible anchor points from the datasets, i.e., instances with large estimated class-posterior probability P (Y |X), before corrupting the training and validation sets. For MNIST we removed 40% of the instances with the largest estimated class posterior probabilities in each class. For CIFAR-10 and CIFAR-100, we removed 10% of the instances with the largest estimated class posterior probabilities in each class. We add "/NA" following the dataset's name denote those datasets which are modified by removing possible anchor points. The detailed experimental results are shown in <ref type="figure">Figure 5</ref> (estimation error) and <ref type="table">Table 3</ref> (classification accuracy). The experimental performance shows that our proposed method outperforms the baseline methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 3.(b) shows a situation where the sufficiently scattered assumption is satisfied while the anchor-point assumption is violated. In Figure 3.(c) and 3.(d), both assumptions are violated. However, in Figure 3.(d), only condition (2) of the sufficiently scattered assumption is violated while both conditions of the sufficiently scattered assumption are violated in 3.(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Transition matrix estimation error on MNIST, CIFAR-10 and CIFAR-100. The error bar for the standard deviation in each figure has been shaded. The lower the better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>VolMinNet 98.74 ? 0.08 98.23 ? 0.16 89.58 ? 0.26 83.37 ? 0.25 64.94 ? 0.40 53.89 ? 1.26 VolMinNet 99.01 ? 0.07 99.00 ? 0.07 90.37 ? 0.30 88.54 ? 0.21 68.45 ? 0.69 58.90 ? 0.89 Classification accuracy (percentage) on MNIST, CIFAR-10 and CIFAR-100.</figDesc><table><row><cell>4 depicts estimation errors of transition matrices esti-</cell></row><row><cell>mated by the proposed VolMinNet and other baseline meth-</cell></row><row><cell>ods. For all different settings of noise on three different</cell></row><row><cell>datasets (original intact datasets), VolMinNet consistently</cell></row><row><cell>gives better results compared to the baselines, which shows</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The definition of being diagonally dominant is different from the one in matrix analysis, but it has been commonly used in labelnoise learning<ref type="bibr" target="#b44">(Xu et al., 2019)</ref> </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 5</ref><p>. Transition matrix estimation error on MNIST/NA, CIFAR-10/NA, CIFAR-100/NA. Datasets with "/NA" means that possible anchor points are removed. The error bar for the standard deviation in each figure has been shaded. The lower the better.  <ref type="table">Table 3</ref>. Classification accuracy (percentage) on MNIST, CIFAR-10,CIFAR-100 and MNIST/NA, CIFAR-10/NA, CIFAR-100/NA. Datasets with "/NA" means that possible anchor points are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Confidence scores make instance-dependent label-noise learning possible</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berthon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unbiased risk estimators can mislead: A case study of learning with complementary labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1929" to="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generalization bounds for neural networks via approximate description length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Daniely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13008" to="13016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Log-det heuristic for matrix rank minimization with applications to hankel and euclidean distance matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hindi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACC</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="2156" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning with multiple complementary labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3072" to="3081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Blind separation of quasi-stationary sources: Exploiting convex geometry in covariance domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2306" to="2320" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust volume minimization-based matrix factorization for remote sensing and document clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="6254" to="6268" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On identifiability of nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="332" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training deep neuralnetworks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Masking: A new perspective of noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5836" to="5846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sigua: Forgetting may make learning with noisy labels more robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4006" to="4016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A survey of label-noise representation learning: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mentornet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2309" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Karush</surname></persName>
		</author>
		<title level="m">Minima of functions of several variables with inequalities as side constraints. M. Sc. Dissertation. Dept. of Mathematics</title>
		<imprint>
			<date type="published" when="1939" />
		</imprint>
		<respStmt>
			<orgName>Univ. of Chicago</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust active label correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="308" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Nonlinear programming. In Traces and emergence of nonlinear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Tucker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="247" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Minimum volume simplex analysis: A fast algorithm to unmix hyperspectral data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Bioucas-Dias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>III-250</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Provably End-to-end Label-noise Learning without Anchor Points</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dividemix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<title level="m">Webvision database: Visual learning and understanding from web data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="171" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="447" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Peer loss functions: Learning from noisy labels without knowing noise rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6226" to="6236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3361" to="3370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Endmember extraction from highly mixed data using minimum volume constrained nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="765" to="777" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1196" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self: Learning to filter noisy labels with self-ensembling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Mummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P N</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning with confident examples: Rank pruning for robust classification with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Northcutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">L</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishna Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, Workshop Track Proceedings</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4331" to="4340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A rate of convergence for mixture proportion estimation, with application to learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="838" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Training convolutional networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Estrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robustness of conditional gans to noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Thekumparampil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10271" to="10282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Class2simi: A new perspective on learning with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<editor>ICML. PMLR</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Are anchor points really indispensable in label-noise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6835" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Part-dependent label noise: Towards instance-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NerurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust early-learning: Hindering the memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A novel information-theoretic loss function for training deep nets robust to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>L_Dmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6222" to="6233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Searching to exploit memorization effect in learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10789" to="10798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Dual t: Reducing estimation error for transition matrix in label-noise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning with biased complementary labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">How does disagreement help generalization against label corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7164" to="7173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Label-noise robust domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10913" to="10924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning noise transition matrix from only noisy labels via total variation regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A second-order approach to learning with instance-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Clusterability as an alternative to anchor points when learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
