<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discovering Human-Object Interaction Concepts via Self-Compositional Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
							<email>baosheng.yu@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">JD Explore Academy</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discovering Human-Object Interaction Concepts via Self-Compositional Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human-Object Interaction</term>
					<term>HOI Concept Discovery</term>
					<term>Object Affordance Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A comprehensive understanding of human-object interaction (HOI) requires detecting not only a small portion of predefined HOI concepts (or categories) but also other reasonable HOI concepts, while current approaches usually fail to explore a huge portion of unknown HOI concepts (i.e., unknown but reasonable combinations of verbs and objects). In this paper, 1) we introduce a novel and challenging task for a comprehensive HOI understanding, which is termed as HOI Concept Discovery; and 2) we devise a self-compositional learning framework (or SCL) for HOI concept discovery. Specifically, we maintain an online updated concept confidence matrix during training: 1) we assign pseudo labels for all composite HOI instances according to the concept confidence matrix for self-training; and 2) we update the concept confidence matrix using the predictions of all composite HOI instances. Therefore, the proposed method enables the learning on both known and unknown HOI concepts. We perform extensive experiments on several popular HOI datasets to demonstrate the effectiveness of the proposed method for HOI concept discovery, object affordance recognition and HOI detection. For example, the proposed self-compositional learning framework significantly improves the performance of 1) HOI concept discovery by over 10% on HICO-DET and over 3% on V-COCO, respectively; 2) object affordance recognition by over 9% mAP on MS-COCO and HICO-DET; and 3) rare-first and non-rare-first unknown HOI detection relatively over 30% and 20%, respectively. Code is publicly available at https://github.com/zhihou7/HOI-CL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human-object interaction (HOI) plays a key role in analyzing the relationships between humans and their surrounding objects <ref type="bibr">[23]</ref>, which is of great importance for deep understanding on human activities/behaviors. Human-object interaction understanding has attracted extensive interests from the community, including image-based <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr">40,</ref><ref type="bibr" target="#b33">55]</ref>, video-based visual relationship analysis <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">42]</ref>, arXiv:2203.14272v2 [cs.CV] 24 Jul 2022 video generation <ref type="bibr" target="#b22">[44]</ref>, and scene reconstruction <ref type="bibr" target="#b44">[66]</ref>. However, the distribution of HOI samples is naturally long-tailed: most interactions are rare and some interactions do not even occur in most scenarios, since we can not obtain an interaction between human and object until someone conducts such action in real-world scenarios. Therefore, recent HOI approaches mainly focus on the analysis of very limited predefined HOI concepts/categories, leaving the learning on a huge number of unknown HOI concepts <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3]</ref> poorly investigated, including HOI detection and object affordance recognition <ref type="bibr" target="#b31">[53,</ref><ref type="bibr">27,</ref><ref type="bibr">28]</ref>. For example, there are only 600 HOI categories known in HICO-DET <ref type="bibr" target="#b6">[7]</ref>, while we can find 9,360 possible verb-object combinations from 117 verbs and 80 objects.  <ref type="figure">Fig. 1</ref>. An illustration of unknown HOI detection via concept discovery. Given some known HOI concepts (e.g., "drink with cup", "drink with bottle", and "hold bowl"), the task of concept discovery aims to identify novel HOI concepts (i.e., reasonable combinations between verbs and objects). For example, here we have some novel HOI concepts, "drink with wine glass", "fill bowl", and "fill bottle". Specifically, the proposed self-compositional learning framework jointly optimizes HOI concept discovery and HOI detection on unknown concepts in an end-to-end manner.</p><p>Object affordance is closely related to HOI understanding from an objectcentric perspective. Specifically, two objects with similar attributes usually share the same affordance, i.e. , humans usually interact with similar objects in a similar way <ref type="bibr" target="#b19">[20]</ref>. For example, cup, bowl, and bottle share the same attributes (e.g., hollow), and all of these objects can be used to "drink with". Therefore, object affordance <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">28]</ref> indicates whether each action can be applied into an object, i.e. , if a verb-object combination is reasonable, we then find a novel HOI concept/category. An illustration of unknown HOI detection via concept discovery is shown in <ref type="figure">Fig. 1</ref>. Recently, it has turned out that an HOI model is not only capable of detecting interactions, but also able to recognize object affordances <ref type="bibr">[28]</ref>, especially novel object affordances using the composite HOI features. Particularly, novel object affordance recognition also indicates discovering novel reasonable verb-object combinations or HOI concepts. Inspired by this, we can introduce a simple baseline for HOI concept discovery by averaging the affordance predictions of training dataset into each object category <ref type="bibr">[28]</ref>.</p><p>Nevertheless, there are two main limitations when directly utilizing object affordance prediction <ref type="bibr">[28]</ref> for concept discovery. First, the affordance prediction approach in [28] is time-consuming and unsuitable to be utilized during training phrase, since it requires to predict all possible combinations of verbs and objects using the whole training set. By contrast, we introduce an online HOI concept discovery method, which is able to collect concept confidence in a running mean manner with verb scores of all composite features in mini-batches during training. Second, also more importantly, the compositional learning approach [28] merely optimizes the composite samples with known concepts (e.g., 600 categories on HICO-DET), ignoring a large number of composite samples with unknown concepts (unlabeled composite samples). As a result, the model is inevitably biased to known object affordances (or HOI concepts), and leads to the similar inferior performance to the one in Positive-Unlabeled learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">49]</ref>. That is, without negative samples for training, the network will tend to predict high confidence on those impossible verb-object combinations or overfit verb patterns (please refer to Appendix A for more analysis). Considering that the online concept discovery branch is able to predict concept confidence during optimization, we can then construct pseudo labels <ref type="bibr">[37]</ref> for all composite HOIs belonging to either known or unknown categories. Inspired by this, we introduce a self-compositional learning strategy (or SCL) to jointly optimize all composite representations and improve concept predictions in an iterative manner. Specifically, SCL combines the object representations with different verb representations to compose new samples for optimization, and thus implicitly pays attention to the object representations and improves the discrimination of composite representations. By doing this, we can improve the object affordance learning, and then facilitate the HOI concept discovery.</p><p>Our main contributions can be summarized as follows: 1) we introduce a new task for a better and comprehensive understanding on human-object interactions; 2) we devise a self-compositional learning framework for HOI concept discovery and object affordance recognition simultaneously; and 3) we evaluate the proposed approach on two extended benchmarks, and it significantly improves the performance of HOI concept discovery, facilitates object affordance recognition with HOI model, and also enables HOI detection with novel concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Human-Object Interaction</head><p>HOI understanding [23] is of great importance for visual relationship reasoning <ref type="bibr" target="#b39">[61]</ref> and action understanding <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">67]</ref>. Different approaches have been in-vestigated for HOI understanding from various aspects, including HOI detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">39,</ref><ref type="bibr">40,</ref><ref type="bibr" target="#b46">68,</ref><ref type="bibr">34,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b49">71,</ref><ref type="bibr" target="#b33">55,</ref><ref type="bibr" target="#b43">65]</ref>, HOI recognition <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">33,</ref><ref type="bibr">30]</ref>, video HOI <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">32]</ref>, compositional action recognition [42], 3D scene reconstruction <ref type="bibr" target="#b44">[66,</ref><ref type="bibr" target="#b11">12]</ref>, video generation <ref type="bibr" target="#b22">[44]</ref>, and object affordance reasoning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr">28]</ref>. Recently, compositional approaches (e.g., VCL [27]) have been intensively proposed for HOI understanding using the structural characteristic <ref type="bibr">[33,</ref><ref type="bibr">27,</ref><ref type="bibr" target="#b22">44,</ref><ref type="bibr">38,</ref><ref type="bibr">28]</ref>. Meanwhile, DETR-based methods (e.g., Qpic <ref type="bibr" target="#b33">[55]</ref>) achieve superior performance on HOI detection. However, these approaches mainly consider the perception of known HOI concepts, and pay no attention to HOI concept discovery. To fulfill the gap between learning on known and unknown concepts, a novel task, i.e. , HOI concept discovery, is explored in this paper. Currently, zero-shot HOI detection also attracts massive interests from the community <ref type="bibr" target="#b31">[53,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b26">48,</ref><ref type="bibr">27,</ref><ref type="bibr">29]</ref>. However, those approaches merely consider known concepts and are unable to discover HOI concepts. Some HOI approaches <ref type="bibr" target="#b26">[48,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b37">59,</ref><ref type="bibr" target="#b36">58]</ref> expand the known concepts via leveraging language priors. However, that is limited to existing knowledge and can not discover concepts that never appear in the language prior knowledge. HOI concept discovery is able to address the problem, and enable unknown HOI concept detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Object Affordance Learning</head><p>The notation of affordance is formally introduced in <ref type="bibr" target="#b19">[20]</ref>, where object affordances are usually those action possibilities that are perceivable by an actor <ref type="bibr" target="#b23">[45,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Noticeably, the action possibilities of an object also indicate the HOI concepts related to the object. Therefore, object affordance can also represent the existence of HOI concepts. Recent object affordance approaches mainly focus on the pixel-level affordance learning from human interaction demonstration <ref type="bibr">[36,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b21">43,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b42">64]</ref>. Yao et al. <ref type="bibr" target="#b41">[63]</ref> present a weakly supervised approach to discover object functionalities from HOI data in the musical instrument environment. Zhu et al. <ref type="bibr" target="#b48">[70]</ref> introduce to reason affordances in knowledge-based representation. Recent approaches propose to generalize HOI detection to unseen HOIs via functionality generalization <ref type="bibr" target="#b1">[2]</ref> or analogies <ref type="bibr" target="#b26">[48]</ref>. However those approaches focus on HOI detection, ignoring object affordance recognition. Specifically, Hou et al. <ref type="bibr">[28]</ref> introduce an affordance transfer learning (ATL) framework to enable HOI model to not only detect interactions but also recognize object affordances. Inspired by this, we further develop a self-compositional learning framework to facilitate the object affordance recognition with HOI model to discover novel HOI concepts for downstream HOI tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Semi-Supervised Learning</head><p>Semi-supervised learning is a learning paradigm for constructing models that use both labeled and unlabeled data <ref type="bibr" target="#b40">[62]</ref>. There are a wide variety of Deep Semi-Supervised Learning methods, such as Generative Networks <ref type="bibr">[35,</ref><ref type="bibr" target="#b32">54]</ref>, Graph-Based methods <ref type="bibr" target="#b35">[57,</ref><ref type="bibr">22]</ref>, Pseudo-Labeling methods [37, <ref type="bibr" target="#b38">60,</ref><ref type="bibr">26]</ref>. HOI concept discovery shares a similar characteristic to semi-supervised learning approaches. H OI concept discovery has instances of labeled HOI concepts, but no instances of unknown concepts. We thus compose HOI representations for unknown concepts according to <ref type="bibr" target="#b28">[50]</ref>. With composite HOIs, concept discovery and object affordance recognition can be treated as PU learning <ref type="bibr" target="#b13">[14]</ref>. Moreover, HOI concept discovery requires to discriminate whether the combinations (possible HOI concepts) are reasonable and existing. Considering each value of the concept confidences also represents the possibility of the composite HOI, we construct pseudo labels <ref type="bibr">[37,</ref><ref type="bibr" target="#b28">50]</ref> for composite features from the concept confidence matrix, and optimize the composite HOIs in an end-to-end way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we first formulate the problem of HOI concept discovery and introduce the compositional learning framework. We then describe a baseline for HOI concept discovery via affordance prediction. Lastly, we introduce the proposed self-compositional learning framework for online HOI concept discovery and object affordance recognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>HOI concept discovery aims to discover novel HOI concepts/categories using HOI instances from existing known HOI categories. Given a set of verb categories V and a set of object categories O, let S = V ? O indicate the set of all possible verb-object combinations. Let S k , S u , and S o denote three disjoint sets, known HOI concepts, unknown HOI concepts, and invalid concepts (or impossible verbobject combinations), respectively. That is, we have</p><formula xml:id="formula_0">S k ?S u = ? and S k ?S u = S if S o = ?. Let T = {(h i , c i )} L i=1</formula><p>indicate the training dataset, where h i is a HOI instance (i.e. , verb-object visual representation pair), c i ? S k indicates the label of the i-th HOI instance and L is the total number of HOI instance.</p><p>We would also like to clarify the difference between the notations of "unknown HOI categories" and "unseen HOI categories" in current HOI approaches as follows. Let S z indicate the set of "unseen HOI categories" and we then have S z ? S k . Specifically, "unseen HOI category" indicates that the HOI concept is known but no corresponding HOI instances can be observed in the training data. Current HOI methods usually assume that unseen HOI categories S z are known HOI categories via the prior knowledge <ref type="bibr" target="#b31">[53,</ref><ref type="bibr">33,</ref><ref type="bibr" target="#b26">48,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr">27]</ref>. Therefore, existing HOI methods can not directly detect/recognize HOIs with unknown HOI concepts. HOI concept discovery aims to find S u from the existing HOI instances in T with only known HOI concepts in S k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HOI Compositional Learning</head><p>Inspired by the compositional nature of HOI, i.e. , each HOI consists of a verb and an object, visual compositional learning has been intensively explored for HOI detection by combining visual verb and object representations <ref type="bibr">[33,</ref><ref type="bibr">27,</ref><ref type="bibr">29,</ref><ref type="bibr">28]</ref>. Let h i = ?x vi , x oi ? indicate a HOI instance, where x vi and x oi denote the verb and object representations, respectively. The HOI compositional learning then aims to achieve the following objective,</p><formula xml:id="formula_1">g h (? x vi , x oi ?) ? g h (?x vi , x oi ?),<label>(1)</label></formula><p>where g h indicates the HOI classifier, x vi and x oi indicate the real verb-object representation pair (i.e. , annotated HOI pair in dataset), ? x vi , x oi ? indicates the composite verb-object pair. Specifically, x oi can be obtained from either real HOIs [27], fabricated objects or language embedding [29, <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b26">48]</ref>, or external object datasets <ref type="bibr">[28]</ref>, while x vi can be from real HOIs (annotated verb-object pair) and language embeddings <ref type="bibr">[33,</ref><ref type="bibr" target="#b26">48]</ref>. As a result, when composite HOIs are similar to real HOIs, we are then able to augment HOI training samples in a compositional manner. However, current compositional approaches for HOI detection [27,28] simply remove the composite HOI instances out of the label space, which may also remove a large number of feasible HOIs (e.g., "ride zebra" as shown <ref type="figure">Figure 2</ref>). Furthermore, the compositional approach can not only augment the training data for HOI recognition, but also provide a method to determinate whether x vi and x oi are combinable to form a new HOI or not [28], i.e. , discovering the HOI concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-Compositional Learning</head><p>In this subsection, we introduce the proposed self-compositional learning framework for HOI concept discovery as follows. As shown in <ref type="figure">Figure 2</ref>, the main HOI concept discovery framework falls into the popular two-stage HOI detection framework <ref type="bibr">[27]</ref>. Specifically, we compose novel HOI samples from pair-wise images to optimize the typical HOI branch (annotated HOIs), compositional branch (the composite HOIs out of the label space are removed <ref type="bibr">[27,</ref><ref type="bibr">28]</ref>) and the new concept discovery branch (all composite HOIs are used). The main challenge of HOI concept discovery is the lack of instances for unknown HOI concepts, but we can infer to discover new concepts according to the shared verbs and objects. Specifically, we find that the affordance transfer learning [28] can be used for not only the object affordance recognition but also the HOI concept discovery, and we thus first introduce the affordance-based method as a baseline as follows.</p><p>Affordance Prediction The affordance transfer learning <ref type="bibr">[28]</ref> or ATL is introduced for affordance recognition using the HOI detection model. However, it has been ignored that the affordance prediction can also enable HOI concept discovery, i.e. , predicting a new affordance for an object although the affordance is not labeled during training. We describe a vanilla approach for HOI concept discovery using affordance prediction <ref type="bibr">[28]</ref>. Specifically, we predict the affordances for all objects in the training set according to <ref type="bibr">[28]</ref>. Then, we average the affordance predictions according to each object category to obtain the HOI concept confidence matrix M ? R Nv?No , where each value represents the concept confidence of the corresponding combination between a verb and an object. N v and N o are the numbers of verb and object categories, respectively. For simplicity, we may use both vector and matrix forms of the confidence matrix M ? R NvNo and M ? R Nv?No in this paper. Though affordance prediction can be used for HOI concept discovery, it is time-consuming since it requires to predict affordances of all objects in training set. Specifically, we need an extra offline affordance prediction process to infer concepts with the computational complexity O(N 2 ) in <ref type="bibr">[28]</ref>, where N is the number of total training HOIs, e.g., it takes 8 hours with one GPU to infer the concept matrix M on HICO-DET. However, we can treat the verb representation as affordance representation <ref type="bibr">[28]</ref>, and obtain the affordance predictions for all objects in each mini-batch during training stage. Inspired by the running mean manner in [31], we devise an online HOI concept discovery framework via averaging the predictions in each mini-batch.</p><p>Online Concept Discovery As shown in <ref type="figure">Figure 2</ref>, we keep a HOI concept confidence vector during training, M ? R NvNo , where each value represents the concept confidence of the corresponding combination between a verb and an object. To achieve this, we first extract all verb and object representations among pair-wise images in each batch as x v and x o . We then combine each verb representation and all object representations to generate the composite HOI representations x h . After that, we use the composite HOI representations as the input to the verb classifier and obtain the corresponding verb prediction?  </p><formula xml:id="formula_2">Y v ? R N N ?Nv ,</formula><formula xml:id="formula_3">Y h = Y v ? Y o , where Y h ? R N N ?NvNo ,</formula><formula xml:id="formula_4">M ? M ? C + N N i? h (i, :) ? Y h (i, :) C + N N i Y h (i, :) ,<label>(2)</label></formula><formula xml:id="formula_5">C ? C + N N i Y h (i, :),<label>(3)</label></formula><p>where ? indicates the element-wise multiplication,? h (i, :)?Y h (i, :) aims to filter out predictions whose labels are not Y h (i, :), each value of C ? R NvNo indicates the total number of composite HOI instances in each verb-object pair (including unknown HOI categories). Actually,? h (i, :) ? Y h (i, :) follows the affordance prediction process <ref type="bibr">[28]</ref>. The normalization with C is to avoid the model bias to frequent categories. Specifically, both M and C are zero-initialized. With the optimization of HOI detection, we can obtain the vector M to indicate the HOI concept confidence of each combination between verbs and objects.</p><p>Self-Training Existing HOI compositional learning approaches [27,29,28] usually only consider the known HOI concepts and simply discard the composite HOIs out of label space during optimization. Therefore, there are only positive data for object affordance learning, leaving a large number of unlabeled composite HOIs ignored. Considering that the concept confidence on HOI concept discovery also demonstrates the confidence of affordances (verbs) that can be applied to an object category, we thus try to explore the potential of all composite HOIs, i.e., both labeled and unlabeled composite HOIs, in a semi-supervised way. Inspired by the way used in PU learning <ref type="bibr" target="#b13">[14]</ref> and pseudo-label learning [37], we devise a self-training strategy by assigning the pseudo labels to each verb-object combination instance using the concept confidence matrix M, and optimize the network with the pseudo labels in an end-to-end way. With the self-training, the online concept discovery can gradually improve the concept confidence M, and in turn optimize the HOI model for object affordance learning with the concept confidence. Specifically, we construct the pseudo labels? v ? R N N ?Nv from the concept confidence matrix M ? R Nv?No for composite HOIs x h as follows,</p><formula xml:id="formula_6">Y v (i, :) = No j M(:, j) max(M) ? Y h (i, :, j),<label>(4)</label></formula><p>where 0 ? j &lt; N o indicates the index of object category, 0 ? i &lt; N N is the index of HOI representations. Here, N is the number of HOIs in each minibatch, and is usually very small on HICO-DET and V-COCO. Thus the time complexity of Equation 4 is small. The labels of composite HOIs are reshaped as Y h ? R N N ?Nv?No . Noticeably, in each label Y h (i, :, :), there is only one vector Y h (i, :, j) larger than 0 because each HOI has only one object. As a result, we obtain pseudo verb label? v (i, :) for HOI x hi . Finally, we use composite HOIs with pseudo labels to train the models, and the loss function is defined as follows,</p><formula xml:id="formula_7">L d = 1 N N N N i ( 1 N v Nv k L BCE ( Z(i, k) T ,? v (i, k))),<label>(5)</label></formula><p>where Z(i, :) is the prediction of the i-th composite HOI, 0 ? k &lt; N v means the index of predictions, T is the temperature hyper-parameter to smooth the predictions (the default value is 1 in experiment), L BCE indicates the binary cross entropy loss. Finally, we optimize the network using L d , L h and L c in an end-to-end way, where L h indicate the typical classification loss for known HOIs and L c is the compositional learning loss [27].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first introduce the datasets and evaluation metrics. We then compare the baseline and the proposed method for HOI concept discovery and object affordance recognition. We also demonstrate the effectiveness of the proposed method for HOI detection with unknown concepts and zero-shot HOI detection. Lastly, we provide some visualizations results of self-compositional learning. Moreover, ablation studies and the full results of HOI detection with self-compositional learning are provided in Appendix D, F, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>Datasets. We extend two popular HOI detection datasets, HICO-DET <ref type="bibr" target="#b6">[7]</ref> and V-COCO [24], to evaluate the performance of different methods for HOI concept discovery. Specifically, we first manually annotate all the possible verb-object combinations on HICO-DET (117 verbs and 80 objects) and V-COCO (24 verbs and 80 objects). As a result, we obtain 1,681 concepts on HICO-DET and 401 concepts on V-COCO, i.e. , 1,681 of 9,360 verb-object combinations on HICO-DET and 401 of 1,920 verb-object combinations on V-COCO are reasonable. Besides, 600 of 1,681 HOI concepts on HICO-DET and 222 of 401 HOI concepts on V-COCO are known according to existing annotations. Thus, the HOI concept discovery task requires to discover the other 1,081 concepts on HICO-DET and 179 concepts on V-COCO. See more details about the annotation process, the statistics of annotations, and the novel HOI concepts in Appendix B. Evaluation Metrics. HOI concept discovery aims to discover all reasonable combinations between verbs and objects according to existing HOI training samples. We report the performance by using the average precision (AP) for concept discovery and mean AP (or mAP) for object affordance recognition. For HOI detection, we also report the performance using mAP. We follow <ref type="bibr">[28]</ref> to evaluate object affordance recognition with HOI model on COCO validation 2017 [41], Object 365 validation <ref type="bibr" target="#b30">[52]</ref>, HICO-DET test set <ref type="bibr" target="#b6">[7]</ref> and Novel Objects from Object 365 <ref type="bibr" target="#b30">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We implement the proposed method with TensorFlow <ref type="bibr" target="#b0">[1]</ref>. During training, we have two HOI images (randomly selected) in each mini-batch and we follow <ref type="bibr" target="#b18">[19]</ref> to augment ground truth boxes via random crop and random shift. We use a modified HOI compositional learning framework, i.e. , we directly predict the verb classes and optimize the composite HOIs using SCL. Following <ref type="bibr">[27,</ref><ref type="bibr">29]</ref>, the overall loss function is defined as L = ? 1 L h + ? 2 L c + ? 3 L d , where ? 1 = 2, ? 2 = 0.5, ? 3 = 0.5 on HICO-DET, and ? 1 = 0.5, ? 2 = 0.5, ? 3 = 0.5 on V-COCO, respectively. Following [29], we also include a sigmoid loss for verb representation and the loss weight is 0.3 on HICO-DET. For self-training, we remove the composite HOIs when its corresponding concept confidence is 0, i.e. , the concept confidence has not been updated. If not stated, the backbone is ResNet-101. The Classifier is a two-layer MLP. We train the model for 3.0M iterations on HICO-DET and 300K iterations on HOI-COCO with an initial learning rate of 0.01. For zero-shot HOI detection, we keep human and objects with the score larger than 0.3 and 0.1 on HICO-DET, respectively. See more ablation studies (e.g., hyper-parameters, modules) in Appendix D. Experiments are conducted using a single Tesla V100 GPU (16GB), except for experiments on Qpic <ref type="bibr" target="#b33">[55]</ref>, which uses four V100 GPUs with PyTorch <ref type="bibr" target="#b24">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">HOI Concept Discovery</head><p>Baseline and Methods. We perform experiments to evaluate the effectiveness of our proposed method for HOI concept discovery. For a fair comparison, we build several baselines and methods as follows, -Random: we randomly generate the concept confidence to evaluate the performance. -Affordance: discover concepts via affordance prediction [28] as described in Sec 3.3. -GAT <ref type="bibr" target="#b34">[56]</ref>: build a graph attention network to mine the relationship among verbs during HOI detection, and discover concepts via affordance prediction. -Qpic* <ref type="bibr" target="#b33">[55]</ref>: convert verb and object predictions of <ref type="bibr" target="#b33">[55]</ref> to concept confidence similar as online discovery. -Qpic* <ref type="bibr" target="#b33">[55]</ref> +SCL: utilize concept confidence to update verb labels, and optimize the network (Self-Training). Here, we have no composite HOIs.</p><p>Please refer to the Appendix for more details, comparisons (e.g., re-training, language embedding), and qualitative discovered concepts with analysis.</p><p>Results Comparison. <ref type="table">Table 1</ref> shows affordance prediction is capable of HOI concept discovery since affordance transfer learning [28] also transfers affordances to novel objects. Affordance prediction achieves 24.38% mAP on HICO-DET and 21.36% mAP on V-COCO, respectively, significantly better than the <ref type="table">Table 1</ref>. The performance of the proposed method for HOI concept discovery. We report all performance using the average precision (AP) (%). SCL means selfcompositional learning. SCL? means online concept discovery without self-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>HICO random baseline. With graph attention network, the performance is further improved a bit. Noticeably, [28] completely ignores the possibility of HOI concept discovery via affordance prediction. Due to the strong ability of verb and object prediction, Qpic achieves 27.42% on HICO-DET, better than affordance prediction. However, Qpic has poor performance on V-COCO. The inference process of affordance prediction for concept discovery is time-consuming (over 8 hours with one GPU). Thus we devise an efficient online concept discovery method which directly predicts all concept confidences. Specifically, the online concept discovery method (SCL?) achieves 22.25% mAP on HICO-DET, which is slightly worse than the result of affordance prediction. On V-COCO, the online concept discovery method improves the performance of concept discovery by 3.98% compared to the affordance prediction. The main reason for the above observation might be due to that V-COCO is a small dataset and the HOI model can easily overfit known concepts on V-COCO. Particularly, SCL significantly improves the performance of HOI concept discovery from 22.36% to 33.58% on HICO-DET and from 24.89% to 28.77% on V-COCO, respectively. We find we can also utilize self-training to improve concept discovery on Qpic <ref type="bibr" target="#b33">[55]</ref> (ResNet-50) though the improvement is limited, which might be because verbs and objects are entangled with Qpic. Lastly, we meanwhile find SCL largely improves concept discovery of known concepts on both HICO-DET and V-COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Object Affordance Recognition</head><p>Following [28] that has discussed average precision (AP) is more robust for evaluating object affordance, we evaluate object affordance recognition with AP on HICO-DET.  unknown affordance recognition. We use the released models of [28] to evaluate the results on novel affordance recognition. Here, affordances of novel classes (annotated by hand [28]) are the same in the two settings. We find SCL improves the performance considerably by over 10% on Val2017 and HICO-DET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">HOI Detection with Unknown Concepts</head><p>HOI concept discovery enables zero-shot HOI detection with unknown concepts by first discovering unknown concepts and then performing HOI detection. The experimental results of HOI detection with unknown concepts are shown in Table 3. We follow [27] to evaluate HOI detection with 120 unknown concepts in two settings: rare first selection and non-rare first selection, i.e. , we select 120 unknown concepts from head and tail classes respectively. Different from <ref type="bibr">[27,</ref><ref type="bibr">29]</ref> where the existence of unseen categories is known and the HOI samples for unseen categories are composed during optimization, HOI detection with unknown concepts does not know the existence of unseen categories. Therefore, we select top-K concepts according to the confidence score during inference to evaluate the performance of HOI detection with unknown concepts (that is also zero-shot) in the default mode <ref type="bibr" target="#b6">[7]</ref>.</p><p>As shown in <ref type="table">Table 3</ref>, with more selected unknown concepts according to concept confidence, the proposed approach further improves the performance on unseen categories on both rare first and non-rare first settings. Specifically, it demonstrates a large difference between rare first unknown concepts HOI detection and non-rare first unknown concepts HOI detection in <ref type="table">Table 3</ref>. Considering that the factors (verbs and objects) of rare-first unknown concepts are rare in the training set [29], the recall is very low and thus degrades the performance on <ref type="table">Table 3</ref>. Illustration of HOI detection with unknown concepts and zero-shot HOI detection with SCL. K is the number of selected unknown concepts. HOI detection results are reported by mean average precision (mAP)(%). We also report the recall rate of the unseen categories in the top-K novel concepts. "K = all" indicates the results of selecting all concepts, i.e. , common zero-shot. * means we train Qpic <ref type="bibr">[</ref> unknown categories. However, with concept discovery, the results with top 120 concepts on unknown categories are improved by relatively 34.52% (absolutely 0.58%) on rare first unknown concepts setting and by relatively 20.31% (absolutely 1.19%) on non-rare first setting, respectively. with more concepts, the performance on unknown categories is also increasingly improved.</p><p>We also utilize the discovered concept confidences with SCL to evaluate HOI detection with unknown concepts on Qpic <ref type="bibr" target="#b33">[55]</ref>. For a fair comparison, we use the same concept confidences to SCL. Without concept discovery, the performance of Qpic <ref type="bibr" target="#b33">[55]</ref> degrades to 0 on Unseen categories though Qpic significantly improves zero-shot HOI detection. Lastly, we show zero-shot HOI detection (the unseen categories are known) in <ref type="table">Table 3</ref> (Those rows where K is all). We find that SCL significantly improves Qpic, and forms a new state-of-the-art on zero-shot setting though we merely use ResNet-50 as backbone in Qpic. We consider SCL improves the detection of rare classes (include unseen categories in rare first and seen categories in non-rare first) via stating the distribution of verb and object. See Appendix F for more analysis, e.g., SCL improves Qpic particularly for rare categories on Full HICO-DET. <ref type="figure">Figure 3</ref> illustrates the Grad-CAM under different methods. We find the proposed SCL focus on the details of objects and small objects, while the baseline and VCL mainly highlight the region of human and the interaction region, e.g., SCL highlights the details of the motorbike, particularly the front-wheel (last row). Besides, SCL also helps the model via emphasizing the learning of small objects (e.g., frisbee and bottle in the last two columns), while previous works ignore the small objects. This demonstrates SCL facilitates affordance recognition and HOI concept discovery via exploring more details of objects. A similar trend can be observed in Appedix G (Qpic+SCL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline</head><p>VCL SCL Input <ref type="figure">Fig. 3</ref>. A visual comparison of recent methods using the Grad-CAM <ref type="bibr" target="#b29">[51]</ref> tool. The first row is input image, the second row is baseline without compositional approach, the third row is VCL [27] and the last row is the proposed SCL. We do not compare with ATL <ref type="bibr">[28]</ref>, since that ATL uses extra training datasets. Here, we compare all models using the same dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a novel task, Human-Object Interaction Concept Discovery, which aims to discover all reasonable combinations (i.e. , HOI concepts) between verbs and objects according to a few training samples of known HOI concepts/categories. Furthermore, we introduce a self-compositional learning or SCL framework for HOI concept discovery. SCL maintains an online updated concept confidence matrix, and assigns pseudo labels according to the matrix for all composite HOI features, and thus optimize both known and unknown composite HOI features via self-training. SCL facilitates affordance recognition of HOI model and HOI concept discovery via enabling the learning on both known and unknown HOI concepts. Extensive experiments demonstrate SCL improves HOI concept discovery on HICO-DET and V-COCO and object affordance recognition with HOI model, enables HOI detection with unknown concepts, and improves zeroshot HOI detection. A Detailed Analysis for the Motivation Actually, after we generate the composite HOI features, we have features for both known and unknown concepts. We merely know the HOI features of the known concepts are existing, while we do not know whether the HOI features of unknown concepts are reasonable or not. This actually fall into a typical semisupervised learning, in which part of samples are labeled (known). Therefore, inspired by the popular semi-supervised learning method, we propose to design a self-training strategy with pseudo labels. SCL largely improves concept discovery. At first, during training, SCL involves both HOI instances from known or unknown concepts (via pseudo-labeling). Another important thing is that SCL uses both positive and negative unknown concepts, which prevents the model from only fitting the verb patterns. For example, the classifier may predict a reasonable concept for the verb "eat" regardless of the object representation, if there are no negative unknown concepts, e.g., "eat TV". Lastly, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>, SCL also reduces the risk of overfitting known concepts compared with ATL. e.g., we observe high confidence for the novel concept "squeeze banana"(sort in 2027) in SCL, while the confidence of "squeeze banana" is merely 0.0017 (sort in 7554) in ATL. <ref type="table">Table 4</ref>. The performance of the proposed method for HOI concept discovery under different annotations. Better Annotation indicates we remove some wrongly labeled concepts in annotation. We report all performance using the average precision (AP) (%). UC means unknown concepts and KC means known concepts. SCL means selfcompositional learning. SCL? means online concept discovery without self-training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Annotation</head><p>In order to evaluate the proposed method, we manually annotate the novel concepts for both HICO and V-COCO dataset. Specifically, we annotate the concepts that people can infer from existing concepts. The final set of concepts are provided in the supplemental material.</p><p>Statistically, there are about 1.3% and 1.9% mislabeled pairs on HICO-DET and V-COCO, respectively. Meanwhile, there are about 1.7% and 1.1% unlabeled pairs (including ambiguous verbs) on the remaining categories of HICO-DET and V-COCO.</p><p>To evaluate the effect of annotation quality of concept annotation on HOI concept discovery, we illustrate the result of different models with different annotations. We compare two versions of annotations, both of which are provided in supplemental materials. Specifically, the file "label hoi concept.csv" is the worse version, while "label hoi concept new.csv" is the refined version. <ref type="table">Table 4</ref> shows SCL even achieves better performance when evaluate SCL with better annotation, while the performance of baseline is not improved. This experiments together with <ref type="table">Table 1</ref> in the main paper show the quality of current annotation is enough for the evaluation of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Qualitative illustration</head><p>We also illustrate the discover concepts in this Section. Here, we choose the concepts after removing the known concepts from the prediction list because the confidence of known concepts in the prediction of SCL is usually very higher. We choose 5 concepts with high confidence and 5 concepts with low confidence to illustrate. <ref type="table" target="#tab_7">Table 5</ref> shows the discovered concepts in SCL are usually more reasonable. We provide the full prediction list with confidence in "result conf SCL.txt" in supplementary materials.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Modules</head><p>We conduct ablation studies on three modules: verb auxiliary loss [29], union verb <ref type="bibr">[27]</ref>, and spatial branch <ref type="bibr" target="#b18">[19]</ref>. Union verb indicates that we extract verb representation from the union box of human and object. When we remove the union verb representation, we directly extract verb representation from the human bounding box; In our experiment, we remove the spatial branch. Here, we demonstrate we achieve better performance without the spatial branch. Spatial branch. We remove the spatial branch in <ref type="bibr" target="#b18">[19]</ref>, which is very effective for HOI detection. We find that the spatial branch degrades the performance of HOI concept discovery: the performance of HOI concept discovery increases from 32.56% to 33.26% without spatial branch, as shown in <ref type="table" target="#tab_8">Table 6</ref>. We thus remove spatial branch.</p><p>Verb auxiliary loss. We follow [29] to utilize a verb auxiliary loss to regularize verb representations. As shown in <ref type="table" target="#tab_8">Table 6</ref>, the model without using a verb auxiliary loss drops by nearly 3% on unseen concepts, which demonstrates the importance of verb auxiliary loss for HOI concept discovery.</p><p>Union verb. <ref type="table" target="#tab_8">Table 6</ref> demonstrates that extracting verb representation from union box is of great importance for HOI concept discovery. When we extract verb representation from human bounding box, the result of HOI concept discovery apparently drops from 32.56% to 28.30%.</p><p>Though verb auxiliary loss and union verb representation are very helpful for concept discovery, the performance without the two strategies still outperform our baseline, i.e. , online concept discovery without self-training. To some extent, the self-training approach makes use of all composite HOIs, and thus significantly enriches the training data. As a result, the self-training strategy usually requires more iterations to converge to a better result. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates the comparison of convergence between online concept discovery and self-training. For online concept discovery, we observe that the model begins to overfit the known concepts after 2,000,000 iterations, and we thus have an early stop during the optimization. We notice that the result on unknown concepts of self-training increases to 32.%, while the baseline (i.e. , online concept discovery) begins to overfit after 800,000 iterations. This might be because the self-training utilizes all composite HOIs including many impossible combinations (i.e. , negative samples for HOI concept discovery). <ref type="table">Table 7</ref>. Ablation studies of hyper-parameters on V-COCO. UC means unknown concepts and KC means known concepts. Results are reported by average precision (%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Hyper-parameters</head><p>In the main paper, we have several hyper-parameters (i.e. ? 1 , ? 2 , ? 3 , T , where ? 1 = 2., ? 2 = 0.5, ? 3 = 0.5 and T = 1.). For ? 1 and ? 2 , we follow the settings in <ref type="bibr">[27]</ref>. For ? 3 and T , we perform ablation studies on V-COCO as shown in <ref type="table">Table 7</ref>. We notice that both T and ? 3 have an important effect on the HOI concept discovery. As shown in <ref type="table">Table 7</ref>, the performance increases from 29.52% to 31.33% on unseen concepts when we set ? 3 = 2., which is much better than the results reported in the main paper. This also illustrates that L d is more important than L CL for HOI concept discovery.</p><p>In our experiment, we apply the temperature T to predictions. As shown in <ref type="table">Table 7</ref>, we find that when T decreases to 0.5, the performance also slightly increases from 29.52% to 29.69%. Thus, we further conduct ablation experiments on T in <ref type="table">Table 8</ref>. Specifically, to quickly evaluate the effect of T , we remove spatial branch and run all experiments with 1,000,000 iterations. Noticeably, when we set T = 0.25, the performance on concept discovery further increases from 30.36% to 33.66%, which indicates a smaller temperature helps HOI concept discovery. In our experiments, we also find this result further increases to over 35.% when T = 0.5 after convergence, which is much better than the result (33.26%) of T = 1. This might be because smaller temperature is less sensitive to noise data, since composite HOIs can be regard as noise data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Normalization for Pseudo-labels</head><p>In our experiment, we normalize the confidence matrix for pseudo-labels. <ref type="table" target="#tab_9">Table 9</ref> illustrates the normalization approach has a slight effect on the concept discovery performance.  <ref type="table">Table 10</ref> demonstrates SCL consistent improves the baseline (i.e. , SCL without Self-Training). Here, we use the same concepts for a fair comparison. Thus, the recall is the same. Meanwhile, <ref type="table">Table 10</ref> also shows Self-Training effectively improves the HOI detection. when we select all concepts to evaluate HOI detection, it is common zero-shot HOI detection, i.e. , all unseen classes are known. Particularly, for application, one can directly detect unknown concepts with concept discovery from the model itself, e.g., Qpic <ref type="bibr" target="#b33">[55]</ref>. Here, we mainly demonstrate different methods with the same concept confidence for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Novel Objects</head><p>In the main paper, we illustrate the result on two compositional zero-shot settings. Here, we further illustrate the effectiveness of HOI concept discovery for novel object HOI detection. Novel object HOI detection requires to detect HOI  <ref type="table">Table 11</ref> demonstrates concept discovery largely improves the performance on unseen category from 3.92% to 11.41% (relatively by 191%) with top 100 unknown concepts. We meanwhile find the recall increases to 41.00% with only the top 100 unknown concepts. Nevertheless, when we select all unknown concepts, the performance on unseen category is 17.19%. This shows we should improve the performance of concept discovery. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F HOI Detection</head><p>One-Stage Method. We also evaluate SCL on Qpic <ref type="bibr" target="#b33">[55]</ref>, i.e. , the state-ofthe-art HOI detection method based on Transformer, for HOI detection. Code is provided in https://github.com/zhihou7/SCL. We first obtain concept confidence similar as Section 3.3.2 in the main paper. Denote? v ? R N ?Nv as verb predictions,? o ? R N ?No as verb predictions, we obtain concept predictions? h as follows,? </p><formula xml:id="formula_8">h =? v ?? o .<label>(6)</label></formula><formula xml:id="formula_9">y v = max(y v + M(:, y o ), 1)<label>(7)</label></formula><p>where max means we clip the value to 1 if the value is larger than 1. Then, we obtain pseudo verb label y v to optimize the samples of the HOI similar as Equation 7 (here, we only have annotated HOI samples). We think the running concept confidence M have implicitly counted the distribution of verb and object in the dataset. Meanwhile, the denominator in Equation 2 can also normalize the confidence according to the frequency, and thus ease the long-tailed issue. Thus, with the pseudo labels constructed from M, we can rebalance the distribution of the dataset, which is a bit similar to re-weighting strategy <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. However, SCL does not require to set the weights for each class manually. <ref type="table" target="#tab_7">Table 15</ref> demonstrates SCL greatly improves Qpic on Unseen category on rare first zero-shot detection, while SCL significantly facilitates rare category on non-rare first zero-shot detection. In Full HOI detection on HICO-DET, <ref type="table">Table 14</ref> shows SCL largely facilitates HOI detection on rare category. Particularly, the seen category in rare first setting includes 120 rare classes, while the seen category in non-rare first setting only includes 18 classes (all rare classes are in unseen category in non-rare first setting). Thus, SCL actually improves HOI detection for rare category. We think the concept confidence matrix internally learns the distribution of verb and objects and in the dataset. e.g., given an object, M illustrates the corresponding verb distribution. <ref type="table" target="#tab_7">Table 15</ref>. Zero-Shot HOI detection based on Qpic. Results are reported by mean average precision (%). Here, we split the classes of HOI into four categories in zeroshot setting, i.e. , Seen are categorized into rare and non-rare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Unseen Rare NonRare Full Qpic <ref type="bibr" target="#b33">[55]</ref>  Two-Stage method. Considering the HOI concept discovery is mainly based on two-stage HOI detection approaches <ref type="bibr">[27]</ref>, it is direct and simple to evaluate the performance of self-training on HOI detection. <ref type="table">Table 13</ref> demonstrates the HOI detection results on ground truth boxes. Noticeably, we directly predict the verb category, rather than HOI category. Thus, the baseline of HOI detection (i.e. visual compositional learning [27]) is a bit worse. We can find self-training also slightly improves the performance, especially on rare category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Visualization</head><p>In this section, we provide more visualized illustrations.</p><p>More Grad-CAM Visualizations <ref type="figure" target="#fig_4">Figure 5</ref> demonstrates the visualization of Qpic and Qpic+SCL: the second row is Qpic and the third row is Qpic+SCL, where we observe a similar trend to the Gram-CAM illustration in main paper.</p><p>Concept Visualization. We illustrate the visualized comparisons of concept discovery in <ref type="figure" target="#fig_5">Figure 6</ref>. According to the ground truth and known concepts, we find some verb (affordance) classes can be applied to most of objects (the row is highlighted in the ground truth <ref type="figure">figure)</ref>. This observation is reasonable because some kinds of actions can be applied to most of objects in visual world, e.g., hold. As shown in <ref type="figure" target="#fig_5">Figure 6</ref>, there are many false positive predictions in the results of affordance prediction, and affordance prediction tends to overfit the known concepts, especially those with frequently appeared verbs. Methods of online HOI concept discovery on V-COCO have fewer false positive predictions compared to affordance prediction. However, the two methods tend to predict concepts composed of frequent verbs in known concepts due to the verb and object imbalance issues in HOI dataset <ref type="bibr">[29]</ref>. Particularly, the false positive predictions are largely eased with self-training (e.g., the top right region). In addition, the blank columns in <ref type="figure" target="#fig_5">Figure 6</ref> are because there are only 69 objects in V-COCO training set, and we can ease it via training network with additional object images <ref type="bibr">[28]</ref> as illustrated in the last figure of <ref type="figure" target="#fig_5">Figure 6</ref>. See more visualized results on HICO-DET and V-COCO in the supplemental material. Particularly, we further notice there are dependencies between verb classes (See verb dependency analysis).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Additional Concept Discovery Approaches</head><p>We provide More comparisons in this Section. For a fair comparison with ATL [28] (i.e. , affordance prediction), we use the same number of verbs (21 verbs) on V-COCO. The code includes how to convert V-COCO to 21 verbs, i.e. merge " instr" and " obj" and remove actions without object (e.g., stand, smile, run).</p><p>Language embedding baseline. In the main paper, we illustrate a random baseline. Here we further illustrate the results with language embedding <ref type="bibr" target="#b25">[47]</ref>. Different from extracting verb/object features from real HOI images, we use the . Visualized Comparison of different methods on V-COCO dataset. The column is the object classes and the row represents the verb classes. Known Concepts are the concepts that we have known. SCL? means online concept discovery without selftraining. For better illustration, we filter out known concepts in proposed methods. "+ Novel Objects" means self-training with novel object images.</p><p>corresponding language embedding representations of verb/object as input, i.e. discovering concepts from language embedding. <ref type="table" target="#tab_3">Table 12</ref> shows the performance is just a bit better than random result, and is much worse than online concept discovery. Similar to the main paper, when we evaluate the unknown concepts, we mask out the known concepts to avoid the disturbance from known concepts.</p><p>Re-Training. We first train the HOI model via visual compositional learning <ref type="bibr">[27]</ref>, and then predict the concept confidence. Next, we use the predicted concept confidence to provide pseudo labels for the composite HOIs. <ref type="table" target="#tab_3">Table 12</ref> shows the performance of Re-Training is worse than SCL.</p><p>With COCO dataset. <ref type="table" target="#tab_3">Table 12</ref> also demonstrates the baseline (SCL?) with COCO datasets has poor performance on concept discovery. We think it is because the domain shift between COCO dataset and HICO-DET dataset. However, SCL still achieves significant improvement on concept discovery.</p><p>Qpic+SCL. The details are provided in Section D.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where N indicates the number of real HOI instances (i.e. , verbobject pair) in each mini-batch and N N is then the number of all composite verbobject pairs (including unknown HOI concepts). Let Y v ? R N ?Nv and Y o ? R N ?No denote the label of verb representations x v and object representations x o , respectively. We then have all composite HOI labels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Acknowledgments Mr. Zhi Hou and Dr. Baosheng Yu are supported by ARC FL-170100117, DP-180103424, IC-190100031, and LE-200100049. 22. Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O., Dahl, G.E.: Neural message passing for quantum chemistry. In: ICML. pp. 1263-1272. PMLR (2017) 23. Gupta, A., Kembhavi, A., Davis, L.S.: Observing human-object interactions: Using spatial and functional compatibility for recognition. IEEE PAMI 31(10), 1775-1789 (2009) 24. Gupta, S., Malik, J.: Visual semantic role labeling. arXiv preprint arXiv:1505.04474 (2015) 25. Hassan, M., Dharmaratne, A.: Attribute based affordance detection from humanobject interaction images. In: Image and Video Technology. pp. 220-232. Springer (2015) 26. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015) 27. Hou, Z., Peng, X., Qiao, Y., Tao, D.: Visual compositional learning for humanobject interaction detection. In: ECCV (2020) 28. Hou, Z., Yu, B., Qiao, Y., Peng, X., Tao, D.: Affordance transfer learning for human-object interaction detection. In: CVPR (2021) 29. Hou, Z., Yu, B., Qiao, Y., Peng, X., Tao, D.: Detecting human-object interaction via fabricated compositional learning. In: CVPR (2021) 30. Huynh, D., Elhamifar, E.: Interaction compass: Multi-label zero-shot learning of human-object interactions via spatial relations. In: ICCV. pp. 8472-8483 (2021) 31. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing internal covariate shift. In: ICML. pp. 448-456. PMLR (2015) 32. Ji, J., Desai, R., Niebles, J.C.: Detecting human-object relationships in videos. In: ICCV. pp. 8106-8116 (2021) 33. Kato, K., Li, Y., Gupta, A.: Compositional learning for human object interaction. In: ECCV. pp. 234-251 (2018) 34. Kim, B., Lee, J., Kang, J., Kim, E.S., Kim, H.J.: Hotr: End-to-end human-object interaction detection with transformers. In: CVPR. pp. 74-83 (2021) 35. Kingma, D.P., Mohamed, S., Rezende, D.J., Welling, M.: Semi-supervised learning with deep generative models. In: NIPS (2014) 36. Kjellstr?m, H., Romero, J., Kragi?, D.: Visual object-action recognition: Inferring object affordances from human demonstration. Computer Vision and Image Understanding 115(1), 81-90 (2011) 37. Lee, D.H., et al.: Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In: Workshop on challenges in representation learning, ICML (2013) 38. Li, Y.L., Liu, X., Wu, X., Li, Y., Lu, C.: Hoi analysis: Integrating and decomposing human-object interaction. NeuIPS 33 (2020) 39. Li, Y.L., Zhou, S., Huang, X., Xu, L., Ma, Z., Fang, H.S., Wang, Y.F., Lu, C.: Transferable interactiveness prior for human-object interaction detection. In: CVPR (2019) 40. Liao, Y., Liu, S., Wang, F., Chen, Y., Feng, J.: Ppdm: Parallel point detection and matching for real-time human-object interaction detection. In: CVPR (2020) 41. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll?r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV. pp. 740-755. Springer (2014) 42. Materzynska, J., Xiao, T., Herzig, R., Xu, H., Wang, X., Darrell, T.: Somethingelse: Compositional action recognition with spatial-temporal interaction networks. In: CVPR. pp. 1049-1059 (2020)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of the convergence with self-training strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Visualized Illustration of SCL+Qpic and Qpic<ref type="bibr" target="#b33">[55]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6. Visualized Comparison of different methods on V-COCO dataset. The column is the object classes and the row represents the verb classes. Known Concepts are the concepts that we have known. SCL? means online concept discovery without selftraining. For better illustration, we filter out known concepts in proposed methods. "+ Novel Objects" means self-training with novel object images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Concept Discovery Branch Classifier Compositional Branch Verb Representation Object Representation Fig. 2. Illustration of Self-Compositional Learning for HOI Concept Discovery. Specif- ically, following [27], verb and object features are extracted via RoI-Pooling from union box and object box respectively, which are then used to construct HOI features in HOI branch according to HOI annotation. Following [27], for SCL, verb and object features are further mutually combined to generate composite HOI features. Then, the feasible composite HOI features belonging to the known concepts are directly used to</head><label></label><figDesc>train the network in Compositional Branch. Here the classifier predicts verb classes directly. Meanwhile, we update the concept confidence M ? R Nv ?No , where Nv and No are the number of verb classes and object classes respectively, with the predictions of all composite HOI features. The concept discovery branch is optimized via a self-training approach to learn from composite HOI features with the concept confidence M.</figDesc><table><row><cell></cell><cell>Verb</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ride horse</cell></row><row><cell>ROI Pooling CNN</cell><cell>Object</cell><cell>+</cell><cell></cell><cell>Classifier</cell><cell cols="2">feed zebra HOI Branch ? #</cell></row><row><cell></cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Concept Confidence % ? ' ()?(+</cell></row><row><cell>feed horse ride zebra</cell><cell>? "</cell><cell>Classifier</cell><cell>update ? $</cell><cell>0.0 0.1 ? ? ? ? 0.4 0.0 ?</cell><cell>0.3 ? 0.6</cell><cell>ride horse ride zebra feed horse ?</cell></row><row><cell></cell><cell cols="3">Self-Compositional Learning</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and the superscripts h, v, and o indicate HOI, verb, and object, respectively. Similar to affordance prediction, we repeat? v by N o times to obtain concept predictions? h ? R N N ?NvNo . Finally, we update M in a running mean manner [31] as follows,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>illustrates SCL largely improves SCL? (without self-training) by over 9% on Val2017, Object365, HICO-DET under the same training iterations. SCL requires more iterations to converge, and SCL greatly improves previous methods on all datasets with 3M iterations (Please refer to Appendix D.2 for convergence analysis). Noticeably, SCL directly predicts verb rather than HOI categories, and removes the spatial branch. Thus, SCL without self-training (SCL?) is a bit worse than ATL. Previous approaches ignore the</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Comparison of object affordance recognition with HOI network (trained on HICO-DET) among different datasets. Val2017 is the validation 2017 of COCO [41]. Obj365 is the validation of Object365 [52] with only COCO labels. Novel classes are selected from Object365 with non-COCO labels. ATL * means ATL optimized with COCO data. Numbers are copied from the appendix in [28]. Unknown affordances indicate we evaluate with our annotated affordances. Previous approaches [27,28] are usually trained by less 0.8M iterations (Please refer to the released checkpoint in [27,28]). We thus also illustrate SCL under 0.8M iterations by default. SCL? means SCL without self-training. Results are reported by Mean Average Precision (%). 35.73 43.15 12.05 28.71 27.58 32.76 12.05 ATL [28] 52.01 50.94 59.44 15.64 36.80 34.38 42.00 15.64 ATL * [28] 56.05 40.83 57.41 8.52 37.01 30.21 43.29 8.52 SCL? 50.51 43.52 57.29 14.46 44.21 41.37 48.68 14.46 SCL 59.64 52.70 67.05 14.90 47.68 42.05 52.95 14.90 SCL (3M iters) 72.08 57.53 82.47 18.55 56.19 46.32 64.50 18.55</figDesc><table><row><cell>Method</cell><cell cols="5">Known Affordances Val2017 Obj365 HICO Novel Val2017 Obj365 HICO Novel Unknown Affordances</cell></row><row><cell>FCL [29]</cell><cell>25.11 25.21 37.32 6.80</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VCL [27]</cell><cell>36.74</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>55](ResNet-50) with the released code in zero-shot setting and use the discovered concepts of SCL to evaluate HOI detection with unknown concepts. Un indicates Unknown/Unseen, Kn indicates Known/Seen, while Rec indicates Recall. .18 24.67 21.57 100.00 18.25 18.78 18.67 100.00 FCL [29] all 13.16 24.23 22.01 100.00 18.66 19.55 19.37 100.00 Qpic + SCL all 19.07 30.39 28.08 100.00 21.73 25.00 24.34 100.00</figDesc><table><row><cell>Method</cell><cell>K</cell><cell>Un</cell><cell cols="3">Rare First Kn Full Rec (%) Un</cell><cell cols="2">Non-rare First Kn Full Rec (%)</cell></row><row><cell>SCL</cell><cell>0</cell><cell cols="2">1.68 22.72 18.52</cell><cell>0.00</cell><cell cols="2">5.86 16.70 14.53</cell><cell>0.00</cell></row><row><cell>SCL</cell><cell cols="3">120 2.26 22.72 18.71</cell><cell>10.83</cell><cell cols="2">7.05 16.70 14.77</cell><cell>21.67</cell></row><row><cell>SCL</cell><cell cols="3">240 3.66 22.72 18.91</cell><cell>15.00</cell><cell cols="2">7.17 16.70 14.80</cell><cell>25.00</cell></row><row><cell>SCL</cell><cell cols="3">360 4.09 22.72 19.00</cell><cell>15.83</cell><cell cols="2">7.91 16.70 14.94</cell><cell>30.83</cell></row><row><cell>SCL</cell><cell cols="7">all 9.64 22.72 19.78 100.00 13.30 16.70 16.02 100.00</cell></row><row><cell>Qpic *  [55]</cell><cell>0</cell><cell cols="2">0.0 30.47 24.37</cell><cell>0.00</cell><cell>0.0</cell><cell>23.73 18.98</cell><cell>0.0</cell></row><row><cell>Qpic *  [55]</cell><cell cols="3">120 2.32 30.47 24.84</cell><cell>10.83</cell><cell cols="2">14.90 22.19 20.58</cell><cell>21.67</cell></row><row><cell>Qpic *  [55]</cell><cell cols="3">240 3.35 30.47 25.04</cell><cell>15.00</cell><cell cols="2">14.90 22.79 21.22</cell><cell>25.00</cell></row><row><cell>Qpic *  [55]</cell><cell cols="3">360 3.72 30.47 25.12</cell><cell>15.83</cell><cell cols="2">14.91 23.13 21.48</cell><cell>30.83</cell></row><row><cell>Qpic *  [55]</cell><cell cols="7">all 15.24 30.44 27.40 100.00 21.03 23.73 23.19 100.00</cell></row><row><cell>ATL [28]</cell><cell cols="2">all 9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>The illustration of discovered concepts.</figDesc><table><row><cell cols="4">Method Concepts with high confidence</cell><cell>Concepts with low confidence</cell></row><row><cell cols="2">SCL? type on</cell><cell>sink,inspect</cell><cell>refrig-</cell><cell>zip zebra, sign dog, chase broccoli,</cell></row><row><cell></cell><cell>erator,feed</cell><cell>suitcase,</cell><cell>inspect</cell><cell>set parking meter, tag teddy bear</cell></row><row><cell></cell><cell cols="2">chair,carry stop sign</cell><cell></cell></row><row><cell>SCL</cell><cell cols="3">ride bear, board truck, carry bowl,</cell><cell>zip zebra, flush parking meter,</cell></row><row><cell></cell><cell cols="3">wash fire hydrant, hop on motor-</cell><cell>stop at hair drier, stop at mi-</cell></row><row><cell></cell><cell>cycle</cell><cell></cell><cell></cell><cell>crowave</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Ablation studies of different modules on HICO-DET. UC means unknown concepts and KC means known concepts. Verb aux loss means Verb auxiliary loss (i.e. , binary cross entropy loss). Results are reported by average precision (%).</figDesc><table><row><cell cols="4">Spatial branch Verb aux loss Union Verb UC</cell><cell>KC</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">32.56 94.39</cell></row><row><cell>-</cell><cell>?</cell><cell>?</cell><cell cols="2">33.26 93.06</cell></row><row><cell>?</cell><cell>-</cell><cell>?</cell><cell cols="2">29.56 93.36</cell></row><row><cell>?</cell><cell>?</cell><cell>-</cell><cell cols="2">28.30 94.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>Illustration of normalized pseudo labels on HICO-DET and V-COCO. Experiments results are reported by average precision (%). Here, the SCL model uses spatial branch.</figDesc><table><row><cell></cell><cell></cell><cell>Method</cell><cell cols="4">HICO-DET UC (%) KC (%) UC (%) KC (%) V-COCO</cell><cell></cell></row><row><cell></cell><cell></cell><cell>SCL</cell><cell cols="4">32.56 94.39 29.52 97.57</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">w/o normalization 32.30</cell><cell>94.2</cell><cell cols="2">29.32 97.93</cell><cell></cell></row><row><cell cols="8">Table 10. Illustration of HOI detection with unknown concepts and zero-shot HOI</cell></row><row><cell cols="8">detection with SCL. K is the number of selected unknown concepts. HOI detection</cell></row><row><cell cols="8">results are reported by mean average precision (mAP)(%). We also report the recall</cell></row><row><cell cols="8">of the unseen categories in the top-K novel concepts. K = all indicates the results of</cell></row><row><cell cols="8">selecting all concepts, i.e. , common zero-shot.  *  means we train Qpic [55](ResNet-50)</cell></row><row><cell cols="8">with the released code in zero-shot setting and use the discovered concepts of SCL to</cell></row><row><cell cols="5">evaluate HOI detection with unknown concepts.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Method K</cell><cell cols="6">Rare First Unknown Known Full Recall (%) Unknown Known Full Recall (%) Non-rare First</cell></row><row><cell cols="2">Baseline 0</cell><cell>1.68</cell><cell>22.10 18.52</cell><cell>0.00</cell><cell>5.86</cell><cell>16.30 14.21</cell><cell>0.00</cell></row><row><cell cols="2">Baseline 120</cell><cell>3.06</cell><cell>22.10 18.29</cell><cell>10.83</cell><cell>6.16</cell><cell>16.30 14.27</cell><cell>21.67</cell></row><row><cell cols="2">Baseline 240</cell><cell>3.28</cell><cell>22.10 18.34</cell><cell>13.33</cell><cell>6.90</cell><cell>16.30 14.42</cell><cell>25.00</cell></row><row><cell cols="2">Baseline 360</cell><cell>3.86</cell><cell>22.10 18.45</cell><cell>15.83</cell><cell>7.29</cell><cell>16.30 14.50</cell><cell>30.83</cell></row><row><cell cols="2">Baseline all</cell><cell>9.62</cell><cell>22.10 19.61</cell><cell>100.00</cell><cell>12.82</cell><cell>16.30 15.60</cell><cell>100.00</cell></row><row><cell>SCL</cell><cell>0</cell><cell>1.68</cell><cell>22.72 18.52</cell><cell>0.00</cell><cell>5.86</cell><cell>16.70 14.53</cell><cell>0.00</cell></row><row><cell>SCL</cell><cell>120</cell><cell>2.26</cell><cell>22.72 18.71</cell><cell>10.83</cell><cell>7.05</cell><cell>16.70 14.77</cell><cell>21.67</cell></row><row><cell>SCL</cell><cell>240</cell><cell>3.66</cell><cell>22.72 18.91</cell><cell>15.00</cell><cell>7.17</cell><cell>16.70 14.80</cell><cell>25.00</cell></row><row><cell>SCL</cell><cell>360</cell><cell>4.09</cell><cell>22.72 19.00</cell><cell>15.83</cell><cell>7.91</cell><cell>16.70 14.94</cell><cell>30.83</cell></row><row><cell>SCL</cell><cell>all</cell><cell>9.64</cell><cell>22.72 19.78</cell><cell>100.00</cell><cell>13.30</cell><cell>16.70 16.02</cell><cell>100.00</cell></row><row><cell cols="6">E HOI Detection with Unknown Concepts</cell><cell></cell><cell></cell></row><row><cell cols="4">E.1 Additional Comparisons</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 .Table 12 .</head><label>1112</label><figDesc>Illustration of the effectiveness of HOI concept discovery for HOI detection with unknown concepts (novel objects). K is the number of selected unknown concepts. HOI detection results are reported by mean average precision (mAP)(%). Recall is evaluated for the unseen categories under the top-k novel concepts. The last row indicates the results of selecting all concepts. Additional Comparison on HOI concept discovery. We report all performance using the average precision (AP) (%). UC means unknown concepts and KC means known concepts. SCL means self-compositional learning. SCL? means online concept discovery without self-training. SCL (COCO) means we train the network via composing between verbs from HICO and objects from COCO 2014 training set. with novel objects, i.e. , the object of an unseen HOI is never seen in the HOI training set. We follow [28] to select 100 categories as unknown concepts. The remaining categories do not include the objects of unseen categories. Here we use a unique object detector to detect objects. To enable the novel object HOI detection and novel object HOI concept discovery, we follow [28] to incorporate external objects (e.g.COCO [41]) to compose novel object HOI samples. Specifically, we only choose the novel types of objects from COCO [41] as objects images in the framework [28] for novel object HOI detection with unknown concepts.</figDesc><table><row><cell cols="5">K Unseen Seen Full Recall (%)</cell></row><row><cell>0</cell><cell cols="3">3.92 19.45 16.86</cell><cell>0.00</cell></row><row><cell cols="4">100 11.41 19.45 18.11</cell><cell>41.00</cell></row><row><cell cols="4">200 12.40 19.45 18.28</cell><cell>48.00</cell></row><row><cell cols="4">300 13.52 19.45 18.46</cell><cell>52.00</cell></row><row><cell cols="4">400 13.52 19.45 18.46</cell><cell>52.00</cell></row><row><cell cols="4">500 13.91 19.45 18.53</cell><cell>56.00</cell></row><row><cell cols="4">600 13.91 19.45 18.53</cell><cell>56.00</cell></row><row><cell cols="5">all 17.19 19.45 19.07 100.00</cell></row><row><cell>Method</cell><cell></cell><cell cols="3">HICO-DET UC (%) KC (%) UC (%) KC (%) V-COCO</cell></row><row><cell>Random</cell><cell></cell><cell>12.52</cell><cell>6.56</cell><cell>12.53 13.54</cell></row><row><cell cols="4">language embedding 16.08 29.64</cell><cell>-</cell><cell>-</cell></row><row><cell>Re-Training</cell><cell></cell><cell cols="2">26.09 50.32</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">SCL? (COCO)</cell><cell cols="3">17.01 55.50 26.04 81.47</cell></row><row><cell>SCL (COCO)</cell><cell></cell><cell cols="3">31.92 86.43 27.90 90.04</cell></row><row><cell>SCL?</cell><cell></cell><cell cols="3">22.36 83.04 26.64 95.59</cell></row><row><cell>SCL</cell><cell></cell><cell cols="3">33.26 93.06 29.52 97.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13 .Table 14 .</head><label>1314</label><figDesc>Illustration of the effectiveness of self-training on HOI detection based on ground truth box. Results are reported by mean average precision (%). Self-Training 42.66 35.81 44.70 Illustration of the effectiveness of self-training for Qpic (ResNet-50). Results are reported by mean average precision (%). * means we use the released code to reproduce the results for a fair comparison. S1 means Scenario 1, while S2 means Scenario 2. SCL 29.75 24.78 31.23 61.55 62.38</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="3">Full Rare NonRare</cell></row><row><cell></cell><cell>SCL</cell><cell cols="3">42.92 36.60 44.81</cell></row><row><cell>w/o Method</cell><cell>Full</cell><cell cols="2">HICO-DET Rare NonRare</cell><cell cols="2">V-COCO S1 S2</cell></row><row><cell cols="3">GGNet [69] 23.47 16.48</cell><cell>25.60</cell><cell>-</cell><cell>54.7</cell></row><row><cell>ATL [28]</cell><cell cols="2">23.81 17.43</cell><cell>25.72</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">HOTR [34] 25.10 17.34</cell><cell>27.42</cell><cell>55.2</cell><cell>64.4</cell></row><row><cell>AS-Net[9]</cell><cell cols="2">28.87 24.25</cell><cell>30.25</cell><cell>-</cell><cell>53.9</cell></row><row><cell>Qpic [55]</cell><cell cols="2">29.07 21.85</cell><cell>31.23</cell><cell>58.8</cell><cell>61.0</cell></row><row><cell cols="3">Qpic* [55] 29.19 23.01</cell><cell>31.04</cell><cell cols="2">61.29 62.10</cell></row><row><cell>Qpic +</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Then, we update M according toEquation 2  and Equation 3 in the main paper. After training, we evaluate HOI concept discovery with M .For self-training on Qpic<ref type="bibr" target="#b33">[55]</ref>, we use M to update the verb label Y v ? R N ?Nv for annotated HOIs. Here, we do not have composite HOIs because Qpic has entangled verb and object predictions, and we update verb labels with M. Specifically, given an HOI with a verb labeled as y v ? R N v and an object labeled as y o ? R N o , where 0 ? y o &lt; N o denotes the index of object category, we update y v as follows,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>(non-rare first) 21.03 19.12 25.59 23.19 Qpic+SCL (non-rare first) 21.73 22.43 26.03 24.34</figDesc><table><row><cell>Qpic [55] (rare first)</cell><cell>15.24 16.72</cell><cell>30.98</cell><cell>27.40</cell></row><row><cell>Qpic+SCL (rare first)</cell><cell>19.07 16.19</cell><cell>30.89</cell><cell>28.08</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th symposium on operating systems design and implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Detecting humanobject interactions via functional generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Rambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cognitive psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Best</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>West Publishing Co</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">What is the effect of importance weighting in deep learning? In: ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PMLR</publisher>
			<biblScope unit="page" from="872" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset. In: CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
	<note>ieee winter conference on applications of computer vision (wacv</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1017" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Reformulating hoi detection as adaptive set prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9004" to="9013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Sensation and perception. Handbook of psychology pp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Coren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="85" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gravity-aware monocular 3d human-object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golyanik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12365" to="12374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maria Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="720" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Positive and unlabeled examples help learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Comit?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gilleron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Letouzey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Algorithmic Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="219" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d affordancenet: A benchmark for visual object affordance understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1778" to="1787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning classifiers from only positive and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Noto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="213" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Demo2vec: Reasoning object affordances from online videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">People watching: Human actions as a cue for single view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Delaitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="259" to="274" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ican: Instance-centric attention network for humanobject interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The ecological approach to visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The ecological approach to visual perception: classic edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning affordance landscapes for interaction exploration in 3d environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Generating videos of zero-shot compositions of actions and objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nawhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The Design of Everyday Things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Basic Books, Inc</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alch?-Buc, F., Fox, E., Garnett, R.</editor>
		<imprint>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019" />
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP. pp</title>
		<imprint>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Detecting unseen visual relations using analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Novelty detection: Unlabeled data definitely help</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="464" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Probability of error of some adaptive pattern-recognition machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scudder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="363" to="371" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Gradcam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Scaling human-object interaction recognition through zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unsupervised and semi-supervised learning with categorical generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06390</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">QPIC: Query-based pairwise human-object interaction detection with image-wide contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshinaga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discovering human interactions with large-vocabulary objects via query and multi-scale detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13475" to="13484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Discovering human interactions with novel objects via zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11652" to="11661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A survey on deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00550</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Discovering object functionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">One-shot object affordance detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.03658</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mining the benefits of two-stage and one-stage hoi detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Perceiving 3d human-object spatial arrangements from a single image in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pepose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Skeleton-based interactive graph network for human object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Polysemy deciphering network for humanobject interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="69" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Glance and gaze: Inferring action-aware points for one-stage human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13234" to="13243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Reasoning about object affordances in a knowledge base representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="408" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">End-to-end human object interaction detection with hoi transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="11825" to="11834" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
