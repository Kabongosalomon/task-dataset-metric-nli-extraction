<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
							<email>t.n.kipf@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
							<email>m.welling@uva.nl</email>
							<affiliation key="aff1">
								<orgName type="department">Canadian Institute for Advanced Research (CIFAR)</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>We consider the problem of classifying nodes (such as documents) in a graph (such as a citation network), where labels are only available for a small subset of nodes. This problem can be framed as graph-based semi-supervised learning, where label information is smoothed over the graph via some form of explicit graph-based regularization <ref type="bibr" target="#b31">(Zhu et al., 2003;</ref><ref type="bibr" target="#b30">Zhou et al., 2004;</ref><ref type="bibr" target="#b2">Belkin et al., 2006;</ref><ref type="bibr" target="#b27">Weston et al., 2012)</ref>, e.g. by using a graph Laplacian regularization term in the loss function:</p><formula xml:id="formula_0">L = L 0 + ?L reg , with L reg = i,j A ij f (X i ) ? f (X j ) 2 = f (X) ?f (X) .<label>(1)</label></formula><p>Here, L 0 denotes the supervised loss w.r.t. the labeled part of the graph, f (?) can be a neural networklike differentiable function, ? is a weighing factor and X is a matrix of node feature vectors X i . ? = D ? A denotes the unnormalized graph Laplacian of an undirected graph G = (V, E) with N nodes v i ? V, edges (v i , v j ) ? E, an adjacency matrix A ? R N ?N (binary or weighted) and a degree matrix D ii = j A ij . The formulation of Eq. 1 relies on the assumption that connected nodes in the graph are likely to share the same label. This assumption, however, might restrict modeling capacity, as graph edges need not necessarily encode node similarity, but could contain additional information.</p><p>In this work, we encode the graph structure directly using a neural network model f (X, A) and train on a supervised target L 0 for all nodes with labels, thereby avoiding explicit graph-based regularization in the loss function. Conditioning f (?) on the adjacency matrix of the graph will allow the model to distribute gradient information from the supervised loss L 0 and will enable it to learn representations of nodes both with and without labels.</p><p>Our contributions are two-fold. Firstly, we introduce a simple and well-behaved layer-wise propagation rule for neural network models which operate directly on graphs and show how it can be motivated from a first-order approximation of spectral graph convolutions <ref type="bibr" target="#b12">(Hammond et al., 2011)</ref>. Secondly, we demonstrate how this form of a graph-based neural network model can be used for fast and scalable semi-supervised classification of nodes in a graph. Experiments on a number of datasets demonstrate that our model compares favorably both in classification accuracy and efficiency (measured in wall-clock time) against state-of-the-art methods for semi-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FAST APPROXIMATE CONVOLUTIONS ON GRAPHS</head><p>In this section, we provide theoretical motivation for a specific graph-based neural network model f (X, A) that we will use in the rest of this paper. We consider a multi-layer Graph Convolutional Network (GCN) with the following layer-wise propagation rule:</p><formula xml:id="formula_1">H (l+1) = ? D ? 1 2?D ? 1 2 H (l) W (l) .<label>(2)</label></formula><p>Here,? = A + I N is the adjacency matrix of the undirected graph G with added self-connections. I N is the identity matrix,D ii = j? ij and W (l) is a layer-specific trainable weight matrix. ?(?) denotes an activation function, such as the ReLU(?) = max(0, ?). H (l) ? R N ?D is the matrix of activations in the l th layer; H (0) = X. In the following, we show that the form of this propagation rule can be motivated 1 via a first-order approximation of localized spectral filters on graphs <ref type="bibr" target="#b12">(Hammond et al., 2011;</ref><ref type="bibr" target="#b6">Defferrard et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SPECTRAL GRAPH CONVOLUTIONS</head><p>We consider spectral convolutions on graphs defined as the multiplication of a signal x ? R N (a scalar for every node) with a filter g ? = diag(?) parameterized by ? ? R N in the Fourier domain, i.e.:</p><formula xml:id="formula_2">g ? x = U g ? U x ,<label>(3)</label></formula><p>where U is the matrix of eigenvectors of the normalized graph Laplacian L = I N ? D ? 1 2 AD ? 1 2 = U ?U , with a diagonal matrix of its eigenvalues ? and U x being the graph Fourier transform of x. We can understand g ? as a function of the eigenvalues of L, i.e. g ? (?). Evaluating Eq. 3 is computationally expensive, as multiplication with the eigenvector matrix U is O(N 2 ). Furthermore, computing the eigendecomposition of L in the first place might be prohibitively expensive for large graphs. To circumvent this problem, it was suggested in <ref type="bibr" target="#b12">Hammond et al. (2011)</ref> that g ? (?) can be well-approximated by a truncated expansion in terms of Chebyshev polynomials T k (x) up to K th order:</p><formula xml:id="formula_3">g ? (?) ? K k=0 ? k T k (?) ,<label>(4)</label></formula><p>with a rescaled? = 2 ?max ? ? I N . ? max denotes the largest eigenvalue of L. ? ? R K is now a vector of Chebyshev coefficients. The Chebyshev polynomials are recursively defined as T k (x) = 2xT k?1 (x) ? T k?2 (x), with T 0 (x) = 1 and T 1 (x) = x. The reader is referred to <ref type="bibr" target="#b12">Hammond et al. (2011)</ref> for an in-depth discussion of this approximation.</p><p>Going back to our definition of a convolution of a signal x with a filter g ? , we now have:</p><formula xml:id="formula_4">g ? x ? K k=0 ? k T k (L)x ,<label>(5)</label></formula><p>withL = 2 ?max L ? I N ; as can easily be verified by noticing that (U ?U ) k = U ? k U . Note that this expression is now K-localized since it is a K th -order polynomial in the Laplacian, i.e. it depends only on nodes that are at maximum K steps away from the central node (K th -order neighborhood). The complexity of evaluating Eq. 5 is O(|E|), i.e. linear in the number of edges. <ref type="bibr" target="#b6">Defferrard et al. (2016)</ref> use this K-localized convolution to define a convolutional neural network on graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">LAYER-WISE LINEAR MODEL</head><p>A neural network model based on graph convolutions can therefore be built by stacking multiple convolutional layers of the form of Eq. 5, each layer followed by a point-wise non-linearity. Now, imagine we limited the layer-wise convolution operation to K = 1 (see Eq. 5), i.e. a function that is linear w.r.t. L and therefore a linear function on the graph Laplacian spectrum.</p><p>In this way, we can still recover a rich class of convolutional filter functions by stacking multiple such layers, but we are not limited to the explicit parameterization given by, e.g., the Chebyshev polynomials. We intuitively expect that such a model can alleviate the problem of overfitting on local neighborhood structures for graphs with very wide node degree distributions, such as social networks, citation networks, knowledge graphs and many other real-world graph datasets. Additionally, for a fixed computational budget, this layer-wise linear formulation allows us to build deeper models, a practice that is known to improve modeling capacity on a number of domains <ref type="bibr" target="#b13">(He et al., 2016)</ref>.</p><p>In this linear formulation of a GCN we further approximate ? max ? 2, as we can expect that neural network parameters will adapt to this change in scale during training. Under these approximations Eq. 5 simplifies to:</p><formula xml:id="formula_5">g ? x ? ? 0 x + ? 1 (L ? I N ) x = ? 0 x ? ? 1 D ? 1 2 AD ? 1 2 x ,<label>(6)</label></formula><p>with two free parameters ? 0 and ? 1 . The filter parameters can be shared over the whole graph. Successive application of filters of this form then effectively convolve the k th -order neighborhood of a node, where k is the number of successive filtering operations or convolutional layers in the neural network model.</p><p>In practice, it can be beneficial to constrain the number of parameters further to address overfitting and to minimize the number of operations (such as matrix multiplications) per layer. This leaves us with the following expression:</p><formula xml:id="formula_6">g ? x ? ? I N + D ? 1 2 AD ? 1 2 x ,<label>(7)</label></formula><p>with a single parameter ? = ? 0 = ?? 1 . Note that I N + D ? 1 2 AD ? 1 2 now has eigenvalues in the range [0, 2]. Repeated application of this operator can therefore lead to numerical instabilities and exploding/vanishing gradients when used in a deep neural network model. To alleviate this problem, we introduce the following renormalization trick:</p><formula xml:id="formula_7">I N + D ? 1 2 AD ? 1 2 ?D ? 1 2?D ? 1 2 , with A = A + I N andD ii = j? ij .</formula><p>We can generalize this definition to a signal X ? R N ?C with C input channels (i.e. a C-dimensional feature vector for every node) and F filters or feature maps as follows:</p><formula xml:id="formula_8">Z =D ? 1 2?D ? 1 2 X? ,<label>(8)</label></formula><p>where ? ? R C?F is now a matrix of filter parameters and Z ? R N ?F is the convolved signal matrix. This filtering operation has complexity O(|E|F C), as?X can be efficiently implemented as a product of a sparse matrix with a dense matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SEMI-SUPERVISED NODE CLASSIFICATION</head><p>Having introduced a simple, yet flexible model f (X, A) for efficient information propagation on graphs, we can return to the problem of semi-supervised node classification. As outlined in the introduction, we can relax certain assumptions typically made in graph-based semi-supervised learning by conditioning our model f (X, A) both on the data X and on the adjacency matrix A of the underlying graph structure. We expect this setting to be especially powerful in scenarios where the adjacency matrix contains information not present in the data X, such as citation links between documents in a citation network or relations in a knowledge graph. The overall model, a multi-layer GCN for semi-supervised learning, is schematically depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">EXAMPLE</head><p>In the following, we consider a two-layer GCN for semi-supervised node classification on a graph with a symmetric adjacency matrix A (binary or weighted). We first calculate? =D ? 1 2?D ? 1 2 in a pre-processing step. Our forward model then takes the simple form: Here, W (0) ? R C?H is an input-to-hidden weight matrix for a hidden layer with H feature maps. W (1) ? R H?F is a hidden-to-output weight matrix. The softmax activation function, defined as softmax(</p><formula xml:id="formula_9">Z = f (X, A) = softmax ? ReLU ? XW (0) W (1) . (9) C input layer X 1 X 2 X 3 X 4 F output layer Z 1 Z 2 Z 3 Z 4 hidden layers Y 1 Y 4 1 (a) Graph Convolutional Network</formula><formula xml:id="formula_10">x i ) = 1 Z exp(x i ) with Z = i exp(x i ), is applied row-wise.</formula><p>For semi-supervised multiclass classification, we then evaluate the cross-entropy error over all labeled examples:</p><formula xml:id="formula_11">L = ? l?Y L F f =1 Y lf ln Z lf ,<label>(10)</label></formula><p>where Y L is the set of node indices that have labels.</p><p>The neural network weights W (0) and W (1) are trained using gradient descent. In this work, we perform batch gradient descent using the full dataset for every training iteration, which is a viable option as long as datasets fit in memory. Using a sparse representation for A, memory requirement is O(|E|), i.e. linear in the number of edges. Stochasticity in the training process is introduced via dropout <ref type="bibr" target="#b24">(Srivastava et al., 2014)</ref>. We leave memory-efficient extensions with mini-batch stochastic gradient descent for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">IMPLEMENTATION</head><p>In practice, we make use of TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref> for an efficient GPU-based implementation 2 of Eq. 9 using sparse-dense matrix multiplications. The computational complexity of evaluating Eq. 9 is then O(|E|CHF ), i.e. linear in the number of graph edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Our model draws inspiration both from the field of graph-based semi-supervised learning and from recent work on neural networks that operate on graphs. In what follows, we provide a brief overview on related work in both fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">GRAPH-BASED SEMI-SUPERVISED LEARNING</head><p>A large number of approaches for semi-supervised learning using graph representations have been proposed in recent years, most of which fall into two broad categories: methods that use some form of explicit graph Laplacian regularization and graph embedding-based approaches. Prominent examples for graph Laplacian regularization include label propagation <ref type="bibr" target="#b31">(Zhu et al., 2003)</ref>, manifold regularization <ref type="bibr" target="#b2">(Belkin et al., 2006)</ref> and deep semi-supervised embedding <ref type="bibr" target="#b27">(Weston et al., 2012)</ref>.</p><p>Recently, attention has shifted to models that learn graph embeddings with methods inspired by the skip-gram model <ref type="bibr" target="#b19">(Mikolov et al., 2013)</ref>. DeepWalk <ref type="bibr" target="#b21">(Perozzi et al., 2014)</ref> learns embeddings via the prediction of the local neighborhood of nodes, sampled from random walks on the graph. LINE <ref type="bibr" target="#b25">(Tang et al., 2015)</ref> and node2vec <ref type="bibr" target="#b11">(Grover &amp; Leskovec, 2016)</ref> extend DeepWalk with more sophisticated random walk or breadth-first search schemes. For all these methods, however, a multistep pipeline including random walk generation and semi-supervised training is required where each step has to be optimized separately. Planetoid <ref type="bibr" target="#b28">(Yang et al., 2016)</ref> alleviates this by injecting label information in the process of learning embeddings. A related approach to node classification with a graph-based neural network was recently introduced in <ref type="bibr" target="#b1">Atwood &amp; Towsley (2016)</ref>. They report O(N 2 ) complexity, limiting the range of possible applications. In a different yet related model, <ref type="bibr" target="#b20">Niepert et al. (2016)</ref> convert graphs locally into sequences that are fed into a conventional 1D convolutional neural network, which requires the definition of a node ordering in a pre-processing step.</p><p>Our method is based on spectral graph convolutional neural networks, introduced in Bruna et al.</p><p>(2014) and later extended by <ref type="bibr" target="#b6">Defferrard et al. (2016)</ref> with fast localized convolutions. In contrast to these works, we consider here the task of transductive node classification within networks of significantly larger scale. We show that in this setting, a number of simplifications (see Section 2.2) can be introduced to the original frameworks of <ref type="bibr" target="#b4">Bruna et al. (2014)</ref> and <ref type="bibr" target="#b6">Defferrard et al. (2016)</ref> that improve scalability and classification performance in large-scale networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We test our model in a number of experiments: semi-supervised document classification in citation networks, semi-supervised entity classification in a bipartite graph extracted from a knowledge graph, an evaluation of various graph propagation models and a run-time analysis on random graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DATASETS</head><p>We closely follow the experimental setup in <ref type="bibr" target="#b28">Yang et al. (2016)</ref>. Dataset statistics are summarized in <ref type="table" target="#tab_1">Table 1</ref>. In the citation network datasets-Citeseer, Cora and Pubmed <ref type="bibr" target="#b23">(Sen et al., 2008</ref>)-nodes are documents and edges are citation links. Label rate denotes the number of labeled nodes that are used for training divided by the total number of nodes in each dataset. NELL <ref type="bibr" target="#b5">(Carlson et al., 2010;</ref><ref type="bibr" target="#b28">Yang et al., 2016</ref>) is a bipartite graph dataset extracted from a knowledge graph with 55,864 relation nodes and 9,891 entity nodes. Citation networks We consider three citation network datasets: Citeseer, Cora and Pubmed <ref type="bibr" target="#b23">(Sen et al., 2008)</ref>. The datasets contain sparse bag-of-words feature vectors for each document and a list of citation links between documents. We treat the citation links as (undirected) edges and construct a binary, symmetric adjacency matrix A. Each document has a class label. For training, we only use 20 labels per class, but all feature vectors.</p><p>NELL NELL is a dataset extracted from the knowledge graph introduced in <ref type="bibr" target="#b5">(Carlson et al., 2010)</ref>. A knowledge graph is a set of entities connected with directed, labeled edges (relations). We follow the pre-processing scheme as described in <ref type="bibr" target="#b28">Yang et al. (2016)</ref>. We assign separate relation nodes r 1 and r 2 for each entity pair (e 1 , r, e 2 ) as (e 1 , r 1 ) and (e 2 , r 2 ). Entity nodes are described by sparse feature vectors. We extend the number of features in NELL by assigning a unique one-hot representation for every relation node, effectively resulting in a 61,278-dim sparse feature vector per node. The semi-supervised task here considers the extreme case of only a single labeled example per class in the training set. We construct a binary, symmetric adjacency matrix from this graph by setting entries A ij = 1, if one or more edges are present between nodes i and j.</p><p>Random graphs We simulate random graph datasets of various sizes for experiments where we measure training time per epoch. For a dataset with N nodes we create a random graph assigning 2N edges uniformly at random. We take the identity matrix I N as input feature matrix X, thereby implicitly taking a featureless approach where the model is only informed about the identity of each node, specified by a unique one-hot vector. We add dummy labels Y i = 1 for every node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">EXPERIMENTAL SET-UP</head><p>Unless otherwise noted, we train a two-layer GCN as described in Section 3.1 and evaluate prediction accuracy on a test set of 1,000 labeled examples. We provide additional experiments using deeper models with up to 10 layers in Appendix B. We choose the same dataset splits as in <ref type="bibr" target="#b28">Yang et al. (2016)</ref> with an additional validation set of 500 labeled examples for hyperparameter optimization (dropout rate for all layers, L2 regularization factor for the first GCN layer and number of hidden units). We do not use the validation set labels for training.</p><p>For the citation network datasets, we optimize hyperparameters on Cora only and use the same set of parameters for Citeseer and Pubmed. We train all models for a maximum of 200 epochs (training iterations) using Adam (Kingma &amp; Ba, 2015) with a learning rate of 0.01 and early stopping with a window size of 10, i.e. we stop training if the validation loss does not decrease for 10 consecutive epochs. We initialize weights using the initialization described in <ref type="bibr" target="#b9">Glorot &amp; Bengio (2010)</ref> and accordingly (row-)normalize input feature vectors. On the random graph datasets, we use a hidden layer size of 32 units and omit regularization (i.e. neither dropout nor L2 regularization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">BASELINES</head><p>We compare against the same baseline methods as in <ref type="bibr" target="#b28">Yang et al. (2016)</ref>, i.e. label propagation (LP) <ref type="bibr" target="#b31">(Zhu et al., 2003)</ref>, semi-supervised embedding (SemiEmb) <ref type="bibr" target="#b27">(Weston et al., 2012)</ref>, manifold regularization (ManiReg) <ref type="bibr" target="#b2">(Belkin et al., 2006)</ref> and skip-gram based graph embeddings (DeepWalk) <ref type="bibr" target="#b21">(Perozzi et al., 2014)</ref>. We omit TSVM <ref type="bibr" target="#b14">(Joachims, 1999)</ref>, as it does not scale to the large number of classes in one of our datasets.</p><p>We further compare against the iterative classification algorithm (ICA) proposed in <ref type="bibr" target="#b17">Lu &amp; Getoor (2003)</ref> in conjunction with two logistic regression classifiers, one for local node features alone and one for relational classification using local features and an aggregation operator as described in <ref type="bibr" target="#b23">Sen et al. (2008)</ref>. We first train the local classifier using all labeled training set nodes and use it to bootstrap class labels of unlabeled nodes for relational classifier training. We run iterative classification (relational classifier) with a random node ordering for 10 iterations on all unlabeled nodes (bootstrapped using the local classifier). L2 regularization parameter and aggregation operator (count vs. prop, see <ref type="bibr" target="#b23">Sen et al. (2008)</ref>) are chosen based on validation set performance for each dataset separately.</p><p>Lastly, we compare against Planetoid <ref type="bibr" target="#b28">(Yang et al., 2016)</ref>, where we always choose their bestperforming model variant (transductive vs. inductive) as a baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">SEMI-SUPERVISED NODE CLASSIFICATION</head><p>Results are summarized in <ref type="table" target="#tab_2">Table 2</ref>. Reported numbers denote classification accuracy in percent. For ICA, we report the mean accuracy of 100 runs with random node orderings. Results for all other baseline methods are taken from the Planetoid paper <ref type="bibr" target="#b28">(Yang et al., 2016)</ref>. Planetoid* denotes the best model for the respective dataset out of the variants presented in their paper. GCN (rand. splits) 67.9 ? 0.5 80.1 ? 0.5 78.9 ? 0.7 58.4 ? 1.7</p><p>We further report wall-clock training time in seconds until convergence (in brackets) for our method (incl. evaluation of validation error) and for Planetoid. For the latter, we used an implementation provided by the authors 3 and trained on the same hardware (with GPU) as our GCN model. We trained and tested our model on the same dataset splits as in <ref type="bibr" target="#b28">Yang et al. (2016)</ref> and report mean accuracy of 100 runs with random weight initializations. We used the following sets of hyperparameters for Citeseer, Cora and Pubmed: 0.5 (dropout rate), 5 ? 10 ?4 (L2 regularization) and 16 (number of hidden units); and for NELL: 0.1 (dropout rate), 1 ? 10 ?5 (L2 regularization) and 64 (number of hidden units).</p><p>In addition, we report performance of our model on 10 randomly drawn dataset splits of the same size as in <ref type="bibr" target="#b28">Yang et al. (2016)</ref>, denoted by GCN (rand. splits). Here, we report mean and standard error of prediction accuracy on the test set split in percent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">EVALUATION OF PROPAGATION MODEL</head><p>We compare different variants of our proposed per-layer propagation model on the citation network datasets. We follow the experimental set-up described in the previous section. Results are summarized in <ref type="table" target="#tab_3">Table 3</ref>. The propagation model of our original GCN model is denoted by renormalization trick (in bold). In all other cases, the propagation model of both neural network layers is replaced with the model specified under propagation model. Reported numbers denote mean classification accuracy for 100 repeated runs with random weight matrix initializations. In case of multiple variables ? i per layer, we impose L2 regularization on all weight matrices of the first layer.  Here, we report results for the mean training time per epoch (forward pass, cross-entropy calculation, backward pass) for 100 epochs on simulated random graphs, measured in seconds wall-clock time. See Section 5.1 for a detailed description of the random graph dataset used in these experiments. We compare results on a GPU and on a CPU-only implementation 4 in TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> summarizes the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">SEMI-SUPERVISED MODEL</head><p>In the experiments demonstrated here, our method for semi-supervised node classification outperforms recent related methods by a significant margin. Methods based on graph-Laplacian regularization <ref type="bibr" target="#b31">(Zhu et al., 2003;</ref><ref type="bibr" target="#b2">Belkin et al., 2006;</ref><ref type="bibr" target="#b27">Weston et al., 2012)</ref> are most likely limited due to their assumption that edges encode mere similarity of nodes. Skip-gram based methods on the other hand are limited by the fact that they are based on a multi-step pipeline which is difficult to optimize. Our proposed model can overcome both limitations, while still comparing favorably in terms of efficiency (measured in wall-clock time) to related methods. Propagation of feature information from neighboring nodes in every layer improves classification performance in comparison to methods like ICA <ref type="bibr" target="#b17">(Lu &amp; Getoor, 2003)</ref>, where only label information is aggregated.</p><p>We have further demonstrated that the proposed renormalized propagation model (Eq. 8) offers both improved efficiency (fewer parameters and operations, such as multiplication or addition) and better predictive performance on a number of datasets compared to a na?ve 1 st -order model (Eq. 6) or higher-order graph convolutional models using Chebyshev polynomials (Eq. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">LIMITATIONS AND FUTURE WORK</head><p>Here, we describe several limitations of our current model and outline how these might be overcome in future work.</p><p>Memory requirement In the current setup with full-batch gradient descent, memory requirement grows linearly in the size of the dataset. We have shown that for large graphs that do not fit in GPU memory, training on CPU can still be a viable option. Mini-batch stochastic gradient descent can alleviate this issue. The procedure of generating mini-batches, however, should take into account the number of layers in the GCN model, as the K th -order neighborhood for a GCN with K layers has to be stored in memory for an exact procedure. For very large and densely connected graph datasets, further approximations might be necessary.</p><p>Directed edges and edge features Our framework currently does not naturally support edge features and is limited to undirected graphs (weighted or unweighted). Results on NELL however show that it is possible to handle both directed edges and edge features by representing the original directed graph as an undirected bipartite graph with additional nodes that represent edges in the original graph (see Section 5.1 for details).</p><p>Limiting assumptions Through the approximations introduced in Section 2, we implicitly assume locality (dependence on the K th -order neighborhood for a GCN with K layers) and equal importance of self-connections vs. edges to neighboring nodes. For some datasets, however, it might be beneficial to introduce a trade-off parameter ? in the definition of?:</p><formula xml:id="formula_12">A = A + ?I N .<label>(11)</label></formula><p>This parameter now plays a similar role as the trade-off parameter between supervised and unsupervised loss in the typical semi-supervised setting (see Eq. 1). Here, however, it can be learned via gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We have introduced a novel approach for semi-supervised classification on graph-structured data. Our GCN model uses an efficient layer-wise propagation rule that is based on a first-order approximation of spectral convolutions on graphs. Experiments on a number of network datasets suggest that the proposed GCN model is capable of encoding both graph structure and node features in a way useful for semi-supervised classification. In this setting, our model outperforms several recently proposed methods by a significant margin, while being computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A RELATION TO WEISFEILER-LEHMAN ALGORITHM</head><p>A neural network model for graph-structured data should ideally be able to learn representations of nodes in a graph, taking both the graph structure and feature description of nodes into account. A well-studied framework for the unique assignment of node labels given a graph and (optionally) discrete initial node labels is provided by the 1-dim Weisfeiler-Lehman (WL-1) algorithm <ref type="bibr" target="#b26">(Weisfeiler &amp; Lehmann, 1968)</ref>:</p><p>Algorithm 1: WL-1 algorithm <ref type="bibr" target="#b26">(Weisfeiler &amp; Lehmann, 1968)</ref> Input: Initial node coloring (h</p><formula xml:id="formula_13">(0) 1 , h (0) 2 , ..., h (0) N ) Output: Final node coloring (h (T ) 1 , h (T ) 2 , ..., h (T ) N ) t ? 0; repeat for v i ? V do h (t+1) i ? hash j?Ni h (t) j ; t ? t + 1; until stable node coloring is reached;</formula><p>Here, h (t) i denotes the coloring (label assignment) of node v i (at iteration t) and N i is its set of neighboring node indices (irrespective of whether the graph includes self-connections for every node or not). hash(?) is a hash function. For an in-depth mathematical discussion of the WL-1 algorithm see, e.g., <ref type="bibr" target="#b7">Douglas (2011)</ref>.</p><p>We can replace the hash function in Algorithm 1 with a neural network layer-like differentiable function with trainable parameters as follows:</p><formula xml:id="formula_14">h (l+1) i = ? ? ? j?Ni 1 c ij h (l) j W (l) ? ? ,<label>(12)</label></formula><p>where c ij is an appropriately chosen normalization constant for the edge (v i , v j ). Further, we can take h (l) i now to be a vector of activations of node i in the l th neural network layer. W (l) is a layer-specific weight matrix and ?(?) denotes a differentiable, non-linear activation function.</p><p>By choosing c ij = d i d j , where d i = |N i | denotes the degree of node v i , we recover the propagation rule of our Graph Convolutional Network (GCN) model in vector form (see Eq. 2) 5 .</p><p>This-loosely speaking-allows us to interpret our GCN model as a differentiable and parameterized generalization of the 1-dim Weisfeiler-Lehman algorithm on graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 NODE EMBEDDINGS WITH RANDOM WEIGHTS</head><p>From the analogy with the Weisfeiler-Lehman algorithm, we can understand that even an untrained GCN model with random weights can serve as a powerful feature extractor for nodes in a graph. As an example, consider the following 3-layer GCN model:</p><formula xml:id="formula_15">Z = tanh ? tanh ? tanh ? XW (0) W (1) W (2) ,<label>(13)</label></formula><p>with weight matrices W (l) initialized at random using the initialization described in <ref type="bibr" target="#b9">Glorot &amp; Bengio (2010)</ref>.?, X and Z are defined as in Section 3.1.</p><p>We apply this model on Zachary's karate club network <ref type="bibr" target="#b29">(Zachary, 1977)</ref>. This graph contains 34 nodes, connected by 154 (undirected and unweighted) edges. Every node is labeled by one of four classes, obtained via modularity-based clustering <ref type="bibr" target="#b3">(Brandes et al., 2008)</ref>. See <ref type="figure">Figure 3a</ref> for an illustration.</p><p>(a) Karate club network (b) Random weight embedding <ref type="figure">Figure 3</ref>: Left: Zachary's karate club network <ref type="bibr" target="#b29">(Zachary, 1977)</ref>, colors denote communities obtained via modularity-based clustering <ref type="bibr" target="#b3">(Brandes et al., 2008)</ref>. Right: Embeddings obtained from an untrained 3-layer GCN model (Eq. 13) with random weights applied to the karate club network. Best viewed on a computer screen.</p><p>We take a featureless approach by setting X = I N , where I N is the N by N identity matrix. N is the number of nodes in the graph. Note that nodes are randomly ordered (i.e. ordering contains no information). Furthermore, we choose a hidden layer dimensionality 6 of 4 and a two-dimensional output (so that the output can immediately be visualized in a 2-dim plot). <ref type="figure">Figure 3b</ref> shows a representative example of node embeddings (outputs Z) obtained from an untrained GCN model applied to the karate club network. These results are comparable to embeddings obtained from DeepWalk <ref type="bibr" target="#b21">(Perozzi et al., 2014)</ref>, which uses a more expensive unsupervised training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 SEMI-SUPERVISED NODE EMBEDDINGS</head><p>On this simple example of a GCN applied to the karate club network it is interesting to observe how embeddings react during training on a semi-supervised classification task. Such a visualization (see <ref type="figure" target="#fig_2">Figure 4</ref>) provides insights into how the GCN model can make use of the graph structure (and of features extracted from the graph structure at later layers) to learn embeddings that are useful for a classification task.</p><p>We consider the following semi-supervised learning setup: we add a softmax layer on top of our model (Eq. 13) and train using only a single labeled example per class (i.e. a total number of 4 labeled nodes). We train for 300 training iterations using Adam (Kingma &amp; Ba, 2015) with a learning rate of 0.01 on a cross-entropy loss. <ref type="figure" target="#fig_2">Figure 4</ref> shows the evolution of node embeddings over a number of training iterations. The model succeeds in linearly separating the communities based on minimal supervision and the graph structure alone. A video of the full training process can be found on our website 7 . <ref type="bibr">6</ref> We originally experimented with a hidden layer dimensionality of 2 (i.e. same as output layer), but observed that a dimensionality of 4 resulted in less frequent saturation of tanh(?) units and therefore visually more pleasing results. 7 http://tkipf.github.io/graph-convolutional-networks/ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EXPERIMENTS ON MODEL DEPTH</head><p>In these experiments, we investigate the influence of model depth (number of layers) on classification performance. We report results on a 5-fold cross-validation experiment on the Cora, Citeseer and Pubmed datasets <ref type="bibr" target="#b23">(Sen et al., 2008)</ref> using all labels. In addition to the standard GCN model (Eq. 2), we report results on a model variant where we use residual connections <ref type="bibr" target="#b13">(He et al., 2016)</ref> between hidden layers to facilitate training of deeper models by enabling the model to carry over information from the previous layer's input:</p><formula xml:id="formula_16">H (l+1) = ? D ? 1 2?D ? 1 2 H (l) W (l) + H (l) .<label>(14)</label></formula><p>On each cross-validation split, we train for 400 epochs (without early stopping) using the Adam optimizer (Kingma &amp; Ba, 2015) with a learning rate of 0.01. Other hyperparameters are chosen as follows: 0.5 (dropout rate, first and last layer), 5 ? 10 ?4 (L2 regularization, first layer), 16 (number of units for each hidden layer) and 0.01 (learning rate). Results are summarized in <ref type="figure" target="#fig_3">Figure 5</ref>. For the datasets considered here, best results are obtained with a 2-or 3-layer model. We observe that for models deeper than 7 layers, training without the use of residual connections can become difficult, as the effective context size for each node increases by the size of its K th -order neighborhood (for a model with K layers) with each additional layer. Furthermore, overfitting can become an issue as the number of parameters increases with model depth.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Left: Schematic depiction of multi-layer Graph Convolutional Network (GCN) for semisupervised learning with C input channels and F feature maps in the output layer. The graph structure (edges shown as black lines) is shared over layers, labels are denoted by Y i . Right: t-SNE (Maaten &amp; Hinton, 2008) visualization of hidden layer activations of a two-layer GCN trained on the Cora dataset<ref type="bibr" target="#b23">(Sen et al., 2008)</ref> using 5% of labels. Colors denote document class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Wall-clock time per epoch for random graphs. (*) indicates out-of-memory error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Evolution of karate club network node embeddings obtained from a GCN model after a number of semi-supervised training iterations. Colors denote class. Nodes of which labels were provided during training (one per class) are highlighted (grey outline). Grey links between nodes denote graph edges. Best viewed on a computer screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Influence of model depth (number of layers) on classification performance. Markers denote mean classification accuracy (training vs. testing) for 5-fold cross-validation. Shaded areas denote standard error. We show results both for a standard GCN model (dashed lines) and a model with added residual connections<ref type="bibr" target="#b13">(He et al., 2016)</ref> between hidden layers (solid lines).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>4.2 NEURAL NETWORKS ON GRAPHSNeural networks that operate on graphs have previously been introduced in<ref type="bibr" target="#b10">Gori et al. (2005)</ref>;<ref type="bibr" target="#b22">Scarselli et al. (2009)</ref> as a form of recurrent neural network. Their framework requires the repeated application of contraction maps as propagation functions until node representations reach a stable fixed point. This restriction was later alleviated in<ref type="bibr" target="#b16">Li et al. (2016)</ref> by introducing modern practices for recurrent neural network training to the original graph neural network framework.<ref type="bibr" target="#b8">Duvenaud et al. (2015)</ref> introduced a convolution-like propagation rule on graphs and methods for graph-level classification. Their approach requires to learn node degree-specific weight matrices which does not scale to large graphs with wide node degree distributions. Our model instead uses a single weight matrix per layer and deals with varying node degrees through an appropriate normalization of the adjacency matrix (see Section 3.1).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics, as reported in<ref type="bibr" target="#b28">Yang et al. (2016)</ref>.</figDesc><table><row><cell>Dataset</cell><cell>Type</cell><cell>Nodes</cell><cell cols="4">Edges Classes Features Label rate</cell></row><row><cell cols="2">Citeseer Citation network</cell><cell>3,327</cell><cell>4,732</cell><cell>6</cell><cell>3,703</cell><cell>0.036</cell></row><row><cell>Cora</cell><cell>Citation network</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>1,433</cell><cell>0.052</cell></row><row><cell cols="3">Pubmed Citation network 19,717</cell><cell>44,338</cell><cell>3</cell><cell>500</cell><cell>0.003</cell></row><row><cell>NELL</cell><cell cols="3">Knowledge graph 65,755 266,144</cell><cell>210</cell><cell>5,414</cell><cell>0.001</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary of results in terms of classification accuracy (in percent).</figDesc><table><row><cell>Method</cell><cell>Citeseer</cell><cell>Cora</cell><cell>Pubmed</cell><cell>NELL</cell></row><row><cell>ManiReg [3]</cell><cell>60.1</cell><cell>59.5</cell><cell>70.7</cell><cell>21.8</cell></row><row><cell>SemiEmb [28]</cell><cell>59.6</cell><cell>59.0</cell><cell>71.1</cell><cell>26.7</cell></row><row><cell>LP [32]</cell><cell>45.3</cell><cell>68.0</cell><cell>63.0</cell><cell>26.5</cell></row><row><cell>DeepWalk [22]</cell><cell>43.2</cell><cell>67.2</cell><cell>65.3</cell><cell>58.1</cell></row><row><cell>ICA [18]</cell><cell>69.1</cell><cell>75.1</cell><cell>73.9</cell><cell>23.1</cell></row><row><cell>Planetoid* [29]</cell><cell cols="4">64.7 (26s) 75.7 (13s) 77.2 (25s) 61.9 (185s)</cell></row><row><cell>GCN (this paper)</cell><cell>70.3 (7s)</cell><cell>81.5 (4s)</cell><cell cols="2">79.0 (38s) 66.0 (48s)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of propagation models.</figDesc><table><row><cell>Description</cell><cell></cell><cell>Propagation model</cell><cell cols="3">Citeseer Cora Pubmed</cell></row><row><cell>Chebyshev filter (Eq. 5)</cell><cell>K = 3 K = 2</cell><cell>K k=0 T k (L)X? k</cell><cell>69.8 69.6</cell><cell>79.5 81.2</cell><cell>74.4 73.8</cell></row><row><cell>1 st -order model (Eq. 6) Single parameter (Eq. 7)</cell><cell></cell><cell>X? 0 + D ? 1 2 AD ? 1 2 X? 1 (I N + D ? 1 2 AD ? 1 2 )X?</cell><cell>68.3 69.3</cell><cell>80.0 79.2</cell><cell>77.5 77.4</cell></row><row><cell cols="3">Renormalization trick (Eq. 8)D ? 1 2?D ? 1 2 X?</cell><cell cols="2">70.3 81.5</cell><cell>79.0</cell></row><row><cell>1 st -order term only</cell><cell></cell><cell>D ? 1 2 AD ? 1 2 X?</cell><cell>68.7</cell><cell>80.5</cell><cell>77.8</cell></row><row><cell>Multi-layer perceptron</cell><cell></cell><cell>X?</cell><cell>46.5</cell><cell>55.1</cell><cell>71.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Published as a conference paper at ICLR 2017</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We provide an alternative interpretation of this propagation rule based on the Weisfeiler-Lehman algorithm<ref type="bibr" target="#b26">(Weisfeiler &amp; Lehmann, 1968)</ref> in Appendix A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Code to reproduce our experiments is available at https://github.com/tkipf/gcn.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/kimiyoung/planetoid</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Hardware used: 16-core Intel R Xeon R CPU E5-2640 v3 @ 2.60GHz, GeForce R GTX TITAN X</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Note that we here implicitly assume that self-connections have already been added to every node in the graph (for a clutter-free notation).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Christos Louizos, Taco Cohen, Joan Bruna, Zhilin Yang, Dave Herman, Pramod Sinha and Abdul-Saboor Sheikh for helpful discussions. This research was funded by SAP.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On modularity clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrik</forename><surname>Brandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Delling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gaertler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gorke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoran</forename><surname>Nikoloski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorothea</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="172" to="188" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Estevam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">L</forename><surname>Douglas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1101.5211</idno>
		<title level="m">The Weisfeiler-Lehman method and graph isomorphism testing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AISTATS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Lehmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
	<note type="report_type">Nauchno-Technicheskaya Informatsia</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning via semisupervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An information flow model for conflict and fission in small groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">W</forename><surname>Zachary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of anthropological research</title>
		<imprint>
			<biblScope unit="page" from="452" to="473" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Thomas Navin Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
