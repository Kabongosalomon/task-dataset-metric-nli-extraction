<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DaViT: Dual Attention Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
							<email>mingyuding@hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>2?</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cloud + AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Codella</surname></persName>
							<email>ncodella@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cloud + AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><forename type="middle">Luo</forename><surname>1?</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
							<email>wangjingdong@outlook.com</email>
							<affiliation key="aff2">
								<address>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
							<email>luyuan@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Cloud + AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DaViT: Dual Attention Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we introduce Dual Attention Vision Transformers (DaViT), a simple yet effective vision transformer architecture that is able to capture global context while maintaining computational efficiency. We propose approaching the problem from an orthogonal angle: exploiting self-attention mechanisms with both "spatial tokens" and "channel tokens". With spatial tokens, the spatial dimension defines the token scope, and the channel dimension defines the token feature dimension. With channel tokens, we have the inverse: the channel dimension defines the token scope, and the spatial dimension defines the token feature dimension. We further group tokens along the sequence direction for both spatial and channel tokens to maintain the linear complexity of the entire model. We show that these two self-attentions complement each other: (i) since each channel token contains an abstract representation of the entire image, the channel attention naturally captures global interactions and representations by taking all spatial positions into account when computing attention scores between channels; (ii) the spatial attention refines the local representations by performing fine-grained interactions across spatial locations, which in turn helps the global information modeling in channel attention. Extensive experiments show our DaViT achieves state-of-the-art performance on four different tasks with efficient computations. Without extra data, DaViT-Tiny, DaViT-Small, and DaViT-Base achieve 82.8%, 84.2%, and 84.6% top-1 accuracy on ImageNet-1K with 28.3M, 49.7M, and 87.9M parameters, respectively. When we further scale up DaViT with 1.5B weakly supervised image and text pairs, DaViT-Gaint reaches 90.4% top-1 accuracy on ImageNet-1K. Code is available at https://github.com/dingmyu/davit.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The global context is essential for many computer vision approaches, such as image classification and semantic segmentation. Convolutional neural networks (CNNs) <ref type="bibr" target="#b40">[41]</ref> gradually obtain a global receptive field by multi-layer architectures and down-sampling operators. Recently, vision transformers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref>, which directly capture long-range visual dependencies with a single self-attention layer, have drawn much attention. While these methods present strong capabilities to model the global context, their computational complexity grows quadratically with the token length, limiting its ability to scale up to high-resolution scenarios.  <ref type="figure">Fig. 1</ref>. (a) Spatial window multihead self-attention splits the spatial dimension into local windows, where each window contains multiple spatial tokens. Each token is also divided into multiple heads. (b) Channel group single-head self-attention groups channel tokens into multi groups. Attention is performed in each channel group with an entire image-level channel as a token. A channel-wise token that captures global information is also highlighted in (a). In this work, we alternately use these two types of attention to obtain both local fine-grained and global features.</p><p>Designing an architecture that can capture global contexts while maintaining efficiency to learn from high-resolution inputs is still an open research problem. A substantial body of work has been dedicated to developing vision transformers toward this goal. iGPT <ref type="bibr" target="#b6">[7]</ref> first utilized a standard transformer to solve vision tasks by treating the image as sequences of pixels and performing pixellevel interactions. After that, ViT <ref type="bibr" target="#b17">[18]</ref> used non-overlapped image patches as tokens to model the relationship between small image patches instead of pixels, showing promising performance on middle-resolution tasks such as classification. To further reduce the computational cost, local attention <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b80">82,</ref><ref type="bibr" target="#b57">58]</ref> that limits attention in a spatially local window, and squeezed projection <ref type="bibr" target="#b63">[65,</ref><ref type="bibr" target="#b61">62]</ref> that performs attention on downsampled tokens, were proposed. Though local attention methods benefit from linear complexity with the spatial size, operators like "Shift" <ref type="bibr" target="#b39">[40]</ref>, "Overlapping Patch" <ref type="bibr" target="#b63">[65,</ref><ref type="bibr" target="#b61">62]</ref>, "ConvFFN" <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b77">79,</ref><ref type="bibr" target="#b66">68]</ref> are indispensable to compensate for the loss of global contextual information.</p><p>The general pattern across all prior works is that they attain various tradeoffs between resolution, global context, and computational complexity: pixel-level <ref type="bibr" target="#b6">[7]</ref> and patch-level <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b63">65,</ref><ref type="bibr" target="#b62">63]</ref> self-attentions suffer either the cost of quadratic computational overhead or loss of global contextual information. Beyond variations of pixel-level and patch-level self-attentions, can we design an image-level self-attention mechanism that captures global information but is still efficient concerning the spatial size?</p><p>In this work, we introduce such a self-attention mechanism that is able to capture global context while maintaining computational efficiency. In addition to "spatial tokens" defined by existing works in <ref type="figure">Fig 1(a)</ref> representing the feature of an image patch, we introduce "channel tokens" by applying self-attention to the transpose of the token matrix, as shown in <ref type="figure">Fig 1(b)</ref>. With channel tokens, the channel dimension defines the token scope, and the spatial dimension defines the token feature dimension. In this way, each channel token is global on the spatial dimension, containing an abstract representation of the entire image. Correspondingly, performing self-attention on such channel tokens further captures the global interaction by taking all spatial positions into account when 4 6 8 10 12 <ref type="bibr">14 16</ref> FLOPs (G) <ref type="bibr" target="#b79">81</ref>  <ref type="bibr" target="#b80">82</ref> 83 <ref type="bibr">84 85</ref> Acc (%)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Classification</head><p>Swin-Tiny/Small/Base Twins-SVT-Small/Base/Large Focal-Tiny/Small/Base Dual-Tiny/Small/Base <ref type="bibr">(Ours)</ref> 900 1000 1100 1200 1300 FLOPs (G) <ref type="bibr" target="#b43">44</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance Segmentation</head><p>Swin-Tiny/Small/Base Twins-SVT-Small/Base Focal-Tiny/Small/Base Dual-Tiny/Small/Base(Ours) <ref type="figure">Fig. 2</ref>. Comparisons of the efficiency (i.e., FLOPs) and performance (e.g., Acc, mIoU, mAP) between the proposed approach and existing SoTA methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b70">72,</ref><ref type="bibr" target="#b9">10]</ref> on four computer vision tasks. Each method is represented by a circle, whose size represents the number of parameters. Our approach achieves superior performance under similar FLOPs than its counterparts on all four benchmarks.</p><p>computing attention scores between channels. Compared to conventional selfattention that performs global interactions over local pixels or patch tokens at a quadratic computational cost, the information exchange of channel self-attention is naturally imposed from a global perspective rather than a pixel/patch-wise one. Based on the global receptive field of the channel token, it fuses the representations to produce new global tokens and passes the information to the following layers. Thus, one can take such channel self-attention as a dynamic feature fusion over a series of abstract representations of the entire image.</p><p>Although this approach presents many advantages, a few challenges must be overcome. First, the computational complexity suddenly increases quadratically with the channel dimension, limiting the representation power of the layers. Inspired by spatial local attention <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b80">82,</ref><ref type="bibr" target="#b57">58]</ref>, we propose channel group attention by dividing the feature channels into several groups and performing image-level interactions within each group. By group attention, we reduce the complexity to linear with respect to both the spatial and the channel dimensions. Since each channel token contains an abstract representation of the entire image, selfattention in this setting naturally captures the global interaction even we apply it locally along the channel dimensions.</p><p>Second, though channel-wise self-attention can capture global information easily, image-level tokens hinder local interactions across spatial locations. To solve this problem, we introduce Dual Attention Vision Transformers (DaViT) that alternately applies spatial window attention and channel group attention to capture both short-range and long-range visual dependencies, as shown in Figure 1. Our results show that these two structures complement each other: the channel group attention provides a global receptive field on the spatial dimension and extracts high-level global-image representations by dynamic feature fusion across global channel tokens; the spatial window attention refines the local representations by performing fine-grained local interactions across spatial locations, which in turn helps the global information modeling in channel attention.</p><p>To summarize, we propose DaViT that contains two seamlessly integrated self-attentions: spatial window self-attention with "spatial tokens", and channel group self-attention with "channel tokens". The two forms of attention are complementary and alternatively arranged, providing both local fine-grained and global interactions in a computationally efficient manner. We evaluate the effectiveness of the proposed DaViT via comprehensive empirical studies on image classification, object detection, and segmentation. Results in <ref type="figure">Figure 2</ref> show that DaViT consistently outperforms the SoTA vision transformers across three benchmarks and four tasks with even fewer computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Vision Transformers and MLPs. Transformers <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b14">15]</ref> have dominated a wide range of natural language processing tasks. In computer vision, pioneering works iGPT <ref type="bibr" target="#b6">[7]</ref> and ViT <ref type="bibr" target="#b17">[18]</ref> apply attention directly to a sequence of image pixels or patches. Similarly, follow-up works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b64">66,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b78">80,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref> model global relationships on the patch-level tokens. All these works apply attention to capture interactions over all the local tokens, which can be pixel-level or patch-level. In this work, we model the global relationship from an orthogonal perspective and apply attention mechanisms to both spatial tokens as well as their transpose, which we refer to as image-level (global) channel tokens. In this manner, we capture both fine-grained structural patterns and global interactions.</p><p>Most recently, MLP-based models <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b72">74,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b26">27]</ref> draw attention by using multi-layer perceptrons (MLPs) directly over patch features. MLP-Mixer <ref type="bibr" target="#b52">[53]</ref> leverages feature transpose by applying MLPs across either spatial locations or feature channels repeatedly. However, it resorts to neither transformer architectures nor self-attention layers, and works only on the image classification task. Our work focuses on transformer-based architectures to benefit various tasks, e.g., classification, objection detection, and segmentation. Hierarchical Vision Transformers. Hierarchical designs are widely adopted to transformers <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">65,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b80">82,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b74">76,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b82">84,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b71">73,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b69">71,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b83">85]</ref> in vision. PVT <ref type="bibr" target="#b62">[63]</ref> and CvT <ref type="bibr" target="#b63">[65]</ref> perform attention on the squeezed tokens to reduce the computational cost. Swin Transformer <ref type="bibr" target="#b39">[40]</ref>, ViL <ref type="bibr" target="#b80">[82]</ref>, and HaloNet <ref type="bibr" target="#b57">[58]</ref> apply local windows attention to the patch tokens, which capture fine-grained features and reduce the quadratic complexity to linear, but lose the ability of global modeling. To compensate for the loss of global context, Swin Transformer <ref type="bibr" target="#b39">[40]</ref> conducts attention on the shifted local windows alternatively between consecutive blocks, and ViL <ref type="bibr" target="#b80">[82]</ref> and HaloNet <ref type="bibr" target="#b57">[58]</ref> play on overlapped windows.</p><p>In this work, the proposed approach shares merits of hierarchical architectures and fine-grained local attention, meanwhile our proposed group channel attention still efficiently models the global context. Channel-wise Attentions. The author of <ref type="bibr" target="#b27">[28]</ref> first proposes a Squeeze-and-Excitation (SE) block as channel-wise attention to re-calibrating the channelwise features through the squeezed global feature. Other operators in CNNs related to our work are Dynamic Head <ref type="bibr" target="#b11">[12]</ref> and DANet <ref type="bibr" target="#b18">[19]</ref>. They apply attention along different feature dimensions on top of the CNN backbone for a specific task. Some transformer architectures involve channel-wise operations as well to reduce the computational costs. LambdaNetworks <ref type="bibr" target="#b1">[2]</ref> first transforms the context into a linear function lambda that is applied to the corresponding query. XCiT <ref type="bibr" target="#b0">[1]</ref> proposes cross-covariance attention (XCA) for efficient processing of high-resolution images. Similarly, CoaT <ref type="bibr" target="#b68">[70]</ref> introduces a factorized attention mechanism that works efficiently in a multi-branch transformer backbone. Our work proposes channel group attention to capture global information in transformers. Though it has only linear complexity with respect to both channel and spatial dimensions, we demonstrate its power when combined with spatial window attention, forming our dual attention mechanism. Furthermore, we analyze in detail how our dual attention obtains global interactions as well as fine-grained local features, showing its effectiveness in benefiting various tasks, e.g., classification, objection detection, and segmentation.</p><formula xml:id="formula_0">Norm &amp; FFN Spatial Window Self-attention Add Norm &amp; FFN Channel Group Self-attention Add Projection Q K T V P ? C N w ? N h ? P w ? C h ??? P w ? C h ??? C h ? P w ??? P w ? C h ??? P w ? P w ??? P w ? C h Softmax Reshape Layer Norm ? ? Projection Projection Q K T V P ? C ??? C g ? P ??? P ? C g ??? C g ? P ??? C g ? C g ??? C g ? P Softmax Transpose &amp; Reshape Layer Norm ? ? Projection ? Elementwise Sum</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We propose Dual Attention Vision Transformers (DaViT), a clean, efficient, yet effective transformer backbone containing both local fine-grained features and global representations. In this section, we first introduce the hierarchical layout of our model. We then detail our channel group attention and the combination with spatial window attention <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>We divide the model into four stages, where a patch embedding layer is inserted at the beginning of each stage. We stack our dual attention blocks in each stage with the resolution and feature dimension kept the same. <ref type="figure" target="#fig_1">Figure 3</ref>(a) illustrates the architecture of our dual attention block, consisting of a spatial window attention block and a channel group attention block.</p><p>Preliminaries. Let us assume a R P ?C dimensional visual feature, where P is the number of total patches and C is the number of total channels. Simply applying the standard global self-attention leads to a complexity of O(2P 2 C + 4P C 2 ). It is defined as:</p><formula xml:id="formula_1">A(Q, K, V) = Concat(head 1 , . . . , head N h ) where head i = Attention(Q i , K i , V i ) = softmax Q i (K i ) T ? C h V i (1) where Q i = X i W Q i , K i = X i W K i , and V i = X i W V i are R P ?C h dimensional visual features with N h</formula><p>heads, X i denotes the i th head of the input feature and W i denotes the projection weights of the i th head for Q, K, V, and C = C h * N h . Please note that the output projection W O is omitted here. Considering P can be very large, e.g., 128 ? 128, the computational cost is immoderate.</p><p>In DaViT, we alternatively arrange spatial window attention and channel group attention to obtain both local and global features, but with a linear complexity to the spatial dimension, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatial Window Attention</head><p>Window attention computes self-attention within local windows, as shown in <ref type="figure">Figure 1(a)</ref>. The windows are arranged to partition the image in a non-overlapping manner evenly. Supposing there are N w different windows with each window containing P w patches, where P = P w * N w . Then window attention can be represented by:</p><formula xml:id="formula_2">A window (Q, K, V) = {A(Q i , K i , V i )} Nw i=0 (2) where Q i , K i , V i ? R Pw?C h</formula><p>are local window queries, keys, and values. The computational complexity of a window-based self-attention is O(2P P w C+4P C 2 ) with a linear complexity with the spatial size P . More details of window attention are shown in <ref type="figure" target="#fig_1">Figure 3</ref></p><formula xml:id="formula_3">(b).</formula><p>Though the computation is reduced, window attention loses the ability to model the global information. We will show that our proposed channel attention naturally solves this problem and mutually benefits with window attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Channel Group Attention</head><p>We visit self-attention from another perspective and propose channel-wise attention, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>(c). Previous self-attentions <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b63">65,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b79">81,</ref><ref type="bibr" target="#b6">7]</ref> in vision define tokens with pixels or patches, and gather the information along spatial dimensions. Instead of performing attention on pixel-level or patch-level, we apply attention mechanisms on the transpose of patch-level tokens. To obtain global information in the spatial dimension, we set the number of heads equal to 1. We argue that each transposed token abstracts the global information. In this way, channel tokens interact with global information on the channel dimension in linear spatial-wise complexity, as shown in <ref type="figure">Figure 1</ref></p><formula xml:id="formula_4">(b).</formula><p>Simply transposing the feature can obtain a vanilla channel-level attention with a complexity of O(6P C 2 ). To further reduce the computational complexity, we group channels into multiple groups and perform self-attention within each group. Formally, let N g denotes the number of groups and C g denotes the number of channels in each group, we have C = N g * C g . In this way, our channel group attention is global, with image-level tokens interacting across a group of channels. It is defined as:</p><formula xml:id="formula_5">A channel (Q, K, V) = {A group (Q i , K i , V i ) T } Ng i=0 A group (Q i , K i , V i ) = softmax Q T i K i C g V T i (3) where Q i , K i , V i ? R P ?Cg are grouped channel-wise image-level queries, keys,</formula><p>and values. Note that although we transpose the tokens in channel attention, the projection layers W and the scaling factor 1 ? Cg remain performed and computed along the channel dimension, rather than the spatial one. Considering that the number of spatial patches varies with the image size, the above design ensures our model can generalize to any image size. Complexity analysis. Our channel group attention is performed on the imagelevel tokens across the channel dimension. Compared to window attention that produces an attention map with size P w ? P w , the channel-wise attention map is of C g ? C g -dimensional. The overall computational complexity of our model includes O(2P C(P w + C g )) for window and channel attentions, O(8P C 2 ) for linear projections, and O(16P C 2 ) for FFNs (expand ratio is 4). It can be seen that our dual attention is computationally efficient with linear complexity to both the spatial and channel dimensions. FFN dominates the number of FLOPs and model parameters. Considering our dual attention has both channel-wise and spatial-wise interactions, in this work, we conduct an initial exploration to show the potential of the pure-attention structure without FFNs. Details can be found in Appendix. Global interactions in channel attention. Channel attention naturally captures global information and interactions for visual recognition tasks. (i) After transposing the feature, each channel token itself is global on the spatial dimension, providing a global view of the image. (ii) Given C g tokens with dimension P , the C g ? C g -dimensional attention map is computed by involving all spatial locations, i.e., (C g ? P ) ? (P ? C g ). (iii) With such a global attentive map, channel attention fuses multiple global views of the image dynamically, producing new global tokens and passing the information to the following spatial-wise layers. Compared to spatial-wise global attentions <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b17">18]</ref> that perform interactions across spatial locations, the information exchange of our channel self-attention is imposed from a global perspective rather than a patch-wise one, complementing the spatial window attention. Detailed analysis can be found in Sec. 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Instantiation</head><p>In this work, we follow the design strategy suggested by previous works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b70">72]</ref>.</p><formula xml:id="formula_6">Take an image with H ? W , a C-dimensional feature with a resolution of H 4 ? W 4</formula><p>is obtained after the first patch embedding layer. And its resolution is further</p><formula xml:id="formula_7">reduced into H 8 ? W 8 , H 16 ? W 16 , and H 32 ? W 32</formula><p>with the feature dimension increasing to 2C, 4C, and 8C after the other three patch embedding layer, respectively. Here, our patch embedding layer is implemented by stride convolution. The convolutional kernels and stride values of our four patch embedding layers are {7, 2, 2, 2} and {4, 2, 2, 2}, respectively.</p><p>We consider three different network configurations for image classification, objection detection, and segmentation:</p><formula xml:id="formula_8">-DaViT-Tiny: C = 96, L = {1, 1, 3, 1}, Ng = N h = {3, 6, 12, 24} -DaViT-Small: C = 96, L = {1, 1, 9, 1}, Ng = N h = {3, 6, 12, 24} -DaViT-Base: C = 128, L = {1, 1, 9, 1}, Ng = N h = {4, 8, 16, 32},</formula><p>where L is the layer numbers, N g is the number of groups in channel attention, and N h is the number of heads in window attention for each stage.</p><p>When more training data involved, we further scale up DaViT to large, huge, and giant size to validate the scaling ability of the proposed architecture for image classification: See Appendix for more details of model configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>Interpretation of global interactions. Global interactions in transformers can be summarized into different types. Vanilla ViT <ref type="bibr" target="#b17">[18]</ref> and DeiT <ref type="bibr" target="#b54">[55]</ref> perform information exchange between different patches among the whole image; Focal Transformer <ref type="bibr" target="#b70">[72]</ref> proposes interactions between tokens with different scales to get a larger receptive field; PVT [62] leverages a spatial reduction mechanism to obtain a coarse approximation of global attention; Swin <ref type="bibr" target="#b39">[40]</ref> stacks multiple hierarchical layers to get global information eventually. Unlike them, a single block with our channel attention is able to learn the global interactions from another perspective by taking all spatial positions into account when computing the attention scores, as in (C g ? P ) ? (P ? C g ). It captures the information from multiple global tokens, which represent different abstract views of the entire image. For example, different channels may contain information from different parts of an object; such part information can be aggregated into a global view. <ref type="figure" target="#fig_2">Figure 4</ref> illustrates how our channel attention works by visualizing the featuremaps before and after the attention. For each image in the first column, we randomly choose an output channel (the second column) in the third stage of the network and its corresponding top-7 relevant input channels for visualization. We can see that the channel attention fuses information from multiple tokens, selects globally important regions, and suppresses unimportant regions.</p><p>Relation to Swin and DeiT. To show the effectiveness of our dual selfattention, we make detailed comparisons with two representative clean baselines: Swin <ref type="bibr" target="#b39">[40]</ref> with fine-grained local features, and DeiT <ref type="bibr" target="#b54">[55]</ref> with global coarse features. Note that though our work and Swin both use window attention as a network element, the key design of Swin, i.e., the use of "shifted window partitions" between successive layers to increase the receptive field, is not utilized in our work. We also simplifies its relative position encoding <ref type="bibr" target="#b10">[11]</ref> with a depth-wise convolution before layernorm to get a cleaner structure for arbitrary input sizes. To further keep our architecture clean and efficient, we do not use additional op- <ref type="table">Table 1</ref>. Comparison of image classification on ImageNet-1K for different models. All models are trained and evaluated with 224?224 resolution on ImageNet-1K by default, unless otherwise noted. For fair comparison, token labeling <ref type="bibr" target="#b29">[30]</ref> and distillation <ref type="bibr" target="#b54">[55]</ref> are not used for all models and their counterparts. ? and ? denote the model is evaluated with resolution of 384 ? 384 and 512 ? 512, respectively. erators like "Overlapping Patch" <ref type="bibr" target="#b63">[65,</ref><ref type="bibr" target="#b61">62]</ref> and "ConvFFN" <ref type="bibr" target="#b77">[79,</ref><ref type="bibr" target="#b66">68]</ref>. See Appendix for detailed throughput of our model compared with Swin. <ref type="figure">Figure 5</ref> shows the effectiveness of our channel group attention. We randomly visualize a feature channel in each stage of DaViT, Swin <ref type="bibr" target="#b39">[40]</ref>, and DeiT <ref type="bibr" target="#b54">[55]</ref>. We observe that: (i) Swin captures fine-grained details but no focus in the first two stages as it lacks global information. It can not focus on the main object until the last stage. (ii) DeiT learns coarse-grained global features over the image but loses details hence difficult to focus on the main content. (iii) Our DaViT captures both short-range and long-range visual dependencies by combining two types of self-attention. It shows strong global modeling capabilities by finding out fine-grained details of the main content in stage 1, and further focusing on some keypoints in stage 2. It then gradually refines the regions of interest from both global and local perspectives for final recognition. Relation to channel-wise attention in CNNs. Traditional channel-wise attentions blocks, e.g., SENet <ref type="bibr" target="#b27">[28]</ref> and ECANet <ref type="bibr" target="#b60">[61]</ref>, adaptively reweight the fea- ture maps by global averaged features among channels. In contrast, our channel group self-attention performs dynamic feature fusion across global tokens (different global views of the entire image) in transformers. In this way, our channel group self-attention is more powerful. We make quantitative comparisons by replacing the channel self-attention in our tiny model with SE <ref type="bibr" target="#b27">[28]</ref> and ECA <ref type="bibr" target="#b60">[61]</ref> blocks. See Appendix for detailed results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct experiments on ImageNet-1K image classification <ref type="bibr" target="#b13">[14]</ref>, COCO object detection <ref type="bibr" target="#b37">[38]</ref>, and ADE20K semantic segmentation <ref type="bibr" target="#b81">[83]</ref>. Neither token labeling <ref type="bibr" target="#b29">[30]</ref> nor distillation <ref type="bibr" target="#b54">[55]</ref> is used in all experiments and comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Image Classification</head><p>We compare different methods on ImageNet-1K <ref type="bibr" target="#b13">[14]</ref>. We implement our DaViT on the timm framework [64]. Following <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b63">65,</ref><ref type="bibr" target="#b80">82]</ref>, we use the same set of data augmentation and regularization strategies used in <ref type="bibr" target="#b54">[55]</ref> after excluding repeated augmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref> and exponential moving average (EMA) <ref type="bibr" target="#b43">[44]</ref>. We train all the models for 300 epochs with a batch size 2048 and use AdamW <ref type="bibr" target="#b41">[42]</ref> as the optimizer. The weight decay is set to 0.05 and the maximal gradient norm is clipped to 1.0. We use a simple triangular learning rate schedule <ref type="bibr" target="#b48">[49]</ref> as in <ref type="bibr" target="#b56">[57]</ref>. The stochastic depth drop rates are set to 0.1, 0.2, and 0.4 for our tiny, small, and base models, respectively. During training, we crop images randomly to 224 ? 224, while a center crop is used during evaluation on the validation set.</p><p>In <ref type="table">Table 1</ref>, we summarize the results for baseline models and current state-ofthe-art models on the image classification task. We can find our DaViT achieves new state-of-the-art and consistently outperforms other methods with similar model size (#Params.) and computational complexity (GFLOPs). Specifically, DaViT-Tiny, Small, and Base improve over the Swin Transformer [40] by 1.5%, 1.1%, and 1.2%, respectively. Notably, our DaViT-Small with 49.7M parameters reaches 84.2%, which surpasses all counterpart -Base models using much fewer parameters. For example, our DaViT-Small achieves 0.4% and 0.8% higher accuracy than Focal-Base and Swin-Base, respectively, using near half computations.</p><p>Following <ref type="bibr" target="#b63">[65,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b39">40]</ref>, when 13M images from ImageNet-22k <ref type="bibr" target="#b13">[14]</ref> involved for pre-training, DaViT-Base and DaViT-Large obtained 86.9% and 87.5% top-1 accuracy, respectively. Furthermore, when we further scale up DaViT with 1.5B privately collected weakly supervised image-text pairs data and pre-train DaViT with unified contrastive learning <ref type="bibr" target="#b75">[77]</ref> approach, DaViT-Huge and DaViT-Gaint reach 90.2% and 90.4% top-1 accuracy on ImageNet with 362M and 1.4B parameters, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Object Detection and Instance Segmentation</head><p>We benchmark our models on object detection with COCO 2017 <ref type="bibr" target="#b37">[38]</ref>. The pretrained models are used as visual backbones and then plugged into two representative pipelines, RetinaNet <ref type="bibr" target="#b36">[37]</ref> and Mask R-CNN <ref type="bibr" target="#b22">[23]</ref>. All models are trained on the 118k training images and results reported on the 5K validation set. We follow the standard to use two training schedules, 1? schedule with 12 epochs and 3? schedule with 36 epochs. The same multi-scale training strategy as in <ref type="bibr" target="#b39">[40]</ref> by randomly resizing the shorter side of the image to the range of [480, 800] is used. During training, we use AdamW <ref type="bibr" target="#b41">[42]</ref> for optimization with initial learning rate 10 ?4 and weight decay 0.05. We use 0.1, 0.2, and 0.3 stochastic depth drop rates to regularize the training for our tiny, small, and base models, respectively. The numbers of counterparts <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b80">82]</ref> are borrowed from <ref type="bibr" target="#b70">[72]</ref>.</p><p>In <ref type="table" target="#tab_3">Table 2</ref> and <ref type="table">Table 5</ref>, we show the performance of our models against several state-of-the-art counterparts. The bbox mAP (AP b ) and mask mAP (AP m ) are reported. Results of 1? schedule shown in <ref type="table" target="#tab_3">Table 2</ref> have demonstrated the effectiveness of our method. We observe substantial gains across all settings and metrics compared with several strong transformer baselines.</p><p>To have more comprehensive comparisons, we further train our models with 3? schedule and show the detailed numbers, #parameters, and associated computational costs for RetinaNet and Mask R-CNN in <ref type="table">Table 5</ref>. As we can see, even for the 3? schedule, our models can still achieve 1.5-2.9% gains on RetinaNet 3? and 1.0-1.4% gains on Mask R-CNN 3? over Swin Transformer models.</p><p>Moreover, from <ref type="table">Table 5</ref> we observe a saturated and even degraded mAP in Swin Transformer <ref type="bibr" target="#b39">[40]</ref> and Focal Transformer <ref type="bibr" target="#b70">[72]</ref> from small to base model, while the mAP of our model is continuously increased with larger model size, showing a better scale-up ability. Our base model outperforms the state-of-theart [72] by 1.8% on RetinaNet 3? and 0.9% on Mask R-CNN 3?. <ref type="table">Table 5</ref>. The effect of channel group attention at different network stages. Taking a transformer layout with all spatial window attention blocks as the baseline (n/a), we replace two spatial attention blocks at different stages with a dual attention block to show its effectiveness. The first two spatial attention blocks are selected in the third stage to compare with other stages fairly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Semantic Segmentation on ADE20k</head><p>Besides the instance segmentation results above, we further evaluate our model on semantic segmentation, a task that usually requires high-resolution input and long-range interactions. We benchmark our method on ADE20K <ref type="bibr" target="#b81">[83]</ref>. Specifically, we use UperNet <ref type="bibr" target="#b65">[67]</ref> as the segmentation method and our DaViT as the backbone. We train three models with DaViT-Tiny, DaViT-Small, DaViT-Base, respectively. For all models, we use a standard recipe by setting the input size to 512 ? 512 and train the model for 160k iterations with batch size 16. In <ref type="table" target="#tab_5">Table 4</ref>, we show the comparisons to previous works. As we can see, our tiny, small, and base models consistently outperform recent SoTAs, such as 1.2-1.8% gains over Swin Transformers <ref type="bibr" target="#b39">[40]</ref> with a similar number of parameters and FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>Evaluation of channel group attention at different stages. We make comparisons by inserting a dual attention block at different stages of a window transformer. From the quantitative results in <ref type="table">Table 5</ref>, we observe: (i) The dual attention module consistently boosts performance at each stage. (ii) Dual attention in the second stage improves the most, as the earlier stage requires more global information. We speculate that the relatively small improvement in the first stage is that local texture features dominate the shallow part of the network.</p><p>(iii) We achieve the best results when adding dual attention in all four stages. Dual attention layout. We conduct experiments on the layout of our dual attention. There are three options with similar computations: (i) window attention first; (ii) channel attention first; and (iii) two types of attention are paralleled arranged. The comparison is shown in <ref type="table" target="#tab_6">Table 6</ref>. We can see that the three strategies achieve similar performance, with 'window attention first' slightly better.</p><p>More ablative experiments can be found in Appendix. </p><formula xml:id="formula_9">N 1 h = N 1 g = 3 C 1 h = C 1 g = 32 ? ? ? ? ? 1 ? ? ? ?</formula><p>win. sz. 7 ? 7, Pw = 49</p><formula xml:id="formula_10">N 1 h = N 1 g = 3 C 1 h = C 1 g = 32 ? ? ? ? ? 1 ? ? ? ?</formula><p>win. sz. 7 ? 7, Pw = 49</p><formula xml:id="formula_11">N 1 h = N 1 g = 4 C 1 h = C 1 g = 32 ? ? ? ? ? 1 stage 2 28 ? 28</formula><p>Patch Embedding kernel 2, stride 2, pad 0, C 2 = 192 kernel 2, stride 2, pad 0, C 2 = 192 kernel 2, stride 2, pad 0, C 2 = 256 28 </p><formula xml:id="formula_12">N 2 h = N 2 g = 6 C 2 h = C 2 g = 32 ? ? ? ? ? 1 ? ? ? ?</formula><p>win. sz. 7 ? 7, Pw = 49</p><formula xml:id="formula_13">N 2 h = N 2 g = 6 C 2 h = C 2 g = 32 ? ? ? ? ? 1 ? ? ? ?</formula><p>win. sz. 7 ? 7, Pw = 49 </p><formula xml:id="formula_14">N 2 h = N 2 g = 8 C 2 h = C 2 g = 32 ? ? ? ? ?</formula><formula xml:id="formula_15">N 3 h = N 3 g = 12 C 3 h = C 3 g = 32 ? ? ? ? ? 3 ? ? ? ?</formula><p>win. sz. 7 ? 7, Pw = 49</p><formula xml:id="formula_16">N 3 h = N 3 g = 12 C 3 h = C 3 g = 32 ? ? ? ? ? 9 ? ? ? ?</formula><p>win. sz. 7 ? 7, Pw = 49 </p><formula xml:id="formula_17">N 3 h = N 3 g = 16 C 3 h = C 3 g = 32 ? ? ? ? ? 9</formula><formula xml:id="formula_18">N 4 h = N 4 g = 24 C 4 h = C 4 g = 32 ? ? ? ? ? 1 ? ? ? ? win. sz. 7 ? 7, Pw = 49 N 4 h = N 4 g = 24 C 4 h = C 4 g = 32 ? ? ? ? ? 1 ? ? ? ? win. sz. 7 ? 7, Pw = 49 N 4 h = N 4 g = 32 C 4 h = C 4 g = 32 ? ? ? ? ? 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This work introduces the dual attention mechanism, containing spatial window attention and channel group attention, to capture global contexts while maintaining computational efficiency. We show that these two self-attentions complement each other: (i) since each channel token contains an abstract representation of the entire image, the channel attention naturally captures global interactions and representations by taking all spatial positions into account when computing attention scores between channels; (ii) spatial attention refines the local representations by performing fine-grained interactions across spatial locations, which in turn helps the global information modeling in channel attention. We further visualize how our channel group attention captures global interactions and demonstrate its effectiveness in various benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Details of Model Configuration</head><p>In this work, we follow the design strategy suggested by previous works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b70">72]</ref>. We divide the entire architecture into four stages, where a patch embedding layer is inserted at the beginning of each stage. Here, our patch embedding layer is implemented by stride convolution. The convolutional kernels and stride values of our four patch embedding layers are {7, 2, 2, 2} and {4, 2, 2, 2}, respectively. Note the large kernel in the first layer introduces almost no additional calculations as the number of input channels is only 3. For the rest kernel values, we use 2 to perform non-overlapping patch merging. We stack our dual attention blocks in each stage with the resolution and feature dimension kept the same. These stages jointly produce a hierarchical representation, with the same feature map resolutions as those of typical convolutional networks, e.g., VGG <ref type="bibr" target="#b47">[48]</ref> and ResNet <ref type="bibr" target="#b23">[24]</ref>. As a result, the proposed architecture can conveniently replace the backbone networks in existing methods for various vision tasks. All model training is accelerated by NVIDIA Apex Automatic Mixed Precision (AMP).</p><p>Detailed model configurations of our tiny, small, and base models are shown in <ref type="table" target="#tab_7">Table 7</ref> with the feature dimension increasing to C 2 , C 3 , and C 4 after the other three patch embedding layer, respectively. For simplicity, we set the window size of 7 ? 7 thus P w = 49 for all models. We also set the number of channels per group C g = 32 and the number of channels per head C h = 32 for all blocks of our three models. For DaViT-tiny and DaViT-small, we set the number of heads/groups N h = N g = {3, 6, 12, 24} for four stages, respectively; and we set the number of heads/groups N h = N g = {4, 8, 16, 32} for four stages in DaViT-base. Also, we set the number of dual attention blocks {1, 1, 3, 1} for our tiny model and {1, 1, 9, 1} for the small and base models.</p><p>For the models without FFN, we simply change the number of dual attention blocks to keep the total computation costs similar, though we believe there should be a better configuration specifically for them. We set the number of dual attention blocks {2, 2, 11, 2} for our tiny model (without FFN) and {2, 2, 28, 2} for the small and base models (without FFN).</p><p>When more training data is involved, we further scale up DaViT to large, huge, and giant sizes to validate the scaling ability of the proposed architecture for image classification with larger input resolutions. Considering larger image resolutions and model sizes used, we suspect that the dot products of self-attention grow large in magnitude in this case, as in <ref type="bibr" target="#b58">[59]</ref>. To counteract this effect, we scale the dot products in our channel attention by 1 ? P . We set C 1 = {192, 256, 384}, N 1 h = N 1 g = {6, 8, 12}, and the number of dual attention blocks of the third stage as {9, 9, 12} respectively for large, huge, and giant models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Channel Attention on Vanilla ViT</head><p>We apply our channel group attention on the vanilla DeiT <ref type="bibr" target="#b54">[55]</ref> to show the generalizability and effectiveness of our dual attention mechanism. We alternatively arrange the vanilla patch-level self-attention and our channel group self-attention. We set the number of groups and channels of our channel-wise group attention the same as the number of heads and channels of self-attention in DeiT, to make the number of parameters and FLOPs comparable with the vanilla DeiT. From <ref type="table" target="#tab_10">Table 8</ref> we observe substantial gains of our dual attention across all model sizes, e.g., 2.7% over the tiny model and 0.5% even compared to the base model. The result shows that our channel-wise attention can be combined with window attention and patch-level global attention, improving the performance of both spatial-wise self-attentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Transformer without FFNs</head><p>FFN has been a default component of Transformers with little research on it. However, it dominates the number of FLOPs and model parameters of our DaViT. Considering our dual attention has both channel-wise and spatial-wise interactions, we conduct an initial exploration to show the potential of the pureattention structure without FFNs. We remove FFNs and add more dual attention blocks to match the computational costs. From <ref type="table" target="#tab_12">Table 9</ref> we see that: pure dual attention without FFNs achieves 1.5% and 1.7% better Top-1 accuracy than pure window attention and channel attention, respectively, showing the effectiveness of dual attention that has both channel-wise and spatial-wise interactions. The model without FFN shows comparable and even better performance with models like PVT <ref type="bibr" target="#b62">[63]</ref> and DeiT <ref type="bibr" target="#b54">[55]</ref>, but still inferior to recent SoTAs like Swin <ref type="bibr" target="#b39">[40]</ref>, Focal <ref type="bibr" target="#b70">[72]</ref>, and our full DaViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Model Capacity against Model Depth</head><p>Our DaViT contains both spatial-wise and channel-wise attention, and both local and global interactions in each dual attention block. We conduct experiments to show whether it needs less number of layers to obtain similar modeling capacity as previous works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b70">72]</ref>. We gradually reduce the number of transformer layers at the third stage from original 3 (6 in Swin <ref type="bibr" target="#b39">[40]</ref> and Focal <ref type="bibr" target="#b70">[72]</ref>) to 2 (4 <ref type="table">Table 10</ref>. Impact of the change of model depth. We gradually reduce the number of transformer layers at the third stage from the original 3 (6 in Swin <ref type="bibr" target="#b39">[40]</ref> and Focal <ref type="bibr" target="#b70">[72]</ref>) to 2 (4) and further 1 <ref type="bibr" target="#b1">(2</ref> in Swin <ref type="bibr" target="#b39">[40]</ref> and Focal <ref type="bibr" target="#b70">[72]</ref>) and further 1 (2 in Swin <ref type="bibr" target="#b39">[40]</ref> and Focal <ref type="bibr" target="#b70">[72]</ref>). From <ref type="table">Table 10</ref>, we find our model outperforms existing SoTA models consistently with the same depth. Moreover, using two fewer layers, our model achieves comparable and even better performance to Swin Transformer. Also, with fewer computational costs, our model outperforms Focal Transformer by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Throughput Analysis</head><p>In addition to the main criteria of computational cost in the main paper, i.e., FLOPs and #parameters, we report the real-time inference speed/throughput against Swin Transformer <ref type="bibr" target="#b39">[40]</ref> to show the efficiency of our work. Compared to the strong baseline Swin, our model does have advantages in real-time throughput as the cleaner structure and high efficiency of the group channel attention. For example, we remove the shifted window partition, which depends on torch.roll() to perform cyclic shift and its reverse on features. This operation is not fully optimized by popular inference frameworks. <ref type="table">Table 11</ref> demonstrates the comparison between Swin and DaViT. Nvidia V100 GPU is utilized for the benchmark, and the image resolution is 224 ? 224. It shows that DaViT consistently outperforms Swin across different model sizes in efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Comparisons with SE and ECA Blocks</head><p>To make quantitative comparisons with traditional channel-wise attentions blocks, we did experiments by replacing the channel self-attention in our tiny model with SE block <ref type="bibr" target="#b27">[28]</ref> and ECA block <ref type="bibr" target="#b60">[61]</ref>. The results in <ref type="table" target="#tab_3">Table 12</ref> show that our channel group self-attention is more powerful by performing dynamic feature fusion across global tokens (different global views of the entire image) in transformers. It shows superior performance (1.6%) than both of the two variants using SE <ref type="bibr" target="#b27">[28]</ref> and ECA <ref type="bibr" target="#b60">[61]</ref> blocks, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>?Fig. 3 .</head><label>3</label><figDesc>Matrix Product P : Number of total patches C : Number of total channels N w : Number of windows N h : Number of heads N g : Number of channel groups P w : Patches per window C h : Channels per head C g : Channels per group N g ? 1 ? C g ? P Model architecture for our dual attention block. It contains two transformer blocks: spatial window self-attention and channel group self-attention blocks. By alternately using the two types of attention, our model enjoys the benefit of capturing both local fine-grained and global image-level interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>the channel attention (top-7 channels attending to the output channel, sorted by attention score) Illustrating how channel attention gathering global information by visualizing attended feature maps. The first and second columns denote the original image and a feature channel after our channel attention; while the other columns are the channel tokens with top-7 highest attention scores before the attention. Channel attention is able to select globally important regions and suppress background regions for better recognition. The third network stage of the classification model is used for visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>-</head><label></label><figDesc>DaViT-Large: C = 192, L = {1, 1, 9, 1}, Ng = N h = {6, 12, 24, 48} -DaViT-Huge: C = 256, L = {1, 1, 9, 1}, Ng = N h = {8, 16, 32, 64} -DaViT-Giant: C = 384, L = {1, 1, 12, 3}, Ng = N h = {12, 24, 48, 96}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>stage 4 7</head><label>7</label><figDesc>? 7 Patch Embedding kernel 2, stride 2, pad 0, C 4 = 768 kernel 2, stride 2, pad 0, C 4 = 768 kernel 2, stride 2, pad 0, C 4 . sz. 7 ? 7, Pw = 49</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparisons with CNN and Transformer baselines and SoTA methods on COCO object detection. The box mAP (AP b ) and mask mAP (AP m ) are reported for RetinaNet and Mask R-CNN trained with 1? schedule. FLOPs are measured by 800 ? 1280. More detailed comparisons with 3? schedule are inTable 5.</figDesc><table><row><cell>Backbone</cell><cell>FLOPs (G)</cell><cell>RetinaNet AP b</cell><cell cols="2">Mask R-CNN AP b AP m</cell></row><row><cell>ResNet-50 [24]</cell><cell>239/260</cell><cell>36.3</cell><cell>38.0</cell><cell>34.4</cell></row><row><cell>PVT-Small [63]</cell><cell>226/245</cell><cell>40.4</cell><cell>40.4</cell><cell>37.8</cell></row><row><cell>ViL-Small [82]</cell><cell>252/174</cell><cell>41.6</cell><cell>41.8</cell><cell>38.5</cell></row><row><cell>Swin-Tiny [40]</cell><cell>245/264</cell><cell>42.0</cell><cell>43.7</cell><cell>39.8</cell></row><row><cell>Focal-Tiny [72]</cell><cell>265/291</cell><cell>43.7</cell><cell>44.8</cell><cell>41.0</cell></row><row><cell>DaViT-Tiny (Ours)</cell><cell>244/263</cell><cell>44.0</cell><cell>45.0</cell><cell>41.1</cell></row><row><cell>ResNeXt101-32x4d [69]</cell><cell>319/340</cell><cell>39.9</cell><cell>41.9</cell><cell>37.5</cell></row><row><cell>PVT-Medium [63]</cell><cell>283/302</cell><cell>41.9</cell><cell>42.0</cell><cell>39.0</cell></row><row><cell>ViL-Medium [82]</cell><cell>339/261</cell><cell>42.9</cell><cell>43.4</cell><cell>39.7</cell></row><row><cell>Swin-Small [40]</cell><cell>335/354</cell><cell>45.0</cell><cell>46.5</cell><cell>42.1</cell></row><row><cell>Focal-Small [72]</cell><cell>367/401</cell><cell>45.6</cell><cell>47.4</cell><cell>42.8</cell></row><row><cell>DaViT-Small (Ours)</cell><cell>332/351</cell><cell>46.0</cell><cell>47.7</cell><cell>42.9</cell></row><row><cell>ResNeXt101-64x4d [69]</cell><cell>473/493</cell><cell>41.0</cell><cell>42.8</cell><cell>38.4</cell></row><row><cell>PVT-Large [63]</cell><cell>345/364</cell><cell>42.6</cell><cell>42.9</cell><cell>39.5</cell></row><row><cell>ViL-Base [82]</cell><cell>443/365</cell><cell>44.3</cell><cell>45.1</cell><cell>41.0</cell></row><row><cell>Swin-Base [40]</cell><cell>477/496</cell><cell>45.0</cell><cell>46.9</cell><cell>42.3</cell></row><row><cell>Focal-Base [72]</cell><cell>514/533</cell><cell>46.3</cell><cell>47.8</cell><cell>43.2</cell></row><row><cell>DaViT-Base (Ours)</cell><cell>471/491</cell><cell>46.7</cell><cell>48.2</cell><cell>43.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>COCO object detection and segmentation results with RetinaNet<ref type="bibr" target="#b36">[37]</ref> and Mask R-CNN<ref type="bibr" target="#b23">[24]</ref>. All models are trained with 3? schedule and multi-scale inputs. The numbers before and after "/" at column 2 and 3 are the model size and complexity for RetinaNet and Mask R-CNN, respectively. FLOPs are measured by 800 ? 1280.AP b AP b 50 AP b 75 APS APM APL AP b AP b 50 AP b75 AP m AP m 50 AP m /107.3 471/491 48.7 70.0 52.3 33.7 52.8 62.9 49.9 71.5 54.6 44.6 68.8 47.8</figDesc><table><row><cell>Backbone</cell><cell>#Params FLOPs (M) (G)</cell><cell>RetinaNet 3x</cell><cell>Mask R-CNN 3x</cell><cell>75</cell></row><row><cell>ResNet50 [24]</cell><cell cols="4">37.7/44.2 239/260 39.0 58.4 41.8 22.4 42.8 51.6 41.0 61.7 44.9 37.1 58.4 40.1</cell></row><row><cell>PVT-Small[63]</cell><cell cols="4">34.2/44.1 226/245 42.2 62.7 45.0 26.2 45.2 57.2 43.0 65.3 46.9 39.9 62.5 42.8</cell></row><row><cell>ViL-Small [82]</cell><cell cols="4">35.7/45.0 252/174 42.9 63.8 45.6 27.8 46.4 56.3 43.4 64.9 47.0 39.6 62.1 42.4</cell></row><row><cell>Swin-Tiny [40]</cell><cell cols="4">38.5/47.8 245/264 45.0 65.9 48.4 29.7 48.9 58.1 46.0 68.1 50.3 41.6 65.1 44.9</cell></row><row><cell>Focal-Tiny [72]</cell><cell cols="4">39.4/48.8 265/291 45.5 66.3 48.8 31.2 49.2 58.7 47.2 69.4 51.9 42.7 66.5 45.9</cell></row><row><cell>DaViT-Tiny (Ours)</cell><cell cols="4">38.5/47.8 244/263 46.5 68.1 49.6 32.3 50.6 59.9 47.4 69.5 52.0 42.9 66.8 46.4</cell></row><row><cell cols="5">ResNeXt101-32x4d [69] 56.4/62.8 319/340 41.4 61.0 44.3 23.9 45.5 53.7 44.0 64.4 48.0 39.2 61.4 41.9</cell></row><row><cell>PVT-Medium [63]</cell><cell cols="4">53.9/63.9 283/302 43.2 63.8 46.1 27.3 46.3 58.9 44.2 66.0 48.2 40.5 63.1 43.5</cell></row><row><cell>ViL-Medium [82]</cell><cell cols="4">50.8/60.1 339/261 43.7 64.6 46.4 27.9 47.1 56.9 44.6 66.3 48.5 40.7 63.8 43.7</cell></row><row><cell>Swin-Small [40]</cell><cell cols="4">59.8/69.1 335/354 46.4 67.0 50.1 31.0 50.1 60.3 48.5 70.2 53.5 43.3 67.3 46.6</cell></row><row><cell>Focal-Small [72]</cell><cell cols="4">61.7/71.2 367/401 47.3 67.8 51.0 31.6 50.9 61.1 48.8 70.5 53.6 43.8 67.7 47.2</cell></row><row><cell>DaViT-Small (Ours)</cell><cell cols="4">59.9/69.2 332/351 48.2 69.7 51.7 32.7 52.2 62.9 49.5 71.4 54.7 44.3 68.4 47.6</cell></row><row><cell cols="5">ResNeXt101-64x4d [69] 95.5/102 473/493 41.8 61.5 44.4 25.2 45.4 54.6 44.4 64.9 48.8 39.7 61.9 42.6</cell></row><row><cell>PVT-Large[63]</cell><cell cols="4">71.1/81.0 345/364 43.4 63.6 46.1 26.1 46.0 59.5 44.5 66.0 48.3 40.7 63.4 43.7</cell></row><row><cell>ViL-Base [82]</cell><cell cols="4">66.7/76.1 443/365 44.7 65.5 47.6 29.9 48.0 58.1 45.7 67.2 49.9 41.3 64.4 44.5</cell></row><row><cell>Swin-Base [40]</cell><cell cols="4">98.4/107.0 477/496 45.8 66.4 49.1 29.9 49.4 60.3 48.5 69.8 53.2 43.4 66.8 46.9</cell></row><row><cell>Focal-Base [72]</cell><cell cols="4">100.8/110.0 514/533 46.9 67.8 50.3 31.9 50.3 61.5 49.0 70.1 53.6 43.7 67.6 47.0</cell></row><row><cell>DaViT-Base (Ours)</cell><cell>98.5</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Comparison with SoTA methods for semantic segmentation on ADE20K<ref type="bibr" target="#b81">[83]</ref> val set. Single-scale evaluation is used. FLOPs are measured by 512 ? 2048.</figDesc><table><row><cell>Backbone</cell><cell>Method</cell><cell>#Params (M)</cell><cell>FLOPs (G)</cell><cell>mIoU (%)</cell></row><row><cell>Swin-Tiny [40]</cell><cell>UperNet [67]</cell><cell>60</cell><cell>945</cell><cell>44.5</cell></row><row><cell>PVT-Large [63]</cell><cell>SemanticFPN [36]</cell><cell>65</cell><cell>318</cell><cell>44.8</cell></row><row><cell>HRNet-w48 [60]</cell><cell>OCRNet [78]</cell><cell>71</cell><cell>664</cell><cell>45.7</cell></row><row><cell>Focal-Tiny [72]</cell><cell>UperNet [67]</cell><cell>62</cell><cell>998</cell><cell>45.8</cell></row><row><cell>XCiT-S12/16 [1]</cell><cell>UperNet [67]</cell><cell>52</cell><cell>-</cell><cell>45.9</cell></row><row><cell>Twins-SVT-Small [10]</cell><cell>UperNet [67]</cell><cell>54</cell><cell>912</cell><cell>46.2</cell></row><row><cell>DaViT-Tiny (Ours)</cell><cell>UperNet [67]</cell><cell>60</cell><cell>940</cell><cell>46.3</cell></row><row><cell>ResNet-101 [24]</cell><cell>UperNet [67]</cell><cell>86</cell><cell>1029</cell><cell>44.9</cell></row><row><cell>XCiT-S24/16 [1]</cell><cell>UperNet [67]</cell><cell>73</cell><cell>-</cell><cell>46.9</cell></row><row><cell>Swin-Small [40]</cell><cell>UperNet [67]</cell><cell>81</cell><cell>1038</cell><cell>47.6</cell></row><row><cell>Twins-SVT-Base [10]</cell><cell>UperNet [67]</cell><cell>88</cell><cell>1044</cell><cell>47.7</cell></row><row><cell>Focal-Small [72]</cell><cell>UperNet [67]</cell><cell>85</cell><cell>1130</cell><cell>48.0</cell></row><row><cell>ResNeSt-200 [81]</cell><cell>DLab.v3+ [6]</cell><cell>88</cell><cell>1381</cell><cell>48.4</cell></row><row><cell>DaViT-Small (Ours)</cell><cell>UperNet [67]</cell><cell>81</cell><cell>1030</cell><cell>48.8</cell></row><row><cell>Swin-Base [40]</cell><cell>UperNet [67]</cell><cell>121</cell><cell>1188</cell><cell>48.1</cell></row><row><cell>XCiT-M24/8 [1]</cell><cell>UperNet [67]</cell><cell>109</cell><cell>-</cell><cell>48.4</cell></row><row><cell>Twins-SVT-Large [10]</cell><cell>UperNet [67]</cell><cell>133</cell><cell>1188</cell><cell>48.8</cell></row><row><cell>ViT-Hybrid [45]</cell><cell>DPT [45]</cell><cell>124</cell><cell>1231</cell><cell>49.0</cell></row><row><cell>Focal-Base [72]</cell><cell>UperNet [67]</cell><cell>126</cell><cell>1354</cell><cell>49.0</cell></row><row><cell>DaViT-Base (Ours)</cell><cell>UperNet [67]</cell><cell>121</cell><cell>1175</cell><cell>49.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Quantitative comparisons of different dual attention layouts on ImageNet.</figDesc><table><row><cell>Stage</cell><cell>#Params (M)</cell><cell>FLOPs (G)</cell><cell>Top-1 (%)</cell><cell>Top-5 (%)</cell></row><row><cell>n/a</cell><cell>28.3</cell><cell>4.6</cell><cell>81.1</cell><cell>95.6</cell></row><row><cell>1</cell><cell>28.3</cell><cell>4.5</cell><cell>81.7</cell><cell>95.9</cell></row><row><cell>2</cell><cell>28.3</cell><cell>4.5</cell><cell>82.2</cell><cell>96.1</cell></row><row><cell>3</cell><cell>28.3</cell><cell>4.6</cell><cell>82.1</cell><cell>96.0</cell></row><row><cell>4</cell><cell>28.3</cell><cell>4.6</cell><cell>81.9</cell><cell>95.9</cell></row><row><cell>1-4</cell><cell>28.3</cell><cell>4.6</cell><cell>82.8</cell><cell>96.2</cell></row><row><cell>Model</cell><cell cols="2">#Params (M)</cell><cell>FLOPs (G)</cell><cell>Top-1 (%)</cell></row><row><cell cols="2">Window ? Channel</cell><cell>28.3</cell><cell>4.5</cell><cell>82.8</cell></row><row><cell cols="2">Channel ? Window</cell><cell>28.3</cell><cell>4.5</cell><cell>82.6</cell></row><row><cell cols="2">Hybrid (parallel)</cell><cell>28.3</cell><cell>4.5</cell><cell>82.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Model configurations for our DaViT. We introduce three configurations DaViT-Tiny, DaViT-Small, and DaViT-Base with different model capacities. The size of the input image is set to 224 ? 224. , stride 4, pad 3, C 1 = 96 kernel 7, stride 4, pad 3, C 1 = 96 kernel 7, stride 4, pad 3, C 1 = 128</figDesc><table><row><cell></cell><cell>Output Size</cell><cell>Layer Name</cell><cell>DaViT-Tiny</cell><cell>DaViT-Small</cell><cell>DaViT-Base</cell></row><row><cell>stage 1</cell><cell cols="3">56 ? 56 Patch Embedding kernel 756 ? 56 Transformer ? ? Dual ? win. sz. 7 ? 7, Pw = 49</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Block</cell><cell>?</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Comparison on ImageNet-1K by replacing half of DeiT<ref type="bibr" target="#b54">[55]</ref> attention blocks with our channel attention block. Performance improvement over DeiT is highlighted in blue font.</figDesc><table><row><cell>Model</cell><cell cols="3">#Params (M) FLOPs (G) Top-1 (%)</cell></row><row><cell>DeiT-Tiny [55]</cell><cell>5.7</cell><cell>1.2</cell><cell>72.2</cell></row><row><cell>DeiT-Tiny [55] + Channel Attention</cell><cell>5.7</cell><cell>1.2</cell><cell>74.9 (+2.7)</cell></row><row><cell>DeiT-Small [55]</cell><cell>22.1</cell><cell>4.5</cell><cell>79.8</cell></row><row><cell>DeiT-Small [55] + Channel Attention</cell><cell>22.1</cell><cell>4.5</cell><cell>81.2 (+1.4)</cell></row><row><cell>DeiT-Base [55]</cell><cell>86.7</cell><cell>17.4</cell><cell>81.8</cell></row><row><cell>DeiT-Base [55] + Channel Attention</cell><cell>86.7</cell><cell>17.4</cell><cell>82.3 (+0.5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>. Specifically, take an image with H ? W , a C 1 -dimensional feature with a resolution of H 4 ? W 4 is obtained after the first patch embedding layer. And its resolution is further reduced into H</figDesc><table><row><cell>8 ? W 8 , H 16 ? W 16 , and H 32 ? W 32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 .</head><label>9</label><figDesc>Comparison of Transformers without FFNs on ImageNet-1K. FFN is removed and more attention layers are added to match the computational cost.</figDesc><table><row><cell>Model (w/o FFN)</cell><cell cols="3">#Params FLOPs Top-1 (%)</cell></row><row><cell>Window Attention-Tiny (w/o FFN)</cell><cell>25.8</cell><cell>4.6</cell><cell>79.1</cell></row><row><cell>Channel Attention-Tiny (w/o FFN)</cell><cell>25.8</cell><cell>4.5</cell><cell>79.3</cell></row><row><cell>Dual Attention-Tiny (w/o FFN)</cell><cell>25.8</cell><cell>4.5</cell><cell>80.8</cell></row><row><cell>Dual Attention-Small (w/o FFN)</cell><cell>46.3</cell><cell>8.7</cell><cell>81.9</cell></row><row><cell>Dual Attention-Base (w/o FFN)</cell><cell>81.6</cell><cell>15.2</cell><cell>82.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 .Table 12 .</head><label>1112</label><figDesc>). Throughput of Swin and DaViT. Comparisons to the channelwise attention used in CNNs, by replacing our channel group attention with SENet<ref type="bibr" target="#b27">[28]</ref> and ECANet<ref type="bibr" target="#b60">[61]</ref>, respectively.</figDesc><table><row><cell></cell><cell>Depths</cell><cell>Model</cell><cell cols="2">#Params. (M)</cell><cell>FLOPs (G)</cell><cell>Top-1 (%)</cell></row><row><cell></cell><cell>2-2-2-2</cell><cell>Swin [40] Focal [72]</cell><cell>21.2 21.7</cell><cell></cell><cell>3.1 3.4</cell><cell>78.7 79.9</cell></row><row><cell></cell><cell>1-1-1-1</cell><cell>DaViT (ours)</cell><cell>21.2</cell><cell></cell><cell>3.1</cell><cell>80.2</cell></row><row><cell></cell><cell>2-2-4-2</cell><cell>Swin [40] Focal [72]</cell><cell>24.7 25.4</cell><cell></cell><cell>3.8 4.1</cell><cell>80.2 81.4</cell></row><row><cell></cell><cell>1-1-2-1</cell><cell>DaViT (ours)</cell><cell>24.7</cell><cell></cell><cell>3.8</cell><cell>81.8</cell></row><row><cell></cell><cell>2-2-6-2</cell><cell>Swin [40] Focal [72]</cell><cell>28.3 29.1</cell><cell></cell><cell>4.5 4.9</cell><cell>81.2 82.2</cell></row><row><cell></cell><cell>1-1-3-1</cell><cell>DaViT (ours)</cell><cell>28.3</cell><cell></cell><cell>4.5</cell><cell>82.8</cell></row><row><cell>Model</cell><cell cols="3">Throughput (samples/s)</cell><cell></cell></row><row><cell>Swin-T</cell><cell></cell><cell></cell><cell>1024</cell><cell></cell></row><row><cell>DaViT-T</cell><cell></cell><cell></cell><cell>1059</cell><cell></cell></row><row><cell>Swin-S</cell><cell></cell><cell></cell><cell>655</cell><cell cols="2">Method</cell><cell>Top-1 (%)</cell></row><row><cell>DaViT-S</cell><cell></cell><cell></cell><cell>685</cell><cell cols="2">SE Block [28]</cell><cell>81.2</cell></row><row><cell>Swin-B</cell><cell></cell><cell></cell><cell>496</cell><cell cols="2">ECA Block [61]</cell><cell>81.2</cell></row><row><cell>DaViT-B</cell><cell></cell><cell></cell><cell>523</cell><cell cols="2">Channel Self-Attention</cell><cell>82.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Xcit: Cross-covariance image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08602</idno>
		<title level="m">Lambdanetworks: Modeling long-range interactions without attention</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05509</idno>
		<title level="m">Multigrain: a unified image embedding for classes and instances</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<title level="m">High-performance large-scale image recognition without normalization. arXiv: Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>2020) 1, 2, 4</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cyclemlp: A mlp-like architecture for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Visformer: The vision-friendly transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Twins: Revisiting spatial attention design in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13840</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Conditional positional encodings for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno>2102.10882</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic head: Unifying object detection heads with attentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Hr-nas: Searching efficient high-resolution neural architectures with lightweight transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00652</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>In: ICLR (2021) 1, 2, 4, 7</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09741</idno>
		<title level="m">Visual attention network</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vision permutator: A permutable mlp-like architecture for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Shuffle transformer: Rethinking spatial shuffle for vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03650</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">All tokens matter: Token labeling for training better vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Uniformer: Unifying convolution and self-attention for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09450</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved multiscale vision transformers for classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05707</idno>
		<title level="m">Localvit: Bringing locality to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">As-mlp: An axial shifted mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08391</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08050</idno>
		<title level="m">Pay attention to mlps</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note>In: ICCV (2021) 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03545</idno>
		<title level="m">A convnet for the 2020s</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10619</idno>
		<title level="m">Scalable visual transformers with hierarchical pooling</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scaling vision with sparse mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Susano Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<title level="m">Tokenlearner: What can 8 learned tokens do for images and videos? arXiv: Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Super-convergence: Very fast training of neural networks using large learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Topin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11006</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Ufo-vit: High performance linear vision transformer without softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.14382</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02767</idno>
		<title level="m">Quadtree attention for vision transformers</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">O</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Resmlp: Feedforward networks for image classification with data-efficient training</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note>PMLR (2021) 4, 7, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09792</idno>
		<title level="m">Patches are all you need? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Eca-net: Efficient channel attention for deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13797</idno>
		<title level="m">Pvtv2: Improved baselines with pyramid vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
	</analytic>
	<monogr>
		<title level="m">ICCV (2021) 2, 4</title>
		<meeting><address><addrLine>Wightman, R.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>Pytorch image models</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Rethinking and improving relative position encoding for vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Co-scale conv-attentional image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Vitae: Vision transformer advanced by exploring intrinsic inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00641</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Glance-and-gaze vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">S2-mlp: Spatial-shift mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>WACV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Volo: Vision outlooker for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13112</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11432</idno>
		<title level="m">Florence: A new foundation model for computer vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11065</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Hrformer: High-resolution transformer for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<title level="m">Scaling vision transformers. arXiv: Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Multi-scale vision longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>In: ICCV (2021) 2, 3, 4, 10</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11886</idno>
		<title level="m">Deepvit: Towards deeper vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Elsa: Enhanced local selfattention for vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
