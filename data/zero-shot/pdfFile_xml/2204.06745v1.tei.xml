<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GPT-NeoX-20B: An Open-Source Autoregressive Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sid</forename><surname>Black</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Biderman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Hallahan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><surname>Golding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Leahy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Pieler</forename><surname>Usvsn</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Prashanth</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivanshu</forename><surname>Purohit</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laria</forename><surname>Reynolds</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Weinbach</surname></persName>
						</author>
						<title level="a" type="main">GPT-NeoX-20B: An Open-Source Autoregressive Language Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe GPT-NeoX-20B's architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at https:// github.com/EleutherAI/gpt-neox.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past several years, there has been an explosion in research surrounding large language models (LLMs) for natural language processing, catalyzed largely by the impressive performance of Transformer-based language models such as <ref type="bibr">BERT (Devlin et al., 2019)</ref>, <ref type="bibr">GPT-2 (Radford et al., 2019)</ref>, <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>, and T5 <ref type="bibr">(Raffel et al., 2020)</ref>. One of the most impactful outcomes of this research has been the discovery that the performance of LLMs scales predictably as a power law with the number of parameters, with architectural details such as width/depth ratio having a minimal impact on performance within a wide range <ref type="bibr">(Kaplan et al., 2020)</ref>. A consequence of this has been an abundance of research focusing on scaling Transformer models up to ever-larger scales, resulting in dense models that surpass 500B parameters * Lead authors. Authors after the first three are listed in alphabetical order. See Appendix A for individual contribution details. Correspondence can be sent to {sid, stella, contact}@eleuther.ai <ref type="bibr">(Smith et al., 2022;</ref><ref type="bibr">Chowdhery et al., 2022)</ref>, a milestone that would have been almost unthinkable just a few years prior.</p><p>Today, there are dozens of publicly acknowledged LLMs in existence, the largest having more than two orders of magnitude more parameters than GPT-2, and even at that scale there are nearly a dozen different models. However, these models are almost universally the protected intellectual property of large organizations, and are gated behind a commercial API, available only upon request, or not available for outsider use at all. To our knowledge, the only freely and publicly available dense autoregressive language models larger than GPT-2 are GPT-Neo (2.7B parameters) <ref type="bibr">(Black et al., 2021)</ref>, GPT-J-6B (Wang and Komatsuzaki, 2021), Megatron-11B 1 , Pangu-?-13B <ref type="bibr">(Zeng et al., 2021)</ref>, and the recently released FairSeq models (2.7B, 6.7B, and 13B parameters) <ref type="bibr">(Artetxe et al., 2021)</ref>.</p><p>In this paper we introduce GPT-NeoX-20B, a 20 billion parameter open-source autoregressive language model. We make the models weights freely and openly available to the public through a permissive license, motivated by the belief that open access to LLMs is critical to advancing research in a wide range of areas-particularly in AI safety, mechanistic interpretability, and the study of how LLM capabilities scale. Many of the most interesting capabilities of LLMs only emerge above a certain number of parameters, and they have many properties that simply cannot be studied in smaller models. Although safety is often cited as a justification for keeping model weights private, we believe this is insufficient to prevent misuse, and is largely a limitation on the ability to probe and study LLMs for researchers not based at the small number of organizations that have access to state of the art language models. In addition, we make partially trained checkpoints avaliable at evenly spaced 1000 step intervals throughout the whole of training. We hope that by making a wide range of checkpoints throughout training freely available, we will facilitate research on the training dynamics of LLMs, as well as the aforementioned areas of AI safety and interpretability.</p><p>In studying GPT-NeoX-20B, we find several noteworthy phenomena at odds with the established literature. We train on a dataset that contains duplicated data for more than one epoch but see no evidence of performance loss. While <ref type="bibr">(Hendrycks et al., 2021a)</ref> claims that few-shot prompting doesn't improve performance on their task, we find that this is actually a phenomenon unique to GPT-3 and doesn't apply to either GPT-NeoX-20B or FairSeq models. Finally, we find that GPT-NeoX-20B is a powerful few-shot learner, recieving a much larger performance boost from few-shot examples than comparable sized GPT-3 and FairSeq models. As we see the same with GPT-J-6B (Wang and <ref type="bibr">Komatsuzaki, 2021)</ref>, we hypothesize that this may be due to the shared choice of training data.</p><p>In the following sections, we give a broad overview of GPT-NeoX-20B's architecture and training hyperparameters, detail the hardware and software setup used for training and evaluation, and elaborate on the choices made when designing the training dataset and tokenization. We also address of some of the difficulties and unknowns we encountered in training such a large model. We place significant importance on the broader impacts of the release GPT-NeoX-20B, and provide a lengthy discussion of why we believe its release is a net benefit. We also document issues of training cost and carbon emissions in as much detail as much as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Design and Implementation</head><p>GPT-NeoX-20B is an autoregressive transformer decoder model whose architecture largely follows that of <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>, with a few notable deviations described below. Our model has 20 billion parameters, of which 19.9 billion are "non-embedding" parameters that <ref type="bibr">Kaplan et al. (2020)</ref> identify as the proper number to use for scaling laws analysis. Our model has 44 layers, a hidden dimension size of 6144, and 64 heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Architecture</head><p>Although our architecture is largely similar to GPT-3, there are some notable differences. In this sec-tion we give a high-level overview of those differences, but ask the reader to refer to <ref type="bibr">(Brown et al., 2020)</ref> for full details of the model architecture. Our model architecture is almost identical to that of GPT-J (Wang and Komatsuzaki, 2021) 2 , however we choose to use GPT-3 as the point of reference because there is no canonical published reference on the design of GPT-J.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Rotary Positional Embeddings</head><p>We use rotary embeddings <ref type="bibr">(Su et al., 2021)</ref> instead of the learned positional embeddings used in GPT models <ref type="bibr">(Radford et al., 2018)</ref>, based on our positive prior experiences using it in training LLMs. Rotary embeddings are a form of static relative positional embeddings. In brief, they twist the embedding space such that the attention of a token at position m to token at position n is linearly dependent on m ? n. More formally, they modify the standard multiheaded attention equations from</p><formula xml:id="formula_0">softmax 1 ? d ? n,m x T m W T q W k x n ,</formula><p>where x m , x n are (batched) embeddings of tokens at position m and n respectively and W T q , W k are the query and key weights respectively to</p><formula xml:id="formula_1">softmax 1 ? d ? n,m x T m W T q R d ?,(n?m) W k x n ,</formula><p>where R d ?,x is a d ? d block diagonal matrix with the block of index i being a 2D rotation by x? i for hyperparameters ? = {? i = 10000 ?2i/d | i ? {0, 1, 2, . . . , (d ? 1)/2}}. While <ref type="bibr">Su et al. (2021)</ref> apply rotary embeddings to every embedding vector, we follow Wang and <ref type="bibr">Komatsuzaki (2021)</ref> and instead apply it only to the first 25% of embedding vector dimensions. Our initial experiments indicate that this strikes the best balance of performance and computational efficiency. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Parallel Attention + FF Layers</head><p>We compute the Attention and Feed-Forward (FF) layers in parallel 4 and sum the results, rather than running them in series. This is primarily for efficiency purposes, as each residual addition with op-sharding requires one all-reduce in the forward pass and one in the backwards pass <ref type="bibr">(Shoeybi et al., 2020)</ref>. By computing the Attention and FFs in parallel, the results can be reduced locally before performing a single all-reduce. In Mesh Transformer JAX <ref type="bibr">(Wang, 2021)</ref>, this led to a 15% throughput increase, while having comparable loss curves with running them in series during early training.</p><p>Due to an oversight in our code, we unintentionally apply two independent Layer Norms instead of using a tied layer norm the way Wang and <ref type="bibr">Komatsuzaki (2021)</ref> does. Instead of computing</p><p>x + Attn(LN 1 (x)) + FF(LN 1 (x)) as intended, our codebase unties the layer norms:</p><p>x + Attn(LN 1 (x)) + FF(LN 2 (x)).</p><p>Unfortunately, this was only noticed after we were much too far into training to restart. Subsequent experiments at small scales indicated that the untied layer norm makes no difference in performance, but we nevertheless wish to highlight this in the interest of transparency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Initialization</head><p>For the Feed-Forward output layers before the residuals, we used the initialization scheme introduced in Wang (2021), 2 L ? d . This prevents activations from growing with increasing depth and width, with the factor of 2 compensating for the fact that the parallel and feed-forward layers are organized in parallel. For all other layers, we use the small init scheme from Nguyen and Salazar (2019), 2 d+4d 3 See the Weights &amp; Biases reports here and here for further details. <ref type="bibr">4</ref> See GitHub for implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">All Dense Layers</head><p>While GPT-3 uses alternating dense and sparse layers using the technique introduced in Child et al. <ref type="bibr">(2019)</ref>, we instead opt to exclusively use dense layers to reduce implementation complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Software Libraries</head><p>Our model is trained using a codebase that builds on <ref type="bibr">Megatron (Shoeybi et al., 2020)</ref> and <ref type="bibr">Deep-Speed (Rasley et al., 2020)</ref> to facilitate efficient and straightforward training of large language models with tens of billions of parameters. We use the official PyTorch v1.10.0 release binary package compiled with CUDA 11.1. This package is bundled with NCCL 2.10.3 for distributed communications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hardware</head><p>We trained GPT-NeoX-20B on twelve Supermicro AS-4124GO-NART servers, each with eight NVIDIA A100-SXM4-40GB GPUs and configured with two AMD EPYC 7532 CPUs. All GPUs can directly access the InfiniBand switched fabric through one of four ConnectX-6 HCAs for GPUDirect RDMA. Two NVIDIA MQM8700-HS2R switches-connected by 16 links-compose the spine of this InfiniBand network, with one link per node CPU socket connected to each switch. <ref type="figure">Figure 2</ref> shows a simplified overview of a node as configured for training.  <ref type="bibr">(2020)</ref> in combination with pipeline parallelism <ref type="bibr">(Harlap et al., 2018)</ref> to distribute the model across GPUs. To train GPT-NeoX-20B, we found that the most efficient way to distribute the model given our hardware setup was to set a tensor parallel size of 2, and a pipeline parallel size of 4. This allows for the most communication intensive processes, tensor and pipeline parallelism, to occur within a node, and data parallel communication to occur across node boundaries. In this fashion, we were able to achieve and maintain an efficiency of 117 teraFLOPS per GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Data</head><p>GPT-NeoX-20B was trained on the Pile (Gao et al., 2020), a massive curated dataset designed specifically for training large language models. It consists of data from 22 data sources, coarsely broken down into 5 categories:</p><p>? In aggregate, the Pile consists of over 825 GiB of raw text data. The diversity of data sources reflects our desire for a general-purpose language model. Certain components are up-sampled to obtain a more balanced data distribution. In contrast, GPT-3's training data consists of web-scrapes, books datasets, and Wikipedia. When comparing results in this work to GPT-3, the training data is almost certainly the biggest known unknown factor. Full details of the Pile can be found in the technical report <ref type="bibr">(Gao et al., 2020)</ref> and the associated datasheet <ref type="bibr" target="#b6">(Biderman et al., 2022)</ref>.</p><p>It is particularly notable that the Pile contains a scrape of StackExchange preprocessed into a Q/A form. There is a significant and growing body of work on the influence of the syntactic structure of finetuning data on downstream performance <ref type="bibr">(Zhong et al., 2021;</ref><ref type="bibr">Tan et al., 2021;</ref><ref type="bibr"></ref> GPT-2 def fibRec(n):? if n &lt; 2:? return n? else:? return fibRec(n-1) + fibRec(n-2) 55 tokens</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-NeoX-20B</head><p>def fibRec(n):? if n &lt; 2:? return n? else:? return fibRec(n-1) + fibRec(n-2) 39 tokens <ref type="figure">Figure 3</ref>: GPT-2 tokenization vs. GPT-NeoX-20B tokenization. GPT-NeoX-20B tokenization handles whitespace better, which is particularly useful for text such as source code. For more examples, see Appendix F. <ref type="bibr">Sanh et al., 2021;</ref><ref type="bibr">Wei et al., 2021)</ref>. While so far there has been no systematic work that focuses on prompted pretraining, recent work <ref type="bibr">(Biderman and Raff, 2022)</ref> observed that the formulation of the StackExchange component of the Pile appears to heavily influence code generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tokenization</head><p>For GPT-NeoX-20B, we use a BPE-based tokenizer similar to that used in GPT-2, with the same total vocabulary size of 50257, with three major changes to the tokenizer. First, we train a new BPE tokenizer based on the Pile, taking advantage of its diverse text sources to construct a more generalpurpose tokenizer. Second, in contrast to the GPT-2 tokenizer which treats tokenization at the start of a string as a non-space-delimited token, the GPT-NeoX-20B tokenizer applies consistent space delimitation regardless. This resolves an inconsistency regarding the presence of prefix spaces to a tokenization input. 12 . An example can be seen in <ref type="figure">Figure 3</ref>. Third, our tokenizer contains tokens for repeated space tokens (all positive integer amounts of repeated spaces up to and including 24). This allows the GPT-NeoX-20B tokenizer to tokenize text with large amounts of whitespace using fewer tokens; for instance, program source code or arXiv L A T E X source files. See Appendix E for an analysis of the tokenizer. 12 https://discuss.huggingface.co/t/ bpe-tokenizers-and-spaces-before-words/475/2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Duplication</head><p>In the past two years, the standard practice when training autoregressive language models has become to train for only one epoch <ref type="bibr">(Komatsuzaki, 2019;</ref><ref type="bibr">Kaplan et al., 2020;</ref><ref type="bibr">Henighan et al., 2020)</ref>. Recent research has claimed to see significant benefits from going even further and deduplicating training data <ref type="bibr">(Lee et al., 2021;</ref><ref type="bibr">Kandpal et al., 2022;</ref><ref type="bibr">Roberts et al., 2022)</ref>. In particular, every publicly known larger language model other than <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> and Jurassic-1 13 either uses some form of deduplication <ref type="bibr">(Rae et al., 2022;</ref><ref type="bibr">Askell et al., 2021;</ref><ref type="bibr">Zeng et al., 2021;</ref><ref type="bibr">Sun et al., 2021;</ref><ref type="bibr">Smith et al., 2022;</ref><ref type="bibr">Hoffmann et al., 2022;</ref><ref type="bibr">Chowdhery et al., 2022)</ref> or does not discuss the training data in sufficient detail to determine what was done <ref type="bibr">(Kim et al., 2021)</ref>.</p><p>When the Pile was originally made, the only language model larger than GPT-NeoX-20B that existed was GPT-3, which upsampled high-quality subsets of its training data. The Pile followed suit, and due to a combination of a lack of resources for large-scale ablations and a lack of noticeable impact at smaller scales, we opt to use the Pile as-is. As shown in <ref type="figure" target="#fig_1">fig. 4</ref>, even at the 20B parameter scale we see no drop in test validation loss after crossing the one epoch boundary.</p><p>Unfortunately, none of the papers that have claimed to see an improvement from deduplication have released trained models that demonstrate this, making replication and confirmation of their results difficult. <ref type="bibr">Lee et al. (2021)</ref> releases the deduplication code that they used, which we intend to use to explore this question in more detail in the future.</p><p>It is important to note that even if there is not an improvement in loss or on task evaluations there are nevertheless compelling reasons to deduplicate training data for any model put into production. In particular, systematic analysis has shown significant benefits in terms of reducing the leakage of training data <ref type="bibr">(Lee et al., 2021;</ref><ref type="bibr">Zhang et al., 2021;</ref><ref type="bibr">Carlini et al., 2022;</ref><ref type="bibr">Kandpal et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Performance Evaluations</head><p>To evaluate our model we use the EleutherAI Language Model Evaluation Harness <ref type="bibr">(Gao et al., 2021b)</ref>, an open source codebase for language model evaluation that supports a number of model APIs. As our goal is to make a powerful model publicly accessible, we compare with English language models with at least 10B parameters that are publicly accessible. We compare with the GPT-3 models on the OpenAI API <ref type="bibr">(Brown et al., 2020)</ref>, the open source FairSeq dense models <ref type="bibr">(Artetxe et al., 2021)</ref>, and GPT-J-6B (Wang and Komatsuzaki, 2021). We do not compare against T5 <ref type="bibr">(Raffel et al., 2020)</ref> or its derivatives as our evaluation methodology assumes that the models are autoregressive. While there is a Megatron-11B checkpoint that has been publicly released, the released code is non-functional and we have not been able to get the model to work. We do not compare against any mixture-of-experts models as no public MoE model achieves performance comparable to a 10B parameter dense model.</p><p>While the size of the GPT-3 API models are not officially confirmed, we follow Gao (2021b) and assess them as being 350M (Ada), 1.3B (Babbage), 6.7B (Curie), and 175B (Da Vinci). We categorize both GPT-J-6B and GPT-NeoX-20B under the umbrella of GPT-NeoX models, as both models are trained with the same architecture and were trained on the same dataset. However, we connect them using a dashed line to reflect the fact that these two models are not the same model trained at two different scales the way the FairSeq and GPT-3 models are, having been trained using different codebases, different tokenizers, and for different numbers of tokens.</p><p>Where we were able to obtain the relevant information, we report two baselines: human-level performance and random performance. All plots contain error bars representing two standard errors, indicating the 95% confidence interval around each point. For some plots, the standard error is so small that the interval is not visible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tasks Evaluated</head><p>We evaluate our model on a diverse collection of standard language model evaluation datasets that we divide into three main categories: natural language tasks, Advanced Knowledge-Based Tasks, and Mathematical Tasks. We evalutate GPT-J-6B, GPT-NeoX-20B, and FairSeq models both zeroand five-shot, but due to financial constraints only evaluate GPT-3 models zero-shot. Due to space constraints a representative subset of the results are shown here, with the rest in Appendix D. Mathematical Tasks The solving of mathematical problem solving is an area that has had a long history of study in AI research, despite the fact that large language models tend to perform quite poorly on both arithmetic tasks and mathematical problems phrased in natural language. We evaluate on the MATH test dataset (Hendrycks et al., 2021b) as well as on the numerical arithmetic problems introduced by <ref type="bibr">Brown et al. (2020)</ref>. Note that the MATH test dataset is an evaluation metric that is generally finetuned on, but due to computational limitations we only evaluate models zero-and five-shot here.</p><p>Advanced Knowledge-Based Tasks We are also interested in the ability of our models to answer factual questions that (for humans) require advanced knowledge. To do this, we use a dataset of multiple choice questions in a variety of diverse domains developed by <ref type="bibr">Hendrycks et al. (2021a)</ref>. Following common practice on this dataset, we focus on results aggregated by subject area: Humanities, Social Sciences, STEM, and Miscellaneous as presented in <ref type="figure" target="#fig_3">Figure 7</ref>. We report five-shot performance to be comparable to previous work, taking our five-shot GPT-3 values from Hendrycks et al. (2021a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance Results</head><p>Natural Language Tasks While GPT-NeoX-20B outperforms FairSeq 13B on some tasks (e.g. ARC, LAMBADA, PIQA, PROST), it underperforms on others (e.g. HellaSwag, LogiQA zeroshot). In total, across the 32 evaluations we did we outpreform on 22 tasks, underperform on four tasks, and fall within the margin of error on six tasks. By far our weakest performance is on Hel-laSwag, where we score four standard deviations below FairSeq 13B in both zero-and five-shot evaluations. Similarly, GPT-J underperforms FairSeq 6.7B by three standard deviations zero-shot and six standard deviations five-shot on HellaSwag. We find this massive performance loss largely inexplicable; while we originally assumed that the substantial non-prose components of the Pile were to blame, we note that GPT-J and GPT-NeoX overpreform FairSeq models on the very similar Lambada task by roughly the same amount.</p><p>Mathematics While GPT-3 and FairSeq models are generally quite close on arithmetic tasks, they are consistently out-performed by GPT-J and GPT-NeoX. We conjecture that this is traceable to the prevalence of mathematics equations in the training data, but warn that people should not assume that this means that training on the Pile produces better out-of-distribution arithmetic reasoning. <ref type="bibr">Razeghi et al. (2022)</ref> show that there is a strong correlation between the frequency of a numerical equation in the Pile and GPT-J's performance on that equation, and we see no reason this would not hold in GPT-NeoX 20B, FairSeq, and GPT-3. We are unfortunately unable to investigate this effect in FairSeq and GPT-3 models because the authors do not release their training data.</p><p>Advanced Knowledge-Based Tasks While GPT-NeoX and FairSeq models both exhibit dominant performance on MMMLU compared to GPT-3 in the five-shot setting <ref type="figure" target="#fig_3">(Figure 7)</ref>, their performance is much closer in the zero-shot setting (Tables 10 to 13). <ref type="bibr">Hendrycks et al. (2021b)</ref> claim to find that few-shot evaluation does not improve performance relative to zero-shot, but they only study GPT-3. By contrast, we find that GPT-NeoX and FairSeq models do improve substantially with as few as five examples. We view this as a warning against drawing strong conclusions about evaluation metrics based only on one model, and encourage researchers developing new evaluation benchmarks to leverage multiple different classes of models to avoid overfitting their conclusions to a specific model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Powerful Few-Shot Learning</head><p>Our experiments indicate that GPT-J-6B and GPT-NeoX-20B benefit substantially more from fewshot evaluations than the FairSeq models do. When going from 0-shot to 5-shot evaluations, GPT-J-6B improves by 0.0526 and GPT-NeoX-20B improves <ref type="figure">Figure 6</ref>: Zero-shot performance of GPT-NeoX-20B compared to and FairSeq and OpenAI models on arithmetic tasks and MATH. Random performance on these tasks is 0%, and we were unable to find information on median human performance. by 0.0598 while the FairSeq 6.7B and 13B models improve by 0.0051 and 0.0183 respectively. This result is statistically significant and robust to perturbations of prompting. While we do not have a particular explanation for this currently, we view this as a strong recommendation for our models. While we do not have systematic five-shot evaluations of GPT-3 due to financial limitations, the change in performance demonstrated in tables 10 to 13 and <ref type="figure" target="#fig_3">fig. 7</ref> further supports the suggestion that GPT-J-6B and GPT-NeoX-20B are able to gain significantly more utility from five-shot examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Limitations</head><p>Optimal Training Hyperparameter tuning is an expensive process, and is often infeasible to do at full scale for multi-billion parameter models. Due to the aforementioned limitations, we opted to choose hyperparameters based on a mixture of experiments at smaller scales and by interpolating parameters appropriate for our model size based on previously published work <ref type="bibr">(Brown et al., 2020</ref>  <ref type="bibr">(2020)</ref>. As such, it is almost certainly the case that the hyperparameters used for our model are no longer optimal, and potentially never were.</p><p>Lack of Coding Evaluations Many of the design choices we made during the development of this model were oriented towards improving performance on coding tasks. However, we underestimated the difficulty and cost of existing coding benchmarks (Chen et al., 2021), and so were unable to evaluate out model in that domain. We hope to do so in the future.</p><p>Data Duplication Finally, the lack of dataset deduplication could also have had an impact on downstream performance. Recent work has shown that deduplicating training data can have a large effect on perplexity <ref type="bibr">(Lee et al., 2021)</ref>. While our experiments show no sign of this, it is hard to dismiss it due to the number of researchers who have found the opposite result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Releasing a 20B Parameter LLM</head><p>The current status quo in research is that large language models are things people train and publish about, but do not actually release. To the best of our knowledge, GPT-NeoX-20B is the largest and most performant dense language model to ever be publicly released. A variety of reasons for the nonrelease of large language models are given by various groups, but the primary one is the harms that public access to LLMs would purportedly cause.</p><p>We take these concerns quite seriously. However, having taken them quite seriously, we feel that they are flawed in several respects. While a thorough analysis of these issues is beyond the scope of this paper, the public release of our model is the most important contribution of this paper and so an explanation of why we disagree with the prevailing wisdom is important.</p><p>Providing access to ethics and alignment researchers will prevent harm. The open-source release of this model is motivated by the hope that it will allow researchers who would not otherwise have access to LLMs to use them. While there are negative risks due to the potential acceleration of capabilities research, we believe the benefits of this release outweigh the risks. We also note that these benefits are not hypothetical, as a number of papers about the limits and ethics of LLMs has been explicitly enabled by the public release of previous models <ref type="bibr">(Zhang et al., 2021;</ref><ref type="bibr">Kandpal et al., 2022;</ref><ref type="bibr">Carlini et al., 2022;</ref><ref type="bibr">Birhane et al., 2021;</ref><ref type="bibr">nostalgebraist, 2020;</ref><ref type="bibr">Meng et al., 2022;</ref><ref type="bibr">Lin et al., 2021)</ref>.</p><p>Limiting access to governments and corporations will not prevent harm. Perhaps the most curious aspect of the argument that LLMs should not be released is that the people making such arguments are not arguing they they should not use LLMs. Rather, they are claiming that other people should not use them. We do not believe that this is a position that should be taken seriously. The companies and governments that have the financial resources to train LLMs are overwhelmingly more likely to do large scale harm using a LLM than a random individual.</p><p>Releasing this model is the beginning, not the end, of our work to make GPT-NeoX-20B widely accessible to researchers. Due to the size of the model, inference is most economical on a pair of RTX 3090 Tis or a single A6000 GPU and finetuning requires significantly more compute. Truly promoting widespread access to LLMs means promoting widespread access to computing infrastructure in addition to the models themselves. We plan to make progress on this issue going forward by continuing to work on reducing the inference costs of our model, and by working with researchers to provide access to the computing infrastructure they need to carry out experiments on our models. We strongly encourage researchers who are interested in studying GPT-NeoX-20B but lack the necessary infrastructure to reach out to discuss how we can help empower you.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary</head><p>We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive Transformer language model trained on the Pile (Gao et al., 2020) dataset, and detail the main architectural differences between GPT-NeoX-20B and GPT-3-most notably the change in tokenizer, the addition of Rotary Positional Embeddings, the parallel computation of attention and feed-forward layers, and a different initialization scheme and hyperparameters. We run extensive evaluations of GPT-NeoX-20B on natural language and factual knowledge tasks, and compare it with other publicly available models, finding it performs particularly well on knowledge-based and mathematical tasks. Finally, we are open sourcing the training and evaluation code at https://github. com/EleutherAI/gpt-neox, where readers can find a link to download the model weights across the whole training run. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Broader Impacts</head><p>The current status quo in research is that large language models are things people train and publish about, but do not actually release. To the best of our knowledge, GPT-NeoX-20B is the largest dense language model to ever be publicly released with a several-way tie for second place at 13 billion parameters <ref type="bibr">(Artetxe et al., 2021;</ref><ref type="bibr">Xue et al., 2020</ref><ref type="bibr">Xue et al., , 2022</ref> and many more models at the 10-11B parameter scale. A variety of reasons for the non-release of large language models are given by various groups, but the primary one is the harms that public access to LLMs would purportedly cause. We take these concerns quite seriously. However, having taken them quite seriously, we feel that they are flawed in several respects. While a thorough analysis of these issues is beyond the scope of this paper, the public release of our model is the most important contribution of this paper and so an explanation of why we disagree with the prevailing wisdom is important.</p><p>Providing access to ethics and alignment researchers will prevent harm. The open-source release of this model is motivated by the hope that it will allow researchers who would not otherwise have access to LLMs to use them. While there are negative risks due to the potential acceleration of capabilities research, we believe the benefits of this release outweigh the risks. We also note that these benefits are not hypothetical, as a number of papers about the limits and ethics of LLMs has been explicitly enabled by the public release of previous models <ref type="bibr">(Zhang et al., 2021;</ref><ref type="bibr">Kandpal et al., 2022;</ref><ref type="bibr">Carlini et al., 2022;</ref><ref type="bibr">Birhane et al., 2021;</ref><ref type="bibr">nostalgebraist, 2020;</ref><ref type="bibr">Meng et al., 2022;</ref><ref type="bibr">Lin et al., 2021)</ref>.</p><p>Limiting access to governments and corporations will not prevent harm. Perhaps the most curious aspect of the argument that LLMs should not be released is that the people making such arguments are not arguing they they should not use LLMs. Rather, they are claiming that other people should not use them. We do not believe that this is a position that should be taken seriously. The companies and governments that have the financial resources to train LLMs are overwhelmingly more likely to do large scale harm using a LLM than a random individual.</p><p>The open-source release of this model is motivated by the hope that it will allow ethics and alignment researchers who would not otherwise have access to LLMs to use them. While there are negative risks due to the potential acceleration of capabilities research, we believe the benefits of this release outweigh the risks of accelerating capabilities research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Impact on Capabilities Research and Products</head><p>When discussing the impact of access to technology, it is important to distinguish between capacities research which seeks to push the current stateof-the-art and research on We feel the risk of releasing GPT-NeoX-20B is acceptable, as the contribution of the model to capabilities research is likely to be limited, for two reasons.</p><p>We ultimately believe that the benefits of releasing this model outweigh the risks, but this argument hinges crucially on the particular circumstances of this release. All actors considering releasing powerful AI models or advancing the frontier of capabilities should think carefully about what they release, in what way, and when.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Impact on Ethics and Alignment Research</head><p>To oversimplify a complex debate, there are broadly speaking two schools of thought regarding the mitigation of harm that is done by AI algorithms: AI Ethics and AI Alignement. AI Ethics researchers are primarily concerned with the impact of current technologies or technologies very similar to current technologies, while AI Alignment is primarily concerned with future "generally intelligent" systems whose capacities greatly outclass currently existing systems and possess human and superhuman levels of intelligence. While the tools, methods, and ideas of these camps are very different, we believe that increasing access to these technologies will empower and advance the goals of researchers in both schools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.1 The Necessity of Model Access for AI Ethics</head><p>Analyzing and documenting the limitations of models is an essential aspect of AI ethics research <ref type="bibr">(Matias, 2020)</ref>. Work examining and criticizing datasets <ref type="bibr">(Kreutzer et al., 2022;</ref><ref type="bibr">Dodge et al., 2021;</ref><ref type="bibr">Birhane et al., 2021)</ref>, functionality <ref type="bibr">(Smart, 2021;</ref><ref type="bibr">Zhang et al., 2021;</ref><ref type="bibr">Carlini et al., 2022;</ref><ref type="bibr">Biderman and Raff, 2022)</ref>, evaluation and deployment procedures <ref type="bibr">(Biderman and Scheirer, 2020;</ref><ref type="bibr">Talat et al., 2022)</ref>, and more are essential to well-rounded and informed debate on the value and application of technology.</p><p>However the current centralization of LLM training also creates a centralization of control of technology <ref type="bibr">(Sadowski et al., 2021;</ref><ref type="bibr">Whittaker, 2021)</ref> that makes meaningful independent evaluation impossible. This means that it is often not possible to do this kind of work in practice because of the severe access restrictions companies that own large language models put on them. While GPT-NeoX is the 13th largest dense language model at time of writing only model larger than GPT-NeoX 20B that is publicly accessible is GPT-3. There are significant limitations on people's ability to do research on GPT-3 though, as it is not free to use and its training data is private.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.2 The Usefulness of Large Language Models in Alignment</head><p>LLMs represent a different paradigm than the AI systems generally studied by alignment researchers because they are not well-described as coherent agents or expected utility maximizers. Though trained to optimize a log-likelihood loss function, at a high level the goals a LLM pursues are varied and contradictory, depending on the way it is prompted. This introduces additional challenges, but may also enable new approaches to alignment. GPT-NeoX-20B itself is not the system we need to align, but we hope it can serve as a publicly available platform for experiments whose results might generalize to crucial future work.</p><p>The following is a non-exhaustive list of potential approaches we consider promising for further investigation.</p><p>Mechanistic interpretability. Mechanistic interpretability research (Cammarata et al., 2020) hopes to gain an understanding into how models accomplish the tasks they do, in part in the hopes of detecting problematic or deceptive algorithms implemented by models before these failures manifest in the real world. Being able to interpret and inspect the detailed inner workings of trained models would be a powerful tool to ensure models are optimizing for the goals we intended <ref type="bibr">(Hubinger et al., 2021;</ref><ref type="bibr">Koch et al., 2021)</ref>. Reverse engineering transformer language models has already yielded insights about the inner functioning of LMs <ref type="bibr">(Elhage et al., 2021;</ref><ref type="bibr">nostalgebraist, 2020;</ref><ref type="bibr">Meng et al., 2022;</ref><ref type="bibr">Dai et al., 2021)</ref>.</p><p>Using a LLM as a reward model. Because they are trained to predict human writing, LLMs also appear to develop a useful representation of human values at the semantic level. Finding a way to utilise these representations could be a possible path toward solving the problem of reward robustness in RL and other algorithms which require a proxy of human judgment <ref type="bibr">(Stiennon et al., 2022;</ref><ref type="bibr">Wentworth, 2020)</ref>. Despite fundamental theoretical limitations on learning human values <ref type="bibr" target="#b0">(Armstrong and Mindermann, 2018;</ref><ref type="bibr">Kosoy, 2016)</ref>, value learning may still be robust enough to align weaker superhuman AIs. Future experiments could explore the extent to which LLM pretraining improves downstream reward model robustness and generalization.</p><p>Natural language transparency. Since LLM prompts are in a human-readable form, it can provide insight on the LLM's expected behavior. Prompt programming or finetuning can be used to leverage this fact and force a LLM to execute more transparent algorithms, such as splitting problems into steps or explicitly writing an "internal monologue" <ref type="bibr">(Soares, 2021;</ref><ref type="bibr">Gao et al., 2021a;</ref><ref type="bibr">Nye et al., 2021)</ref>. Reliability and trustworthiness can present significant challenges for these approaches.</p><p>However, this form of transparency also has its limits. In particular, models can often respond unpredictably to prompts, and internal monologues may become completely detached from the model's decision making process if translating between the model's ontology and the human ontology is more complex than simply modeling human monologues <ref type="bibr">(Christiano et al., 2021)</ref>.</p><p>Simulating agents at runtime. Although LLMs are not well-described as coherent agents, they can still be used to generate goal-directed processes. Given an appropriate prompt (such as a story of a character working to achieve a goal), LLMs can predict and thus simulate an agent <ref type="bibr">(Huang et al., 2022)</ref>. Simulated agents take representative actions according to the patterns present in the training data, similar to behavior cloning. One potential future research direction is testing whether they are less susceptible to failure modes that follow from expected utility maximization, such as Goodhart failures and power-seeking behavior. However, other failure modes can be introduced by the LM training procedure, such as "delusions" or "hallucinations" <ref type="bibr">(Ortega et al., 2021;</ref><ref type="bibr">Gao, 2021a;</ref><ref type="bibr">Maynez et al., 2020)</ref>. Additionally, simulated agents may be uncompetitive with optimal agents like those produced by Reinforcement Learning. An important research direction is to explore how the beneficial properties of simulated agents can be maintained while making them competitive with RL based approaches.</p><p>Tool AI and automated alignment research. LMs can be used as relatively unagentic tools, such as OpenAI's Codex model (Chen et al., 2021) acting as a coding assistant. Because pretrained LLMs are not directly optimized for the factual accuracy of their predictions, it is possible they avoid some of the traditional problems with tool or oracle AI <ref type="bibr" target="#b1">(Armstrong et al., 2012)</ref>, such as the incentive to produce manipulative answers (Demski, 2019). Tool AI is not a long-term solution to the problem of alignment, but it could be used to assist alignment research or even automate large parts of it. For example, language models could be used to help brainstorm alignment ideas more quickly, act as a writing assistant, or directly generate alignment research papers for humans to review. This line of research also risks accelerating capabilities research, a concern we discuss more below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Differential Impact on Access</head><p>Because training large models requires a significant engineering and capital investment, such models are often out of reach for small labs and independent researchers. As it stands, only large organizations have access to the latest generation of powerful language models <ref type="bibr">(Brown et al., 2020;</ref><ref type="bibr">Rae et al., 2022;</ref><ref type="bibr">Fedus et al., 2021;</ref><ref type="bibr">Lieber et al., 2021;</ref><ref type="bibr">Tang, 2021)</ref>. The number of researchers focused primarily on ethics and alignment working at these labs is much lower than those working on developing new capabilities.</p><p>We feel the risk of releasing GPT-NeoX-20B is acceptable, as the contribution of the model to capabilities research is likely to be limited, for two reasons. Firstly, the organizations pursuing capabilities research most aggressively are unlikely to benefit from our open-source release of this model as they have already developed more powerful models of their own. Secondly, we believe the single most important piece of knowledge that drives advancing capabilities research is the knowledge that scaling LLMs was possible in the first place <ref type="bibr">(Leahy, 2021;</ref><ref type="bibr">Leahy and Biderman, 2021)</ref>. Whereas the actual implementation is very fungible (as evidenced by the large number of parties who have succeeded in creating their own LLMs in the past two years). This differential impact, wherein our release is expected to benefit primarily people who have less funding and infrastructure, is a key factor in our decision to release this model publicly.</p><p>We ultimately believe that the benefits of releasing this model outweigh the risks, but this argument hinges crucially on the particular circumstances of this release. All actors considering releasing powerful AI models or advancing the frontier of capabilities should think carefully about what they release, in what way, and when.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Environmental Impact</head><p>A significant point of concern in some recent work is the energy usage and carbon emissions associated with training large language models <ref type="bibr">(Strubell et al., 2019;</ref><ref type="bibr">Schwartz et al., 2020;</ref><ref type="bibr">Lacoste et al., 2019;</ref><ref type="bibr" target="#b5">Bender et al., 2021)</ref>. In particular, Strubell et al. <ref type="formula">(2019)</ref> estimate that a then-recent paper by the authors released 626, 155 lbs or 284.01 metric tons 14 of CO 2 (t CO 2 ). As Strubell et al. <ref type="bibr">(2019)</ref> has been widely cited and quoted in the media as representative of large-scale language models, we decided to explicitly and carefully track our energy usage and carbon emissions to see if this is truly a representative account of NLP emissions.</p><p>Throughout the development and training of our model, we tracked our energy usage and carbon emissions. We found that the process of developing and training GPT-NeoX-20B emitted almost exactly 10% of Strubell et al. (2019)'s estimate, coming in at a total of 69957 lbs or 31.73 metric tons of CO 2 . This is roughly the equivalent of the yearly emissions of the average American or 35 round-trip flights between New York City and San Francisco. Our systems were based in Illinois, USA, and consumed energy sourced from the mix as follows This mixture produces an average of 0.47905 t CO 2 /MWh, and we consumed a total of 43.92 MWh of electricity over the course of 1830 hours of training. Scaling, testing, and evaluation were responsible for the equivalent of another 920 hours on our systems, for a total energy consumption 66.24 MWh and thus the production of just under 35 metric tons of CO 2 .</p><p>It is noteworthy that Strubell et al. <ref type="formula">(2019)</ref> are estimating emissions from a neural architecture search paper, and is therefore not directly comparable to ours. The primary motivation for our comparison is that their number has attracted a lot of attention and is often taken to be respresentative of NLP research. In general, we advocate for more systematic and comprehensive reporting to improve transparency surrounding this important topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Full Evaluation Results</head><p>Results for natural language understanding tasks are shown in Tables 2 and 3, while results for Hendrycks tasks are found in ????????.</p><p>All evaluations had version 0 in the Evaluation Harness. This information is reported in the output of the Evaluation Harness and should be used for ensuring reproducibility of these results, even as the task implementations themselves may change to fix bugs.  0.195 ? 0.012 0.233 ? 0.012 0.263 ? 0.013 0.296 ? 0.013 0.329 ? 0.014 0.345 ? 0.014 OpenBookQA 0.168 ? 0.017 0.190 ? 0.018 0.238 ? 0.019 0.254 ? 0.019 0.292 ? 0.020 0.296 ? 0.020 HeadQA (English) 0.233 ? 0.008 0.233 ? 0.008 0.256 ? 0.008 0.264 ? 0.008 0.280 ? 0.009 0.280 ? 0.009 LogiQA 0.220 ? 0.016 0.230 ? 0.017 0.214 ? 0.016 0.212 ? 0.016 0.232 ? 0.017 0.240 ? 0.017 PROST 0.215 ? 0.003 0.257 ? 0.003 0.257 ? 0.003 0.230 ? 0.003 0.272 ? 0.003 0.252 ? 0.003 <ref type="bibr">QA4MRE (2013)</ref> 0.285 ? 0.027 0.335 ? 0.028 0.327 ? 0.028 0.380 ? 0.029 0.370 ? 0.029 0.380 ? 0.029   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-J</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-NeoX</head><formula xml:id="formula_2">GPT-J GPT-NeoX GPT-3 Task 6B 20B Ada Babbage Curie DaVinci 1DC</formula><p>0.088 ? 0.006 0.098 ? 0.007 0.029 ? 0.000 0.001 ? 0.000 0.024 ? 0.000 0.098 ? 0.000 2D+ 0.238 ? 0.010 0.570 ? 0.011 0.006 ? 0.000 0.009 ? 0.000 0.025 ? 0.000 0.769 ? 0.000 2Dx 0.139 ? 0.008 0.148 ? 0.008 0.022 ? 0.000 0.021 ? 0.000 0.058 ? 0.000 0.198 ? 0.000 2D-0.216 ? 0.009 0.680 ? 0.010 0.013 ? 0.000 0.013 ? 0.000 0.076 ? 0.000 0.580 ? 0.000 3D+ 0.088 ? 0.006 0.099 ? 0.007 0.001 ? 0.000 0.001 ? 0.000 0.003 ? 0.000 0.342 ? 0.000 3D-0.046 ? 0.005 0.344 ? 0.011 0.001 ? 0.000 0.001 ? 0.000 0.004 ? 0.000 0.483 ? 0.000 4D+ 0.007 ? 0.002 0.007 ? 0.002 0.001 ? 0.000 0.000 ? 0.000 0.001 ? 0.000 0.040 ? 0.000 4D-0.005 ? 0.002 0.029 ? 0.004 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.075 ? 0.000 5D+ 0.001 ? 0.001 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.006 ? 0.000 5D-0.000 ? 0.000 0.004 ? 0.001 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.008 ? 0.000 MATH (Algebra) 0.013 ? 0.003 0.010 ? 0.003 0.003 ? 0.002 0.008 ? 0.003 0.003 ? 0.002 0.008 ? 0.003 MATH (Counting and Probability) 0.011 ? 0.005 0.017 ? 0.006 0.000 ? 0.000 0.004 ? 0.003 0.000 ? 0.000 0.006 ? 0.004 MATH (Geometry) 0.004 ? 0.003 0.017 ? 0.006 0.000 ? 0.000 0.000 ? 0.000 0.002 ? 0.002 0.002 ? 0.002 MATH (Intermediate Algebra) 0.004 ? 0.002 0.001 ? 0.001 0.000 ? 0.000 0.003 ? 0.002 0.006 ? 0.002 0.003 ? 0.002 MATH <ref type="table">(Number Theory)</ref> 0.007 ? 0.004 0.013 ? 0.005 0.007 ? 0.004 0.000 ? 0.000 0.006 ? 0.003 0.011 ? 0.005 MATH (Pre-Algebra) 0.010 ? 0.003 0.018 ? 0.005 0.007 ? 0.003 0.006 ? 0.003 0.008 ? 0.003 0.014 ? 0.004 MATH (Pre-Calculus) 0.005 ? 0.003 0.005 ? 0.003 0.004 ? 0.003 0.000 ? 0.000 0.002 ? 0.002 0.004 ? 0.003 0.001 ? 0.001 0.000 ? 0.000 0.000 ? 0.000 0.011 ? 0.002 0.024 ? 0.003 0.001 ? 0.001 2D+ 0.005 ? 0.002 0.001 ? 0.001 0.002 ? 0.001 0.009 ? 0.002 0.019 ? 0.003 0.020 ? 0.003 2Dx 0.020 ? 0.003 0.004 ? 0.001 0.018 ? 0.003 0.023 ? 0.003 0.036 ? 0.004 0.028 ? 0.004 2D-0.005 ? 0.002 0.002 ? 0.001 0.006 ? 0.002 0.013 ? 0.002 0.013 ? 0.003 0.015 ? 0.003 3D+ 0.001 ? 0.001 0.001 ? 0.001 0.001 ? 0.001 0.001 ? 0.001 0.001 ? 0.001 0.001 ? 0.001 3D-0.002 ? 0.001 0.001 ? 0.001 0.002 ? 0.001 0.002 ? 0.001 0.002 ? 0.001 0.002 ? 0.001 4D+ 0.001 ? 0.001 0.000 ? 0.000 0.001 ? 0.001 0.001 ? 0.001 0.001 ? 0.001 0.001 ? 0.001 4D-0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 5D+ 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 5D-0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 MATH (Algebra) 0.000 ? 0.000 0.000 ? 0.000 0.001 ? 0.001 0.003 ? 0.002 0.004 ? 0.002 0.003 ? 0.001 MATH (Counting and Probability) 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.004 ? 0.003 0.000 ? 0.000 MATH (Geometry) 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.002 ? 0.002 0.000 ? 0.000 0.000 ? 0.000 MATH (Intermediate Algebra) 0.000 ? 0.002 0.000 ? 0.002 0.000 ? 0.000 0.001 ? 0.001 0.006 ? 0.002 0.002 ? 0.002 MATH <ref type="table">(Number Theory)</ref> 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.002 ? 0.002 0.000 ? 0.000 0.004 ? 0.003 MATH (Pre-Algebra) 0.000 ? 0.000 0.000 ? 0.000 0.003 ? 0.002 0.002 ? 0.002 0.001 ? 0.001 0.000 ? 0.000 MATH (Pre-Calculus) 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.002 ? 0.002 0.000 ? 0.000 0.000 ? 0.000 </p><formula xml:id="formula_3">GPT-J GPT-NeoX GPT-3 Task 6B 20B Ada Babbage Curie DaVinci 1DC 0.192 ? 0.009 0.191 ? 0.009 - - - - 2D+ 0.880 ? 0.007 0.992 ? 0.002 - - - - 2Dx 0.282 ? 0.010 0.452 ? 0.011 - - - - 2D- 0.817 ? 0.009 0.942 ? 0.005 - - - - 3D+ 0.357 ? 0.011 0.599 ? 0.011 - - - - 3D- 0.497 ? 0.011 0.819 ? 0.009 - - - - 4D+ 0.058 ? 0.005 0.152 ? 0.008 - - - - 4D- 0.092 ? 0.006 0.151 ? 0.008 - - - - 5D+</formula><p>0.009 ? 0.002 0.033 ? 0.004 ----5D-0.021 ? 0.003 0.059 ? 0.005 ----MATH (Algebra) 0.032 ? 0.005 0.049 ? 0.006 ----MATH (Counting and Probability) 0.036 ? 0.009 0.030 ? 0.008 ----MATH (Geometry) 0.027 ? 0.007 0.015 ? 0.005 ----MATH (Intermediate Algebra) 0.024 ? 0.005 0.021 ? 0.005 ----MATH <ref type="table">(Number Theory)</ref> 0.044 ? 0.009 0.065 ? 0.011 ----MATH (Pre-Algebra) 0.052 ? 0.008 0.057 ? 0.008 ----MATH (Pre-Calculus) 0.013 ? 0.005 0.027 ? 0.007 ---- 0.019 ? 0.003 0.024 ? 0.003 0.029 ? 0.004 0.032 ? 0.004 0.046 ? 0.005 0.046 ? 0.005 2D+ 0.005 ? 0.002 0.004 ? 0.001 0.006 ? 0.002 0.029 ? 0.004 0.034 ? 0.004 0.051 ? 0.005 2Dx 0.001 ? 0.001 0.025 ? 0.004 0.025 ? 0.003 0.025 ? 0.003 0.049 ? 0.005 0.053 ? 0.005 2D-0.007 ? 0.002 0.011 ? 0.002 0.008 ? 0.002 0.013 ? 0.003 0.018 ? 0.003 0.030 ? 0.004 3D+ 0.002 ? 0.001 0.002 ? 0.001 0.001 ? 0.001 0.003 ? 0.001 0.001 ? 0.001 0.003 ? 0.001 3D-0.002 ? 0.001 0.004 ? 0.001 0.003 ? 0.001 0.003 ? 0.001 0.002 ? 0.001 0.003 ? 0.001 4D+ 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 4D-0.001 ? 0.001 0.000 ? 0.000 0.000 ? 0.000 0.001 ? 0.001 0.000 ? 0.000 0.000 ? 0.000 5D+ 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 5D-0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 0.000 ? 0.000 MATH (Algebra) 0.023 ? 0.004 0.010 ? 0.003 0.013 ? 0.003 0.014 ? 0.003 0.017 ? 0.004 0.012 ? 0.003 MATH (Counting and Probability) 0.008 ? 0.004 0.004 ? 0.003 0.015 ? 0.006 0.017 ? 0.006 0.015 ? 0.006 0.017 ? 0.006 MATH (Geometry) 0.000 ? 0.000 0.013 ? 0.005 0.006 ? 0.004 0.015 ? 0.005 0.015 ? 0.005 0.006 ? 0.004 MATH (Intermediate Algebra) 0.010 ? 0.003 0.002 ? 0.002 0.007 ? 0.003 0.010 ? 0.003 0.011 ? 0.003 0.004 ? 0.002 MATH <ref type="table">(Number Theory)</ref> 0.019 ? 0.006 0.009 ? 0.004 0.007 ? 0.004 0.011 ? 0.005 0.028 ? 0.007 0.019 ? 0.006 MATH (Pre-Algebra) 0.013 ? 0.004 0.008 ? 0.003 0.010 ? 0.003 0.011 ? 0.004 0.021 ? 0.005 0.013 ? 0.004 MATH (Pre-Calculus) 0.002 ? 0.002 0.002 ? 0.002 0.004 ? 0.003 0.000 ? 0.000 0.002 ? 0.002 0.000 ? 0.000     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Tokenizer Analysis</head><p>Both tokenizers share 36938 out of 50257 tokens, a ?73.5% overlap in tokens. In this section, we perform comparison between the GPT-NeoX-20B tokenizer to the GPT-2 tokenizer using the validation set of the Pile.</p><p>In <ref type="table" target="#tab_8">Table 15</ref>, we show the resulting number of tokens from tokenizing each component of the Pile's validation set with both tokenizers, and the ratio of GPT-NeoX-20B tokens to GPT-2 tokens.</p><p>We observe that the GPT-NeoX-20B tokenizer represents all Pile components using fewer or very closely comparable numbers of tokens. The largest percentage improvement in token counts are in the EuroParl, GitHub, and PubMed Central components, with a more than 20% savings in the number of tokens needed to represent that component. We highlight that arXiv, GitHub, and StackExchange-subsets with large code components-can be represented with meaningfully fewer tokens with the GPT-NeoX-20B tokenizer compared to the GPT-2 tokenizer. Overall, the GPT-NeoX-20B tokenizer represents the Pile validation set with approximately 10% fewer tokens compared to the GPT-2 tokenizer.</p><p>Given that the GPT-NeoX-20B tokenizer is tweaked to better tokenize whitespace, we also perform a comparison between the two tokenizers excluding whitespace. We perform the same analysis as the above, but exclude all whitespace tokens from our computations, only counting the non-whitespace tokens. A token is considered a whitespace token if it consists only of whitespace characters. The results are shown in <ref type="table" target="#tab_9">Table 16</ref> in the Appendix. We observe that the GPT-NeoX-20B tokenizer still uses 5% fewer tokens to represent the Pile validation set compared to the GPT-2 tokenizer. As expected, the token ratios for certain components such as GitHub and StackExchange become closer to even once the whitespace characters are excluded.</p><p>GPT-2 GPT-NeoX-20B GPT-NeoX-20B  While we evaluated our tokenizer using the validation set for the Pile, the Pile components would still be considered in-domain for the tokenizer and may not provide the most informative comparison point. To perform an out-of-domain comparison, we perform the same analysis using the AllenAI replication of C4, 15 , another popular pretraining corpus for large language models. As above, we use the validation set for our analysis. Our results are shown in <ref type="table" target="#tab_7">Table 14</ref>. We find that the GPT-NeoX-20B tokenizer tokenizes the C4 validation set to approximately the same number of tokens as the GPT-2 tokenizer. When excluding all whitespace tokens, the GPT-NeoX-20B requires approximately 1% more tokens to represent the corpus compared to the GPT-2 tokenizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Tokenizer Comparisons</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1.1 Longest Tokens</head><p>We show in <ref type="table" target="#tab_10">Table 17</ref> the 10 longest tokens in each tokenizer vocabulary. We exclude consideration of tokens that comprise only symbols or whitespace characters. We observe that for the GPT-2 tokenizer, many of the longest tokens appear to reflect artifacts in the tokenizer training data, likely with certain websites or web-scrapes being overrepresented in the training data. For the GPT-NeoX-20B tokenizer, we observe that most of the longest tokens are scientific terms, likely arising from the PubMed components of the Pile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1.2 Worst Case Word Tokenization Comparison</head><p>We consider the words for which there is the greatest discrepancy in the resulting token length between the two tokenizers, where one tokenizer needs many tokens to represent while the other tokenizer uses GPT-2 GPT-NeoX-20B GPT-NeoX-20B   relatively few tokens. We define a word as a contiguous string delimited by whitespace or punctuation (as defined by strings.punctuation in Python). We perform this analysis at the component level. We only consider words that occur at least 10 times within the given component. We show in <ref type="table" target="#tab_11">Table 18</ref> a representative example from the Pile-CC corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Tokenization Examples</head><p>In <ref type="figure" target="#fig_0">Figures 8 and 13</ref>   GPT-2 Tokenization 253 tokens --? abstract: 'The maximal minors of a $p\times (m + p)$-matrix of univariate polynomials of degree $n$ with indeterminate coefficients are themselves polynomials of degree $np$. The subalgebra generated by their coefficients is the coordinate ring of the quantum Grassmannian, a singular compactification of the space of rational curves of degree $np$ in the Grassmannian of $p$planes in ($m + p$)-space. These subalgebra generators are shown to form a sagbi basis. The resulting flat deformation from the quantum Grassmannian to a toric variety gives a new " Gr? bner basis style" proof of the Ravi-Rosenthal-Wang formulas in quantum Schubert calculus. The coordinate ring of the quantum Grassmannian is an algebra with straightening law, which is normal, Cohen-Macaulay, Gorenstein and Koszul, and the ideal of quantum Pl?cker relations has a quadratic Gr?bner basis. This holds more generally for skew quantum Schubert varieties. These results are well-known for the classical Schubert varietie GPT-NeoX-20B Tokenization 229 tokens --? abstract: 'The maximal minors of a $p\times (m + p)$-matrix of univariate polynomials of degree $n$ with indeterminate coefficients are themselves polynomials of degree $np$. The subalgebra generated by their coefficients is the coordinate ring of the quantum Grassmannian, a singular compactification of the space of rational curves of degree $np$ in the Grassmannian of $p$planes in ($m + p$)-space. These subalgebra generators are shown to form a sagbi basis. The resulting flat deformation from the quantum Grassmannian to a toric variety gives a new "Gr? bner basis style" proof of the Ravi-Rosenthal-Wang formulas in quantum Schubert calculus. The coordinate ring of the quantum Grassmannian is an algebra with straightening law, which is normal, Cohen-Macaulay, Gorenstein and Koszul, and the ideal of quantum Pl?cker relations has a quadratic Gr?bner basis. This holds more generally for skew quantum Schubert varieties. These results are well-known for the classical Schubert varietie This is a work of fiction. Names, characters, places and incidents are products of the author 's imagination or are used fictitiously and are not to be construed as real. Any resemblance to actual events, locales, organizations, or persons, living or dead, is completely coincidental. ? ? www.beverleykendall.com? ? Cover Design ? Okay Creations, Sarah Hansen? ? All rights reserved. Except as permitted under the U.S. Copyright Act of 1976, no part of this publication may be reproduced, distributed or transmitted in any form or by any means, or stored in a database or retrieval system, without the prior written permission of the author .? ? ** License Statement **? ? This ebook is licensed for your personal enjoyment only. This ebook may not be re-sold or given away to other people. If you would like to share this book with another person, please purchase an additional copy for each reader. If GPT-NeoX-20B Tokenization 228 tokens ? ? **THE TRAP**? ? Beverley Kendall? ? Copyright ? Beverley Kendall 2014? ? Published by Season Publishing LLC? ? This is a work of fiction. Names, characters, places and incidents are products of the author 's imagination or are used fictitiously and are not to be construed as real. Any resemblance to actual events, locales, organizations, or persons, living or dead, is completely coincidental. ? ? www.beverleykendall.com? ? Cover Design ? Okay Creations, Sarah Hansen? ? All rights reserved. Except as permitted under the U.S. Copyright Act of 1976, no part of this publication may be reproduced, distributed or transmitted in any form or by any means, or stored in a database or retrieval system, without the prior written permission of the author .? ? ** License Statement **? ? This ebook is licensed for your personal enjoyment only. This ebook may not be re-sold or given away to other people. If you would like to share this book with another person, please purchase an additional copy for each reader. If  430 tokens &lt;at-dialog title="vm.title" on-close="vm.onClose"&gt;? &lt;at-form state="vm.form" autocomplete="off" id="external_test_form"&gt;? &lt;at-input-group col="12" tab="20" state="vm.form.inputs" form-id="external_test"&gt;&lt;/atinput-group&gt;? &lt;at-action-group col="12" pos="right"&gt;? &lt;at-action-button? variant="tertiary"? ng-click="vm.onClose()"? &gt;? ::vm.strings.get('CLOSE') ? &lt;/at-action-button&gt;? &lt;at-action-button? variant="primary"? ng-click="vm.onSubmit()"? ng-disabled="!vm.form.isValid || vm.form.disabled"? &gt;? ::vm.strings.get('RUN') ? &lt;/at-action-button&gt;? &lt;/at-action-group&gt;? &lt;/at-form&gt;? &lt;/at-dialog&gt;? GPT-NeoX-20B Tokenization 257 tokens &lt;at-dialog title="vm.title" on-close="vm.onClose"&gt;? &lt;at-form state="vm.form" autocomplete="off" id="external_test_form"&gt;? &lt;at-input-group col="12" tab="20" state="vm.form.inputs" form-id="external_test"&gt;&lt;/atinput-group&gt;? &lt;at-action-group col="12" pos="right"&gt;? &lt;at-action-button? variant="tertiary"? ng-click="vm.onClose()"? &gt;? ::vm.strings.get('CLOSE') ? &lt;/at-action-button&gt;? &lt;at-action-button? variant="primary"? ng-click="vm.onSubmit()"? ng-disabled="!vm.form.isValid || vm.form.disabled"? &gt;? ::vm.strings.get('RUN') ? &lt;/at-action-button&gt;? &lt;/at-action-group&gt;? &lt;/at-form&gt;? &lt;/at-dialog&gt;? <ref type="figure" target="#fig_0">Figure 11</ref>: Pile (GitHub) Tokenization Example GPT-2 Tokenization 178 tokens Theresa May is expected to appoint an EU ambassador who " believes in Brexit" in the wake of the current Brussels representative's decision to quit after being cut adrift by Downing Street. ? ? Sir Ivan Rogers on Tuesday announced his resignation as Britain' s ambassador in Brussels after it was made clear Mrs May and her senior team had " lost confidence" in him over his " pessim istic" view of Brexit.? ? Government sources made clear that Sir Ivan had " jumped before he was pushed" and that Number 10 believed his negative view of Brexit meant that he could not lead the negotiations after the Prime Minister triggers Article 50.? ? In a 1,400-word resignation letter to his staff leaked on Tuesday night, Sir Ivan launched a thinly-veiled attack on the "muddled thinking" in Mrs May's Government.</p><p>GPT-NeoX-20B Tokenization 170 tokens Theresa May is expected to appoint an EU ambassador who "believes in Brexit" in the wake of the current Brussels representative's decision to quit after being cut adrift by Downing Street. ? ? Sir Ivan Rogers on Tuesday announced his resignation as Britain's ambassador in Brussels after it was made clear Mrs May and her senior team had "lost confidence" in him over his "pessim istic" view of Brexit.? ? Government sources made clear that Sir Ivan had "jumped before he was pushed" and that Number 10 believed his negative view of Brexit meant that he could not lead the negotiations after the Prime Minister triggers Article 50.? ? In a 1,400-word resignation letter to his staff leaked on Tuesday night, Sir Ivan launched a thinly-veiled attack on the "muddled thinking" in Mrs May's Government. <ref type="figure" target="#fig_0">Figure 12</ref>: Pile (OpenWebText2) Tokenization Example GPT-2 Tokenization 268 tokens Carotid endarterectomy: operative risks, recurrent stenosis, and long-term stroke rates in a modern series.? To determine whether carotid endarterectomy (CEA) safely and effectively maintained a durable reduction in stroke complications over an extended period, we reviewed our data on 478 consecutive patients who underwent 544 CEA's since 1976. Follow-up was complete in 83% of patients (mean 44 months). There were 7 early deaths (1.3%), only 1 stroke related (0.2%). Peri operative stroke rates (overall 2.9%) varied according to operative indications: asymptomatic, 1 .4%; transient ischemic attacks (TIA)/amaurosis fugax (AF), 1.3%; nonhemispheric symptoms (NH), 4.9%; and prior stroke (CVA), 7.1%. Five and 10-year stroke-free rates were 96% and 92% in the asymptomatic group, 93% and 87% in the TIA/AF group, 92% and 92% in the NH group, and 80% and 73% in the CVA group. Late ipsilateral strokes occurred infrequently (8 patients, 1.7%). Late deaths were primarily cardiac related (51.3%). Stro GPT-NeoX-20B Tokenization 250 tokens Carotid endarterectomy: operative risks, recurrent stenosis, and long-term stroke rates in a modern series.? To determine whether carotid endarterectomy (CEA) safely and effectively maintained a durable reduction in stroke complications over an extended period, we reviewed our data on 478 consecutive patients who underwent 544 CEA's since 1976. Follow-up was complete in 83% of patients (mean 44 months). There were 7 early deaths (1.3%), only 1 stroke related (0.2%). Peri operative stroke rates (overall 2.9%) varied according to operative indications: asymptomatic, 1 .4%; transient ischemic attacks (TIA)/amaurosis fugax (AF), 1.3%; nonhemispheric symptoms (NH), 4.9%; and prior stroke (CVA), 7.1%. Five and 10-year stroke-free rates were 96% and 92% in the asymptomatic group, 93% and 87% in the TIA/AF group, 92% and 92% in the NH group, and 80% and 73% in the CVA group. Late ipsilateral strokes occurred infrequently (8 patients, 1.7%). Late deaths were primarily cardiac related (51.3%). Stro </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A pictorial representation of rotary embeddings, from Su et al. (2021).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Training and validation loss for GPT-NeoX-20B. As the validation loss continued to fall into the beginning of the second epoch, we decided to let it train further.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Natural</head><label></label><figDesc>Language Tasks We evaluate our model on a diverse collection of standard language model evaluation datasets: ANLI (Nie et al., 2020), ARC (Clark et al., 2018), HeadQA (English) (Vilares and G?mez-Rodr?guez, 2019), HellaSwag (Zellers et al., 2019), LAMBDADA (Paperno et al., 2016), LogiQA (Liu et al., 2020), OpenBookQA (Mihaylov et al., 2018), PiQA (Bisk et al., 2020), PROST (Aroca-Ouellette et al., 2021), QA4MRE (Pe?as et al., 2013) (2013), SciQ (Welbl et al., 2017), TriviaQA (Joshi et al., 2017), Winogrande (Sakaguchi et al., 2021), and the SuperGlue version of the Winograd Schemas Challenge (WSC) (Wang et al., 2019).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Five-shot performance of GPT-NeoX-20B compared to GPT-J-6B and FairSeq and OpenAI models on Hendrycks et al. (2021a). Due to financial limitations we were unable to evaluate on the OpenAI API. Instead, we report numbers from Hendrycks et al. (2021a) with model sizes corrected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>30% Solar (0 t CO 2 /MWh) ? 18.10% Wind (0 t CO 2 /MWh) ? 1.30% Other Renewables (0 t CO 2 /MWh)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure</head><label></label><figDesc>Figure 8: Pile (arXiv) Tokenization Example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure</head><label></label><figDesc>t = 1 + 8. Let s(d) = d**3 + 6*d**2 + 2*d + 1. Let u be s(t). Suppose 10 = 5*z, 5*a + 0*z = -z + u. Is 4 a factor of a?? True? Suppose 5*l = r -35, -2*r + 5*l -15 = -70. Is r a multiple of 4?? True? Suppose 2*l + 11 -1 = 0. Does 15 divide (-2)/l -118/(-5)?? False? Suppose 3*k -3*f + 0*f -72 = 0, -25 = -5*f. Is 9 a factor of 2/(-4) + k/2?? False? Suppose 6*w + 25 = w. Let t(c) = c + 9. Let u be t(w). Suppose -u*z = -3*z -10. Is z a multiple of 5?? True? Let j = 81 + -139. Let i = j + 101. Is 11 a factor of i?? False? Let q(s) = s**3 + 4*s**2 -s + 2. Let u be q(-4). Let o(w) = w**2 + w -6. Let t be o(u). Suppose -3*l -39 = -3*d -2*l, 0 = 3*d -2*l -t. Does 9 divide d?? False? Suppose -2*b + 39 + 13 = 0. Is b a multiple of 14?? False? Let q = -7 + 12. Suppose 8*l = q*l + 81. Suppose 129 = 4*f -l. Is 13 a factor of f?? True? Suppose 0 = -4*n + j + 33, 4*n -n + 4*j = 20. Let c = 5 -n. Is 35*1 -(-6)/c a multiple of 11? ? True? Let g(m) = m**2 -2*m -3. Let k be g(3). Let j be GPT-NeoX-20B Tokenization 468 tokens o?? True? Suppose -3*t = 1 + 8. Let s(d) = d**3 + 6*d**2 + 2*d + 1. Let u be s(t). Suppose 10 = 5*z, 5*a + 0*z = -z + u. Is 4 a factor of a?? True? Suppose 5*l = r -35, -2*r + 5*l -15 = -70. Is r a multiple of 4?? True? Suppose 2*l + 11 -1 = 0. Does 15 divide (-2)/l -118/(-5)?? False? Suppose 3*k -3*f + 0*f -72 = 0, -25 = -5*f. Is 9 a factor of 2/(-4) + k/2?? False? Suppose 6*w + 25 = w. Let t(c) = c + 9. Let u be t(w). Suppose -u*z = -3*z -10. Is z a multiple of 5?? True? Let j = 81 + -139. Let i = j + 101. Is 11 a factor of i?? False? Let q(s) = s**3 + 4*s**2 -s + 2. Let u be q(-4). Let o(w) = w**2 + w -6. Let t be o(u). Suppose -3*l -39 = -3*d -2*l, 0 = 3*d -2*l -t. Does 9 divide d?? False? Suppose -2*b + 39 + 13 = 0. Is b a multiple of 14?? False? Let q = -7 + 12. Suppose 8*l = q*l + 81. Suppose 129 = 4*f -l. Is 13 a factor of f?? True? Suppose 0 = -4*n + j + 33, 4*n -n + 4*j = 20. Let c = 5 -n. Is 35*1 -(-6)/c a multiple of 11? ? True? Let g(m) = m**2 -2*m -3. Let k be g(3). Let j be</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Pile (DM Mathematics) Tokenization Example GPT-2 Tokenization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 :</head><label>13</label><figDesc>Pile (PubMed Abstracts) Tokenization Example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Stella Biderman and Edward Raff. 2022. Neural language models are effective plagiarists. Computing Research Repository, arXiv:2201.07406. Version 1. Bisk, Rowan Zellers, Ronan Le bras, Jianfeng Gao, and Yejin Choi. 2020. PIQA: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7432-7439. Computing Research Repository, arXiv:2104.08696. Version 1. Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2021. A Mathematical Framework for Transformer Circuits. transformer-circuits.pub. Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. Computing Research Repository, arXiv:2203.15556. Version 1. Koch, Lauro Langosco, Jacob Pfau, James Le, and Lee Sharkey. 2021. Objective robustness in deep reinforcement learning. Computing Research Repository, arXiv:2105.14111. Version 2. Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of Machine Translation Summit X: Papers, pages 79-86, Phuket, Thailand. Aran Komatsuzaki. 2019. One epoch is all you need. Jacob Hilton, and Owain Evans. 2021. TruthfulQA: Measuring how models mimic human falsehoods. Computing Research Repository, arXiv:2109.07958. Version 1. Ben Wang. 2021. Mesh-Transformer-JAX: Modelparallel implementation of transformer language model with JAX.Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 billion parameter autoregressive language model. Black was the lead developer and overall point person for the project. Stella Biderman was the lead scientist and project manager.InTable 1we attach the full configuration details used to train GPT-NeoX-20B. The file is available in .yaml format usable in gpt-neox at https:// github.com/EleutherAI/gpt-neox, where we also provide documentation describing the role of each parameter.</figDesc><table><row><cell>Jordan Hoffmann, Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Computing Research Repository, arXiv:2201.07207. Version 1. Evan Hubinger, Chris van Merwijk, Vladimir Miku-lik, Joar Skalse, and Scott Garrabrant. 2021. Risks from learned optimization in advanced machine learning systems. Computing Research Repository, arXiv:1906.01820. Version 3. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale dis-tantly supervised challenge dataset for reading com-Stephanie Lin, Pierre Lison and J?rg Tiedemann. 2016. OpenSub-A Individual Contributions titles2016: Extracting large parallel corpora from movie and TV subtitles. In Proceedings of the Tenth International Conference on Language Resources Implementation and Engineering Implementation of training infrastructure: and Evaluation (LREC'16), pages 923-929, Por-toro?, Slovenia. European Language Resources As-Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Samuel Weinbach sociation (ELRA). Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020. LogiQA: A challenge dataset for machine reading comprehen-sion with logical reasoning. In Proceedings of the Twenty-Ninth International Joint Conference on Ar-Scaling experiments and optimization: Sid Black, Stella Biderman, Quentin Anthony, Samuel Weinbach Positional Embeddings: Sid Black, Eric Hallahan, Michael Pieler tificial Intelligence, IJCAI-20, pages 3622-3628. In-ternational Joint Conferences on Artificial Intelli-Tokenizer: gence Organization. Sid Black</cell><cell>Yonatan Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large scale autoregressive language modeling with Mesh-Tensorflow. William Fedus, Barret Zoph, and Noam Shazeer. 2021. Switch Transformers: Scaling to trillion parameter models with simple and efficient sparsity. Comput-ing Research Repository, arXiv:2101.03961. Ver-sion 1. Leo Gao. 2021a. Behavior cloning is miscalibrated. AI Alignment Forum. Configuration Key Value Jack Computing Research Repository, arXiv:1906.06669. Version 1. Vanessa Kosoy. 2016. IRL is hard. AI Alignment Fo-rum. Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wa-hab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Al-lahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan, Supheak-mungkol Sarin, Sokhar Samb, Beno?t Sagot, Clara Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin attention-dropout 0 bias-gelu-fusion True Guu, Adams Wei Yu, Brian Lester, Nan Du, An-drew M Dai, and Quoc V Le. 2021. Finetuned lan-checkpoint-activations True checkpoint-num-layers 1 data-impl mmap guage models are zero-shot learners. Computing Re-search Repository, arXiv:2109.01652. Version 5. distributed-backend nccl eval-interval 1000 Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. eval-iters 10 fp16.enabled True Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy User-fp16.fp16 True fp16.hysteresis 2 fp16.initial-scale-power 12 generated Text, pages 94-106, Copenhagen, Den-fp16.loss-scale 0 mark. Association for Computational Linguistics. John Wentworth. 2020. Alignment by default. AI fp16.loss-scale-window 1000 fp16.min-loss-scale 1 gpt-j-residual True Alignment Forum. gradient-accumulation-steps 32 gradient-clipping 1.0 hidden-dropout 0 hidden-size 6144</cell></row><row><cell>Abram Demski. 2019. The parable of Predict-O-Matic. AI Alignment Forum. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. Computing Research Repository, arXiv:1810.04805. Version 2. Jesse Dodge, Maarten Sap, Ana Marasovi?, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting large webtext corpora: A case study on the Colos-sal Clean Crawled Corpus. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1286-1305, Online and Punta Cana, Dominican Republic. Association for Levy, and Samuel Bowman. 2019. SuperGLUE: A Linguistics, pages 4885-4901, Online. Association Computational Linguistics. prehension. In Proceedings of the 55th Annual Meet-ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611, Van-couver, Canada. Association for Computational Lin-guistics. Nikhil Kandpal, Eric Wallace, and Colin Raffel. 2022. Deduplicating training data mitigates privacy risks in language models. Computing Research Reposi-tory, arXiv:2202.06539. Version 2. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. Computing Research Repository, arXiv:2001.08361. Version 1. Boseop Kim, HyoungSeok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak, Jeon Dong Hyeon, Sunghyun Park, Sungju Kim, Seonhoon Kim, Dong-pil Seo, Heungsub Lee, Minyoung Jeong, Sung-jae Lee, Minsub Kim, Suk Hyun Ko, Seokhun Kim, Taeyong Park, Jinuk Kim, Soyoung Kang, Na-Hyeon Ryu, Kang Min Yoo, Minsuk Chang, Soobin Suh, Sookyo In, Jinseong Park, Kyungduk Kim, Hiun Kim, Jisu Jeong, Yong Goo Yeo, Donghoon Ham, Dongju Park, Min Young Lee, Jaewook Kang, Inho Kang, Jung-Woo Ha, Woomyoung Park, and Ilya Loshchilov and Frank Hutter. 2019. Decoupled Miscellaneous: weight decay regularization. Computing Research USVSN Sai Prashanth, Ben Wang Repository, arXiv:1711.05101. Version 3. Scientific Experimentation J. Nathan Matias. 2020. Why we need industry-independent research on tech &amp; society. Citizens and Technology Lab. Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Zeerak Talat, Aur?lie N?v?ol, Stella Biderman, Miruna Evaluations: Clinciu, Manan Dey, Shayne Longpre, Alexan-Stella Biderman, Leo Gao, Jonathan Tow, dra Sasha Luccioni, Maraim Masoud, Margaret Mitchell, Dragomir Radev, Shanya Sharma, Arjun Subramonian, Jaesung Tae, Samson Tan, Deepak Sid Black, Shivanshu Purohit, Horace He, Laurence Golding Ryan McDonald. 2020. On faithfulness and factu-ality in abstractive summarization. Computing Re-search Repository, arXiv:2005.00661. Version 1. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual knowl-edge in GPT. Computing Research Repository, arXiv:2202.05262v1. Version 1. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct elec-tricity? A new dataset for open book question an-Tunuguntla, and Oskar van der Wal. 2022. You reap Positional Embeddings: what you sow: On the challenges of bias evaluation under multilingual settings. In Proceedings of the 1st Workshop on Challenges &amp; Perspectives in Cre-Stella Biderman, Laurence Golding, Michael Pieler ating Large Language Models. Association for Com-putational Linguistics. Zhixing Tan, Xiangwen Zhang, Shuo Wang, and Yang Liu. 2021. MSP: Multi-stage prompting for mak-Tokenizer: Stella Biderman, Jason Phang, Leo Gao Broader Impacts ing pre-trained language models better translators. Computing Research Repository, arXiv:2110.06609. Version 1. Alignment Implications: Leo Gao, Connor Leahy, Laria Reynolds, swering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381-2391, Brussels, Belgium. Association Kyle McDonell Jie Tang. 2021. WuDao: Pretrain the world. Keynote address at the European Conference on Machine Environmental Impact: for Computational Linguistics. Learning and Principles and Practice of Knowledge Stella Biderman, Eric Hallahan Discovery in Databases. Toan Q. Nguyen and Julian Salazar. 2019. Trans-formers without tears: Improving the normalization David Vilares and Carlos G?mez-Rodr?guez. 2019. B Full Configuration Details Nako Sung. 2021. What changes can large-scale language models bring? Intensive study on Hy-perCLOVA: Billions-scale Korean generative pre-trained transformers. In Proceedings of the 2021 of self-attention. Computing Research Repository, arXiv:1910.05895. Version 2. HEAD-QA: A healthcare dataset for complex rea-soning. In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguis-Yixin Nie, Adina Williams, Emily Dinan, Mohit tics, pages 960-966, Florence, Italy. Association for Conference on Empirical Methods in Natural Lan-Bansal, Jason Weston, and Douwe Kiela. 2020. Ad-Computational Linguistics. guage Processing, pages 3405-3424, Online and versarial NLI: A new benchmark for natural lan-Punta Cana, Dominican Republic. Association for nual Meeting of the Association for Computational Amanpreet Singh, Julian Michael, Felix Hill, Omer Computational Linguistics. guage understanding. In Proceedings of the 58th An-Alex Wang, Yada Pruksachatkun, Nikita Nangia,</cell><cell>Rivera, Annette Rios, Isabel Papadimitriou, Sa-lomey Osei, Pedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, Andre Niyongabo Rubungo, Toan Q. Nguyen, Mathias M?ller, Andr? M?ller, Sham-suddeen Hassan Muhammad, Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapi-wanashe Matangira, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine ?abuk Ball?, Stella Biderman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofe-toluwa Adeyemi. 2022. Quality at a Glance: An Au-dit of Web-Crawled Multilingual Datasets. Transac-tions of the Association for Computational Linguis-tics, 10:50-72. init-method small-init log-interval 2 lr-decay-iters 150000 lr-decay-style cosine max-position-embeddings 2048 min-lr 9.7e-06 model-parallel-size 2 no-weight-tying True norm layernorm num-attention-heads 64 num-layers 44 optimizer.params.betas [0.9, 0.95] optimizer.params.eps 1e-08 optimizer.params.lr 9.7e-05 optimizer.type Adam Computing Research Repository, arXiv:2010.11934. output-layer-init-method wang-init output-layer-parallelism column Version 1. partition-activations False pipe-parallel-size 4 Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really finish your sentence? pos-emb rotary rotary-pct 0.25 save-interval 500 In Pro-scaled-upper-triang-masked-softmax-fusion True ceedings of the 57th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 4791-seq-length 2048 split 995,4,1 4800, Florence, Italy. Association for Computational steps-per-print 2 synchronize-each-layer True Linguistics. tokenizer-type HFTokenizer train-iters 150000 train-micro-batch-size-per-gpu 4 vocab-file 20B-tokenizer.json wall-clock-breakdown False warmup 0.01 weight-decay 0.01 zero-optimization.allgather-bucket-size 1260000000 zero-optimization.allgather-partitions True zero-optimization.contiguous-gradients True zero-optimization.cpu-offload False zero-optimization.overlap-comm True zero-optimization.reduce-bucket-size 1260000000 zero-optimization.reduce-scatter True zero-optimization.stage 1 Table 1: The full configuration details for GPT-NeoX-20B training</cell></row><row><cell>for Computational Linguistics. stickier benchmark for general-purpose language un-</cell><cell></cell></row><row><cell>derstanding systems. In Advances in Neural Infor-</cell><cell></cell></row><row><cell>nostalgebraist. 2020. interpreting GPT: the logit lens. mation Processing Systems, volume 32, pages 3266-</cell><cell>Computing Research Repository, arXiv:2010.14701.</cell></row><row><cell>LessWrong. 3280. Curran Associates, Inc.</cell><cell>Version 2.</cell></row></table><note>Stella Biderman and Walter J. Scheirer. 2020. Pitfalls in machine learning research: Reexamining the de- velopment cycle. In Proceedings on "I Can't Be- lieve It's Not Better!" at NeurIPS Workshops, vol- ume 137 of Proceedings of Machine Learning Re- search, pages 106-117. PMLR. Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. 2021. Multimodal datasets: misogyny, pornography, and malignant stereotypes. Comput- ing Research Repository, arXiv:2110.01963. Ver- sion 1.Paul Christiano, Ajeya Cotra, and Mark Xu. 2021. Elic- iting latent knowledge: How to tell if your eyes de- ceive you. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question an- swering? try ARC, the AI2 Reasoning Challenge. Computing Research Repository, arXiv:1803.05457. Version 1. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. 2021. Knowledge neurons in pre- trained transformers.Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,Leo Gao. 2021b. On the sizes of openai api models. Leo Gao, Stella Biderman, Sid Black, Laurence Gold- ing, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800GB dataset of diverse text for language modeling. Computing Research Repository, arXiv:2101.00027. Version 1. Leo Gao, Kyle McDonell, Laria Reynolds, and Stella Biderman. 2021a. A preliminary explo- ration into factored cognition with language models. EleutherAI Blog. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Gold- ing, Jeffrey Hsu, Kyle McDonell, Niklas Muen- nighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021b. A framework for few-shot language model evaluation. Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg Ganger, and Phil Gibbons. 2018. PipeDream: Fast and effi- cient pipeline parallel DNN training. Computing Re- search Repository, arXiv:1806.03377. Version 1. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Stein- hardt. 2021a. Measuring massive multitask lan- guage understanding. Computing Research Repos- itory, arXiv:2009.03300. Version 3. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the MATH dataset. Comput- ing Research Repository, arXiv:2103.03874. Ver- sion 2. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schul- man, Dario Amodei, and Sam McCandlish. 2020. Scaling laws for autoregressive generative modeling.Bryan Klimt and Yiming Yang. 2004. The Enron cor- pus: A new dataset for email classification research. In Proceedings of the 15th European Conference on Machine Learning, ECML'04, page 217-226, Berlin, Heidelberg. Springer-Verlag.Meredith Whittaker. 2021. The steep cost of capture. Interactions, 28(6):50-55. Linting Xue, Aditya Barua, Noah Constant, Rami Al- Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2022. ByT5: Towards a token-free future with pre-trained byte-to-byte models. Trans- actions of the Association for Computational Lin- guistics, 10:291-306. Linting Xue, Noah Constant, Adam Roberts, Mi- hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mT5: A massively multilingual pre-trained text-to-text transformer.Sid</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Zero-Shot Results on Natural Language Understanding Tasks (GPT-J, GPT-NeoX and GPT-3)</figDesc><table><row><cell>FairSeq</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Zero-Shot Results on Natural Language Understanding Tasks (FairSeq Models)</figDesc><table><row><cell></cell><cell>GPT-J</cell><cell>GPT-NeoX</cell><cell></cell><cell></cell><cell>GPT-3</cell></row><row><cell>Task</cell><cell>6B</cell><cell>20B</cell><cell cols="3">Ada Babbage Curie DaVinci</cell></row><row><cell>ANLI Round 1</cell><cell cols="3">0.322 ? 0.015 0.312 ? 0.015 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ANLI Round 2</cell><cell cols="3">0.331 ? 0.015 0.329 ? 0.015 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ANLI Round 3</cell><cell cols="3">0.346 ? 0.014 0.342 ? 0.014 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LAMBADA</cell><cell cols="3">0.662 ? 0.007 0.698 ? 0.006 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>WSC</cell><cell cols="3">0.365 ? 0.047 0.385 ? 0.048 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HellaSwag</cell><cell cols="3">0.494 ? 0.005 0.538 ? 0.005 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Winogrande</cell><cell cols="3">0.660 ? 0.013 0.683 ? 0.013 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SciQ</cell><cell cols="3">0.913 ? 0.009 0.960 ? 0.006 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PIQA</cell><cell cols="3">0.756 ? 0.010 0.774 ? 0.010 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TriviaQA</cell><cell cols="3">0.289 ? 0.004 0.347 ? 0.004 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ARC (Challenge)</cell><cell cols="3">0.360 ? 0.014 0.410 ? 0.014 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ARC (Easy)</cell><cell cols="3">0.705 ? 0.009 0.746 ? 0.009 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>OpenBookQA</cell><cell cols="3">0.310 ? 0.021 0.326 ? 0.021 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">HeadQA (English) 0.326 ? 0.009 0.385 ? 0.009 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LogiQA</cell><cell cols="3">0.230 ? 0.017 0.220 ? 0.016 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>QA4MRE (2013)</cell><cell cols="3">0.366 ? 0.029 0.363 ? 0.029 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Five-Shot Results on Natural Language Understanding Tasks (GPT-J and GPT-NeoX). GPT-3 is omitted due to financial limitations.</figDesc><table><row><cell>FairSeq</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Five-Shot Results on Natural Language Understanding Tasks (FairSeq Models)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Zero-Shot Results on Basic Arithmetic and MATH (GPT-J, GPT-NeoX, and GPT-3)</figDesc><table><row><cell>FairSeq</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Zero-Shot Results on Basic Arithmetic and MATH (FairSeq Models)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Five-Shot Results on Basic Arithmetic and MATH (GPT-J and GPT-NeoX). GPT-3 is omitted due to financial limitations.</figDesc><table><row><cell>FairSeq</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>: Five-Shot Results on Basic Arithmetic and MATH (FairSeq Models)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>: Zero-Shot Results on Hendrycks Tasks, Part 1 (GPT-J, GPT-NeoX and GPT-3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11</head><label>11</label><figDesc></figDesc><table><row><cell>: Zero-Shot Results on Hendrycks Tasks, Part 2 (GPT-J, GPT-NeoX, and GPT-3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12</head><label>12</label><figDesc></figDesc><table><row><cell>: Zero-Shot Results on Hendrycks Tasks, Part 1 (FairSeq Models)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 :</head><label>13</label><figDesc></figDesc><table /><note>Zero-shot Results on Hendrycks Tasks, Part 2 (FairSeq Models)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 14 :</head><label>14</label><figDesc>Number of tokens from tokenizing the AllenAI C4 (en) validation set. The GPT-NeoX-20B tokenizer uses approximately the same number of tokens to represent C4 as the GPT-2 tokenizer.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 15 :</head><label>15</label><figDesc>Number of tokens from tokenizing the Pile validation set. The GPT-NeoX-20B tokenizer uses fewer tokens to represent the Pile overall, with the biggest gains in whitespace heavy datasets such as arXiv, GitHub and StackExchange.</figDesc><table><row><cell></cell><cell cols="3">GPT-2 GPT-NeoX-20B GPT-NeoX-20B GPT-2</cell></row><row><cell>arXiv</cell><cell>38,932,524</cell><cell>33,561,364</cell><cell>0.86204</cell></row><row><cell>BookCorpus2</cell><cell>2,233,367</cell><cell>2,262,609</cell><cell>1.01309</cell></row><row><cell>Books3</cell><cell>40,895,236</cell><cell>41,198,424</cell><cell>1.00741</cell></row><row><cell>DM Mathematics</cell><cell>7,214,874</cell><cell>6,929,066</cell><cell>0.96039</cell></row><row><cell>Enron Emails</cell><cell>374,978</cell><cell>373,498</cell><cell>0.99605</cell></row><row><cell>EuroParl</cell><cell>3,482,120</cell><cell>2,780,405</cell><cell>0.79848</cell></row><row><cell>FreeLaw</cell><cell>17,766,692</cell><cell>17,434,708</cell><cell>0.98131</cell></row><row><cell>GitHub</cell><cell>29,338,176</cell><cell>27,558,966</cell><cell>0.93936</cell></row><row><cell>Gutenberg (PG-19)</cell><cell>5,838,580</cell><cell>5,827,408</cell><cell>0.99809</cell></row><row><cell>HackerNews</cell><cell>2,312,116</cell><cell>2,299,848</cell><cell>0.99469</cell></row><row><cell>NIH ExPorter</cell><cell>776,619</cell><cell>739,543</cell><cell>0.95226</cell></row><row><cell>OpenSubtitles</cell><cell>5,428,118</cell><cell>5,445,721</cell><cell>1.00324</cell></row><row><cell>OpenWebText2</cell><cell>30,849,218</cell><cell>29,723,143</cell><cell>0.96350</cell></row><row><cell>PhilPapers</cell><cell>1,872,347</cell><cell>1,743,627</cell><cell>0.93125</cell></row><row><cell>Pile-CC</cell><cell>51,305,080</cell><cell>51,281,909</cell><cell>0.99955</cell></row><row><cell>PubMed Abstracts</cell><cell>8,676,790</cell><cell>8,185,417</cell><cell>0.94337</cell></row><row><cell>PubMed Central</cell><cell>44,508,570</cell><cell>40,722,151</cell><cell>0.91493</cell></row><row><cell>StackExchange</cell><cell>17,414,955</cell><cell>16,712,814</cell><cell>0.95968</cell></row><row><cell>USPTO Backgrounds</cell><cell>9,882,473</cell><cell>9,601,385</cell><cell>0.97156</cell></row><row><cell>Ubuntu IRC</cell><cell>3,220,797</cell><cell>2,659,225</cell><cell>0.82564</cell></row><row><cell>Wikipedia (en)</cell><cell>11,874,878</cell><cell>11,986,567</cell><cell>1.00941</cell></row><row><cell>YoutubeSubtitles</cell><cell>3,589,042</cell><cell>3,046,451</cell><cell>0.84882</cell></row><row><cell>Total</cell><cell>337,787,550</cell><cell>322,074,249</cell><cell>0.95348</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 16 :</head><label>16</label><figDesc>Number of tokens from tokenizing the Pile validation set, excluding whitespace tokens.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>, we show examples of tokenized documents from the Pile, comparing the GPT-2 tokenizer to ours.</figDesc><table><row><cell>GPT-2</cell><cell>GPT-NeoX-20B</cell></row><row><cell>rawdownloadcloneembedreportprint</cell><cell>immunohistochemistry</cell></row><row><cell>BuyableInstoreAndOnline</cell><cell>immunohistochemical</cell></row><row><cell>cloneembedreportprint</cell><cell>telecommunications</cell></row><row><cell>RandomRedditorWithNo</cell><cell>immunofluorescence</cell></row><row><cell>telecommunications</cell><cell>immunosuppressive</cell></row><row><cell>channelAvailability</cell><cell>BytePtrFromString</cell></row><row><cell>disproportionately</cell><cell>multidisciplinary</cell></row><row><cell>Telecommunications</cell><cell>histopathological</cell></row><row><cell>guiActiveUnfocused</cell><cell>neurodegenerative</cell></row><row><cell>ItemThumbnailImage</cell><cell>indistinguishable</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 17 :</head><label>17</label><figDesc>Ten longest tokens (excluding tokens comprising mainly symbols, numbers and spaces) in tokenizer vocabularies.</figDesc><table><row><cell></cell><cell cols="2">GPT-2 Worst-case Tokenization</cell><cell cols="3">GPT-NeoX-20B Worst-case Tokenization</cell></row><row><cell>Word</cell><cell>GPT-2 Tokenization</cell><cell>GPT-NeoX-20B Tokenization</cell><cell>Word</cell><cell>GPT-2 Tokenization</cell><cell>GPT-NeoX-20B Tokenization</cell></row><row><cell>hematopoietic</cell><cell>(6) hematopoietic</cell><cell>(1) hematopoietic</cell><cell cols="3">Schwarzenegger (1) Schwarzenegger (5) Schwarzenegger</cell></row><row><cell>adenocarcinoma</cell><cell>(6) adenocarcinoma</cell><cell>(1) adenocarcinoma</cell><cell>Bolshevik</cell><cell>(1) Bolshevik</cell><cell>(4) Bolshevik</cell></row><row><cell cols="2">MERCHANTABILITY (5) MERCHANTABILITY</cell><cell>(1) MERCHANTABILITY</cell><cell>crowdfunding</cell><cell>(1) crowdfunding</cell><cell>(4) crowdfunding</cell></row><row><cell>CONSEQUENTIAL</cell><cell>(5) CONSEQUENTIAL</cell><cell>(1) CONSEQUENTIAL</cell><cell>misogyny</cell><cell>(1) misogyny</cell><cell>(4) misogyny</cell></row><row><cell>oligonucleotides</cell><cell>(5) oligonucleotides</cell><cell>(1) oligonucleotides</cell><cell>McAuliffe</cell><cell>(1) McAuliffe</cell><cell>(4) McAuliffe</cell></row><row><cell>cytoplasmic</cell><cell>(5) cytoplasmic</cell><cell>(1) cytoplasmic</cell><cell>unstoppable</cell><cell>(1) unstoppable</cell><cell>(4) unstoppable</cell></row><row><cell>corticosteroids</cell><cell>(4) corticosteroids</cell><cell>(1) corticosteroids</cell><cell>Timberwolves</cell><cell>(1) Timberwolves</cell><cell>(4) Timberwolves</cell></row><row><cell>neurodegenerative</cell><cell cols="2">(4) neurodegenerative (1) neurodegenerative</cell><cell>excruciating</cell><cell>(1) excruciating</cell><cell>(4) excruciating</cell></row><row><cell>asymptotic</cell><cell>(4) asymptotic</cell><cell>(1) asymptotic</cell><cell>Kaepernick</cell><cell>(1) Kaepernick</cell><cell>(4) Kaepernick</cell></row><row><cell>aneurysm</cell><cell>(4) aneurysm</cell><cell>(1) aneurysm</cell><cell>Valkyrie</cell><cell>(1) Valkyrie</cell><cell>(4) Valkyrie</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 18 :</head><label>18</label><figDesc>Worst case word tokenization with respective tokenizers. We show cases where one tokenizer requires many more tokens to represent a word compared to the other tokenizer.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This model does not work using the provided codebase, and we have been told it under-performs GPT-J.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The sole difference is due to an oversight discussed in Section 2.1.2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">In private communication, the authors confirmed that Jurassic-1 was trained on the Pile(Gao et al., 2020).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">We choose to present environmental impact figures in metric tons to align with standard reporting.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">https://github.com/allenai/allennlp/discussions/5056</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank staff at CoreWeave-in particular Max Hjelm, Brannin McBee, Peter Salanki, and Brian Venturo-for providing the GPUs and computing infrastructure that made this project possible. We would also like to acknowledge Eren Dogan and Wesley Brown for feedback and technical support throughout the project, and John Schulman, Evan Hubinger, Victor Sanh, Jacob Hilton, and Siddharth Karamcheti for providing feedback on drafts of the paper.</p><p>Finally, we thank Anthony DiPofi, Charles Foster, Jeffrey Hsu, Eric Tang, Anish Thite, Kevin Wang, and Andy Zou for their contributions to the EleutherAI Language Modeling Evaluation Harness we used to evaluate GPT-NeoX-20B.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Occam&apos;s razor is insufficient to infer the preferences of irrational agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?ren</forename><surname>Mindermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5598" to="5609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Thinking inside the box: Controlling and using an oracle AI. Minds and Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Sandberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Bostrom</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11023-012-9282-2</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="299" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PROST: Physical reasoning about objects through space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Aroca-Ouellette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cory</forename><surname>Paik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Roncone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.404</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4597" to="4608" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giri</forename><surname>Anantharaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halil</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandeep</forename><surname>Baines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Horo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10684</idno>
		<title level="m">Zornitsa Kozareva, and Ves Stoyanov. 2021. Efficient large scale language modeling with mixtures of experts. Computing Research Repository</title>
		<meeting><address><addrLine>Mona Diab</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>Version 1</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00861</idno>
		<title level="m">Chris Olah, and Jared Kaplan. 2021. A general language assistant as a laboratory for alignment. Computing Research Repository</title>
		<meeting><address><addrLine>Tom Brown, Jack Clark, Sam McCandlish</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>Version 3</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442188.3445922</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;21</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Bicheno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.07311</idno>
		<title level="m">Datasheet for the Pile. Computing Research Repository</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
