<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning from All Vehicles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning from All Vehicles</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a system to train driving policies from experiences collected not just from the ego-vehicle, but all vehicles that it observes. This system uses the behaviors of other agents to create more diverse driving scenarios without collecting additional data. The main difficulty in learning from other vehicles is that there is no sensor information. We use a set of supervisory tasks to learn an intermediate representation that is invariant to the viewpoint of the controlling vehicle. This not only provides a richer signal at training time but also allows more complex reasoning during inference. Learning how all vehicles drive helps predict their behavior at test time and can avoid collisions. We evaluate this system in closed-loop driving simulations. Our system outperforms all prior methods on the public CARLA Leaderboard by a wide margin, improving driving score by 25 and route completion rate by 24 points.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Autonomous driving has been one of the most anticipated technologies since the advent of modern-day artificial intelligence. However, even after decades of exploration, we have yet to see self-driving cars deployed at scale. One main reason is the generalization. The world and its drivers are more diverse than current planning approaches can handle. Hand-designed classical planning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45]</ref> does not generalize gracefully to unseen or unfamiliar scenarios. Learning based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b36">37]</ref> fare better, but suffer from a long tail of driving scenarios. The majority of driving data consist of easy and uninteresting behaviors. After all, humans drive thousands of hours before observing a traffic accident <ref type="bibr" target="#b42">[43]</ref>, especially when driving an expensive autonomous test vehicle. How do we tame the long-tail of driving scenes? While many approaches rely on carefully crafted safety-critical scenarios in simulation <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>, or collect massive data in the real world <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41]</ref>, in this paper we focus on an orthogonal direction.</p><p>We observe that, although many of us have not experienced traffic accidents ourselves, everyone has at least observed several accidents throughout our driving career. The <ref type="figure">Figure 1</ref>. We present LAV, a mapless, learning-based end-to-end driving system. LAV takes as input multi-modal sensor readings and learns from all nearby vehicles in the scene for both perception and planning. At test time, LAV predicts multi-modal future trajectories for all detected vehicles, including the ego-vehicle. Picture credit -Waymo open dataset <ref type="bibr" target="#b40">[41]</ref>. same applies to safety-critical driving scenarios: While the data-collecting ego-vehicle might not experience accidentprone situations itself, it is likely its driving logs contain states that are interesting or safety-critical, but experienced by other vehicles. Training on other vehicles' trajectories helps not only with sample efficiency, but also greatly increase the chance that the model sees interesting scenarios. Moreover, knowing other vehicles' future trajectories helps the ego-vehicle avoid collisions.</p><p>The main challenge with training on all vehicles lies in the partial observability of other vehicles. Unlike the egovehicle, other vehicles have only partially observed motion trajectories, exposing no control commands or higher-level goals. This makes direct training <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b37">38]</ref> on other vehicles' traces close to impossible. More importantly, other vehicles have no accessible sensors. To learn from other vehicles, a model has to infer their surrounding state using the ego-vehicle's sensors.</p><p>Our framework, Learning from All Vehicles (LAV), handles the partial observability of both perception and motion in one joint recognition, prediction, and planning stack. We decouple the partial observability challenge of perception and action using a privileged distillation approach <ref type="bibr" target="#b10">[11]</ref>. LAV first learns a perception model that outputs a viewpoint invariant representation using auxiliary supervision from 3D detection and segmentation tasks. By definition, this auxil-iary task does not distinguish between the ego-vehicle and other vehicles in the scene and thus learns a viewpoint invariant representation. It handles the partial observability of sensors. In parallel, LAV learns a privileged motion planner <ref type="bibr" target="#b10">[11]</ref>. Instead of predicting steering and acceleration, which are only available for the ego-vehicle, we use future waypoints to represent the motion plan. We use ground-truth computer-vision labels as inputs to the privileged motion planner. Computer-vision labels ensure viewpoint invariance, waypoints provide an invariant representation of motion. The privileged motion planner predicts trajectories of all nearby vehicles and infers their high-level commands. Finally, we combine the two models in a joint framework using privileged distillation <ref type="bibr" target="#b10">[11]</ref>. This final distillation learns a motion prediction model from all vehicles using the viewpoint invariant vision features of the perception model. The distilled policy drives from raw sensor inputs alone.</p><p>We validate our method in the CARLA driving simulator <ref type="bibr" target="#b16">[17]</ref>. At the time of submission, our method ranks first on the CARLA public leaderboard 1 . It attains a 61.85 driving score and a 94.46 route completion rate. Both are the highest among all methods and outperform the prior state-of-the-art method by a wide margin, increasing driving score and route completion rate by 25 and 24 points respectively. Our method won the 2021 CARLA Autonomous Driving challenge 2 . Code available at https: //github.com/dotchen/LAV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Perception for autonomous driving is driven by advances in visual understanding and recognition. The perception system of a self-driving vehicle understands the scene by inferring its nearby objects and surrounding road structures. A typical perception system takes as input LiDAR scans and performs object detection and tracking <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref>. Liang et al. <ref type="bibr" target="#b31">[32]</ref>, Vora et al. <ref type="bibr" target="#b45">[46]</ref> fuse RGB camera and LiDAR scans for richer semantic information. For roads, perception systems are categorized based on whether they require pre-recorded HD-Map: map-based systems localize themselves in the pre-recoded maps <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b50">51]</ref>; mapless systems either perform online mapping <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>, or they implicitly predict road-related affordances <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>. Bansal et al. <ref type="bibr" target="#b3">[4]</ref>, Zeng et al. <ref type="bibr" target="#b47">[48]</ref> represent the perception outputs as bird's-eye-viewed (BEV) spatial grids; more recently, Gao et al. <ref type="bibr" target="#b19">[20]</ref>, Li et al. <ref type="bibr" target="#b30">[31]</ref> represent perception outputs in a parameterized vector space for a more compact representation. Our approach takes multi-modal sensor data as input and performs online mapping and object detection. However, we do not directly use the predicted map to perform classical planning. Instead, we learn a planner from data using imitation learning. This planner uses every vehicle it encounters on the road as a supervisory signal to enhance the diversity of the training data.</p><p>Behavior prediction focuses on forecasting the future state of driving scenes. In autonomous driving, a behavior predictor takes as either the input representations obtained from perception or raw sensor data; it predicts trajectories of the dynamic objects in the driving scene. Luo et al. <ref type="bibr" target="#b33">[34]</ref>, Zeng et al. <ref type="bibr" target="#b47">[48]</ref> predict single, deterministic future trajectories of the detected vehicles. Casas et al. <ref type="bibr" target="#b4">[5]</ref>, Zhao et al. <ref type="bibr" target="#b49">[50]</ref> model multi-modal future trajectories by using conditional models. Chai et al. <ref type="bibr" target="#b6">[7]</ref> predicts trajectories as Gaussian mixtures to represent uncertainty in the euclidean space. Cui et al. <ref type="bibr" target="#b14">[15]</ref>, Lee et al. <ref type="bibr" target="#b27">[28]</ref> use latent variables and VAEs to model actor and scene specific uncertainties. Recently, Casas et al. <ref type="bibr" target="#b5">[6]</ref>, Hu et al. <ref type="bibr" target="#b22">[23]</ref>, Kamenev et al. <ref type="bibr" target="#b24">[25]</ref> combine perception and behavior prediction by directly predicting the occupancy maps. Our approach is highly related to the task of behavior prediction, as it also trains on all nearby vehicles' trajectories. Our approach consists of a behavior predictor. In particular, it applies a conditional motion planner on all nearby vehicles, including the ego-vehicle.</p><p>Learning-based motion planning uses imitation learning or reinforcement learning to plan future trajectories. Pioneered by Pomerleau <ref type="bibr" target="#b36">[37]</ref>, imitation learning for autonomous driving regresses sensor inputs to controls by imitating the recorded expert trajectories. Codevilla et al. <ref type="bibr" target="#b13">[14]</ref> use conditional branching and high-level commands to extend imitative models for urban driving. Zeng et al. <ref type="bibr" target="#b47">[48]</ref> use imitation learning to train a cost volume predictor for planning; Chen et al. <ref type="bibr" target="#b8">[9]</ref>, Sauer et al. <ref type="bibr" target="#b39">[40]</ref> predict actions from the learned affordances. Chen et al. <ref type="bibr" target="#b10">[11]</ref> uses on-policy distillation to handle distribution shift as well as to provide stronger imitative supervision signals. Reinforcement learning, on the other hand, trains policies from a user-defined reward function. Kendall et al. <ref type="bibr" target="#b25">[26]</ref> train a lane following driving policy using DDPG; Toromanoff et al. <ref type="bibr" target="#b43">[44]</ref> use distributed Rainbow-IQN to train an urban driving policy with competitive performance. Recently, Chen et al. <ref type="bibr" target="#b9">[10]</ref> use model-based reinforcement learning and distillation to train a driving policy in an offline manner. Our approach builds upon Chen et al. <ref type="bibr" target="#b10">[11]</ref> and trains a motion planner using imitation learning and distillation. However, unlike most prior methods, we train motion planning on data from all nearby vehicles in addition to the ego-vehicle.</p><p>Our idea of training the ego motion planner using data from all vehicles is closely related to Filos et al. <ref type="bibr" target="#b17">[18]</ref> and Zhang and Ohn-Bar <ref type="bibr" target="#b48">[49]</ref>. Filos et al. <ref type="bibr" target="#b17">[18]</ref> extends offline reinforcement learning to learn from other agents' behaviors. Zhang and Ohn-Bar <ref type="bibr" target="#b48">[49]</ref> train a privileged imitation learning policy that learns from other vehicles in a scene. Their policy   side-steps partial observability by training a policy that acts only on the ground truth state of the simulator. It assumes perfect perception or access to other agents' sensors. LAV, on the other hand, operates on raw sensor inputs and learns a viewpoint invariant intermediate representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning from All Vehicles</head><p>We aim to build a deterministic driving model ? that at each timestep t maps sensor readings, high-level navigational command, and vehicle state to raw control command a t . We opt for an end-to-end differentiable three-stage modular pipeline: A perception module, a motion planner, and a low-level controller. See <ref type="figure" target="#fig_2">Figure 2a</ref> for an overview.</p><p>The perception module is trained from massive labeled supervision with two goals in mind: To create a robust and generalizable representation of the surrounding world, and to build vehicle-invariant features that help supervise the motion planner. Section 3.1 describes the overall architecture and training setup of the perception module. It maps raw sensor readings to a map-view feature representation.</p><p>The motion planner uses the map-view features of the perception model to produce a series of waypoints describing the future trajectory of the vehicles. Motion planners commonly use supervision from just the ego-vehicle for this prediction <ref type="bibr" target="#b10">[11]</ref>. This supervision is quite sparse and provides the motion planner with just a single series of labels per collected data point. In our framework, we learn motion planning from all vehicles that surround the ego-vehicle. This is possible because our perception system produces vehicle-invariant features as inputs; it is also because the outputs of the motion planner, the future trajectories, can be easily obtained from ground truth driving logs. <ref type="figure" target="#fig_2">Figure 2b</ref> shows an overview of the motion planner training. Section 3.2 describes the motion planner and its training setup. Finally, a low-level controller converts motion plans into actual steering and acceleration commands that are executed on the vehicle. At test time, the low-level controller consid-ers other vehicles' motion plans to make emergency stop decisions. Section 3.3 describes the controller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A vehicle-independent perception model</head><p>The core objective of any perception module is to build an intermediate representation that readily generalizes from training to test conditions. In our setup, a secondary goal is to build input features to the motion planner that are indistinguishable between the current vehicle and nearby vehicles. The closer the output representations of the ego-vehicle and other vehicles are, the better motion plans transfer between those vehicles. Here, we opt for a metric map-based output representation. In a metric map, rotated ROI pooling extracts fixed-sized feature representations for training vehicles.</p><p>Specifically, we use three RGB cameras I t = {I 1 t , I 2 t , I 3 t } surrounding the vehicle and one LiDAR sensor L t as an input. We combine the color and LiDAR inputs using pointpainting <ref type="bibr" target="#b45">[46]</ref> from RGB inputs and a light-weight Center-Point <ref type="bibr" target="#b46">[47]</ref> with PointPillars <ref type="bibr" target="#b26">[27]</ref> 3D backbone. The backbone provides us with a map-view feature representation f ? R W ?H?C of width W and height H with C channels.</p><p>We train the backbone network using a combination of semantic segmentation and detection losses. See <ref type="figure" target="#fig_4">Figure 3a</ref> for an overview. For every pixel in map-view, we predict a road mask, solid and broken lane boundaries. We use a binary classifier, and binary cross-entropy loss, as road and lanemarking can overlap. In addition, we train a CenterPointstyle detector <ref type="bibr" target="#b46">[47]</ref> for pedestrians and vehicles. Most importantly, we explicitly label the ego-vehicle in this detector. This minimizes the feature distance between ego-vehicle and other vehicles and enables better transfer. We pre-train the perception model using fully labeled data and use rotation augmentations around the ego-vehicle to increase the robustness of the learned model. Supervised pre-training has two advantages. It generalizes better to unseen test conditions. It also learns a similar feature representation for all vehicles. This feature represen-  tation is next used in the motion planner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning to plan motion from all vehicles</head><p>The motion planner uses the output of the perception system to predict a series of future waypoints describing positions the vehicles should steer towards. Here, we propose a novel two-stage motion planner that combines geometric GPS targets and discrete high-level commands. We use a standard RNN formulation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b37">38]</ref> to predict n = 10 future waypoints y 1 , . . . , y n ? R 2 . We use n = 20 for our second leaderboard submission. The motion planner uses a highlevel command c and intermediate GNSS coordinate goal g ? R 2 to perform different driving maneuvers. In CARLA, GNSS goals are sampled every 50-100 meters and contain a measurement error of around one meter. Possible highlevel commands c include turn-left, turn-right, go-straight, follow-lane, change-lane-to-left, change-lane-to-right.</p><p>Let M (f , c) :? R n?2 be the motion planner conditioned on high-level command c and warped featuresf for the Region of Interest (RoI) at the location and orientation of the vehicle in question. For all vehicles, we observe their future trajectory to obtain supervision for future waypoints y. For the ego-vehicle, the simulator provides a ground truth high-level command? and provides sufficient supervision to train the motion planner</p><formula xml:id="formula_0">L ego M = Ef ,y,? y ? M (f ,?) 1 .<label>(1)</label></formula><p>However, other vehicles do not expose their high-level commands. While it may be possible to infer this command from future trajectories alone, any rule-based inference will be ambiguous and noisy. We instead allow the model to infer the high-level command directly and optimize the plan for the most fitting high-level command.</p><formula xml:id="formula_1">L other M = Ef ,y min c y ? M (f , c) 1 .<label>(2)</label></formula><p>At training time we optimize both losses L ego M and L other M jointly. To avoid mode collapse, we use an adaptive weight ? on L other M which starts small and converges to 1 as training progresses. We found ? = 1 ? 0.8 it/4000 to work well in practice. This is only applied during the privileged training stage.</p><p>The resulting motion planner M finds good coarse trajectories for a wide range of traffic scenarios. It learns to plan for all vehicles it sees. However, the resulting motion plan may be noisy as high-level commands c are ambiguous.</p><p>In a second stage, we refine the motion plan using an additional RNN-based motion planning network M (f , g,?) ? R n?2 . The motion refinement network uses the same ROIwarped featuref , the previously predicted motion plan?, and the more fine-grain GNSS goal g as input. We normalize g in the ego-vehicle's coordinate. It then produces a delta to the original trajectory as output. Since GNSS goals are only available for the ego-vehicle, we train the refinement M only on ego-vehicle trajectories</p><formula xml:id="formula_2">L ref ine M = Ef ,y,?,? ? + M (f ,?,?) ? y 1 . (3)</formula><p>To increase robustness of the refinement model, we use the output of M from all high-level commands during training. During both training and testing, we roll out the same refinement network multiple times to recursively refine the predicted trajectory. We use the refined trajectories for planning unless the high-level command is lane-changing. The above loss then applies to each step of the rollout.</p><p>In practice, we learn the motion planner in a privileged distillation framework <ref type="bibr" target="#b10">[11]</ref>. See <ref type="figure" target="#fig_4">Figure 3b</ref> and <ref type="figure" target="#fig_4">Figure 3c</ref> for an overview. We first learn motion planning on ground truth trajectories and ground-truth perception outputs and regions of interest using the losses (1)-(3). We then use the privileged motion planner to supervise a motion planner that uses the inferred perception outputs. During this second stage, we supervise predictions on all high-level commands which leads to a richer supervisory signal <ref type="bibr" target="#b10">[11]</ref>. We additionally distill a high-level command classifier for other vehicles which we use later in the vehicle-aware controller. This stage trains end-to-end by backpropagating gradients from motion prediction and planning to the perception backbone, allowing perception models to attend to the low-level details in the scenes. We keep the pre-training perception loss in the previous stage as an auxilliary supervision to regularize the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Vehicle-aware control</head><p>The controller translates a motion plan into actual driving commands. We use two PID controllers for latitudinal (steering) and longitudinal (acceleration) control. Both PID controllers use basic statistics of the refined motion plan as an input to produce a continuous output command. The longitudinal PID controller additionally uses the current speed as an input to compute acceleration. We overwrite braking using a separate neural network classifier B in case of traffic light and hazard stoppages. The classifier uses the same image inputs as the perception module plus one additional camera with telephoto lenses to capture distant traffic lights. The classifier learns the braking behavior of the datacollecting ego-vehicle using recorded brake actions. Finally, we reuse the motion plans learned from other vehicles to detect potential collisions and perform hazard stops. Specifically, we use the 3D detections of the backbone to find all nearby vehicles. For each, we use the motion planner M to produce future trajectories over each high-level command. We use all motion plans above the high-level command likelihood threshold to check for collisions with the ego-vehicle's motion plan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation details</head><p>Perception. We use PointPillars <ref type="bibr" target="#b26">[27]</ref> with PointPainting <ref type="bibr" target="#b45">[46]</ref> as our multi-modal 3D perception backbone P B . In particular, given RGB images captured from three frontal facing camera {I 0 t , I 1 t , I 2 t } with extrinsic matrices E = {E 0 , E 1 , E 2 }, we use a ERFNet <ref type="bibr" target="#b38">[39]</ref> to compute their semantic segmentation scores S t = {S 0 t , S 1 t , S 2 t }. We use five semantic classes: "background", "vehicles", "roads", "lane markings" and "pedestrians". For each LiDAR point l ? L t , we use PointPainting <ref type="bibr" target="#b45">[46]</ref> to concatenate its corresponding semantic classes using the segmentation scores:</p><formula xml:id="formula_3">l s t = PointPaint(S t , T E , l t ). T E is the perspective transform function.</formula><p>For PointPillars, we use FC-64-64 with BatchNorm [24] as its PointNet. We create pillars for LiDAR points for x ? [?10m, 70m] and y ? [?40m, 40m]. Each pillar represents a 0.25m ? 0.25m spatial region. We use the default 2D CNNs with multi-scale features to obtain the spatial features ? t ? R 192?160?160 with 0.5? resolution of the original pillars. Unlike the original PointPillars which directly builds dense pillars specified by the hyperparameters, we sparsely represent the pillars. We also use a sparse PointNet to process the corresponding sparse pillar features. This allows us to process all pillars efficiently both in space and time.</p><p>We use a branching architecture for the detection and mapping heads. We use a simplified one-stage Center-Point <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b52">53]</ref> formulation for BEV object detection. In particular, we predict two "centerness" maps, one for vehicles and one for pedestrians; we also predict an orientation and bounding-box maps that are both class-agnostic. For mapping, we predict a BEV semantic map for roads, solid lane markings and broken lane markings. Each map is generated using a separate 3 ? 3 convolution followed by a 3 ? 3 up-convolution with stride 2, all from the shared backbone P B . At test time, we use a 2D max pooling layer as a simplified version of NMS.</p><p>We additionally train a binary brake classifier that takes as input all the four camera RGB images. We feed the telephoto lens image and the stitched other three images to a ResNet-18 followed by a global average pooling layer. This gives us fixed-sized embeddings of z 1 , z 2 ? R 512 . We concatenate z 1 , z 2 and feed it to a linear layer to predict the binary brake.</p><p>Prediction and Planning. Given the ego-vehicle and a list of vehicle detection, we use differentiable warping to crop a rotated region of interest (RoI)f i for each vehicle location and yaw angle. A CNN followed by global average pooling takes as input the rotated RoI features and returns a fixedsized embedding z i for each vehicle i. z i is shared among M and M . The motion planner M uses a separate GRU <ref type="bibr" target="#b12">[13]</ref> for each high-level command. The GRU is rolled out n times to produce an offset between consecutive waypoints. The refinement motion planner M uses two forms of recursions and rollouts: rollouts along waypoint and rollouts along refinement iterations. It predicts an offset from the prior motion plan for each iteration. The refinement motion plan relies on just a single GRU unit that takes the GNSS goal g as an additional input. Both motion planners use a linear layer to transform GRU states into the desired outputs.</p><p>Control. The controller C takes as input refined egotrajectory ? = M (f ,?,?). See <ref type="figure">Figure 4</ref> for an overview. If predicted trajectory ? leads to a collision with other traffic participants, we adjust it. For now we perform a hard stop using a hard-coded braking logic. If the predicted trajectory is collision free, we follow it directly. We use two PID controllers for latitudinal and longitudinal control respectively. For latitudinal control, we use the 5-th point in ? 5 as the aim point to compute the steering error. For longitudinal control, we use the difference between target speed inferred from ? k+1 ?? k and the current speed v t to compute acceleration. We use K P = 1.0, K I = 0.5, K D = 0.2 for the latitudinal PID controller, and we use K P = 5.0, K I = 0.5, K D = 1.0  <ref type="table">Table 1</ref>. Comparison of the driving score (main metric), route completion and infraction score on the public CARLA leaderboard <ref type="bibr" target="#b0">[1]</ref> (accessed Jan 2022). All three metrics are higher the better. We outperform prior methods by a wide margin. Detailed infraction numbers reported in the supplement for reference.</p><p>for the longitudinal PID controller. We overwrite the brake control with the predicted brake score if it is larger than the brake computed from the longitudinal controller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our method on the CARLA simulator [17] using closed-loop driving. We compare our approach with the state-of-the-art methods on the public leaderboard, and we perform ablation study on the effect of our design choices locally. For our online leaderboard submission, we train on all the 8 publicly available towns using a dataset of 400K frames, collected with the CARLA behavior agent under randomized weathers. For ablations, we only train on 4 out of the 8 towns, resulting in a dataset of 186K frames. We test on two other unseen towns.</p><p>More details of the dataset statistics are provided in the supplement for reference. <ref type="table" target="#tab_5">Table 6</ref> compares our method with prior state-of-theart methods on the CARLA public leaderboard <ref type="bibr" target="#b0">[1]</ref>. The CARLA leaderboard evaluates autonomous driving systems under unseen and partially adversarial conditions. Vehicles are tasked to complete a set of predefined routes in new towns. For each route, the simulator adds dangerous scenarios such as suddenly crossing pedestrians or aggressive lane-changing vehicles. These scenarios are modeled after the NHTSA typology <ref type="bibr" target="#b0">[1]</ref>. The leaderboard measures how far self-driving vehicles proceed along a route within a fixed time budget, and how often they cause traffic infractions. In <ref type="table" target="#tab_5">Table 6</ref>, we list three key metrics of the leaderboard: Driving Score, Route Completion, and Infraction Score. Route Completion measures the distance percentage an agent is able to complete; Infraction Score measures how often an agent drives without causing infractions; Driving Score measures route completion rate weighted by infractions per route. Driving Score and Route Completion are the two main metrics of comparison. A vehicle standing perfectly still will receive an infraction score of 1. All three metrics are higher the better. We refer readers to the official leaderboard <ref type="bibr" target="#b0">[1]</ref> for a more detailed description of the metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison with state-of-the-art</head><p>We compare to the top 7 entries on the leaderboard. GRIAD <ref type="bibr" target="#b7">[8]</ref> and Transfuser+ are unpublished, concurrent submissions. Rails <ref type="bibr" target="#b9">[10]</ref> is a model-based reinforcementlearning method that trains vision-based driving policies from offline driving logs. IARL <ref type="bibr" target="#b43">[44]</ref> is based on state-ofthe-art model-free reinforcement-learning with distributed training. NEAT <ref type="bibr" target="#b11">[12]</ref> uses imitation-learning with attention and implicit functions. Transfuser <ref type="bibr" target="#b37">[38]</ref> uses imitationlearning with attention-based sensor fusion. LBC <ref type="bibr" target="#b10">[11]</ref> relies on knowledge distillation with imitation learning. LBC is the closest comparison to our approach, as we also use knowledge distillation and imitation learning as a supervisory signal. However, LAV additionally uses other observed vehicles to train the control policy.</p><p>LAV ranks first on the leaderboard, and it outperforms the prior leading entry by a wide margin. It achieves 61.85 driving score, the highest among all methods, and 25 points higher than on the previous state-of-the-art GRIAD. It also achieves a 94.46 Route Completion, the highest among all methods and 32 points higher than the previous state-ofthe-art. Moreover, previous top methods, such as Rails and IARL, require 1M and 40M frames to train the policies. Our method uses only 400K training frames. Our approach has a relatively high infraction score; however, we note that higher Route Completion naturally leads to more infractions. A vehicle that drives slowly or stands still causes fewer infractions but struggles to complete its routes. See LBC for example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation study</head><p>We answer few important questions on our design choices. We again evaluate on Driving Score, Route Completion and Infraction Score. However, we cannot use the online leader- board directly for these additional experiments. We instead use a local setup with similar characteristics to the Leaderboard. In particular, we train on 4 out of 8 towns (Town01, Town03, Town04 and Town06), and evaluate on 2 unseen towns (Town02 and Town05). We select 4 representative routes, 2 from each town, and we evaluate each route with 4 weathers: "Clear Noon", "Cloudy Sunset", "Soft Rain Dawn" and "Hard Rain Night". We evaluate each setup for 3 runs and report the mean and standard deviation. This results in 48 trials for each model. All ablated models use a similar but slightly outdated setup as our leaderboard entry. The main differences are: 1. the ablated models use U-Net as the semantic segmentation backbone 2. the ablated models use FC-32-32 in PointPillars and 3. slightly different controller hyperparameters. <ref type="table">Table 2</ref> studies the effects of our key design choices. We compare to two variants of our approach: 1) One that only trains on ego-vehicle data, 2) One that does not perform privileged distillation. We find that training on other vehicles' trajectories and viewpoints results in lower performance on both route completion and infraction. The performance degradation is smaller than expected likely because our auxiliary perception supervision makes the motion models generalize well to distribution shifts, caused by both test time errors and viewpoints changes. Not using privileged distillation results in a larger performance drop. Without distillation, the motion models need to train from both noisy inputs and labels, thus tackling a much harder learning problem. Our full model achieves the highest scores in all three metrics. <ref type="table" target="#tab_3">Table 3</ref> studies the degree to which training on other vehicles' experiences affect the driving performance. We evaluate three standards, where we only train vehicles within 5, 15 and 25 meters within the ego-vehicle, with 15 meters being our default value. ? 5m and ? 15m performs equally well, while ? 25m is slightly worse. We think this is due to the fact that vehicles at range have too different of an appearance in the sensor inputs which the auxiliary supervision is unable to correct for. The LiDAR sensor in CARLA mimics the Velodyne-64 rays model, which produces at most a few dozen measurements for cars at a distance of 25m.  variants of joint training: 1) No perception pre-training (No Pretrain.), and joint perception and motion training (Joint). The first variant only optimizes the distillation loss, and the latter optimize perception and distillation simultaneously. Both variants do not freeze the 3D backbone. As expected, models without perception training perform poorly. They suffer from the distribution shifts caused by viewpoint changes. Joint training also performs worse than staged straining, because solving perception and planning simultaneously is harder than solving them in a disentangled manner, as also observed by Chen et al. <ref type="bibr" target="#b10">[11]</ref>. <ref type="table">Table 5</ref> studies the effect of our iterative refinement module. K = 0 means we directly use the trajectories predicted by the motion planner M f to drive. The default option K = 5 performs the best, showing the benefits of iterative motion refinement. Iterative refinement allows the model to elastically figure out what residuals to learn. It also naturally combines the semantic information from the high-level command and the geometric information from the goals.</p><p>Detailed infraction numbers for <ref type="table" target="#tab_3">Table 3</ref>, <ref type="table" target="#tab_2">Table 4</ref> and <ref type="table">Table 5</ref> are provided in the supplement for reference. <ref type="figure" target="#fig_5">Figure 5</ref>. Visualizations of the outputs from our system. Each row visualizes RGB camera inputs, predicted road geometries, and detection and motion predictions respectively. Detection and motion prediction are used during inference; mapping is used for training only. For mapping, we predict road, broken and solid (white) lane markings. For detection, we predict pedestrians' and vehicles' poses and bounding boxes. We forecast multi-modal future trajectories with their corresponding likelihoods. Best viewed on screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Refinement Iteration</head><p>Driving Score</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Route Completion</head><p>Infraction Score  <ref type="table">Table 5</ref>. Driving performance ablation on the effect of motion refinement. All models are the same except for number of refinement iterations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Qualitative analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this paper, we present a mapless, end-to-end driving system that trains from the experiences of all nearby vehicles. Our system achieves state-of-the-art performance in closed-loop driving simulation, and it outperforms prior leading methods by a wide margin. Limitations and potential negative social impacts: Our approach is trained and evaluated in simulation alone and still incurs traffic infractions. If directly deployed in the real world, it would most likely result in traffic accidents (negative social impacts). On the technical side, our current behavior predictor instantiated by the conditional motion planner does not consider multi-modality beyond the high-level commands. Extending our work with a probabilistic formulation will strengthen its ability in handling the diverse behaviors of both the ego vehicle and the other road users. Improving the motion predictor beyond its raster representation is also an exciting direction.  <ref type="figure">Figure 6</ref>. Overview of our motion model architectures. Motion prediction outputs future trajectories on different high-level commands for all vehicles, as well as their likelihoods. For the ego-vehicle, we additionally refine it using an iterative refinement module, conditioned on?t. We detach the gradient flow from refinement to prediction in order avoid undesired causal effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Detailed Infractions</head><p>In this section we report additional infraction numbers of our experiments in the main manuscript. All infractions are measured as the number of occurrences normalized per 1 kilometer traveled. <ref type="table" target="#tab_5">Table 6</ref> compares our method with baselines on the CARLA public Leaderboard <ref type="bibr" target="#b0">[1]</ref>. Our method also leads the red light, offroad and blocked infraction numbers among all the methods. <ref type="figure">Figure 6</ref> provdes an overview of architectures of M and M . We detach the gradient of coarse trajectories predicted by M to remove any effect M r might have on M f during training. <ref type="table">Table 8</ref> provides a list of hyperparameters. For all our experiments, we train our models on a 4 Titan Pascal GPU machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Comparison with state-of-the-art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. More Details</head><p>We use a ERFNet as our semantic segmentation architecture for PointPainting. We use the following image augmentations when we train the image-based semantic segmentation and brake prediction models: Gaussian Blur, Additive Gaussian Noise, Pixel Dropout, Multiply (scaling), Linear Contrast, Grayscale, ElasticTransformation. <ref type="table">Table 10</ref> studies the effects of our key design choices. We additionally compare to a variant where we removes Point-Painting <ref type="bibr" target="#b45">[46]</ref> (LiDAR only b.b.). The rest of the backbone is the same. Both Driving Score and Route Completion of this variant are lower than the full LAV. This shows the benefit of multi-modal sensor fusion. <ref type="table" target="#tab_7">Table 11</ref> studies the degree to which training on other vehicles' experiences affect the driving performance. <ref type="table">Table 12</ref> studies different perception training schemes. <ref type="table" target="#tab_3">Table 13</ref> studies the effect of our iterative refinement module. Note that our system does not rely on HD-Maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Ablation study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. CARLA Leaderboard</head><p>Our online CARLA leaderboard entry additionally process temporal information by concatenating LiDAR scans normalized to the current vehicle pose. During training, this is done by stacking normalized frames using ground truth ego-vehicle poses. During testing, ego-vehicles poses are tracked across time via GNSS signals, smoothed by an Extended Kalman Filter (EKF) with a bicycle kinematics forward model. Following prior work <ref type="bibr" target="#b43">[44]</ref>, we use a "stuck counter" to avoid getting blocked. We creep forward if the vehicle is stuck for too long. <ref type="table">Table 9</ref> provide a detailed description of the sensor configurations for the ego-vehicle. We use the compass readings from IMU and GNSS readings to convert the target locations, represented in the GNSS format, to the ego-vehicle coordinate. <ref type="table">Table 7</ref> describes the dataset statistics on the training towns and their corresponding layouts. Our online leaderboard submission trains on all towns, whereas our local ablation models train on Town01, Town03, Town04 and Town06. They test on Town02 and Town05. <ref type="figure" target="#fig_7">Figure 7</ref> provides a visualization of the four routes on which the ablation models are tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Onboard Sensors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Dataset Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. License of Assets</head><p>We use the open source CARLA driving simulator <ref type="bibr" target="#b16">[17]</ref>. CARLA is released under the MIT license. Its assets are under the CC-BY license.</p><p>Our teaser figure in the main paper uses a picture from the Waymo open dataset <ref type="bibr" target="#b40">[41]</ref> The Waymo open dataset uses a customized non-commercial license <ref type="bibr" target="#b2">3</ref>   </p><formula xml:id="formula_4">1 LiDAR R L?4 Velodyne-64 1 RGB R 3?288?480 FOV=40 ? 3 RGB R 3?288?256 FOV=64 ? each 60 ? apart 1 IMU ? 1 GNSS R 2 1</formula><p>Speedometer R <ref type="table">Table 9</ref>. Configuration of our ego-vehicle's on-board sensors. The four RGB cameras are mounted at x = 1.5m, y = 0m, z = 2.4m with respect to the ego-vehicle's centroid. 45.20 ? 6.35 91.55 ? 5.61 0.49 ? 0.06 0.92 ? 0.42 0.00 ? 0.00 0.33 ? 0.50 0.28 ? 0.28 0.27 ? 0.01 0.01 ? 0.02</p><p>Ego-vehicle only 38.56 ? 1.86 84.76 ? 5.12 0.46 ? 0.02 1.17 ? 0.50 0.00 ? 0.00 1.82 ? 0.06 0.34 ? 0.20 0.37 ? 0.09 0.09 ? 0.08 No distillation 28.23 ? 2.27 81.05 ? 6.04 0.36 ? 0.04 2.08 ? 0.34 0.00 ? 0.00 7.87 ? 0.15 0.21 ? 0.04 1.01 ? 0.13 0.05 ? 0.05 LiDAR only b.b. 26.37 ? 2.62 74.96 ? 4.21 0.31 ? 0.04 6.51 ? 3.03 0.00 ? 0.00 0.02 ? 0.03 0.26 ? 0.23 1.56 ? 0.71 0.09 ? 0.04 <ref type="table">Table 10</ref>. Driving performance ablation of the key components of our approach on test towns. Infractions are measured as number of occurrences per kilometer traveled. Mean and standard deviation are computed over three runs. All models are the same despite the ablated option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vehicles Range</head><p>Driving Score</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Route Completion</head><p>Infraction Score</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vehicle Collisions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pedestrian Collisions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layout Collisions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Red light Violations</head><p>Offroad Infractions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blocked Infractions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? 5m</head><p>46.06 ? 1.70 88.77 ? 1.01 0.51 ? 0.02 1.27 ? 0.22 0.00 ? 0.00 0.05 ? 0.09 0.43 ? 0.11 0.69 ? 0.09 0.11 ? 0.10 ? 15m 45.20 ? 6.35 91.55 ? 5.61 0.49 ? 0.06 0.92 ? 0.42 0.00 ? 0.00 0.33 ? 0.50 0.28 ? 0.28 0.27 ? 0.01 0.01 ? 0.02 ? 25m 37.42 ? 3.09 89.56 ? 5.61 0.61 ? 0.12 0.85 ? 0.26 0.00 ? 0.00 0.61 ? 0.12 0.23 ? 0.13 0.43 ? 0.07 0.06 ? 0.10 45.20 ? 6.35 91.55 ? 5.61 0.49 ? 0.06 0.92 ? 0.42 0.00 ? 0.00 0.33 ? 0.50 0.28 ? 0.28 0.27 ? 0.01 0.01 ? 0.02 <ref type="table" target="#tab_3">Table 13</ref>. Driving performance ablation on the effect of motion refinement. All models are the same except for number of refinement iterations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Overview of the inference pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Overview of the training pipeline for the motion planning module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the agent's pipeline. A 3D Backbone fuses LiDAR measurements and semantic segmentation from RGB cameras to produce a 2D spatial feature map. This shared feature map serves as an input to a motion planner. At inference time (a), we use the central crop to predict the ego-vehicles trajectory. At training time (b), we additionally use ground-truth detections of nearby vehicles to train a motion planner from all visible vehicles. Detection results use rotated regions of interest (RoIs) of the shared feature map. Finally, at inference time, a controller aggregates multiple motion predictions into a single steering and acceleration command.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Overview of our training pipeline. (a) We train a 3D perception model using detection and semantic mapping as the supervision signal. Both tasks help learn a viewpoint-invariant spatial representation. Detection additionally predicts other vehicles' poses which we use to forecast their future trajectories at inference. The perception module produces a vehicle-independent feature representation used in motion planning. (b) In parallel, we train a motion planner over ground truth perception. We train the model using traces from all nearby vehicles using their future trajectory as supervision. (c) Finally, we combine the models learned in (a) and (b) using distillation. This model learns how all vehicles plan in an end-to-end manner using only the ego-vehicles sensor inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5</head><label>5</label><figDesc>provides a qualitative analysis of our system. It shows the combined input images, LiDAR point cloud, the auxiliary segmentation predictions, detections, and predicted plans for all vehicles in the scene. The ego-vehicles plan uses the provided high-level command, while all other vehicles predict a distribution over possible future plans. Note how all vehicles predict a reasonable and consistent set of future plans aligning well with the inferred map-view representation of the road and the potential other vehicles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Visualization of the test routes in unseen towns of the ablation models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>LAV 45.20 ? 6.35 91.55 ? 5.61 0.49 ? 0.06 0.92 ? 0.42 0.00 ? 0.00 0.33 ? 0.50 0.28 ? 0.28 Ego-vehicle only 38.56 ? 1.86 84.76 ? 5.12 0.46 ? 0.02 1.17 ? 0.50 0.00 ? 0.00 1.82 ? 0.06 0.34 ? 0.20 No distillation 28.23 ? 2.27 81.05 ? 6.04 0.36 ? 0.04 2.08 ? 0.34 0.00 ? 0.00 7.87 ? 0.15 0.21 ? 0.04 Table 2. Driving performance ablation of the key components of our approach on test towns. Infractions are measured as number of occurrences per kilometer traveled. Mean and standard deviation are computed over three runs.</figDesc><table><row><cell>Driving</cell><cell>Route</cell><cell>Infraction</cell><cell>Vehicle</cell><cell>Pedestrian</cell><cell>Layout</cell><cell>Red light</cell></row><row><cell>Score</cell><cell>Completion</cell><cell>Score</cell><cell>Collisions</cell><cell>Collisions</cell><cell>Collisions</cell><cell>Violations</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>? 1.70 88.77 ? 1.01 0.51 ? 0.02 ? 15m 45.20 ? 6.35 91.55 ? 5.61 0.49 ? 0.06 ? 25m 37.42 ? 3.09 89.56 ? 5.61 0.61 ? 0.12</figDesc><table><row><cell>studies different perception training schemes. We</cell></row><row><cell>compare the default staged training scheme (staged) with two</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Driving performance in test towns of models trained with different range of other vehicles. All models are the same except for other vehicles' maximum range used during training. No Pretrain. 8.47 ? 0.83 9.34 ? 0.35 0.90 ? 0.07 Joint 28.36 ? 2.11 79.58 ? 4.99 0.34 ? 0.02 Staged 45.20 ? 6.35 91.55 ? 5.61 0.49 ? 0.06</figDesc><table><row><cell>Perception</cell><cell>Driving</cell><cell>Route</cell><cell>Infraction</cell></row><row><cell>Training</cell><cell>Score</cell><cell>Completion</cell><cell>Score</cell></row><row><cell cols="4">Table 4. Driving performance in test towns of models with different</cell></row><row><cell cols="4">perception training scheme. All models are the same except for</cell></row><row><cell>perception training.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>K = 0 12.69 ? 2.86 35.85 ? 2.91 0.42 ? 0.03 K = 1 21.30 ? 1.10 85.90 ? 2.46 0.25 ? 0.01 K = 5 45.20 ? 6.35 91.55 ? 5.61 0.49 ? 0.06</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>. Part of our codebase uses the official ResNet and ERFNet implementation. The codes are under the MIT license and the CC-BY-NC license respectively. Comparison of our method and the state-of-the-art on the public CARLA leaderboard<ref type="bibr" target="#b0">[1]</ref> (accessed Jan 2022). Methods are ranked by the driving score as the main metric. Driving Score, Route Completion, Infraction Score are higher the better, whereas the rest are lower the better. Infractions are measured as number of occurences per kilometer traveled. We best all other methods by a wide margin. We significantly outperform the prior best entry by 24 points on the driving score, and 25 points on the route completion. We also lead the red light, offroad and blocked infraction numbers among all the methods.</figDesc><table><row><cell>Rank</cell><cell>Method</cell><cell>Driving Score</cell><cell>Route Completion</cell><cell>Infraction Score</cell><cell>Vehicle Collisions</cell><cell>Pedestrian Collisions</cell><cell>Layout Collisions</cell><cell>Red light Violations</cell><cell>Offroad Infractions</cell><cell>Blocked Infractions</cell></row><row><cell>1</cell><cell>LAV</cell><cell>61.85</cell><cell>94.46</cell><cell>0.64</cell><cell>0.70</cell><cell>0.04</cell><cell>0.02</cell><cell>0.17</cell><cell>0.25</cell><cell>0.10</cell></row><row><cell>2</cell><cell>GRIAD</cell><cell>36.79</cell><cell>61.85</cell><cell>0.60</cell><cell>2.77</cell><cell>0.00</cell><cell>0.41</cell><cell>0.48</cell><cell>1.39</cell><cell>0.84</cell></row><row><cell>3</cell><cell>TransFuser+</cell><cell>34.58</cell><cell>69.84</cell><cell>0.56</cell><cell>0.70</cell><cell>0.04</cell><cell>0.03</cell><cell>0.75</cell><cell>0.18</cell><cell>2.41</cell></row><row><cell>4</cell><cell>Rails [10]</cell><cell>31.37</cell><cell>57.65</cell><cell>0.56</cell><cell>1.35</cell><cell>0.61</cell><cell>1.02</cell><cell>0.79</cell><cell>0.96</cell><cell>0.47</cell></row><row><cell>5</cell><cell>IARL [44]</cell><cell>24.98</cell><cell>46.97</cell><cell>0.52</cell><cell>2.33</cell><cell>0.00</cell><cell>2.47</cell><cell>0.55</cell><cell>1.82</cell><cell>0.94</cell></row><row><cell>6</cell><cell>NEAT [12]</cell><cell>21.83</cell><cell>41.71</cell><cell>0.65</cell><cell>0.74</cell><cell>0.04</cell><cell>0.62</cell><cell>0.70</cell><cell>2.68</cell><cell>5.22</cell></row><row><cell>7</cell><cell cols="2">Transfuser [38] 16.93</cell><cell>51.82</cell><cell>0.42</cell><cell>1.09</cell><cell>0.91</cell><cell>0.19</cell><cell>1.26</cell><cell>0.57</cell><cell>1.96</cell></row><row><cell>8</cell><cell>LBC [11]</cell><cell>8.94</cell><cell>17.54</cell><cell>0.73</cell><cell>0.40</cell><cell>0.00</cell><cell>1.16</cell><cell>0.71</cell><cell>1.52</cell><cell>4.69</cell></row><row><cell cols="2">Town</cell><cell>Town</cell><cell cols="2">Number of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Name</cell><cell>Layout</cell><cell></cell><cell>Frames</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Town01 small, EU town</cell><cell></cell><cell>46559</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Town02 small, EU town</cell><cell></cell><cell>63564</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Town03 large, US town</cell><cell></cell><cell>51896</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Town04 large, US town</cell><cell></cell><cell>46244</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Town05 large, US town</cell><cell></cell><cell>51489</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Town06 large, US town&amp;highway</cell><cell>41812</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Town07 small, US rural</cell><cell></cell><cell>55465</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Town10 small, US city center</cell><cell>42747</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Total</cell><cell></cell><cell></cell><cell>399776</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Number of frames and layouts of the training towns. List of hyperparameters.</figDesc><table><row><cell>Stage</cell><cell>Hyperameter</cell><cell>Values</cell></row><row><cell></cell><cell>batch size</cell><cell>512</cell></row><row><cell>Privileged Motion</cell><cell>learning rate others weight ? other</cell><cell>3e-4 0.5</cell></row><row><cell></cell><cell cols="2">command weight ? cmd 0.1</cell></row><row><cell>Perception &amp; Distill.</cell><cell>batch size learning rate</cell><cell>32 3e-4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 11 .</head><label>11</label><figDesc>Driving performance in test towns of models trained with different range of other vehicles. All models are the same except for other vehicles' maximum range used during training. ? 0.83 9.34 ? 0.35 0.90 ? 0.07 2.37 ? 2.08 0.00 ? 0.00 0.00 ? 0.00 0.19 ? 0.32 0.15 ? 0.26 0.00 ? 0.00Joint 28.36 ? 2.11 79.58 ? 4.99 0.34 ? 0.02 1.65 ? 0.72 0.00 ? 0.00 7.75 ? 1.70 0.45 ? 0.32 0.49 ? 0.03 0.24 ? 0.21 Staged 45.20 ? 6.35 91.55 ? 5.61 0.49 ? 0.06 0.92 ? 0.42 0.00 ? 0.00 0.33 ? 0.50 0.28 ? 0.28 0.27 ? 0.01 0.01 ? 0.02 Table 12. Driving performance in test towns of models with different perception training scheme. All models are the same except for perception training. ? 2.86 35.85 ? 2.91 0.42 ? 0.03 9.15 ? 3.88 0.00 ? 0.00 9.50 ? 2.02 0.33 ? 0.36 4.11 ? 2.11 1.41 ? 1.22 K = 1 21.30 ? 1.10 85.90 ? 2.46 0.25 ? 0.01 2.09 ? 0.10 0.00 ? 0.00 5.58 ? 0.28 0.35 ? 0.26 0.93 ? 0.08 0.03 ? 0.03 K = 5</figDesc><table><row><cell>Perception</cell><cell>Driving</cell><cell>Route</cell><cell>Infraction</cell><cell>Vehicle</cell><cell>Pedestrian</cell><cell>Layout</cell><cell>Red light</cell><cell>Offroad</cell><cell>Blocked</cell></row><row><cell>Training</cell><cell>Score</cell><cell>Completion</cell><cell>Score</cell><cell>Collisions</cell><cell>Collisions</cell><cell>Collisions</cell><cell>Violations</cell><cell>Infractions</cell><cell>Infractions</cell></row><row><cell cols="2">None 8.47 Refinement Driving</cell><cell>Route</cell><cell>Infraction</cell><cell>Vehicle</cell><cell>Pedestrian</cell><cell>Layout</cell><cell>Red light</cell><cell>Offroad</cell><cell>Blocked</cell></row><row><cell>Iteration</cell><cell>Score</cell><cell>Completion</cell><cell>Score</cell><cell>Collisions</cell><cell>Collisions</cell><cell>Collisions</cell><cell>Violations</cell><cell>Infractions</cell><cell>Infractions</cell></row><row><cell>K = 0</cell><cell>12.69</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://leaderboard.carla.org/leaderboard/ 2 https://ml4ad.github.io/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://waymo.com/intl/en_us/dataset-downloadterms/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowlegments</head><p>We thank Tianwei Yin for his help on pillar generation codes, Xingyi Zhou and Jeffrey Zhang for feedback on writing and figures. We thank TACC for providing part of our computing resources. This work was supported by the NSF Institute for Foundations of Machine Learning and NSF award #1845485.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">Carla autonomous driving leaderboard</title>
		<imprint>
			<date type="published" when="2021-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Expert drivers for autonomous driving</title>
		<ptr target="https://kait0.github.io/files/masterthesisbernhardjaeger.pdf" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Team victortango&apos;s entry in the darpa urban challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bacha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheryl</forename><surname>Bauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruel</forename><surname>Faruque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Terwelp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Reinholtz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Wicks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Alberi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of field Robotics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Ogale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Intentnet: Learning to predict intention from raw sensor data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mp3: A unified model to map, perceive, predict and plan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Sadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">GRI: general reinforced imitation and its application to vision-based autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Chekroun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Toromanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Hornauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Moutarde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deepdriving: Learning affordance for direct perception in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to drive from a world on rails</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning by cheating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brady</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neat: Neural attention fields for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021. 1</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer Caglar Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Fethi Bougares Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end driving via conditional imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lookout: Diverse multifuture prediction and planning for self-driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Sadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Practical search techniques in path planning for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitri</forename><surname>Dolgov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Montemerlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Diebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STAIR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Carla: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Psiphi-learning: Reinforcement learning with demonstrations using successor features and inverse temporal difference learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Filos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><surname>Lyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natasha</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Farquhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12560</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Strobe: Streaming object detection from lidar packets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davi</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vectornet: Encoding hd maps and agent dynamics from vectorized representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congcong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d-lanenet: end-to-end 3d multiple lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafi</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Pe&amp;apos;er</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Lahav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Levi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Genlanenet: A generalized and scalable approach for 3d lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peitao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weide</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghao</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae</forename><forename type="middle">Eun</forename><surname>Choe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fiery: Future instance prediction in bird&apos;s-eye view from surround monocular cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zak</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sof?a</forename><surname>Dudas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Hawke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Predictionnet: Real-time joint probabilistic traffic prediction for planning, control, and simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kamenev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ollin</forename><surname>Boer Bohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishwar</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nist?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolai</forename><surname>Smolyanskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.11094</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to drive in a day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Hawke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Janz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Przemyslaw</forename><surname>Mazur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John-Mark</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinh-Dieu</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Desire: Distant future prediction in dynamic scenes with interacting agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A perception-driven autonomous urban vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>How</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitch</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaston</forename><surname>Fiore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Frazzoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Field Robotics</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Map-based precision vehicle localization in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Levinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Montemerlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hdmapnet: An online hd map construction and evaluation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microscopic traffic simulation using sumo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">Alvarez</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Behrisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Bieker-Walz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Erdmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Pang</forename><surname>Fl?tter?d</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Hilbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonhard</forename><surname>L?cken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Rummel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evamarie</forename><surname>Wie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITSC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gellert Mattyus, Namdar Homayounfar, Shrinidhi Kowshika Lakshmikanth, Andrei Pokrovsky, and Raquel Urtasun</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Tartavull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Ioan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>B?rsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Exploiting sparse semantic hd maps for self-driving vehicle localization</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to simulate self-driven particles system with coordinated policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><forename type="middle">Ming</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Alvinn: An autonomous land vehicle in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pomerleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-modal fusion transformer for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arroyo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Conditional affordance learning for driving in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CoRL</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scalability in perception for autonomous driving: Waymo open dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Trafficsim: Learning to simulate realistic multi-agent behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Regalado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rates of motor vehicle crashes, injuries and deaths in relation to driver age, united states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Tefft</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">AAA Foundation for Traffic Safety</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-to-end model-free reinforcement learning for urban driving using implicit affordances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Toromanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilie</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Moutarde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Autonomous driving in urban environments: Boss and the urban challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Urmson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Anhalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tugrul</forename><surname>Duggins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Galatali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pointpainting: Sequential fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bassam</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Center-based 3d object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Abbas Sadat, Bin Yang, Sergio Casas, and Raquel Urtasun. Endto-end interpretable neural motion planner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning by watching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimuyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eshed</forename><surname>Ohn-Bar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Tnt: Targetdriven trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">High definition mapbased vehicle localization for highly automated driving: Geometric analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinling</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICL-GNSS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Tracking objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">End-to-end multi-view fusion for 3d object detection in lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
