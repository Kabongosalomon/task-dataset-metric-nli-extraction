<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cloning Outfits from Real-World Images to 3D Characters for Generalizable Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Wang</surname></persName>
							<email>yanan.wang.cs@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Liang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Masdar City</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
							<email>scliao@ieee.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence (IIAI) 1 Mohamed bin Zayed University of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cloning Outfits from Real-World Images to 3D Characters for Generalizable Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Shengcai Liao is the Corresponding Author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, large-scale synthetic datasets are shown to be very useful for generalizable person re-identification. However, synthesized persons in existing datasets are mostly cartoon-like and in random dress collocation, which limits their performance. To address this, in this work, an automatic approach is proposed to directly clone the whole outfits from real-world person images to virtual 3D characters, such that any virtual person thus created will appear very similar to its real-world counterpart. Specifically, based on UV texture mapping, two cloning methods are designed, namely registered clothes mapping and homogeneous cloth expansion. Given clothes keypoints detected on person images and labeled on regular UV maps with clear clothes structures, registered mapping applies perspective homography to warp real-world clothes to the counterparts on the UV map. As for invisible clothes parts and irregular UV maps, homogeneous expansion segments a homogeneous area on clothes as a realistic cloth pattern or cell, and expand the cell to fill the UV map. Furthermore, a similaritydiversity expansion strategy is proposed, by clustering person images, sampling images per cluster, and cloning outfits for 3D character generation. This way, virtual persons can be scaled up densely in visual similarity to challenge model learning, and diversely in population to enrich sample distribution. Finally, by rendering the cloned characters in Unity3D scenes, a more realistic virtual dataset called ClonedPerson is created, with 5,621 identities and 887,766 images. Experimental results show that the model trained on ClonedPerson has a better generalization performance, superior to that trained on other popular real-world and synthetic person re-identification datasets. The ClonedPerson project is available at https:// github.com/ Yanan-Wangcs/ ClonedPerson.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The generalization of person re-identification has gained increasing attention in recent years. One way to improve generalization is to develop large-scale and diverse training datasets. However, collecting person images from surveillance videos is privacy sensitive, and the further data annotation is expensive. Therefore, recently, synthetic person re-identification datasets have been actively developed due to their advantages of no privacy concern and no annotation cost <ref type="bibr">[3,</ref><ref type="bibr">4,</ref><ref type="bibr" target="#b24">25]</ref>. For example, RandPerson <ref type="bibr" target="#b28">[29]</ref> automatically creates large-scale random 3D characters with 8,000 identities, rendered from simulation of surveillance environments in Unity3D <ref type="bibr" target="#b26">[27]</ref>. It is also proved in <ref type="bibr" target="#b28">[29]</ref> that large-scale synthetic datasets are very useful to improve generalization. Similar findings are also observed in the following work UnrealPerson <ref type="bibr" target="#b32">[33]</ref>. However, synthesized persons in existing datasets are quite different from realistic persons, because synthesized persons are mostly cartoon-like and dress in random collocation. This clear domain gap limits the performance of models trained on such synthetic datasets.</p><p>On the other hand, some researchers proposed to generate 3D human body models from real-world person images <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>, targeting at high-fidelity reconstruction. These methods try to generate 3D body shapes and the associated textures simultaneously, through deep neural networks. They can help reduce the gap between synthetic and realistic person images to some extent due to the input of real-world clothes textures. However, current methods are still not satisfactory as the results are usually blur, and there are many artifacts, e.g. in back views (see <ref type="figure" target="#fig_10">Fig. 8c</ref>).</p><p>Considering the above, in this work, an automatic approach is proposed to directly clone the whole outfits from real-world person images to virtual 3D characters. By doing so, we would like to achieve two goals 1 : (1) the directly cloned clothes textures are clear and sharp in looking; and (2) by cloning the whole outfit, the virtual person thus created will appear very similar to its real-world counterpart, in similar clothes and dress collocation. Specifically, inspired from the UV texture mapping <ref type="bibr">[5]</ref> method developed in RandPerson <ref type="bibr" target="#b28">[29]</ref>, in this work, two cloning methods for UV maps are designed, namely registered clothes mapping and homogeneous cloth expansion. Registered mapping targets at regular UV maps where clothes appear in regular shapes and structures. Based on clothes keypoints detected on real-world person images and labeled on UV maps, registered mapping applies perspective homography <ref type="bibr" target="#b25">[26]</ref> to warp real-world clothes to the counterparts on the UV map. Homogeneous expansion is for invisible clothes parts and irregular UV maps. An optimization algorithm is proposed to find a large homogeneous area on clothes, use it as a realistic cloth pattern or cell, and expand the cell to fill the UV map. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the pipeline of the proposed method.</p><p>Furthermore, a general principle is established to scale up virtual 3D character creation, that is, it should expand both densely in similarity and diversely in population. The former one is to challenge discriminative model learning by providing similar persons, while the latter is to enrich the diversity in sample space. A similarity-diversity expansion strategy is thus proposed. Thanks to the proposed clothes cloning method, this can conveniently be achieved by clustering real-world person images and a controlled sampling of the clustered images for 3D character generation.</p><p>Eventually, the generated 3D characters are imported into Unity3D virtual environments to render a more realistic virtual dataset, called ClonedPerson, with 763,953 images from 4,826 characters for training, and 123,813 images of 795 characters for testing. Experimental results show that the similarity-diversity expansion strategy is effective, and the model trained on the ClonedPerson dataset has a better generalization performance, surpassing the models trained on various real-world and synthetic datasets.</p><p>In summary, our main contributions are: <ref type="bibr" target="#b36">(1)</ref> We propose fidelity reconstruction of identifiable biometric signatures, e.g. faces, may also raise privacy concerns.</p><p>Dataset #ID #Cam #BBox Sur RealOutfit SOMAset <ref type="bibr">[4]</ref> 50 250 100,000 No No No SyRI <ref type="bibr">[3]</ref> 100 280 56,000 No No No PersonX <ref type="bibr" target="#b24">[25]</ref> 1,266 <ref type="bibr">6 273,</ref><ref type="bibr">456 No</ref> No No RandPerson 2 <ref type="bibr" target="#b28">[29]</ref> 8,000 <ref type="bibr">19 1,801,816</ref>  an automatic pipeline to clone outfits from real-world person images to virtual 3D characters, such that they look very similar to their real-world counterparts, with clear clothing textures; (2) two cloning methods, registered clothes mapping and homogeneous cloth expansion, are designed to fulfill this task; (3) a similarity-diversity expansion strategy is proposed, based on clustering of person images and controlled sampling, to scale up 3D character creation densely in similarity and diversely in population; and (4) a largescale synthetic dataset called ClonedPerson, with 887,766 images of 5,621 characters, is created, which results in a better generalization performance than other popular realworld and virtual person re-identification datasets. All used and designed methods are listed in <ref type="table" target="#tab_4">Table A</ref> of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Collecting and manually labeling real-world person reidentification datasets are expensive and privacy-sensitive. In contrast, the use of synthetic data can reduce the cost of manual labeling, and synthetic datasets do not have privacy issues. For synthetic datasets, SyRI <ref type="bibr">[3]</ref> and PersonX <ref type="bibr" target="#b24">[25]</ref> used limited hand-made characters to generate data. In contrast, RandPerson <ref type="bibr" target="#b28">[29]</ref> proposed a clever way to generate new-looking clothes models by replacing UV maps of exist-ing 3D clothes models with neutral images or random color and texture patterns, and designed an automatic pipeline in MakeHuman <ref type="bibr">[6]</ref> to scale up character generation. Besides, similar to real-world environments, <ref type="bibr" target="#b28">[29]</ref> simulated camera networks in Unity3D to render and record moving person videos. Moreover, <ref type="bibr" target="#b28">[29]</ref> proved that models trained on synthetic data generalize well on real-world datasets. Following RandPerson, UnrealPerson <ref type="bibr" target="#b32">[33]</ref> improved the accuracy by using real-world person images to create virtual characters, and rendering with the powerful Unreal Engine 4 (UE4) <ref type="bibr" target="#b36">[1]</ref> with four large and realistic scenes. Specifically, it cropped blocks from segmented clothing images to directly replace UV maps of existing 3D clothes models. However, as shown in <ref type="figure" target="#fig_12">Fig. 7</ref>, this way still results in unrealistic-looking characters due to scale alignment issue. Statistics of some synthetic datasets are shown in <ref type="table" target="#tab_4">Table 1</ref>.</p><p>On the other hand, one may consider using virtual tryon methods to generate synthesized persons. These methods aim to transfer a target clothing onto a reference person. However, existing virtual try-on methods <ref type="bibr">[11,</ref><ref type="bibr" target="#b30">31]</ref> are mostly in 2D, which cannot generate 3D clothed human models, and thus cannot import them into virtual environments for comprehensive rendering. On the other hand, some existing methods, e.g. PIFu <ref type="bibr" target="#b22">[23]</ref>, targets at high-fidelity reconstruction of 3D persons from 2D images. However, such methods require ground-truth of 3D shapes and textures for training, which is quite expensive and limited in scale. Recently, some methods, e.g. HPBTT <ref type="bibr" target="#b33">[34]</ref> tried training 3D reconstruction models from only 2D images. However, they are based on generative models, which usually result in blurred textures and artifacts. Besides, Pix2Surf <ref type="bibr" target="#b20">[21]</ref> proposed to transfer texture from clothing images to 3D humans by neural networks. It achieved a good quality by training a specific model for every category of clothes. However, extending to other categories is costly. Furthermore, since HPBTT and Pix2Surf are both based on SMPL <ref type="bibr" target="#b19">[20]</ref>, they are not able to handle long skirts, as shown in <ref type="figure" target="#fig_12">Fig. 8</ref>.</p><p>Therefore, to further reduce the gap between virtual characters and realistic persons, we follow the way of Rand-Person in repainting UV maps of existing 3D clothing models. However, different from RandPerson and UnrealPerson which directly replace existing UV maps by other images, we design two cloning methods for structure-aware, finegrained repainting of UV maps. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the pipeline of the proposed ClonedPerson approach, which includes the following steps. Firstly, we apply pre-processing steps, including pedestrian detection, pose detection, clothes detection, and clothes keypoint detection to get qualified frontal-view person images and ob-  tain the clothes positions, categories, and clothes keypoints. Next, two cloning methods, registered mapping and homogeneous expansion, are applied to clone clothes from person images to UV texture maps and generate 3D characters. Finally, following RandPerson <ref type="bibr" target="#b28">[29]</ref>, these characters are imported into Unity3D to render synthesized person images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">3D Virtual Character Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pipeline Overview</head><p>Several pre-processing steps are implemented to fulfill our target. For example, to clone the full-body outfits from real-world person images to virtual 3D characters, we apply person detection to localize full-body person images, and remove standalone and occluded clothes. Besides, we design a number of rules based on pose detection to cherrypick 4 non-occluded frontal-view person images, since they best show clothing patterns and collocations. Due to space limits, pre-processing steps are introduced in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Registered Clothes Mapping</head><p>With 3D clothes models available in the MakeHuman community, we obtain some clothes models with regular UV maps, where clothes appear in regular shapes and structures, as <ref type="figure" target="#fig_1">Fig. 2b</ref> shows. With these regular UV maps, we apply perspective homography <ref type="bibr" target="#b25">[26]</ref> to map real-world clothes textures to UV maps of 3D characters, so that the original texture structures in the clothes can be well kept, and will appear to be clear and sharp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Perspective homography</head><p>Perspective homography is also known as perspective transformation <ref type="bibr">[2,</ref><ref type="bibr" target="#b25">26]</ref>. Given a set of 2D points {p i } and a corresponding set of points {p i }, augmented with homogeneous coordinates (appending 1 as the z coordinate), perspective homography maps each p i to p i by a homography matrix H ? R 3?3 , that is, p = Hp.</p><p>Then, we can compute the homography matrix H by solving the following optimization problem:</p><formula xml:id="formula_0">min H n i=1 p i ? Hp i 2 2 ,<label>(1)</label></formula><p>where n is the number of the corresponding points. Eq.</p><p>(1) defines a least squares problem and thus can be easily solved. In addition, the computed homography matrix H can be refined with the Levenberg-Marquardt method <ref type="bibr" target="#b12">[14]</ref> to further reduce the re-projection error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Perspective warping</head><p>In our task, given labeled clothes keypoints p i on regular UV texture maps (e.g. <ref type="figure" target="#fig_1">Fig. 2b</ref>), and the corresponding detected clothes keypoints p i on real-world person images (e.g. <ref type="figure" target="#fig_1">Fig. 2a</ref>), we can solve Eq. (1) and get the homography matrix H. Then, each pixel location p on the UV map will have a corresponding pixel location on the input image, by [x, y, z] T = Hp. Besides, we need to set the z coordinate of all the resulting points to 1 before the warping process, as the transformation operates on homograph coordinates. That is, p = x z , y z , where p represents the corresponding point on the clothes image. Finally, the perspective warping can be done by bilinear interpolation on the clothes image and use the resulting pixel values to fill the UV map <ref type="bibr" target="#b25">[26]</ref>. Specifically, by traversing p on UV map in turn, a corresponding point p =Hp with float numbers of coordinates on the clothing image will be determined. Then, four pixels around p will be bilinearly interpreted into p. An example is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, where the red dots represent the corresponding keypoints. To reduce background influence, we set the outer part of the clothes in black.</p><p>In this paper, with most regular UV maps we directly calculate perspective homography through all the clothes keypoints. However, as shown in <ref type="figure" target="#fig_1">Fig. 2b</ref>, the shapes of the long-sleeved and trousers on the UV map are quite different from those usually appear in person images. In these cases, we calculate the perspective homography on each part of them separately, then warp the clothes parts to the UV texture map and combine the results. For example, pants could be split into left and right sides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Homogeneous Cloth Expansion</head><p>Registered clothes mapping can handle the clothes texture of the frontal side very well. However, the back side is usually different from the frontal side but invisible. Therefore, we further design the homogeneous cloth expansion method to find a homogeneous area on clothes as a realistic cloth cell, and expand the cell to fill the UV map. Besides, as <ref type="figure" target="#fig_1">Fig. 2c</ref> shows, the UV texture maps of some 3D clothes models are irregular, with unclear clothes structures. This also prevents the application of the registered clothes mapping. Therefore, we use the homogeneous cloth expansion to handle irregular UV maps, and enable more clothes models and styles. In our experiments, we have 161 3D clothes models with regular UV maps, and 17 models with irregular UV maps, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Cloth segmentation</head><p>The homogeneous cloth expansion includes two steps, cloth segmentation and cloth expansion. For cloth segmentation, an optimization algorithm is proposed to find a large homogeneous area on clothes, and use it as a realistic cloth cell. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we first crop the clothes area, and use a model trained on MSMT17 <ref type="bibr" target="#b29">[30]</ref> by QAConv 2.0 <ref type="bibr" target="#b16">[18]</ref> to extract the layer2 feature map (48 ? 16) of this clothes image. The purpose of the QAConv model is to extract discriminant feature maps to find homogeneous cells, so as to reduce the influence of image noises. Then, square blocks of various scales are defined on the feature map. Within each block, the average and standard deviation of the feature values are computed, as follows:</p><formula xml:id="formula_1">? k = 1 n k n k i=1 x k i , ? k j = 1 n k ? 1 n k i=1 (x k ij ? ? k j ) 2 ,<label>(2)</label></formula><p>where k denotes the kth block, n k is the number of elements in that block, x k i ? R d is the feature vector of the ith element in block k, with d = 512 dimensions, and j denotes the jth dimension. Note that the standard deviation is computed per feature channel. This value estimates the variations within each block, and thus reflects how homogeneous the cloth is within that block. Furthermore, we would also like the selected block to be as large as possible. Therefore, we further compute the area A k of each block k, and define a ratio R as our objective function for the optimization problem, as follows:</p><formula xml:id="formula_2">K min k=1 R k = 1 d d j=1 ? k j A k ,<label>(3)</label></formula><p>where K denotes the number of blocks. By optimizing the above objective, we obtain a cloth area, with textures within it as homogeneous as possible, and with the area as large as possible. Then, we locate this block on the input clothes image and crop it, resulting in a patch which we call cloth cell. Appendix <ref type="figure" target="#fig_12">Fig. I</ref> shows some cloth cells thus obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Cloth Expansion</head><p>As described above, the homogeneous cloth expansion is applied for both regular UV maps and irregular UV maps. For regular UV maps, it is used to fill the back side of the clothes area, as well as the background. Since we already apply the registered clothes mapping for the frontal side of the clothes on regular UV maps, there exists a scale alignment problem for the cloth cell to be filled on the same UV map. Therefore, to maintain the consistency of the texture of the clothes, we need to scale the homogeneous cloth cell. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, let W c and H c be the width and height, respectively, of the clothes image, W a and H a be the width and height, respectively, of the cropped cloth cell from the clothes image, W t and H t be the width and height, respectively, of the target area of the clothes after registered mapping, then, W s and H s , the width and height, respectively, of the cell to be scaled can be computed as follows:</p><formula xml:id="formula_3">W s = W a W c ? W t , H s = H a H c ? H t .<label>(4)</label></formula><p>Then, the scaled cloth cell is expanded on the whole UV map besides the registered clothes mapping area, by alternating flipping and tiling. As for irregular UV maps, since there is no reference of the scale, we simply use the original shape of the homogeneous cloth cell to flip and tile until fully fill the whole UV map, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>Note that besides the homogeneous cloth expansion , a simple way is to resize the cloth cell directly as a UV map, as done in RandPerson and UnrealPerson. However, simply resizing the cloth cells may result in blur textures and unrealistic patterns, as compared in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Similarity-Diversity Expansion</head><p>We use both DeepFashion (Apache License 2.0) <ref type="bibr" target="#b18">[19]</ref> and DeepFashion2 <ref type="bibr">[9]</ref> images for our virtual data creation. Through the pre-processing steps, there are still tens of thousands of images that are qualified and can be cloned to virtual characters. However, because of the enormous volume of images and repeating images of the same person, using these images directly to create characters is not efficient. To address this, two principles are considered. First, the more diverse samples, the better generalization performance should be. Second, similar person images are able to make the model training pay more attention to subtle differences. According to <ref type="bibr" target="#b32">[33]</ref>, similar characters as hard samples take a positive effect for person re-identification with large number of identities and cameras. Therefore, we propose a similarity-diversity expansion strategy to scale up virtual character creation while improving along both the similarity and diversity aspects, as illustrated in <ref type="figure" target="#fig_12">Fig. 4</ref>. By clustering person images, we can create similar characters from the same cluster, while increase diversity by including more and more clusters. This way, the created characters can expand densely in similarity and diversely in population. Specifically, this strategy first applies DBSCAN <ref type="bibr">[7]</ref> to cluster person images, then it samples a certain number of images per cluster, and finally clones outfits from these images for 3D character generation. In this way, we can generate similar characters in the same cluster and diverse characters with different clusters <ref type="bibr">5</ref> .</p><p>For the clustering, we use the same model trained on MSMT17 <ref type="bibr" target="#b29">[30]</ref> by QAConv 2.0 <ref type="bibr" target="#b16">[18]</ref> to extract feature maps and compute similarity scores between person images. Then, DBSCAN is applied, with different parameters to control the degree of similarity. Specifically, to remove repeating persons, we set =0.4 to cluster the same person with the same outfits. <ref type="figure" target="#fig_7">Fig. 5a</ref> shows two examples where images from the same cluster are with the same person. Next, we select one image per cluster (closest to the cluster center) and remove other redundant images. Together with other images failed to be clustered (with label -1), the second round of clustering is performed, with =0.5. As shown in <ref type="figure" target="#fig_8">Fig. 5b</ref>, this time images from the same cluster are visually similar but generally from different persons.</p><p>Finally, we select seven images (five for training and two for testing) per cluster to generate characters. Following RandPerson <ref type="bibr" target="#b28">[29]</ref>, these characters are imported into Unity3D to render synthesized person images. We implement some adjustments to improve the rendering, as detailed in Appendix E. Accordingly, we create 5,621 characters with 887,766 images, as the ClonedPerson dataset, with 763,953 images from 4,826 characters for training, and 123,813 images from 795 characters for testing. The Statistics of the dataset are shown in <ref type="table" target="#tab_4">Table 1</ref>. We summarize the details and statistics of each step in our pipeline in Appendix F. <ref type="figure" target="#fig_12">Fig. 4</ref> and <ref type="figure" target="#fig_10">Fig. 7c</ref> (more in the Appendix) show some characters created in ClonedPerson.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>Three real-world person re-identification datasets, CUHK03 <ref type="bibr" target="#b13">[15]</ref>, Market-1501 <ref type="bibr" target="#b34">[35]</ref>, and MSMT17 <ref type="bibr" target="#b29">[30]</ref>, are used for generalization evaluation. The CUHK03 dataset includes 14,097 images of 1,467 individuals. There are 7,365 images of 767 identities in the training set, and 6,732 images of 700 identities in the test set, according to the CUHK03-NP protocol <ref type="bibr" target="#b35">[36]</ref>. The detected bounding boxes are used. The Market-1501 dataset includes 32,668 images of 1,501 identities captured from six cameras. 12,936 images of 751 identities are included in the training set, and the remaining 19,732 images of 750 identities are used for the test set. The MSMT17 dataset includes 126,441 images of 4,101 identities and divided into 32,621 images of 1,041 identities for training, and the remaining 93,820 images of 3,060 identities for testing.</p><p>We use two other synthetic datasets, RandPerson <ref type="bibr" target="#b28">[29]</ref> and UnrealPerson <ref type="bibr" target="#b32">[33]</ref>, for comparison, since they are shown to be superior to other synthetic datasets for generalizable person re-identification. RandPerson contains 8,000 identities and 1,801,816 images with 19 cameras. Both the full set and the suggested subset (132,145 images of the 8,000 identities) are used for our experiments. Besides, since some rendering setups are modified in this work, we further render RandPerson characters in our conditions for a fair comparison. This is denoted by RandPerson * (RP * ). UnrealPerson releases 6,799 characters with 1,256,381 images. Both the full set and the suggested subset (120,000 images of 3,000 identities) are used for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Methods</head><p>The validation of the proposed ClonedPerson is through person re-identification experiments. We mainly consider two tasks, generalizable person re-identification <ref type="bibr">[12,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b31">32]</ref>, and unsupervised domain adaptation (UDA). We apply QA-Conv 2.0 <ref type="bibr" target="#b16">[18]</ref> and TransMatcher <ref type="bibr" target="#b15">[17]</ref> for the former, and SpCL <ref type="bibr">[10]</ref> for the latter. All of them are under the MIT License. We keep the same settings for each method.</p><p>All evaluations follow the single-query evaluation protocol <ref type="bibr">[8]</ref>. We use the Cumulative Matching Characteristic (CMC) <ref type="bibr" target="#b21">[22]</ref>, especially the Rank-1 accuracy, and the mean Average Precision (mAP) <ref type="bibr" target="#b23">[24]</ref> as the performance metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Generalizable Person Re-Identification</head><p>The mAP results of direct cross-dataset evaluation are shown in <ref type="table" target="#tab_2">Table 2</ref> comparing real-world datasets with QA-Conv 2.0, and <ref type="table" target="#tab_4">Table 4</ref> comparing synthetic datasets with QAConv 2.0 and TransMatcher. Rank-1 results are reported in <ref type="table" target="#tab_4">Appendix Table C</ref>. In overall, ClonedPerson achieves the best performance, surpassing existing datasets of both synthetic and real-world. The better performance over existing real-world datasets further confirms the findings in <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b32">[33]</ref>. Besides, ClonedPerson is much better than Un-realPerson on CUHK03 and Market-1501, while they are comparable on MSMT17. Note that scenes used by Unre-alPerson are more large and realistic than ours, due to the powerful UE4 engine. Besides, UnrealPerson has ten more cameras than ClonedPerson. Note also that, by comparing to both full set and subset results of RandPerson and Unre-alPerson, it is clear that ClonedPerson's better performance is not because it is larger, but because of its capability of cloning the whole outfits from person images.</p><p>Moreover, by comparing RandPerson * to RandPerson, the new rendering settings are more effective. Besides, compared to RandPerson * with the same rendering setting, ClonedPerson has gained an averaged improvement of about 2% in mAP. This is encouraging since ClonedPerson has only 4,826 identities, compared to 8,000 in RandPerson. Furthermore, with the same learned QAConv models in <ref type="table" target="#tab_2">Tables 2 and 4</ref> the ClonedPerson testing set, with results shown in <ref type="table" target="#tab_4">Table  3</ref>. First, with within-dataset evaluation, QAConv achieves 91.1% in Rank-1 and 68.9% in mAP, indicating that this synthetic domain can be reasonably fitted by a representative method. Second, with cross-dataset evaluation, it can be seen that all models trained on real-world datasets perform not satisfactory on ClonedPerson, indicating that ClonedPerson is quite different and challenging. Nevertheless, it can still be observed that MSMT17 and Market-1501 are more diverse for generalization than CUHK03.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Unsupervised Domain Adaptation</head><p>As for UDA, we conducted experiments with SpCL, using ClonedPerson as source training data or target testing data. Since the whole training set of ClonedPerson is too large for SpCL to handle, e.g. in its clustering stage, we also selected a training subset of ClonedPerson for SpCL, with one image per camera, and 75,830 images in total from the 4,826 subjects. With ClonedPerson as source training data, the results are shown in <ref type="table" target="#tab_4">Table 4</ref>. It shows that on average ClonedPerson outperforms both RandPerson and Un-realPerson, especially, with a large margin on CUHK03.</p><p>With ClonedPerson as target dataset, the results are shown in <ref type="table" target="#tab_4">Table 3</ref>. Similar as cross-dataset evaulation, the UDA results on ClonedPerson are also poor. Furthermore, we also conduct an unsupervised learning task on Cloned-Person by SpCL, as shown in <ref type="table" target="#tab_4">Table 3</ref>. Again, the results are poor. Therefore, it appears that, for SpCL, realworld source training data does not help much in domain adaptation to ClonedPerson, and thus the poor performance  is mainly due to the unique challenges in ClonedPerson for clustering-based identity label reasoning. For example, there are a large number of diverse cameras and a lot of similar persons created by the proposed similarity-diversity expansion strategy. Consequently, though ClonedPerson is a synthesized dataset, it may provide a good test bed for both domain generalization and domain adaptation, and challenge researchers in developing more effective algorithms. After the clustering procedure described in Sec. 4, we obtain 968 clusters. Then, first, we use all the clusters for maximum diversity, and select different numbers of images per cluster for experiments, indicating increasing similarity. <ref type="figure" target="#fig_4">Fig. 6a</ref> shows the performance. As the number of selected images increases, the performance clearly increases. Therefore, it proves that creating similar persons is indeed important for discriminant model learning, since it has to pay more attention to fine details of characters. However, it is saturating when the number of images per cluster reaches five. Therefore, to avoid data redundancy and improve efficiency, we select five images per cluster for the training set, and treat the remaining as a separate testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparison of Different Generation Settings</head><p>Next, we keep the similarity level consistent, with five images per cluster, and select different numbers of clusters for experiments, indicating increasing diversity. As <ref type="figure" target="#fig_4">Fig. 6b</ref> shows, the performance increases as the number of clusters increases, which aligns with our expectations that performance raises as the diversity increases.</p><p>However, the adjustment of diversity and similarity will inevitably cause changes in identities, which might influence performance. Therefore, we keep the number of identities consistent by balancing the variation of similarity and diversity. That is, the selected clusters are gradually reduced when the number of images per cluster increases. The results are shown in <ref type="figure" target="#fig_4">Fig. 6c</ref>. The performance fluctuates in a small range within 0.3%, indicating that both similarity and diversity are important in our virtual data creation.</p><p>Finally, we also compare the random creation method that randomly selects person images for texture cloning and character creation before our clustering step. <ref type="figure" target="#fig_4">Fig. 6d</ref> shows the comparison of this random creation method to our strategies in <ref type="figure" target="#fig_4">Fig. 6a</ref> and <ref type="figure" target="#fig_4">Fig. 6b</ref>. From the results it is clear that with random creation after 3,000 characters the performance is saturating. In contrast, the proposed similarity-diversity expansion strategy is much more efficient in scaling up the virtual character creation, especially with larger number of identities. Therefore, the above analysis proved that the similaritydiversity expansion is effective and efficient in scaling up the virtual character creation, and is potentially useful in creating an even larger and effective dataset when more person image sources are considered, considering the trend in <ref type="figure" target="#fig_4">Fig. 6b</ref>. In contrast, in UnrealPerson <ref type="bibr" target="#b32">[33]</ref> the conclusion is that it can only achieve the best performance with 3,000 characters, but more characters do not help. This is also verified in our experiments with UnrealPerson in Tab. 4. real clothes textures, most of its created characters do not match real-life clothes due to the scale alignment issue of cloth patterns. In contrast, thanks to the designed cloning pipeline, the ClonedPerson characters are more realistic and dressed more like real-life persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Qualitative Comparisons</head><p>Furthermore, <ref type="figure" target="#fig_12">Fig. 8</ref> shows a qualitative comparison between models created by our method and HPBTT <ref type="bibr" target="#b33">[34]</ref>. It can be observed that characters created by the proposed method have clearer and sharper clothes textures, and better back-view looking of the clothes, than that generated by HPBTT. Besides, from the results shown in the first row, it can be seen that in ClonedPerson the clothes category is preserved, while HPBTT fails to deal with long skirts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>This paper contributes an automatic approach to clone the whole outfits from real-world person images to virtual 3D characters. Two critical cloning methods are proposed, registered clothes mapping and homogeneous cloth expansion. As a result, these characters bridge the gap between synthesized and realistic persons, and so models trained by our synthesized persons have better generalization ability for person re-identification. In addition, a similarity-diversity expansion strategy is proposed to scale up virtual characters. We show that similarity can help improve model's discrimination, while diversity can improve the generalization ability of the model. In the future, we could exploit more in developing different types of clothes models and exploit more data sources. Moreover, we show some limitations of this research in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cloning Outfits from Real-World Images to 3D Characters for Generalizable Person Re-Identification: Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Introduction</head><p>Due to space limits, we are not able to explain everything in detail in the main paper. In this Appendix, we further present more details of our implementations, and demonstrate more illustrations to explain our design choices. Besides, we present more experimental results for further understanding.</p><p>All methods used and designed in the project are listed in Tab. A, including existing methods, adapted methods, and the proposed methods. For example, we adapted some existing methods in the person image pre-processing stage to help cherry-pick best-viewing person images and determine clothes positions, categories and clothes keypoint locations. At the same time, we also propose some new methods, such as Registered Clothes Mapping, Homogeneous cloth Expansion and Similarity-Diversity Expansion, to achieve the goal of mapping real clothes to virtual people. Finally, we create the ClonedPerson dataset that can improve the generalization performance of person re-identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Person Image Pre-Processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Pedestrian detection</head><p>Our target is to clone the full-body outfits from realworld person images to virtual 3D characters. However, considering the variety of clothing images in real life, we need to avoid images of standalone clothes and incomplete person images. Therefore, we apply a person detection model, Pedestron <ref type="bibr">[3]</ref>, to detect qualified person images. We keep the original configuration of the Pedestron <ref type="bibr">[3]</ref> and set the detection threshold to be 0.8 to avoid images of standalone clothes and incomplete person images. Different situations of person images are shown in <ref type="figure" target="#fig_7">Fig. A</ref>. Furthermore, we set the area of the detected bounding boxes to be at least 20% of the input image to remove low-resolution persons and some false positives. The detected person images are cropped for the following pose detection procedure.</p><p>Characters in some existing synthetic datasets are dressed in random collocation, such as in RandPerson <ref type="bibr">[11]</ref> and UnrealPerson <ref type="bibr">[12]</ref>. However, random collocation sometimes creates incongruous characters, as shown in <ref type="figure" target="#fig_8">Fig. B</ref>. The left side shows person images and characters created by the proposed cloning method. The right side shows characters created by randomly combining some upper-body and lower-body clothes. We can see that the collocation on the right is inconsistent. Therefore, the use of person detection in localizing full-body person images is also for the purpose of cloning the full-body outfits from real-world person images to virtual 3D characters. As a result, the proposed method follows the original collocations of real-life persons, and so the sample distributions of our data would be more consistent with real-life persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Person view qualification by pose detection</head><p>After person detection, another problem is that person images may have different viewpoints, such as frontal view, back view, and side view. Furthermore, the frontal view images are divided into two situations: occluded and nonoccluded. For our purpose, back-view, side-view, and occluded front-view images are all incomplete displays of clothes, so they are regarded as noisy data. To reduce these noisy data, we use person pose estimation model for automatic judgment. Specifically, we apply the HRNet <ref type="bibr">[9]</ref> model from MMDetection <ref type="bibr" target="#b36">[1]</ref> to do person pose estimation. It is trained on the COCO dataset <ref type="bibr">[6]</ref>. HRNet predicts 17 body keypoints and their visibility probabilities, from which we use 12 keypoints on the body, including shoulders, elbows, hands, hips, knees, and feet. According to the positions of the shoulders, back-view images could be classified. With the width-to-height aspect ratio of the upper body, side-view images could be distinguished. Based on the position of hands and elbows, we can identify occluded images. The definition and locations of the specific keypoints used in our pipeline are shown in <ref type="figure" target="#fig_0">Fig. C(1)</ref>. Then, we can classify different situations according to the following rules: 1) Back view. The right shoulder (P 6) is on the right side of the left shoulder (P 5) on the image.</p><p>2) Side view. The width to height aspect ratio W/H of the person's upper body is less than 0.3.</p><p>3) Occluded 6 . Any hand or elbow point (P 7, P 8, P 9, P 10) is in the upper body area (the area surrounded by P 6, P 5, P 11, and P 12) or the lower body area (the area enclosed by P 12, P 11, P 13, and P 14).</p><p>For the width-to-height aspect ratio of the upper body (Rule 2), as shown in <ref type="figure" target="#fig_0">Fig. C(1)</ref> and <ref type="figure" target="#fig_2">Fig. C(3)</ref>, we consider the Euclidean distance between shoulders (P 5 and P 6) as the upper-body width W , and that between the center of the shoulder (the middle point of P 5 and P 6) and the center of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Category</head><p>Notes Person Detection Existing Pedestron <ref type="bibr">[3]</ref> Pose Detection Adapted</p><p>We used the existing HRNet [9] model from MMDetection <ref type="bibr" target="#b36">[1]</ref>. Specific rules are designed based on the detected keypoints to cherry-pick best-viewing person images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clothes Detection and Classification Adapted</head><p>We trained a model based on Faster-RCNN <ref type="bibr">[8]</ref> with the annotated clothes bounding boxes and categories in DeepFashion2 <ref type="bibr">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clothes Keypoint Detection Adapted</head><p>We annotated clothes keypoints and trained a model based on PIPNet <ref type="bibr">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Registered Clothes Mapping Proposed</head><p>We annotated clothes keypoints on regular UV maps, detected clothes keypoints on person images, and applied the perspective homography method to warp real clothes texture to UV maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Homogeneous Cloth Expansion Proposed</head><p>A new method is proposed to find a homogeneous area as large as possible on clothes images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity-Diversity Expansion</head><p>Proposed A new method is proposed to scale up virtual character creation.  the butt (the middle point of P 11, and P 12) as the height H. Then, we select qualified frontal-view images with W/H ? 0.3.</p><p>For the judgment of occlusion (Rule 3), since the detected points are not on the edge of the body, we define the upper-body and lower-body areas by expanding the surrounding points. First, we define each area's width according to the top corner-point distance of that area. Specifically, as shown in <ref type="figure" target="#fig_10">Fig. C(4)</ref>, w 1 is the width of the upperbody area, and w 2 is the width of the lower-body area. Then, we extend the upper-body area and lower-body area horizontally by w 1 =0.1?w 1 and w 2 = 0.1?w 2 , respectively.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. C, Fig. C(1)</ref> is a qualified frontalview and non-occluded image. According to the position of the shoulders (Rule 1), <ref type="figure" target="#fig_1">Fig. C(2)</ref> is a back-view image. <ref type="figure" target="#fig_0">Fig. C(1)</ref> shows an example of W/H ? 0.3, while <ref type="figure" target="#fig_2">Fig. C(3)</ref> is a side-view image because W/H &lt; 0.3 (Rule 2). <ref type="figure" target="#fig_10">Fig. C(4)</ref> is classified as an occluded image based on the position of hands (Rule 3).</p><p>Unqualified images of person views may cause some common problems for the proposed cloning method, as shown in <ref type="figure" target="#fig_12">Fig. D</ref>. For example, characters generated from back-view images may contain hairs ( <ref type="figure" target="#fig_0">Fig. D(1)</ref>). Characters created from side-view images may have strange textures ( <ref type="figure" target="#fig_1">Fig. D(2)</ref>). Besides, clothes occluded by hands may cause the generated characters containing ghost hands ( <ref type="figure" target="#fig_2">Fig. D(3)</ref>). Therefore, the proposed person view qualification step by pose detection is useful to get qualified frontal-view and non-occluded images, and thus facilitate the cloning of clean clothes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Clothes and keypoint detection</head><p>Through Appendix B.1 and Appendix B.2, we obtained images that contain persons' entire bodies and are completely visible. To achieve the mapping from real-world image to virtual character, we need to get the clothes position and type, and positions of the clothing keypoints in the image. Therefore, we further train two models: the clothes detection model and their corresponding key points detection model. (1) Qualified.</p><p>(2) Back-view.   <ref type="figure" target="#fig_0">Fig. E(1)</ref> shows the types of clothes we use, and the red points display positions of keypoints. The clothing models include eight models (long sleeves, short sleeves, sleeveless, trousers, shorts, skirts, short dresses, and long dresses). After obtaining the labeling information of the clothes keypoints, the clothes detection models and the keypoint detection models are trained separately for eight clothes models. The clothes detection model is based on the faster RCNN <ref type="bibr">[8]</ref> which predicts the bounding box localization and clothes category jointly. The keypoint detection model is based on PIPNet <ref type="bibr">[4]</ref> without Neighbor Regression Module. Finally, we detect all pose qualified images and get the clothing category and the keypoint locations of clothes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Registered Clothes Mapping</head><p>With 3D clothes models available in the MakeHuman community, we obtain some clothes models with regular UV maps, where clothes appear in regular shapes and structures, as <ref type="figure" target="#fig_1">Fig. E(2)</ref> shows. With these regular UV maps, it is possible to apply Registered Clothes Mapping to map real-world clothes textures to virtual models. However, the structure of some UV maps is not clear, so we need a way to find out its structure.</p><p>Changing the UV map will change the appearance of the 3D model because there is a correspondence between the UV map and the model. As <ref type="figure" target="#fig_12">Fig. F</ref> shows, firstly, we use a pure black image as the UV map, and get the model's frontview image as a reference image. Next, a 50 ? 50 white square is used to traverse the UV texture map and get many corresponding front-view images as response images. Then, by comparing these response images and the reference image, we could find out which area in UV maps would be mapped to the front of the model. Finally, by stacking these squares, we can get the approximate area of the texture on the front of the model. <ref type="figure" target="#fig_12">Fig. F</ref> shows some frontal areas founded by this method. Accordingly, different region division and keypoint labeling and mapping rules are designed according to different structures of the UV maps.</p><p>Multi-view strategy. Note that We aim at developing a general system that requires only one single image, as multi-view images are not always available. However, when multi-view images are available as inputs, it is quite straightforward to integrate them into different parts of regular UV maps. An example is shown in <ref type="figure" target="#fig_12">Fig. H.</ref> (1) From back-view images (1) Eight clothes types with labeled keypoints.</p><p>(2) Regular UV maps where clothes appear in regular shapes and structures.</p><p>(3) Irregular UV maps. <ref type="figure" target="#fig_12">Figure E</ref>. Different types of clothes and UV texture maps of the corresponding 3D clothes models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Homogeneous Cloth Expansion</head><p>As discussed in the main paper, to generate clothes textures for irregular UV maps, and textures on regular UV maps corresponding to invisible person parts, we further design a homogeneous cloth expansion method to find a homogeneous area on clothes as a realistic cloth cell, and expand the cell to fill the UV map. <ref type="figure" target="#fig_12">Fig. I</ref> shows some examples of the optimized cloth cells by the proposed algorithm. From these examples, we can see that the proposed method is able to find a homogeneous cloth patch as large as possible.</p><p>Besides the proposed homogeneous cloth expansion method, given a cropped cloth cell, a simple way to create a UV map is to resize the cloth cell directly as a UV map, as proposed in RandPerson <ref type="bibr">[11]</ref>, and also used in UnrealPerson <ref type="bibr">[12]</ref>. However, simply resizing the cloth cells may result in blur textures and unrealistic patterns. For example, <ref type="figure" target="#fig_12">Fig. J</ref> shows a comparison between resizing and the proposed expansion methods. As can be seen, characters created by the proposed expansion method have more realistic textures, while those created by resizing are usually blur. Besides, textures created by resizing usually do not match the pattern scale of the original clothes, and thus are not able to represent the original clothes. This can also be observed from synthesized images of UnrealPerson . In contrast, the proposed expansion method usually has a better consistency of pattern scales.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Unity3D Simulation and Rendering</head><p>As for the rendering process, we follow RandPerson <ref type="bibr">[11]</ref> for the Unity3D environment settings, including the scenes, the configuration of camera networks and character movements, video capturing, and image cropping. In addition, we implement some adjustments to improve the rendering:</p><p>Camera filter. We set post-processing effects for some cameras to increase the imaging variations and make the data more diverse. Post-processing effects include color grading, bloom, grain, and vignette provided in Unity3D.</p><p>Actions. To make the generated data closer to the realworld data, we remove the running and uncommon walking actions in RandPerson. Instead, we include the situation of hanging out in place, allowing the character to stand in place and move hands or turn around, enriching the data diversity.</p><p>Scenes and cameras. The number of cameras in each scene should be expanded to increase rendering efficiency and viewpoint diversity. Since some scenes in RandPerson are too small to expand cameras, we select five out of 11 scenes in RandPerson (scene2, 3, 5, 6, and 10) and create a new scene ourselves to get more complex lighting. We expand the number of cameras in each scene to four, making each scene's proportion in the database more balanced. In total, RandPerson uses 19 cameras in 11 scenes, while we use six scenes with 24 cameras. <ref type="figure" target="#fig_12">Fig. K</ref> shows the six scenes with 24 cameras we use.</p><p>Image cropping. We make further improvements with RandPerson's image cropping strategy by introducing random disturbances to the cropping. Cropped persons in RandPerson are mostly complete and well-aligned. However, there are many incomplete and misaligned person images in real-world datasets. Therefore, we make random disturbances to the cropping to simulate partially visible and misaligned person images. Specifically, let the width and height of the original image be W and H, respectively. For each image, with a probability ?=30% we randomly choose to further crop the image. Then, for the selected image with further cropping, we remove the top 0-0.1H part of  the image randomly, and remove the bottom 0-0.5H part randomly. Then we randomly use one of the three strategies (left side only, right side only, and both sides) to remove some content randomly in 0-? W of the original image, where the side rate ? =0.3 by default. <ref type="figure" target="#fig_12">Fig. L</ref> illustrates the process and some cropped examples. Tab. B shows the results of using different cropping strategies.</p><p>With the above setup, the generated 3D characters are imported into Unity3D environments to render and crop person images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. ClonedPerson Dataset</head><p>An automatic pipeline is described in the main paper to directly clone the whole outfits from real-world person images to virtual 3D characters. <ref type="figure" target="#fig_12">Fig. M</ref> shows some examples of 3D characters in ClonedPerson. For the whole process of creating the ClonedPerson dataset, the specific information in each step is detailed as follows. For each image, person detection needs 0.28s, pose detection needs 0.15s, clothes and keypoint detection needs 0.23s, clothes mapping and 3D creation needs 27.65s, and Unity3D rendering needs 16.5s. Therefore, for the whole pipeline each image costs 44.8s in total.</p><p>For training clothes detection, we use 191k diverse images of 13 popular clothing categories from DeepFash-ion2 <ref type="bibr">[2]</ref>. The clothes keypoint detection training data is composed of DeepFashion2 and crawled clothes images, in which we annotate 17k images manually. After removing the invalid images, we finally select about 10k images of eight clothing categories that we use in this paper to train the clothes detection and clothes keypoint detection models.</p><p>For cloning clothes from real-world person images to virtual characters, we use images from both DeepFashion <ref type="bibr">[7]</ref> and DeepFashion2, with a total of 409k images as our source data. By employing person detection, 146k person images are selected which contain detected persons. Then, 83k images are qualified by viewpoint judgment employing pose detection. Among them, 65k images are successfully detected with clothes bounding boxes and categories, as well as clothes keypoint positions.</p><p>In the clustering stage, we use eps=0.4 to remove 29k images due to repeating persons with the same outfits. Then, we set eps=0.5, and obtain 968 clusters with 6,340 images to create characters. Among the valid 968 clusters, we use all of the clusters and select seven images in each cluster to create our 3D characters. Since some clusters are less than seven images, finally, we get 5,621 person images as inputs and create 5,621 characters accordingly by the proposed method. After rendering and cropping, we obtain 887,766 images for the 5,621 virtual persons, and this forms our ClonedPerson dataset. Among them, we use 763,953 images from 4,826 characters for training, and 123,813 im-ages of 795 characters for testing.</p><p>Besides person re-identification, our data can also be used for other tasks e.g. person detection, person keypoint detection, multi object tracking (with videos), multi-camera multi object tracking, etc. <ref type="figure" target="#fig_12">Fig. N</ref> shows some examples of person keypoint detection on real-world images with a model trained on the ClonedPerson dataset, with automatically recorded keypoint annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Comparison of Different Cropping Strategies</head><p>Tab. B shows the performance of different cropping strategies with the cropping probability ? and side rate ? as introduced in Appendix E. Firstly, we only change the cropping probability ?. From the results shown in Tab. B, it can be observed that the best result is achieved with ?=30%. Then, we keep ?=30%, and change the side rate ? . Finally, from Tab. B it can be observed that it achieves the best performance with the cropping probability ?=30% and the side rate ? =30%. Therefore, the two values are kept as default values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Comparison to Existing Datasets</head><p>Due to space limits of the main paper, we report the detailed results of different datasets for different tasks in Tab. C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Limitations</head><p>As summarized below, this research leaves some aspects for improvements.</p><p>(1) Limited virtual character clothes models. The models we used are from the MakeHuman community, where the available models are limited. Because of the limited models, the categories of clothes can be applied are thus limited. (2) Limited data source. We mainly use images from DeepFashion and DeepFashion2 datasets to create our virtual characters. This makes the data source not diversified enough. We show a distribution of the DeepFashion and DeepFashion2 images in <ref type="figure" target="#fig_12">Fig. O</ref>. We use the same model trained on MSMT17 by QAConv 2.0 to compute similarity scores between images, and draw a sample distribution by t-SNE <ref type="bibr">[10]</ref>. By this plot, we can find that clothes in DeepFashion datasets are not diversified enough. For example, most of the images are summer clothes in white or black. Therefore, we need to exploit more data sources in the future.   clones clothes from person images, but is not capable of high-fidelity reconstruction of 3D models from person images. However, our motivation is to create diversified characters with realistic clothing and create a dataset for improved generalization. High-fidelity reconstruction is challenging and not efficient for our purpose. On the other hand, high-fidelity reconstruction of identifiable biometric signatures, e.g. faces, may also raise privacy concerns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The proposed ClonedPerson pipeline, which automatically creates similarly dressed 3D characters from person images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Different categories of clothes and UV texture maps of the corresponding 3D clothes models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Process pipeline of registered clothes mapping (top) and homogeneous cloth expansion (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 . 5 Figure 5 .</head><label>455</label><figDesc>Illustration of similarity-diversity expansion. Clustering of different values. (a) Examples of two clusters (up and down) with =0.4. (b) Examples of two clusters (left and right) with =0.5, with person images (first row) and the generated characters (second row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Performance of different character scaling up methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6</head><label>6</label><figDesc>shows performance (averaged Rank-1 and mAP of the three real-world testing datasets) with different character scaling up methods, including different settings of the proposed similarity-diversity expansion strategy, and a straightforward random scaling up strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 Figure 7 .Figure 8 .</head><label>778</label><figDesc>shows some images of characters created by three different methods, RandPerson, UnrealPerson, and the proposed ClonedPerson. As can be seen, RandPerson is the most cartoon-like. As for UnrealPerson, though it also uses Examples from different synthetic datasets. Qualitative comparison of different synthesis methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A .</head><label>A</label><figDesc>Examples of person detection results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure B .</head><label>B</label><figDesc>Examples of different combinations of upper-body and lower-body clothes. Left: the proposed cloning of the full-body outfits, in their original collocations. Right: random combination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>( 3 )</head><label>3</label><figDesc>Side-view. (4) Occluded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure C .</head><label>C</label><figDesc>Different viewpoints judged by pose detection. (1) A qualified image, where the left shoulder P 5 is on the right side of the image, W/H ? 0.3, and hands are not in the body area. (2) A back-view image, where the left shoulder P 5 is on the left side of the image. (3) A side-view image, where W/H &lt; 0.3. (4) An occluded image, with hands in the body area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>( 2 )</head><label>2</label><figDesc>From side-view images (3) From occluded images Figure D. Characters created from images of different person views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure F .</head><label>F</label><figDesc>Find out clear structure in UV maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure G .</head><label>G</label><figDesc>Examples of founded frontal areas in UV maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure H .</head><label>H</label><figDesc>Character generated by multi-view images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure I .</head><label>I</label><figDesc>Examples of optimized cloth cells by the proposed algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure J .</head><label>J</label><figDesc>Comparison of expansion and resizing methods in generating UV maps and characters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>( 3 )</head><label>3</label><figDesc>Only clothes considered. The proposed method only Figure L. Illustration of image cropping. Based on the result of the RandPerson's image cropping strategy, the occluded area on the left image shows the possible range of our random removals, and the images on the right are three examples of the cropped results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure M .</head><label>M</label><figDesc>Examples of 3D characters in ClonedPerson. Each group contains input image, generated 3D character, and rendered person image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure N .</head><label>N</label><figDesc>Illustrations of keypoint detection on real-world images with a model trained on the ClonedPerson dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Yes No No UnrealPerson 3 [33] 6,799 34 1,256,381 Yes Yes No ClonedPerson 5,621 24 887,766 Yes Yes Yes</figDesc><table /><note>Table 1. Statistics of some synthetic person re-identification datasets. "Sur": surveillance simulation. "Real": realistic clothes textures. "Outfit": cloning full-body outfits from person images.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>a</head><label></label><figDesc>Eight clothes categories with labeled keypoints. b Regular UV maps where clothes appear in regular shapes and structures.</figDesc><table /><note>c Irregular UV maps.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Direct cross-dataset evaluation results of models trained on different datasets with QAConv 2.0.</figDesc><table><row><cell>, we also evaluate their performances on</cell></row></table><note>Table 3. Evaluation results on the ClonedPerson testing set with different tasks. Green: Cross-dataset evaluation. Gray: Within- dataset evaluation. Blue: UDA. Pink: Unsupervised Learning.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>,801k 16.0 46.9 14.0 25.6 RP 8,000 132k 15.1 45.9 13.8 24.9 RP * 8,000 1,239k 20.1 56.4 17.6 31.4 U P 6,799 1,256k 17.2 56.1 17.5 30.3</figDesc><table><row><cell cols="3">Method Dataset #ID</cell><cell cols="2">#Imgs CUHKMarketMSMTAvg</cell></row><row><cell>QAConv</cell><cell cols="3">RP 8,000 1U P 3,000 120k</cell><cell>17.8 55.9 19.3 31.0</cell></row><row><cell></cell><cell cols="2">CP 4,826</cell><cell>763k</cell><cell>21.8 59.9 18.5 33.4</cell></row><row><cell></cell><cell>RP</cell><cell cols="3">8,000 1,801k 18.7 49.6 16.4 28.2</cell></row><row><cell></cell><cell>RP</cell><cell>8,000</cell><cell>132k</cell><cell>16.9 49.0 15.8 27.2</cell></row><row><cell>TransM</cell><cell cols="4">RP  *  8,000 1,239k 22.9 58.0 20.9 33.9 U P 6,799 1,256k 19.7 60.2 18.4 32.8</cell></row><row><cell></cell><cell cols="2">U P 3,000</cell><cell>120k</cell><cell>19.6 59.4 21.6 33.5</cell></row><row><cell></cell><cell cols="2">CP 4,826</cell><cell>763k</cell><cell>24.4 62.3 20.8 35.8</cell></row><row><cell></cell><cell>RP</cell><cell>8,000</cell><cell>132k</cell><cell>4.7 67.2 27.2 33.0</cell></row><row><cell>SpCL</cell><cell cols="2">U P 3,000</cell><cell>120k</cell><cell>5.3 71.7 28.4 35.1</cell></row><row><cell></cell><cell cols="2">CP 4,826</cell><cell>75k</cell><cell>12.0 72.7 24.2 36.3</cell></row></table><note>Table 4. mAP results with different datasets for different tasks. TransM: TransMatcher. RP : RandPerson. RP* : RandPerson* in new rendering settings. U P : UnrealPerson. CP : ClonedPerson.a Different number of images per cluster b Different number of clusters c Same number of identities d Different number of identities</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table A .</head><label>A</label><figDesc>Methods used and designed in the ClonedPerson pipeline.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table B. Results of different cropping strategies with the cropping probability ? and side rate ? .Figure K. Unity3D virtual environments utilized in this work.Table C. Results with different datasets for different tasks. RandPerson * means an adapted RandPerson dataset rendered with the same settings of the ClonedPerson.</figDesc><table><row><cell>Method</cell><cell>Dataset</cell><cell>#ID</cell><cell>#Imgs</cell><cell cols="8">CUHK03-NP Rank-1 mAP Rank-1 mAP Rank-1 mAP Rank-1 mAP Market-1501 MSMT17 Average</cell></row><row><cell></cell><cell>RandPerson</cell><cell cols="2">8,000 1,801k</cell><cell>17.8</cell><cell>16.0</cell><cell>74.5</cell><cell>46.9</cell><cell>40.6</cell><cell>14.0</cell><cell>44.3</cell><cell>25.6</cell></row><row><cell></cell><cell>RandPerson</cell><cell>8,000</cell><cell>132k</cell><cell>16.8</cell><cell>15.1</cell><cell>75.9</cell><cell>45.9</cell><cell>40.8</cell><cell>13.8</cell><cell>44.5</cell><cell>24.9</cell></row><row><cell>QAConv</cell><cell cols="3">RandPerson  *  8,000 1,239k UnrealPerson 6,799 1,256k</cell><cell>20.5 19.2</cell><cell>20.1 17.2</cell><cell>81.6 80.0</cell><cell>56.4 56.1</cell><cell>46.8 46.0</cell><cell>17.6 17.5</cell><cell>49.6 48.4</cell><cell>31.4 30.3</cell></row><row><cell></cell><cell cols="2">UnrealPerson 3,000</cell><cell>120k</cell><cell>18.8</cell><cell>17.8</cell><cell>80.6</cell><cell>55.9</cell><cell>49.5</cell><cell>19.3</cell><cell>49.6</cell><cell>31.0</cell></row><row><cell></cell><cell cols="2">ClonedPerson 4,826</cell><cell>763k</cell><cell>22.6</cell><cell>21.8</cell><cell>84.5</cell><cell>59.9</cell><cell>49.1</cell><cell>18.5</cell><cell>52.1</cell><cell>33.4</cell></row><row><cell></cell><cell>Market [5]</cell><cell>-</cell><cell>-</cell><cell>22.2</cell><cell>21.4</cell><cell>-</cell><cell>-</cell><cell>47.3</cell><cell>18.4</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>MSMT17 [5]</cell><cell>-</cell><cell>-</cell><cell>23.7</cell><cell>22.5</cell><cell>80.1</cell><cell>52.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>RandPerson</cell><cell cols="2">8,000 1,801k</cell><cell>21.2</cell><cell>18.7</cell><cell>77.6</cell><cell>49.6</cell><cell>45.3</cell><cell>16.4</cell><cell>48.0</cell><cell>28.2</cell></row><row><cell>TransMatcher</cell><cell cols="3">RandPerson RandPerson  *  8,000 1,239k 8,000 132k</cell><cell>18.4 22.9</cell><cell>16.9 22.9</cell><cell>77.3 83.6</cell><cell>49.0 58.0</cell><cell>44.3 51.4</cell><cell>15.8 20.9</cell><cell>46.7 52.6</cell><cell>27.2 33.9</cell></row><row><cell></cell><cell cols="3">UnrealPerson 6,799 1,256k</cell><cell>21.8</cell><cell>19.7</cell><cell>81.1</cell><cell>60.2</cell><cell>44.8</cell><cell>18.4</cell><cell>49.2</cell><cell>32.8</cell></row><row><cell></cell><cell cols="2">UnrealPerson 3,000</cell><cell>120k</cell><cell>21.4</cell><cell>19.6</cell><cell>81.6</cell><cell>59.4</cell><cell>52.0</cell><cell>21.6</cell><cell>51.7</cell><cell>33.5</cell></row><row><cell></cell><cell cols="2">ClonedPerson 4,826</cell><cell>763k</cell><cell>25.4</cell><cell>24.4</cell><cell>84.8</cell><cell>62.3</cell><cell>51.6</cell><cell>20.8</cell><cell>53.9</cell><cell>35.8</cell></row><row><cell>SpCL</cell><cell cols="2">RandPerson UnrealPerson 3,000 8,000 ClonedPerson 4,826</cell><cell>132k 120k 75k</cell><cell>3.9 4.2 11.7</cell><cell cols="7">Prob. ?Side Rate ? #ID 4.7 83.4 67.2 0 0 4,826 763,953 53.7 27.2 #Images Rank-1 mAP 47.0 33.0 45.7 5.3 86.5 71.7 55.2 28.4 48.6 35.1 26.7 10% 0 4,826 763,953 48.9 29.9 12.0 88.0 72.7 49.3 24.2 49.7 36.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20%</cell><cell>0</cell><cell cols="3">4,826 763,953</cell><cell>48.7</cell><cell>29.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30%</cell><cell>0</cell><cell cols="3">4,826 763,953</cell><cell>49.0</cell><cell>30.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40%</cell><cell>0</cell><cell cols="3">4,826 763,953</cell><cell>48.8</cell><cell>30.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50%</cell><cell>0</cell><cell cols="3">4,826 763,953</cell><cell>48.8</cell><cell>29.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30%</cell><cell>10%</cell><cell cols="3">4,826 763,953</cell><cell>49.7</cell><cell>31.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30%</cell><cell>20%</cell><cell cols="3">4,826 763,953</cell><cell>51.2</cell><cell>32.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30%</cell><cell>30%</cell><cell cols="3">4,826 763,953</cell><cell>52.1</cell><cell>33.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30%</cell><cell>40%</cell><cell cols="3">4,826 763,953</cell><cell>51.3</cell><cell>32.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">However, high-fidelity 3D reconstruction of person bodies, for example, heads and 3D body shapes, is not our target. On the other hand, high-</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Suggested subset from<ref type="bibr" target="#b28">[29]</ref>: 8,000 characters with 132,145 images.3  Suggested subset from<ref type="bibr" target="#b32">[33]</ref>: 3,000 characters with 120,000 images.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">No need to worry about dropping other images including some images in good conditions, since there are huge available sources.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Note that we only create one 3D character for one person image, since generating similar characters is already considered in the same cluster.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Note that only self-occlusion is considered here. Though, with the visibility probabilities predicted by HRNet we can also infer occlusion by other objects, this is not yet considered in the current pipeline.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision. Kybernetes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex M</forename><surname>Andrew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain adaptation through synthesis for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Francois</forename><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Aleksander Rognhaugen, and Theoharis Theoharis. Looking beyond appearances: Synthetic training data for deep cnns in re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><forename type="middle">Barros</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="50" to="62" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Modeling, UV Mapping, and Texturing 3D Game Weapons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Wordware Publishing, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">MakeHuman: Open Source Tool for Making 3D Characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makehuman</forename><surname>Community</surname></persName>
		</author>
		<ptr target="http://www.makehumancommunity.org.3" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Density-based spatial clustering of applications with noise (dbscan)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Second International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>of the Second International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michela</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A versatile benchmark for detection, pose estimation, segmentation and re-identification of clothing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-paced contrastive learning with hybrid memory for domain adaptive object re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Viton: An image-based virtual try-on network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7543" to="7552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cross dataset person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="650" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">360-degree textures of people in clothing from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verica</forename><surname>Lazova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A method for the solution of certain non-linear problems in least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Levenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly of Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="164" to="168" />
			<date type="published" when="1944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interpretable and generalizable person re-identification with query-adaptive convolution and temporal lifting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="456" to="474" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XI 16</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transmatcher: Deep image matching through transformers for generalizable person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Graph Sampling Based Deep Metric Learning for Generalizable Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to transfer texture from clothing images to 3d humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aymen</forename><surname>Mir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7023" to="7034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Evaluation methods in face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Micheals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Face Recognition, chapter 21</title>
		<editor>S. Z. Li and A. K. Jain</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="551" to="574" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2304" to="2314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sobh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Elleithy</surname></persName>
		</author>
		<title level="m">Innovations in Computing Sciences and Software Engineering</title>
		<meeting><address><addrLine>Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dissecting person re-identification from the viewpoint of viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="608" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Computer vision: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unity3D: Cross-platform game engine</title>
		<ptr target="https://unity.com.1" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Unity Technologies</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Re-identification supervised texture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunshan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yachun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11846" to="11856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Surpassing real-world source training data: Random 3d characters for generalizable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards photo-realistic virtual try-on by adaptively generating-preserving image content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7850" to="7859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 22nd International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unrealperson: An adaptive pipeline towards costless person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Human parsing based texture transfer from single image to 3d human via cross-view consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A versatile benchmark for detection, pose estimation, segmentation and re-identification of clothing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generalizable pedestrian detection: The elephant in the room</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irtiza</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Saad Ullah Akram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pixel-in-pixel net: Towards efficient facial landmark detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2021-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transmatcher: Deep image matching through transformers for generalizable person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Surpassing real-world source training data: Random 3d characters for generalizable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unrealperson: An adaptive pipeline towards costless person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Figure O. Distribution of DeepFashion and DeepFashion2 images</title>
		<meeting>the IEEE/CVF Conference on Computer Figure O. Distribution of DeepFashion and DeepFashion2 images</meeting>
		<imprint/>
	</monogr>
	<note>made by t-SNE [10</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<title level="m">Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
