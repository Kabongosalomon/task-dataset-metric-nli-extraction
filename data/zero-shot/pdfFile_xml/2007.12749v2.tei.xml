<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hard negative examples are hard, but useful</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Xuan</surname></persName>
							<email>xuanhong@gwu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The George Washington University</orgName>
								<address>
									<postCode>20052</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Stylianou</surname></persName>
							<email>abby.stylianou@slu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Saint Louis University</orgName>
								<address>
									<postCode>63103</postCode>
									<settlement>St. Louis</settlement>
									<region>MO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotong</forename><surname>Liu</surname></persName>
							<email>liuxiaotong2017@gwu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The George Washington University</orgName>
								<address>
									<postCode>20052</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
							<email>pless@gwu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The George Washington University</orgName>
								<address>
									<postCode>20052</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hard negative examples are hard, but useful</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hard Negative</term>
					<term>Deep Metric Learning</term>
					<term>Triplet Loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Triplet loss is an extremely common approach to distance metric learning. Representations of images from the same class are optimized to be mapped closer together in an embedding space than representations of images from different classes. Much work on triplet losses focuses on selecting the most useful triplets of images to consider, with strategies that select dissimilar examples from the same class or similar examples from different classes. The consensus of previous research is that optimizing with the hardest negative examples leads to bad training behavior. That's a problem -these hardest negatives are literally the cases where the distance metric fails to capture semantic similarity. In this paper, we characterize the space of triplets and derive why hard negatives make triplet loss training fail. We offer a simple fix to the loss function and show that, with this fix, optimizing with hard negative examples becomes feasible. This leads to more generalizable features, and image retrieval results that outperform state of the art for datasets with high intra-class variance. Code is available at: https://github.com/littleredxh/HardNegative.git</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep metric learning optimizes an embedding function that maps semantically similar images to relatively nearby locations and maps semantically dissimilar images to distant locations. A number of approaches have been proposed for this problem <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26]</ref>. One common way to learn the mapping is to define a loss function based on triplets of images: an anchor image, a positive image from the same class, and a negative image from a different class. The loss penalizes cases where the anchor is mapped closer to the negative image than it is to the positive image.</p><p>In practice, the performance of triplet loss is highly dependent on the triplet selection strategy. A large number of triplets are possible, but for a large part of the optimization, most triplet candidates already have the anchor much closer to the positive than the negative, so they are redundant. Triplet mining refers to the process of finding useful triplets.</p><p>Inspiration comes from Neil DeGrasse Tyson, the famous American Astrophysicist and science educator who says, while encouraging students, "In whatever you choose to <ref type="figure">Fig. 1</ref>: The triplet diagram plots a triplet as a dot defined by the anchor-positive similarity S ap on the x-axis and the anchor-negative similarity S an on the y-axis. Dots below the diagonal correspond to triplets that are "correct", in the sense that the same class example is closer than the different class example. Triplets above the diagonal of the diagram are candidates for the hard negative triplets. They are important because they indicate locations where the semantic mapping is not yet correct. However, previous works have typically avoided these triplets because of optimization challenges. do, do it because it is hard, not because it is easy". Directly mapping this to our case suggests hard negative mining, where triplets include an anchor image where the positive image from the same class is less similar than the negative image from a different class.</p><p>Optimizing for hard negative triplets is consistent with the actual use of the network in image retrieval (in fact, hard negative triplets are essentially errors in the trained image mappings), and considering challenging combinations of images has proven critical in triplet based distance metric learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref>. But challenges in optimizing with the hardest negative examples are widely reported in work on deep metric learning for face recognition, people re-identification and fine-grained visual recognition tasks. A variety of work shows that optimizing with the hardest negative examples for deep metric learning leads to bad local minima in the early phase of the optimization <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>A standard version of deep metric learning uses triplet loss as the optimization function to learn the weights of a CNN to map images to a feature vector. Very commonly, these feature vectors are normalized before computing the similarity because this makes comparison intuitive and efficient, allowing the similarity between feature vectors to be computed as a simple dot-product. We consider this network to project points to the hypersphere (even though that projection only happens during the similarity computation). We show there are two problems in this implementation.</p><p>First, when the gradient of the loss function does not consider the normalization to a hypersphere during the gradient backward propagation, a large part of the gradient is lost when points are re-projected back to the sphere, especially in the cases of triplets including nearby points. Second, when optimizing the parameters (the weights) of the network for images from different classes that are already mapped to similar feature points, the gradient of the loss function may actually pull these points together instead of separating them (the opposite of the desired behavior).</p><p>We give a systematic derivation showing when and where these challenging triplets arise and diagram the sets of triplets where standard gradient descent leads to bad local minima, and do a simple modification to the triplet loss function to avoid bad optimization outcomes.</p><p>Briefly, our main contributions are to:</p><p>introduce the triplet diagram as a visualization to help systematically characterize triplet selection strategies, understand optimization failures through analysis of the triplet diagram, propose a simple modification to a standard loss function to fix bad optimization behavior with hard negative examples, and demonstrate this modification improves current state of the art results on datasets with high intra-class variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Triplet loss approaches penalize the relative similarities of three examples -two from the same class, and a third from a different class. There has been significant effort in the deep metric learning literature to understand the most effective sampling of informative triplets during training. Including challenging examples from different classes (ones that are similar to the anchor image) is an important technique to speed up the convergence rate, and improve the clustering performance. Currently, many works are devoted to finding such challenging examples within datasets. Hierarchical triplet loss (HTL) <ref type="bibr" target="#b3">[4]</ref> seeks informative triplets based on a pre-defined hierarchy of which classes may be similar. There are also stochastic approaches <ref type="bibr" target="#b20">[21]</ref> that sample triplets judged to be informative based on approximate class signatures that can be efficiently updated during training. However, in practice, current approaches cannot focus on the hardest negative examples, as they lead to bad local minima early on in training as reported in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28]</ref>. The avoid this, authors have developed alternative approaches, such as semi-hard triplet mining <ref type="bibr" target="#b15">[16]</ref>, which focuses on triplets with negative examples that are almost as close to the anchor as positive examples. Easy positive mining <ref type="bibr" target="#b26">[27]</ref> selects only the closest anchor-positive pairs and ensures that they are closer than nearby negative examples.</p><p>Avoiding triplets with hard negative examples remedies the problem that the optimization often fails for these triplets. But hard negative examples are important. The hardest negative examples are literally the cases where the distance metric fails to capture semantic similarity, and would return nearest neighbors of the incorrect class. Interesting datasets like CUB <ref type="bibr" target="#b23">[24]</ref> and CAR <ref type="bibr" target="#b9">[10]</ref> which focus on birds and cars, respectively, have high intra-class variance -often similar to or even larger than the inter-class variance. For example, two images of the same species in different lighting and different viewpoints may look quite different. And two images of different bird species on similar branches in front of similar backgrounds may look quite similar. These hard negative examples are the most important examples for the network to learn discriminative features, and approaches that avoid these examples because of optimization challenges may never achieve optimal performance.</p><p>There has been other attention on ensure that the embedding is more spread out. A non-parametric approach <ref type="bibr" target="#b24">[25]</ref> treats each image as a distinct class of its own, and trains a classifier to distinguish between individual images in order to spread feature points across the whole embedding space. In <ref type="bibr" target="#b28">[29]</ref>, the authors proposed a spread out regularization to let local feature descriptors fully utilize the expressive power of the space. The easy positive approach <ref type="bibr" target="#b26">[27]</ref> only optimizes examples that are similar, leading to more spread out features and feature representations that seem to generalize better to unseen data.</p><p>The next section introduces a diagram to systematically organize these triplet selection approaches, and to explore why the hardest negative examples lead to bad local minima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Triplet diagram</head><p>Triplet loss is trained with triplets of images, (x a ,x p ,x n ), where x a is an anchor image, x p is a positive image of the same class as the anchor, and x n is a negative image of a different class. We consider a convolution neural network, f(?), that embeds the images on a unit hypersphere, (f(x a ),f(x p ),f(x n )). We use (f a ,f p ,f n ) to simplify the representation of the normalized feature vectors. When embedded on a hypersphere, the cosine similarity is a convenient metric to measure the similarity between anchor-positive pair S ap = f a f p and anchor-negative pair S an = f a f n , and this similarity is bounded in the range [?1,1].</p><p>The triplet diagram is an approach to characterizing a given set of triplets. -Hard triplets: Triplets that are not in the correct configuration, where the anchor-positive similarity is less than the anchor-negative similarity (dots above the S an = S ap diagonal). Dots representing triplets in the wrong configuration are drawn in red. Triplets that are not hard triplets we call Easy Triplets, and are drawn in blue. -Hard negative mining: A triplet selection strategy that seeks hard triplets, by selecting for an anchor, the most similar negative example. They are on the top of the diagram. We circle these red dots with a blue ring and call them hard negative triplets in the following discussion. -Semi-hard negative mining <ref type="bibr" target="#b15">[16]</ref>: A triplet selection strategy that selects, for an anchor, the most similar negative example which is less similar than the corresponding positive example. In all cases, they are under S an =S ap diagonal. We circle these blue dots with a red dashed ring. -Easy positive mining <ref type="bibr" target="#b26">[27]</ref>: A triplet selection strategy that selects, for an anchor, the most similar positive example. They tend to be on the right side of the diagram because the anchor-positive similarity tends to be close to 1. We circle these blue dots with a red ring. -Easy positive, Hard negative mining <ref type="bibr" target="#b26">[27]</ref>: A related triplet selection strategy that selects, for an anchor, the most similar positive example and most similar negative example. The pink dot surrounded by a blue dashed circle represents one such example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Why some triplets are hard to optimize</head><p>The triplet diagram offers the ability to understand when the gradient-based optimization of the network parameters is effective and when it fails. The triplets are used to train a network whose loss function encourages the anchor to be more similar to its positive example (drawn from the same class) than to its negative example (drawn from a different class). While there are several possible choices, we consider NCA <ref type="bibr" target="#b4">[5]</ref> as the loss function:</p><formula xml:id="formula_0">L(S ap ,S an )=?log exp(S ap ) exp(S ap )+exp(S an )<label>(1)</label></formula><p>All of the following derivations can also be done for the margin-based triplet loss formulation used in <ref type="bibr" target="#b15">[16]</ref>. We use the NCA-based of triplet loss because the following gradient derivation is clear and simple. Analysis of the margin-based loss is similar and is derived in the Appendix.</p><p>The gradient of this NCA-based triplet loss L(S ap ,S an ) can be decomposed into two parts: a single gradient with respect to feature vectors f a , f p , f n :</p><formula xml:id="formula_1">?L=( ?L ?S ap ?S ap ?f a + ?L ?S an ?S an ?f a )?f a + ?L ?S ap ?S ap ?f p ?f p + ?L ?S an ?S an ?f n ?f n<label>(2)</label></formula><p>and subsequently being clear that these feature vectors respond to changes in the model parameters (the CNN network weights), ?:</p><formula xml:id="formula_2">?L=( ?L ?S ap ?S ap ?f a + ?L ?S an ?S an ?f a ) ?f a ?? ??+ ?L ?S ap ?S ap ?f p ?f p ?? ??+ ?L ?S an ?S an ?f n ?f n ?? ??</formula><p>(3) The gradient optimization only affects the feature embedding through variations in ?, but we first highlight problems with hypersphere embedding assuming that the optimization could directly affect the embedding locations without considering the gradient effect caused by ?. To do this, we derive the loss gradient, g a , g p , g n , with respect to the feature vectors, f a , f p , f n , and use this gradient to update the feature locations where the error should decrease:</p><formula xml:id="formula_3">f p new =f p ??g p =f p ?? ?L ?f p =f p +?f a (4) f n new =f n ??g n =f n ?? ?L ?f n =f n ??f a (5) f a new =f a ??g a =f a ?? ?L ?f a =f a ??f n +?f p<label>(6)</label></formula><p>where ? =? exp(San) exp(Sap)+exp(San) and ? is the learning rate. This gradient update has a clear geometric meaning: the positive point f p is encouraged to move along the direction of the vector f a ; the negative point f n is encouraged to move along the opposite direction of the vector f a ; the anchor point f a is encouraged to move along the direction of the sum of f p and ?f n . All of these are weighted by the same weighting factor ?. Then we can get the new anchor-positive similarity and anchor-negative similarity (the complete derivation is given in the Appendix):</p><formula xml:id="formula_4">S new ap =(1+? 2 )S ap +2???S pn ?? 2 S an (7) S new an =(1+? 2 )S an ?2?+?S pn ?? 2 S ap (8)</formula><p>The first problem is these gradients, g a , g p , g n , have components that move them off the sphere; computing the cosine similarity requires that we compute the norm of f a new , f p new and f n new (the derivation for these is shown in Appendix). Given the norm of the updated feature vector, we can calculate the similarity change after the gradient update: The second problem is that the optimization can only control the feature vectors based on the network parameters, ?. Changes to ? are likely to affect nearby points in similar ways. For example, if there is a hard negative triplet, as defined in Section 3, where the anchor is very close to a negative example, then changing ? to move the anchor closer to the positive example is likely to pull the negative example along with it. We call this effect "entanglement" and propose a simple model to capture its effect on how the gradient update affects the similarities.</p><formula xml:id="formula_5">?S ap = S new ap fa new fp new ?S ap (9) ?S an = S new an fa new fn new ?S an<label>(10)</label></formula><p>We use a scalar, p, and a similarity related factor q = S ap S an , to quantify this entanglement effect. When all three examples in a triplet are nearby to each other, both S ap and S an will be large, and therefore q will increase the entanglement effect; when either the positive or the negative example is far away from the anchor, one of S ap and S an will be small and q will reduce the entanglement effect.</p><p>The total similarity changes with entanglement will be modeled as follows:  When the entanglement increases, the problem gets worse; more anchor-negative pairs are in a region where they are pushed to be more similar, and more anchorpositive pairs are in a region where they are pushed to be less similar. The anchorpositive behavior is less problematic because the effect stops while the triplet is still in a good configuration (with the positive closer to the anchor than the negative), while the anchor-negative has not limit and pushes the anchor and negative to be completely similar.</p><p>The plots predict the potential movement for triplets on the triplet diagram. We will verify this prediction in the Section 6.</p><p>Local minima caused by hard negative triplets In <ref type="figure" target="#fig_1">Figure 2</ref>, the top region indicates that hard negative triplets with very high anchor-negative similarity get pushed towards <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b0">1)</ref>. Because, in that region, S an will move upward to 1 and S ap will move right to 1. The result of the motion is that a network cannot effectively separate the anchor-negative pairs and instead pushes all features together. This problem was described in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28]</ref> as bad local minima of the optimization.</p><p>When will hard triplets appear During triplet loss training, a mini-batch of images is samples random examples from numerous classes. This means that for every image in a batch, there are many possible negative examples, but a smaller number of possible positive examples. In datasets with low intra-class variance and high inter-class variance, an anchor image is less likely to be more similar to its hardest negative example than its random positive example, resulting in more easy triplets.</p><p>However, in datasets with relatively higher intra-class variance and lower inter-class variance, an anchor image is more likely to be more similar to its hardest negative example than its random positive example, and form hard triplets. Even after several epochs of training, it's difficult to cluster instances from same class with extremely high intra-class variance tightly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Modification to triplet loss</head><p>Our solution for the challenge with hard negative triplets is to decouple them into anchor-positive pairs and anchor-negative pairs, and ignore the anchor-positive pairs, and introduce a contrastive loss that penalizes the anchor-negative similarity. We call this Selectively Contrastive Triplet loss L SC , and define this as follows:</p><p>L SC (S ap ,S an )= ?S an if S an &gt;S ap L(S ap ,S an ) others <ref type="bibr" target="#b12">(13)</ref> In most triplet loss training, anchor-positive pairs from the same class will be always pulled to be tightly clustered. With our new loss function, the anchor-positive pairs in triplets will not be updated, resulting in less tight clusters for a class of instances (we discuss later how this results in more generalizable features that are less over-fit to the training data). The network can then 'focus' on directly pushing apart the hard negative examples.</p><p>We denote triplet loss with a Hard Negative mining strategy (HN), triplet loss trained with Semi-Hard Negative mining strategy (SHN), and our Selectively Contrastive Triplet loss with hard negative mining strategy (SCT) in the following discussion. <ref type="figure">Figure 3</ref> shows four examples of triplets from the CUB200(CUB) <ref type="bibr" target="#b23">[24]</ref> and CAR196(CAR) <ref type="bibr" target="#b9">[10]</ref> datasets at the very start of training, and <ref type="figure">Figure 4</ref> shows four examples of triplets at the end of training. The CUB dataset consists of various classes of birds, while the CAR196 dataset consists of different classes of cars. In both of the example triplet figures, the left column shows a positive example, the second column shows the anchor image, and then we show the hard negative example selected with SCT and SHN approach.</p><p>At the beginning of training <ref type="figure">(Figure 3</ref>), both the positive and negative examples appear somewhat random, with little semantic similarity. This is consistent with its + Anchor SCT Hard -SHN Hard - <ref type="figure">Fig. 3</ref>: Example triplets from the CAR and CUB datasets at the start of training. The positive example is randomly selected from a batch, and we show the hard negative example selected by SCT and SHN approach.</p><p>initialization from a pretrained model trained on ImageNet, which contains classes such as birds and cars -images of birds all produce feature vectors that point in generally the same direction in the embedding space, and likewise for images of cars. <ref type="figure">Figure 4</ref> shows that the model trained with SCT approach has truly hard negative examples -ones that even as humans are difficult to distinguish. The negative examples in the model trained with SHN approach, on the other hand, remain quite random. This may be because when the network was initialized, these anchor-negative pairs were accidentally very similar (very hard negatives) and were never included in the semi-hard negative (SHN) optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Results</head><p>We run a set of experiments on the CUB200 (CUB) <ref type="bibr" target="#b23">[24]</ref>, CAR196 (CAR) <ref type="bibr" target="#b9">[10]</ref>, Stanford Online Products (SOP) <ref type="bibr" target="#b17">[18]</ref>, In-shop Cloth (In-shop) <ref type="bibr" target="#b10">[11]</ref> and Hotels-50K(Hotel) <ref type="bibr" target="#b19">[20]</ref> datasets. All tests are run on the PyTorch platform <ref type="bibr" target="#b13">[14]</ref>, using ResNet50 <ref type="bibr" target="#b6">[7]</ref> architectures, pre-trained on ILSVRC 2012-CLS data <ref type="bibr" target="#b14">[15]</ref>. Training images are re-sized to 256 by 256 pixels. We adopt a standard data augmentation scheme (random horizontal flip and random crops padded by 10 pixels on each side). For pre-processing, we normalize the images using the channel means and standard deviations. All networks are trained using stochastic gradient descent (SGD) with momentum 0. The batch size is 128 for CUB and CAR, 512 for SOP, In-shop and Hotel50k. In a batch of images, each class contains 2 examples and all classes are randomly selected from the training data. Empirically, we set ?=1 for all datasets tested.</p><p>We calculate Recall@K as the measurement for retrieval quality. On the CUB and CAR datasets, both the query set and gallery set refer to the testing set. During the query process, the top-K retrieved images exclude the query image itself. In the + Anchor SCT Hard -SHN Hard - <ref type="figure">Fig. 4</ref>: Example triplets from the CAR and CUB datasets at the end of training. The positive example is randomly selected from a batch, and we show the hard negative example selected by SCT and SHN approach.</p><p>Hotels-50K dataset, the training set is used as the gallery for all query images in the test set, as per the protocol described by the authors in <ref type="bibr" target="#b19">[20]</ref>. <ref type="figure" target="#fig_4">Figure 5</ref> helps to visualize what happens with hard negative triplets as the network trains using the triplet diagram described in Section 3. We show the triplet diagram over several iterations, for the HN approach (top), SHN approach (middle), and the SCT approach introduced in this paper (bottom). In the HN approach (top row), most of the movements of hard negative triplets coincide with the movement prediction of the vector field in the <ref type="figure" target="#fig_1">Figure 2</ref> -all of the triplets are pushed towards the bad minima at the location <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b0">1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Hard negative triplets during training</head><p>During the training of SHN approach (middle row), it can avoid this local minima problem, but the approach does not do a good job of separating the hard negative pairs. The motion from the red starting point to the blue point after the gradient update is small, and the points are not being pushed below the diagonal line (where the positive example is closer than the negative example).</p><p>SCT approach (bottom) does not have any of these problems, because the hard negative examples are more effectively separated early on in the optimization, and the blue points after the gradient update are being pushed towards or below the diagonal line.</p><p>In <ref type="figure" target="#fig_5">Figure 6</ref>, we display the percentage of hard triplets as defined in Section 3 in a batch of each training iteration on CUB, CAR, SOP, In-shop Cloth and Hotels-50k datasets (left), and compare the Recall@1 performance of HN, SHN and SCT approaches (right). In the initial training phase, if a high ratio of hard triplets appear in a batch such as CUB, CAR and Hotels-50K dataset, the HN approach converges to the local minima seen in <ref type="figure" target="#fig_4">Figure 5</ref>.    <ref type="table">Table 1</ref>: Retrieval performance on the Hotels-50K dataset. All methods are trained with Resnet-50 and embedding size is 256.</p><p>We find the improvement is related to the percentage of hard triplets when it drops to a stable level. At this stage, there is few hard triplets in In-shop Cloth dataset, and a small portion of hard triplets in CUB, CAR and SOP datasets, a large portion of hard triplets in Hotels-50K dataset. In <ref type="figure" target="#fig_5">Figure 6</ref>, the model trained with SCT approach improves R@1 accuracy relatively small improvement on CUB, CAR, SOP and In-shop datasets but large improvement on Hotels-50K datasets with respect to the model trained with the SHN approach in <ref type="table">Table 1</ref>, we show the new state-of-the-art result on Hotels-50K dataset, and tables of the other datasets are shown in Appendix (this data is visualized in <ref type="figure" target="#fig_5">Figure 6 (right)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Generalizability of SCT Features</head><p>Improving the recall accuracy on unseen classes indicates that the SCT features are more generalizable -the features learned from the training data transfer well to the new unseen classes, rather than overfitting on the training data. The intuition for why the SCT approach would allow us to learn more generalizable features is because forcing the network to give the same feature representation to very different examples from the same class is essentially memorizing that class, and that is not likely to translate to new classes. Because SCT uses a contrastive approach on hard negative triplets, and only works to decrease the anchor-negative similarity, there is less work to push dis-similar anchor-positive pairs together. This effectively results in training data being more spread out in embedding space which previous works have suggested leads to generalizable features <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>We observe this spread out embedding property in the triplet diagrams seen in <ref type="figure">Figure 7</ref>. On training data, a network trained with SCT approach has anchor-positive pairs that are more spread out than a network trained with SHN approach (this is visible in the greater variability of where points are along the x-axis), because the SCT approach sometimes removes the gradient that pulls anchor-positive pairs together. However, the triplet diagrams on test set show that in new classes the triplets have similar distributions, with SCT creating features that are overall slightly higher anchor-positive similarity.</p><p>A different qualitative visualization in <ref type="figure" target="#fig_6">Figure 8</ref>, shows the embedding similarity visualization from <ref type="bibr" target="#b18">[19]</ref>, which highlights the regions of one image that make it look similar to another image. In the top set of figures from the SHN approach, the blue regions that indicate similarity are diffuse, spreading over the entire car, while in the bottom <ref type="figure">Fig. 7</ref>: We train a network on the CAR dataset with the SHN and SCT approach for 80 epochs. Testing data comes from object classes not seen in training. We make a triplet for every image in the training and testing data set, based on its easiest positive (most similar same class image) and hardest negative (most similar different class image), and plot these on the triplet diagram. We see the SHN (left) have a more similar anchor-positive than SCT (right) on the training data, but the SCT distribution of anchor-positive similarities is greater on images from unseen testing classes, indicating improved generalization performance. visualization from the SCT approach, the blue regions are focused on specific features (like the headlights). These specific features are more likely to generalize to new, unseen data, while the features that represent the entire car are less likely to generalize well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Substantial literature has highlighted that hard negative triplets are the most informative for deep metric learning -they are the examples where the distance metric fails to accurately capture semantic similarity. But most approaches have avoided directly optimizing these hard negative triplets, and reported challenges in optimizing with them. This paper introduces the triplet diagram as a way to characterize the effect of different triplet selection strategies. We use the triplet diagram to explore the behavior of the gradient descent optimization, and how it changes the anchor-positive and anchor-negative similarities within triplet. We find that hard negative triplets have gradients that (incorrectly) force negative examples closer to the anchor, and situations that encourage triplets of images that are all similar to become even more similar. This explains previously observed bad behavior when trying to optimize with hard negative triplets.</p><p>We suggest a simple modification to the desired gradients, and derive a loss function that yields those gradients. Experimentally we show that this improves the The heatmaps in the SCT approach visualizations are significantly more concentrated on individual features, as opposed to being more diffuse over the entire car. This suggests the SCT approach learns specific semantic features rather than overfitting and memorizing entire vehicles.</p><p>convergence for triplets with hard negative mining strategy. With this modification, we no longer observe challenges in optimization leading to bad local minima and show that hard-negative mining gives results that exceed or are competitive with state of the art approaches. We additionally provide visualizations that explore the improved generalization of features learned by a network trained with hard negative triplets.</p><p>On P ap , g a can be decomposed into 3 components: component in the plane and along f a , component in the plane and vertical f a , and component vertical to P ap . Then,</p><formula xml:id="formula_6">f a new 2 =(1+?S ap ??S an ) 2 +(? 1?S 2 ap ??? 1?S 2 an ) 2 +(? 1?? 2 1?S 2 an ) 2<label>(19)</label></formula><p>.   The main paper uses NCA-based triplet loss. Another margin-based triplet-loss function is derived to guarantee a specific margin. This can be expressed in terms of f a ,f p ,f n as: </p><p>The updated similarity S new ap and S new an will be:</p><p>S new ap =(1??+? 2 )S ap +2??? 2 ??(1??)S pn ?? 2 S an (30)</p><p>S new an =(1+?+? 2 )S an ?2??? 2 +?(1+?)S pn ?? 2 S ap (31)</p><p>Comparing to the S new ap and S new an in equation 7 and 8 of main paper, margin-based triplet loss behavior is similar to the NCA-based triplet loss. And we simulate the ?S ap and ?S an with the margin-based triplet loss in <ref type="figure" target="#fig_8">figure 9</ref>. These plots show that the behavior of problematic regions are qualitatively similar for both methods with different values of entanglement strength p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.4 More Results on other dataset</head><p>We also compare our results with state-of-the-art embedding approaches such as BIER <ref type="bibr" target="#b12">[13]</ref>,ABE <ref type="bibr" target="#b8">[9]</ref>,FaseAP <ref type="bibr" target="#b0">[1]</ref> Multi-Similarity <ref type="bibr" target="#b22">[23]</ref> and Easy Positive <ref type="bibr" target="#b26">[27]</ref> on SOP <ref type="bibr" target="#b17">[18]</ref> and In-shop <ref type="bibr" target="#b10">[11]</ref> dataset. Tables 3 shows the SC-triplet loss outperforms the best previously reported results on the SOP dataset. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1represents each triplet as a 2D dot (S ap ,S an ), describing how similar the positive and negative examples are to the anchor. This diagram is useful because the location on the diagram describes important features of the triplet:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 (</head><label>2</label><figDesc>left column) shows calculations of the change in the anchor-positive similarity and the change in the anchor-negative similarity. There is an area along the right side of the ?S ap plot (top row, left column) highlighting locations where the anchor and positive are not strongly pulled together. There is also a region along the top side of the ?S an plot (bottom row, left column) highlighting locations where the anchor and negative can not be strongly separated. This behavior arises because the gradient is pushing the feature off the hypersphere and therefore, after normalization, the effect is lost when anchor-positive pairs or anchor-negative pairs are close to each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Numerical simulation of how the optimization changes triplets, with 0 entanglement (left), some entanglement (middle) and complete entanglement (right). The top row shows effects on anchor-positive similarity the bottom row shows effects on anchor-negative similarity. The scale of the arrows indicates the gradient strength. The top region of the bottom-middle and bottom-right plots highlight that the hard negative triplets regions are not well optimized with standard triplet loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 (</head><label>2</label><figDesc>middle and right column) shows vector fields on the diagram where S ap and S an will move based on the gradient of their loss function. It highlights the region along right side of the plots where that anchor and positive examples become less similar (?S total ap &lt; 0), and the region along top side of the plots where that anchor and negative examples become more similar (?S total an &gt; 0) for different parameters of the entanglement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Hard negative triplets of a batch in training iterations 0, 4, 8, 12. 1st row: Triplet loss with hard negative mining (HN); 2nd row: Triplet loss with semi hard negative mining (SHN). Although the hard negative triplets are not selected for training, their position may still change as the network weights are updated; 3rd row: Selectively Contrastive Triplet loss with hard negative mining (SCT). In each case we show where a set of triplets move before an after the iteration, with the starting triplet location shown in red and the ending location in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Left: the percentage of hard triplets in a batch for SOP, In-shop Cloth and Hotels-50K datasets. Right: Recall@1 performance comparison between HN, SHN and SCT approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>The above figures show two query images from the CAR dataset (middle left in each set of images) , and the top five results returned by our model trained with hard negative examples. The Similarity Visualization approach from [19] show what makes the query image similar to these result images (blue regions contribute more to similarity than red regions). In all figures, the visualization of what makes the query image look like the result image is on top, and the visualization of what makes the result image look like the query image is on the bottom. The top two visualizations (a) and (b) show the visualization obtained from the network trained with SHN approach, while the bottom two visualizations (c) and (d) show the visualization obtained from the network trained with SCT approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>L=max( f a ?f p 2 ? 2 +(? 1 ?? 2 1?S 2 an ) 2</head><label>2212</label><figDesc>f a ?f n 2 +?,0)=max(D,0) (20)g p = ?L ?fp = ??(f a ?f p ) if D &gt;0 0 otherwise (21) g n = ?L ?fn = ?(f a ?f n ) if D &gt;0 0 otherwise (22) g a = ?L ?fa = ?(f n ?f p ) if D &gt;0 0 otherwise (23) where D = ( f a ? f p 2 ? f a ? f n 2 + ?) and ? = 2.For simplicity, in the following discussion, we set D &gt; 0 for margin-based triplet loss. Then we can get the f a new , f p new and f n new and their norm:f p new = f p +?(f a ?f p ) =(1??)f p +?f a (24) f n new = f n ??(f a ?f n ) =(1+?)f n ??f a (25) f a new = f a ??f n +?f p (26) f p new 2 =(1??+?S ap ) 2 +? 2 (1?S 2 ap ) (27) f n new 2 =(1+???S an ) 2 +? 2 (1?S 2 an )(28)f a new 2 =(1+?S ap ??S an ) 2 +(? 1?S 2 ap ??? 1?S 2 an )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Numerical simulation for ?S ap , ?S an , ?S total ap and ?S total an change of L nca and L margin with p=0.5,1.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Retrieval Performance on the CUB and CAR datasets comparing to the best reported results with embedding dimension 64 trained on ResNet50.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>SOP</cell><cell></cell><cell></cell><cell>In-shop</cell><cell></cell></row><row><cell>Method</cell><cell>R@1</cell><cell>R@10</cell><cell>R@100</cell><cell>R@1</cell><cell>R@10</cell><cell>R@20</cell></row><row><cell>BIER [13] 512</cell><cell>72.7</cell><cell>86.5</cell><cell>94.0</cell><cell>76.9</cell><cell>92.8</cell><cell>95.2</cell></row><row><cell>ABE [9] 512</cell><cell>76.3</cell><cell>88.4</cell><cell>94.8</cell><cell>87.3</cell><cell>96.7</cell><cell>97.9</cell></row><row><cell>FastAP [1] 512</cell><cell>76.4</cell><cell>89.0</cell><cell>95.1</cell><cell>90.9</cell><cell>97.7</cell><cell>98.5</cell></row><row><cell>MS [23] 512</cell><cell>78.2</cell><cell>90.5</cell><cell>96.0</cell><cell>89.7</cell><cell>97.9</cell><cell>98.5</cell></row><row><cell>EasyPositive [27] 512</cell><cell>78.3</cell><cell>90.7</cell><cell>96.3</cell><cell>87.8</cell><cell>95.7</cell><cell>96.8</cell></row><row><cell>SHN 512</cell><cell>81.0</cell><cell>92.3</cell><cell>96.8</cell><cell>90.6</cell><cell>97.4</cell><cell>98.1</cell></row><row><cell>SCT 512</cell><cell>81.9</cell><cell>92.6</cell><cell>96.8</cell><cell>90.9</cell><cell>97.5</cell><cell>98.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Retrieval Performance on the SOP and In-shop datasets comparing to the best reported results with embedding dimension 512 trained on ResNet50</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This research was partially funded by the Department of Energy, ARPA-E award #DE-AR0000594, and NIJ award 2018-75-CX-0038. Work was partially completed while the first author was an intern with Microsoft Bing Multimedia team.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the calculation of S new ap , we construct two hyper-planes: P ap spanned by f a and f p , and P an spanned by f a and f n . On P ap , f p can be decomposed into two components: f a p (the direction along f a ) and f a? p (the direction vertical to f a ). On P an , f n can be decomposed into two components: f a n (the direction along f a ) and f a? n (the direction vertical to f a ). Then the S pn should be:</p><p>S pn =f p f n =(f a p +f a? p )(f a n +f a? n )</p><p>where ? = f a? p f a? n f a? p f a? n , which represents the projection factor between P ap and P an . When f a , f n , and f p are close enough so that locally the hypersphere is a plane, then ? is the dot-product of normalized vector from f a to f p and f a to f n . If f p ,f a ,f n are co-planer then ? =1, and if moving from f a to f p is orthogonal to the direction from f a to f n , then ? =0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.2 Norm of updated features(NCA-based triplet loss)</head><p>The following derivation shows how to derive f a new , f p new and f n new in equation 9 and 10. On P ap , g p can be decomposed into the direction along f p and the direction vertical to f p . On P an , g n can be decomposed into the direction along f n and the direction vertical to f n . Then, </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep metric learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cakir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Visual-textual association with hardest and semi-hard negative pairs mining for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03083</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Saul, L.K., Weiss, Y., Bottou, L.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2821" to="2829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">Defense of the Triplet Loss for Person Re-Identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention-based ensemble for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bier -boosting independent embeddings robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visualizing deep similarity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hotels-50k: A global hotel recognition dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stochastic class-based hard example mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How to train triplet networks with 100k identities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5022" to="5030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep randomized ensembles for metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved embeddings with easy positive triplet mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Correcting the triplet selection bias for triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="71" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning spread-out local feature descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
