<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimal Transport Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benson</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>B?cigneul</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Optimal Transport Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current graph neural network (GNN) architectures naively average or sum node embeddings into an aggregated graph representation-potentially losing structural or semantic information. We here introduce OT-GNN, a model that computes graph embeddings using parametric prototypes that highlight key facets of different graph aspects. Towards this goal, we successfully combine optimal transport (OT) with parametric graph models. Graph representations are obtained from Wasserstein distances between the set of GNN node embeddings and "prototype" point clouds as free parameters. We theoretically prove that, unlike traditional sum aggregation, our function class on point clouds satisfies a fundamental universal approximation theorem. Empirically, we address an inherent collapse optimization issue by proposing a noise contrastive regularizer to steer the model towards truly exploiting the OT geometry. Finally, we outperform popular methods on several molecular property prediction tasks, while exhibiting smoother graph representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, there has been considerable interest in developing learning algorithms for structured data such as graphs. For example, molecular property prediction has many applications in chemistry and drug discovery <ref type="bibr" target="#b56">(Yang et al. 2019;</ref><ref type="bibr" target="#b49">Vamathevan et al. 2019)</ref>. Historically, graphs were decomposed into features such as molecular fingerprints, or turned into non-parametric graph kernels <ref type="bibr" target="#b52">(Vishwanathan et al. 2010;</ref><ref type="bibr" target="#b46">Shervashidze et al. 2011)</ref>. More recently, learned representations via graph neural networks (GNNs) have achieved state-of-the-art on graph prediction tasks <ref type="bibr" target="#b14">(Duvenaud et al. 2015;</ref><ref type="bibr" target="#b12">Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b27">Kipf and Welling 2017;</ref><ref type="bibr" target="#b56">Yang et al. 2019)</ref>.</p><p>Despite these successes, GNNs are often underutilized in whole graph prediction tasks such as molecule property prediction. Specifically, GNN node embeddings are typically aggregated via simple operations such as a sum or average, turning the molecule into a single vector prior to classification or regression. As a result, some of the information naturally extracted by node embeddings may be lost.</p><p>Departing from this simple aggregation step, <ref type="bibr" target="#b48">(Togninalli et al. 2019</ref>) proposed a kernel function over graphs by directly These distances are then used as the molecular representation (c) for supervised tasks, e.g. property prediction. We assume that a few prototypes, e.g. some functional groups, highlight key facets or structural features of graphs relevant to a particular downstream task at hand. We express graphs by relating them to these abstract prototypes represented as free point cloud parameters.</p><p>comparing non-parametric node embeddings as point clouds through optimal transport (Wasserstein distance). Their nonparametric model yields better empirical performance over popular graph kernels, but this idea hasn't been extended to the more challenging parametric case where optimization difficulties have to be reconciled with the combinatorial aspects of OT solvers.</p><p>Motivated by these observations and drawing inspiration from prior work on prototype learning, we introduce a new class of GNNs where the key representational step consists of comparing each input graph to a set of abstract prototypes ( <ref type="figure" target="#fig_0">fig. 1</ref>). Our desire is to learn prototypical graphs and represent data by some form of distance (OT based) to these prototypical graphs; however, for the OT distance computation it suffices to directly learn the point cloud that represents each prototype, so learning a graph structure (which would be difficult) is not necessary. In short, these prototypes play the role of basis functions and are stored as point clouds as if they were encoded from real graphs. Each input graph is first encoded into a set of node embeddings using any existing GNN architecture. The resulting embedding point cloud is then compared to the prototype embedding sets, where the distance between two point clouds is measured by their Wasserstein distance. The prototypes as abstract basis functions can be understood as keys that highlight property values associated with different graph structural features. In contrast to previous kernel methods, the prototypes are learned together with the GNN parameters in an end-to-end manner.</p><p>Our notion of prototypes is inspired from the vast prior work on prototype learning (see section 6). In our case, prototypes are not required to be the mean of a cluster of data, but instead they are entities living in the data embedding space that capture helpful information for the task under consideration. The closest analogy are the centers of radial basis function networks <ref type="bibr" target="#b8">(Chen, Cowan, and Grant 1991;</ref><ref type="bibr" target="#b40">Poggio and Girosi 1990</ref>), but we also inspire from learning vector quantization approaches <ref type="bibr" target="#b28">(Kohonen 1995)</ref> and prototypical networks <ref type="bibr" target="#b47">(Snell, Swersky, and Zemel 2017)</ref>.</p><p>Our model improves upon traditional aggregation by explicitly tapping into the full set of node embeddings without collapsing them first to a single vector. We theoretically prove that, unlike standard GNN aggregation, our model defines a class of set functions that is a universal approximator.</p><p>Introducing prototype points clouds as free parameters trained using combinatorial optimal transport solvers creates a challenging optimization problem. Indeed, as the models are trained end-to-end, the primary signal is initially available only in aggregate form. If trained as is, the prototypes often collapse to single points, reducing the Wasserstein distance between point clouds to Euclidean comparisons of their means. To counter this effect, we introduce a contrastive regularizer which effectively prevents the model from collapsing, and we demonstrate its merits empirically.</p><p>Our contributions. First, we introduce an efficiently trainable class of graph neural networks enhanced with OT primitives for computing graph representations based on relations with abstract prototypes. Second, we train parametric graph models together with combinatorial OT distances, despite optimization difficulties. A key element is our noise contrastive regularizer that prevents the model from collapsing back to standard summation, thus fully exploiting the OT geometry. Third, we provide a theoretical justification of the increased representational power compared to the standard GNN aggregation method. Finally, our model shows consistent empirical improvements over previous state-of-the-art on molecular datasets, yielding also smoother graph embedding spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries Directed Message Passing Neural Networks (D-MPNN)</head><p>We briefly remind here of the simplified D-MPNN <ref type="bibr" target="#b11">(Dai, Dai, and Song 2016)</ref> architecture which was adapted for state-ofthe-art molecular property prediction by <ref type="bibr" target="#b56">(Yang et al. 2019</ref>). This model takes as input a directed graph G = (V, E), with node and edge features denoted by x v and e vw respectively, for v, w in the vertex set V and v ? w in the edge set E. The parameters of D-MPNN are the matrices {W i , W m , W o }. It keeps track of messages m t vw and hidden states h t vw for each step t, defined as follows. An initial hidden state is set to h 0 vw := ReLU (W i cat(x v , e vw )) where "cat" denotes concatenation. Then, the updates are:</p><formula xml:id="formula_0">m t+1 vw = k?N (v)\{w} h t kv , h t+1 vw = ReLU (h 0 vw + W m m t+1 vw ) (1) where N (v) = {k ? V |(k, v) ? E} denotes v's incoming neighbors.</formula><p>After T steps of message passing, node embeddings are obtained by summing edge embeddings:</p><formula xml:id="formula_1">m v = w?N (v) h T vw , h v = ReLU (W o cat(x v , m v )).</formula><p>(2) A final graph embedding is then obtained as h = v?V h v , which is usually fed to a multilayer perceptron (MLP) for classification or regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>We illustrate, for a given 2D point cloud, the optimal transport plan obtained from minimizing the Wasserstein costs; c(?, ?) denotes the Euclidean distance. A higher dottedline thickness illustrates a greater mass transport.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimal Transport Geometry</head><p>Optimal Transport <ref type="bibr" target="#b39">(Peyr?, Cuturi et al. 2019</ref>) is a mathematical framework that defines distances or similarities between objects such as probability distributions, either discrete or continuous, as the cost of an optimal transport plan from one to the other.</p><p>Wasserstein distance for point clouds. Let a point cloud X = {x i } n i=1 of size n be a set of n points x i ? R d . Given point clouds X, Y of respective sizes n, m, a transport plan (or coupling) is a matrix T of size n ? m with entries in [0, 1], satisfying the two following marginal constraints: T1 m = 1 n 1 n and T T 1 n = 1 m 1 m . Intuitively, the marginal constraints mean that T preserves the mass from X to Y. We denote the set of such couplings as C XY .</p><p>Given a cost function c on R d ? R d , its associated Wasserstein discrepancy is defined as</p><formula xml:id="formula_2">W(X, Y) = min T?C XY ij T ij c(x i , y j ).</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model &amp; Practice</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture Enhancement</head><p>Reformulating standard architectures. The final graph embedding h = v?V h v obtained by aggregating node embeddings is usually fed to a multilayer perceptron (MLP) performing a matrix-multiplication whose i-th component is (Rh) i = r i , h , where r i is the i-th row of matrix R. Replacing ?, ? by a distance/kernel k(?, ?) allows the processing of more general graph representations than just vectors in R d , such as point clouds or adjacency tensors.</p><p>From a single point to a point cloud. We propose to replace the aggregated graph embedding h = v?V h v by the point cloud (of unaggregated node embeddings) H = {h v } v?V , and the inner-products h, r i by the below written Wasserstein discrepancy:</p><formula xml:id="formula_3">W(H, Q i ) := min T?C HQ i vj T vj c(h v , q j i ),<label>(4)</label></formula><p>where Q i = {q j i } j?{1,...,N } , ?i ? {1, . . . , M } represent M prototype point clouds each being represented as a set of N embeddings as free trainable parameters, and the cost is chosen as c = ? ? ? 2 2 or c = ? ?, ? . Note that both options yield identical optimal transport plans.</p><p>Greater representational power. We formulate mathematically that this kernel has a strictly greater representational power than the kernel corresponding to standard innerproduct on top of a sum aggregation, to distinguish between different point clouds.</p><p>Final architecture. Finally, the vector of all Wasserstein distances in eq. (4) becomes the input to a final MLP with a single scalar as output. This can then be used as the prediction for various downstream tasks, depicted in <ref type="figure" target="#fig_0">fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive Regularization</head><p>What would happen to W(H, Q i ) if all points q j i belonging to point cloud Q i would collapse to the same point q i ? All transport plans would yield the same cost, giving for c = ? ?, ? :</p><formula xml:id="formula_4">W(H, Q i ) = ? vj T vj h v , q j i = ? h, q i /|V | . (5)</formula><p>In this scenario, our proposition would simply overparametrize the standard Euclidean model.</p><p>A first obstacle and its cause. Empirically, OT-enhanced GNNs with only the Wasserstein component sometimes perform similarly to the Euclidean baseline in both train and validation settings, in spite of its greater representational power. Further investigation revealed that the Wasserstein model would naturally displace the points in each of its prototype point clouds in such a way that the optimal transport plan T obtained by maximizing vj T vj h v , q j i was not discriminative, i.e. many other transports would yield a similar Wasserstein cost. Indeed, as shown in eq. (5), if each point cloud collapses to its mean, then the Wasserstein geometry collaspses to Euclidean geometry. In this scenario, any transport plan yields the same Wasserstein cost. However, partial or local collapses are also possible and would still result in non-discriminative transport plans, also being undesirable.</p><p>Intuitively, the existence of multiple optimal transport plans implies that the same prototype can be representative for distinct parts of the molecule. However, we desire that different prototypes disentangle different factors of variation such as different functional groups.</p><p>Contrastive regularization. To address this difficulty, we add a regularizer which encourages the model to displace its prototype point clouds such that the optimal transport plans would be discriminative against chosen contrastive transport plans. Namely, consider a point cloud Y of node embeddings and let T i be an optimal transport plan obtained in the computation of W(Y, Q i ). For each T i , we then build a set N eg(T i ) ? C YQi of noisy/contrastive transports. If we denote by W T (X, Y) := kl T kl c(x k , y l ) the Wasserstein cost obtained for the particular transport T, then our contrastive regularization consists in maximizing the term:</p><formula xml:id="formula_5">i log e ?W T i (Y,Qi) e ?W T i (Y,Qi) + T?N eg(T i ) e ?W T (Y,Qi) ,<label>(6)</label></formula><p>which can be interpreted as the log-likelihood that the correct transport T i be (as it should) a better minimizer of W T (Y, Q i ) than its negative samples. This can be considered as an approximation of log(Pr(T i | Y, Q i )), where the partition function is approximated by our selection of negative examples, as done e.g. by <ref type="bibr" target="#b34">(Nickel and Kiela 2017)</ref>. Its effect is shown in <ref type="figure" target="#fig_2">fig. 3</ref>.</p><p>Remarks. The selection of negative examples should reflect the trade-off: (i) not be too large, for computational efficiency while (ii) containing sufficiently meaningful and challenging contrastive samples. Details about our choice of contrastive samples are in the experiments section. Note that replacing the set N eg(T i ) with a singleton {T} for a contrastive random variable T lets us rewrite (eq. (6)) as 1 </p><formula xml:id="formula_6">i log ?(W T ? W T i ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization &amp; Complexity</head><p>Backpropagating gradients through optimal transport (OT) has been the subject of recent research investigations: <ref type="bibr" target="#b19">(Genevay, Peyr?, and Cuturi 2017)</ref> explain how to unroll and differentiate through the Sinkhorn procedure solving OT, which was extended by <ref type="bibr" target="#b44">(Schmitz et al. 2018)</ref> to Wasserstein barycenters. However, more recently, <ref type="bibr" target="#b54">(Xu 2019)</ref> proposed to simply invoke the envelop theorem <ref type="bibr" target="#b0">(Afriat 1971)</ref> to support the idea of keeping the optimal transport plan fixed during the back-propagation of gradients through Wasserstein distances. For the sake of simplicity and training stability, we resort to the latter procedure: keeping T fixed during backpropagation. We discuss complexity in appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Analysis</head><p>In this section we show that the standard architecture lacks a fundamental property of universal approximation of functions defined on point clouds, and that our proposed architecture recovers this property. We will denote by X n d the set of point clouds</p><formula xml:id="formula_7">X = {x i } n i=1 of size n in R d .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Universality</head><p>As seen in section 3, we have replaced the sum aggregation ? followed by the Euclidean inner-product ? by Wasserstein For instance, the real molecule point cloud (red triangle) is much more dispersed when regularization is applied (right) which is desirable in order to interact with as many embeddings of each prototype as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>discrepancies. How does this affect the function class and representations?</head><p>A common framework used to analyze the geometry inherited from similarities and discrepancies is that of kernel theory. A kernel k on a set X is a symmetric function k : X ? X ? R, which can either measure similarities or discrepancies. An important property of a given kernel on a space X is whether simple functions defined on top of this kernel can approximate any continuous function on the same space. This is called universality: a crucial property to regress unknown target functions. Universal kernels. A kernel k defined on X n d is said to be universal if the following holds: for any compact subset X ? X n d , the set of functions in the form 2 m j=1 ? j ?(k(?, ? j )+? j ) is dense in the set C(X ) of continuous functions from X to R, w.r.t the sup norm ? ?,X , ? denoting the sigmoid.</p><p>Although the notion of universality does not indicate how easy it is in practice to learn the correct function, it at least guarantees the absence of a fundamental bottleneck of the model using this kernel.</p><p>In the following we compare the aggregating kernel agg(X, Y) := i x i , j y j (used by popular GNN models) with the Wasserstein kernels defined as</p><formula xml:id="formula_8">W L2 (X, Y) := min T?C XY ij T ij x i ? y j 2 2 (7) W dot (X, Y) := max T?C XY ij T ij x i , y j .<label>(8)</label></formula><p>Theorem 1. We have that: i) the Wasserstein kernel W L2 is universal, while ii) the aggregation kernel agg is not universal.</p><p>Proof: See appendix B. Universality of the W L2 kernel comes from the fact that its square-root defines a metric, and from the axiom of separation of distances: if d(x, y) = 0 then x = y.</p><p>Implications. Theorem 1 states that our proposed OT-GNN model is strictly more powerful than GNN models with summation or averaging pooling. Nevertheless, this implies we can only distinguish graphs that have distinct multi-sets of node embeddings, e.g. all Weisfeiler-Lehman distinguishable graphs in the case of GNNs. In practice, the shape of the aforementioned functions having universal approximation capabilities gives an indication of how one should leverage the vector of Wasserstein distances to prototypes to perform graph classification -e.g. using a MLP on top like we do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definiteness</head><p>For the sake of simplified mathematical analysis, similarity kernels are often required to be positive definite (p.d.), which corresponds to discrepancy kernels being conditionally negative definite (c.n.d.). Although such a property has the benefit of yielding the mathematical framework of Reproducing Kernel Hilbert Spaces, it essentially implies linearity, i.e. the possibility to embed the geometry defined by that kernel in a linear vector space.</p><p>We now discuss that, interestingly, the Wasserstein kernel we used does not satisfy this property, and hence constitutes an interesting instance of a universal, non p.d. kernel. Let us remind these notions.</p><formula xml:id="formula_9">Kernel definiteness. A kernel k is positive definite (p.d.) on X if for n ? N * , x 1 , ..., x n ? X and c 1 , ..., c n ? R, we have ij c i c j k(x i , x j ) ? 0. It is conditionally negative definite (c.n.d.) on X if for n ? N * , x 1 , ..., x n ? X and c 1 , ..., c n ? R such that i c i = 0, we have ij c i c j k(x i , x j ) ? 0.</formula><p>These two notions relate to each other via the below result <ref type="bibr" target="#b4">(Boughorbel, Tarel, and Boujemaa 2005)</ref>: Proposition 1. Let k be a symmetric kernel on X , let x 0 ? X and define the kernel:</p><formula xml:id="formula_10">k(x, y) := ? 1 2 [k(x, y) ? k(x, x 0 ) ? k(y, x 0 ) + k(x 0 , x 0 )]. (9) Thenk is p.d. if and only if k is c.n.d. Example: k = ??? 2 2 and x 0 = 0 yieldk = ?, ? .</formula><p>One can easily show that agg also defines a p.d. kernel, and that agg(?, ?) ? n 2 W(?, ?). However, the Wasserstein kernel is not p.d., as stated in different variants before (e.g. <ref type="bibr" target="#b51">(Vert 2008)</ref>) and as reminded by the below theorem. We here give a novel proof in appendix B. Theorem 2. We have that: i) The (similarity) Wasserstein kernel W dot is not positive definite, and ii) The (discrepancy) Wasserstein kernel W L2 is not conditionally negative definite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments Experimental Setup</head><p>We experiment on 4 benchmark molecular property prediction datasets <ref type="bibr" target="#b56">(Yang et al. 2019)</ref> including both regression (ESOL, Lipophilicity) and classification (BACE, BBBP) tasks. These datasets cover different complex chemical properties (e.g. ESOL -water solubility, LIPO -octanol/water distribution coefficient, BACE -inhibition of human ?-secretase, BBBP -blood-brain barrier penetration). Fingerprint + MLP applies a MLP over the input features which are hashed graph structures (called a molecular fingerprint). GIN is the Graph Isomorphism Network from , which is a variant of a GNN. The original GIN does not account for edge features, so we adapt their algorithm to our setting. Next, GAT is the Graph Attention Network from <ref type="bibr" target="#b50">(Veli?kovi? et al. 2017)</ref>, which uses multihead attention layers to propagate information. The original GAT model does not account for edge features, so we adapt their algorithm to our setting. More details about our implementation of the GIN and GAT models can be found in the appendix D.</p><p>Chemprop -D-MPNN <ref type="bibr" target="#b56">(Yang et al. 2019</ref>) is a graph network that exhibits state-of-the-art performance for molecular representation learning across multiple classification and regression datasets. Empirically we find that this baseline is indeed the best performing, and so is used as to obtain node embeddings in all our prototype models. Additionally, for comparison to our methods, we also add several graph pooling baselines. We apply the graph pooling methods, SAG pooling <ref type="bibr" target="#b30">(Lee, Lee, and Kang 2019)</ref> and TopK pooling (Gao and Ji 2019), on top of the D-MPNN for fair comparison.</p><p>Different variants of our OT-GNN prototype model are described below: ProtoW-L2/Dot is the model that treats point clouds as point sets, and computes the Wasserstein distances to each point cloud (using either L2 distance or (minus) dot product cost functions) as the molecular embedding. ProtoS-L2 is a special case of ProtoW-L2, in which the point clouds have a single point and instead of using Wasserstein distances, we just compute simple Euclidean distances between the aggregated graph embedding and point clouds. Here, we omit using dot product distances since such a model is mathematically equivalent to the GNN model. We use the the POT library (Flamary and Courty 2017) to compute Wasserstein distances using the Earth Movers Distance algorithm. We define the cost matrix by taking the pairwise L2 or negative dot product distances. As mentioned in section 3, we fix the transport plan, and only backpropagate through the cost matrix for computational efficiency. Additionally, to account for the variable size of each input graph, we multiply the OT distance between two point clouds by their respective sizes. More details about experimental setup are presented in appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>We next delve into further discussions of our experimental results. Specific experimental setup details, model sizes, parameters and runtimes can be found in appendix D.</p><p>Regression and Classification. Results are shown in table 1. Our prototype models outperform popular GNN/D-MPNN baselines on all 4 property prediction tasks. Moreover, the prototype models using Wasserstein distance (ProtoW-L2/Dot) achieve better performance on 3 out of 4 of the datasets compared to the prototype model that uses only Euclidean distances (ProtoS-L2). This indicates that Wasserstein distance confers greater discriminative power compared to traditional aggregation methods. We also find that the baseline pooling methods perform worse than the D-MPNN, and we attribute this to the fact that these models were originally created for large graphs networks without Noise Contrastive Regularizer. Without any constraints, the Wasserstein prototype model will often collapse the set of points in a point cloud into a single point. As mentioned in section 3, we use a contrastive regularizer to force the model to meaningfully distribute point clouds in the embedding space. We show 2D embeddings in <ref type="figure" target="#fig_2">fig. 3</ref>, illustrating that without contrastive regularization, prototype point clouds are often displaced close to their mean, while regularization forces them to nicely scatter. Quantitative results in table 1 also highlight the benefit of this regularization.</p><p>Learned Embedding Space. We further examine the learned embedding space of the best baseline (i.e. D-MPNN) and our best Wasserstein model. We claim that our models learn smoother latent representations. We compute the pairwise difference in embedding vectors and the labels for each test data point on the ESOL dataset. Then, we compute two measures of rank correlation, Spearman correlation coefficient (?) and Pearson correlation coefficient (r). This is reminiscent of evaluation tasks for the correlation of word embedding similarity with human labels <ref type="bibr" target="#b33">(Luong, Socher, and Manning 2013)</ref>.</p><p>Our ProtoW-L2 achieves better ? and r scores compared to the D-MPNN model ( <ref type="figure" target="#fig_5">fig. 6)</ref> indicating that our Wasserstein model constructs more meaningful embeddings with respect to the label distribution, which can be inferred also from <ref type="figure" target="#fig_4">fig. 5</ref>. Our ProtoW-L2 model, trained to optimize distances in the embedding space, produces more meaningful representations w.r.t. the label of interest.</p><p>Moreover, as qualitatively shown in <ref type="figure" target="#fig_3">fig. 4</ref>, our model provides more robust molecular embeddings compared to the baseline, in the following sense: we observe that a small perturbation of a molecular embedding corresponds to a small change in predicted property value -a desirable phenomenon that holds rarely for the baseline D-MPNN model. Our Proto-W-L2 model yields smoother heatmaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Related Work on Graph Neural Networks. Graph Neural Networks were introduced by (Gori, Monfardini, and Scarselli 2005) and <ref type="bibr" target="#b43">(Scarselli et al. 2008</ref>) as a form of recurrent neural networks. Graph convolutional networks (GCN) appeared later on in various forms. <ref type="bibr" target="#b14">(Duvenaud et al. 2015;</ref><ref type="bibr" target="#b1">Atwood and Towsley 2016)</ref> proposed a propagation rule inspired from convolution and diffusion, but these methods do not scale to graphs with either large degree distribution or node cardinality. <ref type="bibr" target="#b35">(Niepert, Ahmed, and Kutzkov 2016)</ref> defined a GCN as a 1D convolution on a chosen node ordering. <ref type="bibr" target="#b26">(Kearnes et al. 2016</ref>) also used graph convolutions to generate high quality molecular fingerprints. Efficient spectral methods were proposed by <ref type="bibr" target="#b5">(Bruna et al. 2013;</ref><ref type="bibr" target="#b12">Defferrard, Bresson, and Vandergheynst 2016)</ref>. (Kipf and Welling 2017) simplified their propagation rule, motivated from spectral graph theory <ref type="bibr" target="#b23">(Hammond, Vandergheynst, and Gribonval 2011)</ref>. Different such architectures were later unified into the message passing neural networks (MPNN) framework by <ref type="bibr" target="#b20">(Gilmer et al. 2017)</ref>. A directed MPNN variant was later used to improve state-of-the-art in molecular property prediction on a wide variety of datasets by <ref type="bibr" target="#b56">(Yang et al. 2019)</ref>. Inspired by DeepSets <ref type="bibr" target="#b58">(Zaheer et al. 2017)</ref>, ) propose a simplified, theoretically powerful, GCN architecture. Other recent approaches modify the sum-aggregation of node embeddings in the GCN architecture to preserve more information <ref type="bibr" target="#b29">(Kondor et al. 2018;</ref><ref type="bibr" target="#b37">Pei et al. 2020)</ref>. In this category there is also the recently growing class of hierarchical graph pooling methods which typically either use deterministic and non-differentiable node clustering <ref type="bibr" target="#b12">(Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b25">Jin, Barzilay, and Jaakkola 2018)</ref>, or differentiable pooling <ref type="bibr" target="#b57">(Ying et al. 2018;</ref><ref type="bibr" target="#b36">Noutahi et al. 2019;</ref><ref type="bibr">Gao and Ji 2019)</ref>. However, these methods are still strugling with small labelled graphs such as molecules where global and local node interconnections cannot be easily cast as a hierarchical interaction. Other recent geometry-inspired GNNs include adaptations to non-Euclidean spaces <ref type="bibr" target="#b32">(Liu, Nickel, and Kiela 2019;</ref><ref type="bibr" target="#b7">Chami et al. 2019;</ref><ref type="bibr" target="#b2">Bachmann, B?cigneul,</ref>  and Ganea 2019; <ref type="bibr" target="#b17">Fey et al. 2020)</ref>, and different metric learning on graphs <ref type="bibr" target="#b41">(Riba et al. 2018;</ref><ref type="bibr" target="#b3">Bai et al. 2019;</ref><ref type="bibr" target="#b31">Li et al. 2019</ref>), but we emphasize our novel direction in learning prototype point clouds.</p><p>Related Work on Prototype Learning. Learning prototypes to solve machine learning tasks started to become popular with the introducton of generalized learning vector quantization (GLVQ) methods <ref type="bibr" target="#b28">(Kohonen 1995;</ref><ref type="bibr" target="#b42">Sato and Yamada 1995)</ref>. These approaches perform classification by assigning the class of the closest neighbor prototype to each data point, where Euclidean distance function was the typical choice. Each class has a prototype set that is jointly optimized such that the closest wrong prototype is moved away, while the correct prototype is brought closer. Several extensions <ref type="bibr" target="#b22">(Hammer and Villmann 2002;</ref><ref type="bibr" target="#b45">Schneider, Biehl, and Hammer 2009;</ref><ref type="bibr" target="#b6">Bunte et al. 2012)</ref> introduce feature weights and parameterized input transformations to leverage more flexible and adaptive metric spaces. Nevertheless, such models are limited to classification tasks and might suffer from extreme gradient sparsity.</p><p>Closer to our work are the radial basis function (RBF) networks <ref type="bibr" target="#b8">(Chen, Cowan, and Grant 1991)</ref> that perform classification/regression based on RBF kernel similarities to prototypes. One such similarity vector is used with a shared  linear output layer to obtain the final prediction per each data point. Prototypes are typically set in an unsupervised fashion, e.g. via k-means clustering, or using the Orthogonal Least Square Learning algorithm, unlike being learned using backpropagation as in our case.</p><p>Combining non-parametric kernel methods with the flexibility of deep learning models have resulted in more expressive and scalable similarity functions, conveniently trained with backpropagation and Gaussian processes <ref type="bibr" target="#b53">(Wilson et al. 2016)</ref>. Learning parametric data embeddings and prototypes was also investigated for few-shot and zero-shot classification scenarios <ref type="bibr" target="#b47">(Snell, Swersky, and Zemel 2017)</ref>. Last, <ref type="bibr" target="#b13">Duin and P?kalska (2012)</ref> use distances to prototypes as opposed to p.d. kernels.</p><p>In contrast with the above line of work, our research focuses on learning parametric prototypes for graphs trained jointly with graph embedding functions for both graph classification and regression problems. Prototypes are modeled as sets (point clouds) of embeddings, while graphs are represented by sets of unaggregated node embeddings obtained using graph neural network models. Disimilarities between prototypes and graph embeddings are then quantified via set distances computed using optimal transport. Additional challenges arise due to the combinatorial nature of the Wasserstein distances between sets, hence our discussion on introducing the noise contrastive regularizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose OT-GNN: one of the first parametric graph models that leverages optimal transport to learn graph representations. It learns abstract prototypes as free parametric point clouds that highlight different aspects of the graph. Empirically, we outperform popular baselines in different molecular property prediction tasks, while the learned representations also exhibit stronger correlation with the target labels. Finally, universal approximation theoretical results enhance the merits of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Further Details on Contrastive Regularization</head><p>Motivation One may speculate that it was locally easier for the model to extract valuable information if it would behave like the Euclidean component, preventing it from exploring other roads of the optimization landscape. To better understand this situation, consider the scenario in which a subset of points in a prototype point cloud "collapses", i.e. points become close to each other (see <ref type="figure" target="#fig_2">fig. 3</ref>), thus sharing similar distances to all the node embeddings of real input graphs. The submatrix of the optimal transport matrix corresponding to these collapsed points can be equally replaced by any other submatrix with the same marginals (i.e. same two vectors obtained by summing rows or columns), meaning that the optimal transport matrix is not discriminative. In general, we want to avoid any two rows or columns in the Wasserstein cost matrix being proportional.</p><p>An optimization difficulty. An additional problem of point collapsing is that it is a non-escaping situation when using gradient-based learning methods. The reason is that gradients of all these collapsed points would become and remain identical, thus nothing will encourage them to "separate" in the future.</p><p>Total versus local collapse. Total collapse of all points in a point cloud to its mean is not the only possible collapse case. We note that the collapses are, in practice, mostly local, i.e. some clusters of the point cloud collapse, not the entire point cloud. We argue that this is still a weakness compared to fully uncollapsed point clouds due to the resulting non-discriminative transport plans and due to optimization difficulties discussed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>On the Choice of Contrastive Samples</head><p>Our experiments were conducted with ten negative samples for each correct transport plan. Five of them were obtained by initializing a matrix with uniform i.i.d entries from [0, 10) and performing around five Sinkhorn iterations <ref type="bibr" target="#b9">(Cuturi 2013)</ref> in order to make the matrix satisfy the marginal constraints. The other five were obtained by randomly permuting the columns of the correct transport plan. The latter choice has the desirable effect of penalizing the points of a prototype point cloud Q i to collapse onto the same point. Indeed, the rows of T i ? C HQi index points in H, while its columns index points in Q i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Theoretical Results</head><p>Proof of theorem 1 1. Let us first justify why agg is not universal. Consider a function f ? C(X ) such that there exists X, Y ? X satisfying both f (X) = f (Y) and k x k = l y l . Clearly, any function of the form i ? i ?(agg(W i , ?) + ? i ) would take equal values on X and Y and hence would not approximate f arbitrarily well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>To justify that W is universal, we take inspiration from the proof of universality of neural networks <ref type="bibr" target="#b10">(Cybenko 1989)</ref>.</p><p>Notation. Denote by M (X ) the space of finite, signed regular Borel measures on X .</p><p>Definition. We say that ? is discriminatory w.r.t a kernel k if for a measure ? ? M (X ),</p><formula xml:id="formula_11">X ?(k(Y, X) + ?)d?(X) = 0</formula><p>for all Y ? X n d and ? ? R implies that ? ? 0. We start by reminding a lemma coming from the original paper on the universality of neural networks by Cybenko <ref type="bibr" target="#b10">(Cybenko 1989)</ref>.</p><p>Lemma. If ? is discriminatory w.r.t. k then k is universal.</p><p>Proof: Let S be the subset of functions of the form m i=1 ? i ?(k(?, Q i ) + ? i ) for any ? i ? R, Q i ? X n d and m ? N * and denote byS the closure 3 of S in C(X ). Assume by contradiction thatS = C(X ). By the Hahn-Banach theorem, there exists a bounded linear functional L on C(X ) such that for all h ?S, L(h) = 0 and such that there exists h ? C(X ) s.t. L(h ) = 0. By the Riesz representation theorem, this bounded linear functional is of the form:</p><formula xml:id="formula_12">L(h) = X?X h(X)d?(X), for all h ? C(X ), for some ? ? M (X ). Since ?(k(Q, ?) + ?) is inS, we have X ?(k(Q, X) + ?)d?(X) = 0</formula><p>for all Q ? X n d and ? ? R. Since ? is discriminatory w.r.t. k, this implies that ? = 0 and hence L ? 0, which is a contradiction with L(h ) = 0. HenceS = C(X ), i.e. S is dense in C(X ) and k is universal. Now let us look at the part of the proof that is new.</p><p>Lemma. ? is discriminatory w.r.t. W L2 .</p><p>Proof: Note that for any X, Y, ?, ?, when ? ? +? we have that ?(?(W L2 (X,</p><formula xml:id="formula_13">Y) + ?) + ?) goes to 1 if W L2 (X, Y) + ? &gt; 0, to 0 if W L2 (X, Y) + ? &lt; 0 and to ?(?) if W L2 (X, Y) + ? = 0.</formula><p>Denote by ? Y,? := {X ? X | W L2 (X, Y) ? ? = 0} and B Y,? := {X ? X | W L2 (X, Y) &lt; ?} for ? ? 0 and ? for ? &lt; 0. By the Lebesgue Bounded Convergence Theorem we have:</p><formula xml:id="formula_14">0 = X?X lim ??+? ?(?(W L2 (X, Y) ? ?) + ?)d?(X) = ?(?)?(? Y,? ) + ?(X \ B Y, ? ? ).</formula><p>Since this is true for any ?, it implies that ?(? Y,? ) = ?(X \ B Y, Combining the previous lemmas with k = W L2 concludes the proof.</p><p>Proof of theorem 2 1. We build a counter example. We consider 4 point clouds of size n = 2 and dimension d = 2. First, define u i = ( i/2 , i%2) for i ? {0, ..., 3}. Then take X 1 = {u 0 , u 1 },</p><formula xml:id="formula_15">X 2 = {u 0 , u 2 }, X 3 = {u 0 , u 3 } and X 4 = {u 1 , u 2 }.</formula><p>On the one hand, if W(X i , X j ) = 0, then all vectors in the two point clouds are orthogonal, which can only happen for {i, j} = {1, 2}. On the other hand, if W(X i , X j ) = 1, then either i = j = 3 or i = j = 4. This yields the following Gram matrix</p><formula xml:id="formula_16">(W(X i , X j )) 0?i,j?3 = ? ? ? 1 0 1 1 0 1 1 1 1 1 2 1 1 1 1 2 ? ? ?<label>(10)</label></formula><p>whose determinant is ?1/16, which implies that this matrix has a negative eigenvalue.</p><p>2. This comes from proposition proposition 1. Choosing k = W L2 and x 0 = 0 to be the trivial point cloud made of n times the zero vector yieldsk = W dot . Sincek is not positive definite from the previous point of the theorem, k is not conditionally negative definite from proposition proposition 1.</p><p>Shape of the optimal transport plan for point clouds of same size</p><p>The below result describes the shape of optimal transport plans for point clouds of same size, which are essentially permutation matrices. For the sake of curiosity, we also illustrate in <ref type="figure">fig. 2</ref> the optimal transport for point clouds of different sizes. However, in the case of different sized point clouds, the optimal transport plans distribute mass from single source points to multiple target points. We note that non-square transports seem to remain relatively sparse as well. This is in line with our empirical observations. Proposition 2. For X, Y ? X n,d there exists a rescaled permutation matrix 1 n (? i?(j) ) 1?i,j?n which is an optimal transport plan, i.e.</p><formula xml:id="formula_17">W L2 (X, Y) = 1 n n j=1 x ?(j) ? y j 2 2<label>(11)</label></formula><p>W dot (X, Y) = 1 n n j=1</p><p>x ?(j) , y j .</p><p>Proof. It is well known from Birkhoff's theorem that every squared doubly-stochastic matrix is a convex combination of permutation matrices. Since the Wasserstein cost for a given transport T is a linear function, it is also a convex/concave function, and hence it is maximized/minimized over the convex compact set of couplings at one of its extremal points, namely one of the rescaled permutations, yielding the desired result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Complexity Wasserstein</head><p>Computing the Wasserstein optimal transport plan between two point clouds consists in the minimization of a linear function under linear constraints. It can either be performed exactly by using network simplex methods or interior point methods as done by <ref type="bibr" target="#b38">(Pele and Werman 2009</ref>) in time?(n 3 ), or approximately up to ? via the Sinkhorn algorithm <ref type="bibr" target="#b9">(Cuturi 2013)</ref> in time?(n 2 /? 3 ). More recently, <ref type="bibr" target="#b15">(Dvurechensky, Gasnikov, and Kroshnin 2018)</ref> proposed an algorithm solving OT up to ? with time complexity?(min{n 9/4 /?, n 2 /? 2 }) via a primal-dual method inspired from accelerated gradient descent.</p><p>In our experiments, we used the Python Optimal Transport (POT) library <ref type="bibr" target="#b18">(Flamary and Courty 2017)</ref>. We noticed empirically that the Earth Mover Distance (EMD) solver yielded faster and more accurate solutions than Sinkhorn for our datasets, because the graphs and point clouds were small enough (&lt; 30 elements). As such, we our final models use EMD.</p><p>Significant speed up could potentially be obtained by rewritting the POT library for it to solve OT in batches over GPUs. In our experiments, we ran all jobs on CPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Further Experimental Details</head><p>Model Sizes Using MPNN hidden dimension as 200, and the final output MLP hidden dimension as 100, the number of parameters for the models are given by table 3. The fingerprint used dimension was 2048, explaining why the MLP has a large number of parameters. The D-MPNN model is much smaller than GIN and GAT models because it shares parameters between layers, unlike the others. Our prototype models are even smaller than the D-MPNN model because we do not require the large MLP at the end, instead we compute distances to a few small prototypes (small number of overall parameters used for these point clouds). The dimensions of the prototype embeddings are also smaller compared to the node embedding dimensions of the D-MPNN and other baselines. We did not see significant improvements in quality by increasing any hyperparameter value.</p><p>Remarkably, our model outperforms all the baselines using between 10 to 1.5 times less parameters.   Runtimes. We report the average total training time (number of epochs might vary depending on the early stopping criteria), as well as average training epoch time for the D-MPNN and our prototype models in table 2. We note that our method is between 1 to 7.1 times slower than the D-MPNN baseline which mostly happens due to the frequent calls to the Earth Mover Distance OT solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup of Experiments</head><p>Each dataset is split randomly 5 times into 80%:10%:10% train, validation and test sets. For each of the 5 splits, we run each model 5 times to reduce the variance in particular data splits (resulting in each model being run 25 times). We search hyperparameters described in table 4 for each split of the data, and then take the average performance over all the splits. The hyperparameters are separately searched for each data split, so that the model performance is based on a completely unseen test set, and that there is no data leakage across data splits. The models are trained for 150 epochs with early stopping if the validation error has not improved in 50 epochs and a batch size of 16. We train the models using the Adam optimizer with a learning rate of 5e-4. For the prototype models, we use different learning rates for the GNN and the point clouds (5e-4 and 5e-3 respectively), because empirically we find that the gradients are much smaller for the point clouds. The molecular datasets used for experiments here are small in size (varying from 1-4k data points), so this is a fair method of comparison, and is indeed what is done in other works on molecular property prediction <ref type="bibr" target="#b56">(Yang et al. 2019</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline models</head><p>Both the GIN ) and GAT <ref type="bibr" target="#b50">(Veli?kovi? et al. 2017</ref>) models were originally used for graphs without edge features. Therefore, we adapt both these algorithms to our use-case, in which edge features are a critical aspect of the prediction task. Here, we expand on the exact architectures that we use for both models. First we introduce common notation that we will use for both models. Each example is defined by a set of vertices and edges (V, E). Let v i ? V denote the ith node in the graph, and let e ij ? E denote the edge between nodes (i, j). Let h (k) vi be the feature representation of node v i at layer k. Let h eij be the input features for the edge between nodes (i, j), and is static because we do updates only on nodes. Let N (v i ) denote the set of neighbors for node i, not including node i; letN (v i ) denote the set of neighbors for node i as well as the node itself.</p><p>GIN The update rule for GIN is then defined as:</p><formula xml:id="formula_19">h k vi = MLP (k) (1 + (k) ) + vj ?N (vi) [h (k?1) u + W (k) b h eij ]</formula><p>(13) As with the original model, the final embedding h G is defined as the concatenation of the summed node embeddings for each layer.</p><formula xml:id="formula_20">h G = CONCAT vi h (k)</formula><p>vi |k = 0, 1, 2...K</p><p>GAT For our implementation of the GAT model, we compute the attention scores for each pairwise node ? (k) ij as follows.</p><formula xml:id="formula_22">? (k) ij = exp f (v i , v j ) v j ?N (vi) exp f (v i , v j ) , where (15) f (v i , v j ) = ?(a (k) W (k) 1 h (k?1) vi ||W (k) 2 [h (k?1) vj +W (k) b h eij ] ) where {W (k) 1 , W (k) 2 , W<label>(k)</label></formula><p>b } are layer specific feature transforms, ? is LeakyReLU, a (k) is a vector that computes the final attention score for each pair of nodes. Note that here we do attention across all of a node's neighbors as well as the node itself.</p><p>The updated node embeddings are as follows:</p><formula xml:id="formula_23">h k vi = vj ?N (vi) ? (k) i,j h (k?1) vj<label>(16)</label></formula><p>The final graph embedding is just a simple sum aggregation of the node representations on the last layer (h G = vi h K vi ). As with <ref type="bibr" target="#b50">(Veli?kovi? et al. 2017)</ref>, we also extend this formulation to utilize multiple attention heads. To better understand if the learned prototypes offer interpretability, we examined the ProtoW-Dot model trained with NC regularization (weight 0.1). For each of the 10 learned prototypes, we computed the set of molecules in the test set that are closer in terms of the corresponding Wasserstein distance to this prototype than to any other prototype. While we noticed that one prototype is closest to the majority of molecules, there are other prototypes that are more interpretable as shown in <ref type="figure">fig. 7</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our OT-GNN prototype model computes graph embeddings from Wasserstein distances between (a) the set of GNN node embeddings and (b) prototype embedding sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>reminiscent of noise contrastive estimation<ref type="bibr" target="#b21">(Gutmann and Hyv?rinen 2010)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>2D embeddings of prototypes and of a real molecule with and without contrastive regularization for same random seed runs on the ESOL dataset. Both prototypes and real molecule point clouds tend to cluster when no regularization is used (left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>2D heatmaps of T-SNE projections of molecular embeddings (before the last linear layer) w.r.t. their associated predicted labels on test molecules. Comparing (a) vs (b) and (c) vs (d), we can observe a smoother space of our model compared to the D-MPNN baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of the correlation between graph embedding distances (X axis) and label distances (Y axis) on the ESOL dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>The Spearman and Pearson correlation coefficients on the ESOL dataset for the D ? MPNN and ProtoW-L2 model w.r.t. the pairwise difference in embedding vectors and labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>? ? ) = 0. From ?(X ) = 0 (because B Y, ? ? = ? for ? &lt; 0), we also have ?(B Y, ? ? ) = 0. Hence ? is zero on all balls defined by the metric ? W L2 . From the Hahn decomposition theorem, there exist disjoint Borel sets P, N such that X = P ? N and ? = ? + ? ? ? where ? + (A) := ?(A ? P ), ? ? (A) := ?(A ? N ) for any Borel set A with ? + , ? ? being positive measures. Since ? + and ? ? coincide on all balls on a finite dimensional metric space, they coincide everywhere (Hoffmann-J?rgensen 1976) and hence ? ? 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc># grphs= 4199 # grphs= 1512 # grphs= 2039 Baselines Fingerprint+MLP .922 ? .017 .885 ? .017 .870 ? .007 .911 ? .005 GIN .665 ? .026 .658 ? .019 .861 ? .013 .900 ? .014 GAT .654 ? .028 .808 ? .047 .860 ? .011 .888 ? .015 D-MPNN .635 ? .027 .646 ? .041 .865 ? .013 .915 ? .010 D-MPNN+SAG Pool .674 ? .034 .720 ? .039 .855 ? .015 .901 ? .034 D-MPNN+TopK Pool .673 ? .087 .675 ? .080 .860 ? .033 .912 ? .032 Results on the property prediction datasets. Best model is in bold, second best is underlined. Lower RMSE and higher AUC are better. Wasserstein models are by default trained with contrastive regularization as described in section 3. GIN, GAT and D-MPNN use summation pooling which outperforms max and mean pooling. SAG and TopK graph pooling methods are also used with D-MPNN.</figDesc><table><row><cell></cell><cell cols="2">ESOL (RMSE) Lipo (RMSE)</cell><cell>BACE (AUC)</cell><cell>BBBP (AUC)</cell></row><row><cell cols="2">Models ProtoS-L2 ProtoW-Dot (no reg.) .608 ? .029 .611 ? .034 ProtoW-Dot .594 ? .031 # grphs = 1128 Ours ProtoW-L2 (no reg.) .616 ? .028</cell><cell>.580 ? .016 .637 ? .018 .629 ? .015 .615 ? .025</cell><cell>.865 ? .010 .867 ? .014 .871 ? .014 .870 ? .012</cell><cell>.918 ? .009 .919 ? .009 .919 ? .009 .920 ? .010</cell></row><row><cell>ProtoW-L2</cell><cell>.605 ? .029</cell><cell>.604 ? .014</cell><cell>.873 ? .015</cell><cell>.920 ? .010</cell></row></table><note>edge features, not for small molecular graphs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>? .029 .393 ? .049 ProtoS-L2 .561 ? .087 .414 ? .141 ProtoW-Dot .592 ? .150 .559 ? .216 ProtoW-L2 .815 ? .026 .828 ? .020</figDesc><table><row><cell></cell><cell>Spearman ?</cell><cell>Pearson r</cell></row><row><cell>D-MPNN</cell><cell>.424</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Training times for each model and dataset in table 1.</figDesc><table><row><cell cols="4">MLP GIN GAT D-MPNN ProtoS ProtoW</cell></row><row><cell>401k 626k 671k</cell><cell>100k</cell><cell>65k</cell><cell>66k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Number of parameters per each model in table 1. Corresponding hyperparameters are in appendix D.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The parameters for our models (the prototype models all use the same GNN base model), and the values that we used for hyperparameter search. When there is only a single value in the search list, it means we did not search over this value, and used the specified value for all models.</figDesc><table><row><cell>Parameter Name</cell><cell>Search Values</cell><cell>Description</cell></row><row><cell>n_epochs</cell><cell>{150}</cell><cell>Number of epochs trained</cell></row><row><cell>batch_size</cell><cell>{16}</cell><cell>Size of each batch</cell></row><row><cell>lr</cell><cell>{5e-4}</cell><cell>Overall learning rate for model</cell></row><row><cell>lr_pc</cell><cell>{5e-3}</cell><cell>Learning rate for the prototype embeddings</cell></row><row><cell>n_layers</cell><cell>{5}</cell><cell>Number of layers in the GNN</cell></row><row><cell>n_hidden</cell><cell>{50, 200}</cell><cell>Size of hidden dimension in GNN</cell></row><row><cell>n_ffn_hidden</cell><cell cols="2">{1e2, 1e3, 1e4} Size of the output feed forward layer</cell></row><row><cell>dropout_gnn</cell><cell>{0.}</cell><cell>Dropout probability for GNN</cell></row><row><cell>dropout_fnn</cell><cell>{0., 0.1, 0.2}</cell><cell>Dropout probability for feed forward layer</cell></row><row><cell>n_pc (M)</cell><cell>{10, 20}</cell><cell>Number of the prototypes (point clouds)</cell></row><row><cell>pc_size (N)</cell><cell>{10}</cell><cell>Number of points in each prototype (point cloud)</cell></row><row><cell>pc_hidden (d)</cell><cell>{5, 10}</cell><cell>Size of hidden dimension of each point in each prototype</cell></row><row><cell>nc_coef</cell><cell cols="2">{0., 0.01, 0.1, 1} Coefficient for noise contrastive regularization</cell></row><row><cell cols="3">E What types of molecules do prototypes</cell></row><row><cell cols="2">capture ?</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">where ?(?) is the sigmoid function.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For m ? N * , ?j?j ? R and ?j ? X n d .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">W.r.t the topology defined by the sup norm f ?,X := sup X?X |f (X)|.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Theory of maxima and the method of Lagrange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afriat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="343" to="357" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>B?cigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-E</forename><surname>Ganea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05076</idno>
		<title level="m">Constant Curvature Graph Convolutional Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simgnn: A neural network approach to fast graph similarity computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="384" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Conditionally positive definite kernels for svm based image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boughorbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="113" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Limited rank matrix learning, discriminative dimension reduction and visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bunte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-M</forename><surname>Schleif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Villmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Biehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="159" to="173" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hyperbolic graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4869" to="4880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Orthogonal least squares learning algorithm for radial basis function networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Grant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="302" to="309" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of control, signals and systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lee, D. D.</editor>
		<editor>Sugiyama, M.</editor>
		<editor>Luxburg, U. V.</editor>
		<editor>Guyon, I.</editor>
		<editor>and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The dissimilarity space: Bridging structural and statistical pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>P?kalska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="826" to="832" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Computational optimal transport: Complexity by accelerated gradient descent is better than by Sinkhorn&apos;s algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dvurechensky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gasnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kroshnin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04367</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The closest molecules to some particular prototypes in terms of the corresponding Wasserstein distance. One can observe that some prototypes are closer to insoluble molecules containing rings (Prototype 2), while others prefer more soluble molecules</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>Prototype 1</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09621</idno>
		<title level="m">Deep graph matching consensus</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pot python optimal transport library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">;</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<ptr target="https://github.com/rflamary/POT" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Gao, H</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
	<note>Graph U-Nets</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning generative models with sinkhorn divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00292</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">Org</forename><surname>Jmlr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
	<note>Proceedings of the 34th International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generalized relevance learning vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Villmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8-9</biblScope>
			<biblScope unit="page" from="1059" to="1068" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Measures which agree on balls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffmann-J?rgensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematica Scandinavica</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="319" to="326" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04364</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Selforganizing maps</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="175" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Covariant compositional networks for learning graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph matching networks for learning the similarity of graph structured objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dullien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3835" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hyperbolic graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8228" to="8239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Poincar? embeddings for learning hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6338" to="6347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Noutahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Horwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gigu?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tossou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11577</idno>
		<title level="m">Towards interpretable sparse graph representation learning with laplacian pooling</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Geom-GCN: Geometric Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">C</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast and robust earth mover&apos;s distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="460" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Computational optimal transport. Foundations and Trends? in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="355" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Networks for approximation and learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1481" to="1497" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning graph distances with message passing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Llad?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Forn?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2239" to="2244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generalized learning vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Wasserstein dictionary learning: Optimal transport-based unsupervised nonlinear dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bonneel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ngole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coeurjolly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Starck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="643" to="678" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adaptive relevance matrices in learning vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Biehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3532" to="3561" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Weisfeilerlehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Wasserstein weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Togninalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ghisu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Llinares-L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6436" to="6446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Applications of machine learning in drug discovery and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vamathevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Czodrowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dunham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Drug Discovery</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="463" to="477" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0801.4061</idno>
		<title level="m">The optimal assignment kernel is not positive definite</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Gromov-Wasserstein Factorization Models for Graph Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08530</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<title level="m">How powerful are graph neural networks? International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Analyzing learned molecular representations for property prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eiden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guzman-Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hopper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3370" to="3388" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
