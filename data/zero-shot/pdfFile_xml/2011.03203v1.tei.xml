<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unleashing the Power of Neural Discourse Parsers - A Context and Structure Aware Approach Using Large Scale Pretraining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorii</forename><surname>Guz</surname></persName>
							<email>gguz@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia Vancouver</orgName>
								<address>
									<postCode>V6T 1Z4</postCode>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Huber</surname></persName>
							<email>huberpat@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia Vancouver</orgName>
								<address>
									<postCode>V6T 1Z4</postCode>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
							<email>carenini@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia Vancouver</orgName>
								<address>
									<postCode>V6T 1Z4</postCode>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unleashing the Power of Neural Discourse Parsers - A Context and Structure Aware Approach Using Large Scale Pretraining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>RST-based discourse parsing is an important NLP task with numerous downstream applications, such as summarization, machine translation and opinion mining. In this paper, we demonstrate a simple, yet highly accurate discourse parser, incorporating recent contextual language models. Our parser establishes the new state-of-the-art (SOTA) performance for predicting structure and nuclearity on two key RST datasets, RST-DT and Instr-DT. We further demonstrate that pretraining our parser on the recently available large-scale "silver-standard" discourse treebank MEGA-DT provides even larger performance benefits, suggesting a novel and promising research direction in the field of discourse analysis.</p><p>This work is licensed under a Creative Commons Attribution 4.0 International License. License details:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Discourse parsing is an important upstream task within the area of Natural Language Processing (NLP) which has been an active field of research over the last decades. In this work, we focus on discourse representations for the English language, where most research has been surrounding one of the two main theories behind discourse, the Rhetorical Structure Theory (RST) proposed by Mann and <ref type="bibr" target="#b26">Thompson (1988)</ref> or interpreting discourse according to PDTB <ref type="bibr" target="#b33">(Prasad et al., 2008)</ref>. While both theories have their strengths, the application of the RST theory, encoding documents into complete constituency discourse trees <ref type="bibr" target="#b29">(Morey et al., 2018)</ref>, has been shown to have many crucial implications on real world problems. A tree is defined on a set of EDUs (Elementary Discourse Units), approximately aligning with clause-like sentence fragments, acting as the leaves of the tree. Adjacent EDUs or sub-trees are hierarchically aggregated to form larger (possibly non-binary) constituents, with internal nodes containing (1) a nuclearity label, defining the importance of the subtree (rooted at the internal node) in the local context and (2) a relation label, defining the type of semantic connection between the two subtrees (e.g., Elaboration, Background). In this work, we focus on structure and nuclearity prediction, not taking relations into account. Previous research has shown that the use of RST-style discourse parsing as a system component can enhance important tasks, such as sentiment analysis, summarization and text categorization <ref type="bibr" target="#b0">(Bhatia et al., 2015;</ref><ref type="bibr" target="#b30">Nejat et al., 2017;</ref><ref type="bibr" target="#b11">Hogenboom et al., 2015;</ref><ref type="bibr" target="#b8">Gerani et al., 2014;</ref><ref type="bibr" target="#b14">Ji and Smith, 2017)</ref>. More recently, it has also been suggested that discourse structures obtained in an RST-style manner can further be complementary to learned contextual embeddings, like the popular BERT approach <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref>. Combining both approaches has shown to support tasks where linguistic information on complete documents is critical, such as argumentation analysis <ref type="bibr" target="#b3">(Chakrabarty et al., 2019)</ref>. Even though discourse parsers appear to enhance the performance on a variety of tasks, the full potential of using more linguistically inspired approaches for downstream applications has not been unleashed yet. The main open challenges of integrating discourse into more NLP downstream tasks and to deliver even greater benefits have been a combination of (1) discourse parsing being a difficult task itself, with an inherently high degree of ambiguity and uncertainty and (2) the lack of large-scale annotated datasets, rendering the initial problem more severe, as data-driven approaches cannot be applied to their full potential.</p><p>The combination of these two limitations has been one of the main reasons for the limited application of neural discourse parsing for more diverse downstream tasks. While there have been neural discourse parsers proposed <ref type="bibr" target="#b1">(Braud et al., 2017;</ref><ref type="bibr" target="#b40">Yu et al., 2018;</ref><ref type="bibr" target="#b25">Mabona et al., 2019)</ref>, they still cannot consistently outperform traditional approaches when applied to the RST-DT dataset, where the amount of training data is arguably insufficient for such data-intensive approaches.</p><p>In this work, we alleviate the restrictions to the effective and efficient use of discourse as mentioned above by introducing a novel approach combining a newly proposed large-scale discourse treebank with our data-driven neural discourse parsing strategy. More specifically, we employ the novel MEGA-DT "silver-standard" discourse treebank published by <ref type="bibr" target="#b12">Huber and Carenini (2020)</ref> containing over 250,000 discourse annotated documents from the Yelp'13 sentiment dataset <ref type="bibr" target="#b36">(Tang et al., 2015)</ref>, nearly three orders of magnitude larger than commonly used RST-style annotated discourse treebanks (RST-DT <ref type="bibr" target="#b2">(Carlson et al., 2002)</ref>, Instructional-DT <ref type="bibr" target="#b35">(Subba and Di Eugenio, 2009)</ref>). Given this new dataset with a previously unseen number of full RST-style discourse trees, we revisit the task of neural discourse parsing, which has been previously attempted by <ref type="bibr" target="#b40">Yu et al. (2018)</ref> and others with rather limited success. We believe that one reason why previous neural models could not yet consistently outperform more traditional approaches, heavily relying on feature engineering <ref type="bibr" target="#b38">(Wang et al., 2017)</ref>, is the lack of generalisation when using deep learning approaches on the small RST-DT dataset, containing only 385 discourse annotated documents. This makes us believe that using a more advanced neural discourse parser in combination with a large training dataset can lead to significant performance gains. Admittedly, even though MEGA-DT contains a huge number of datapoints to train on, it has been automatically annotated, potentially introducing noise and biases, which can negatively influence the performance of our newly proposed neural discourse parser when solely trained on this dataset. A natural and intuitive approach to make use of the neural discourse parser and both datasets ("silver-standard" and gold-standard) is to combine them during training, pretraining on the large-scale "silver-standard" corpus and subsequently fine-tuning on RST-DT or further human annotated datasets. This way, general discourse structures could be learned from the large-scale treebank and then enhanced with human-annotated trees. With the results shown in this paper strongly suggesting that our new discourse parser can encode discourse more effectively, we hope that our efforts will prompt researchers to develop more linguistically inspired applications based on our discourse parser. Our contributions in this paper are:</p><p>(1) We propose a novel neural discourse parsing architecture which combines multiple lines of previous work in a single framework.</p><p>(2) We combine a large-scale "silver-standard" treebank (MEGA-DT) with small, domain-specific gold-standard treebanks in a neural way during the training process, by initially pretraining on the large (domain-independent) dataset and subsequently fine-tuning on the dataset within the domain itself.</p><p>(3) We apply the neural discourse parser on two commonly used disocurse treebanks (RST-DT and Instruction-DT), showing large performance improvements of our model over previous state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The field of discourse parsing has been mainly dominated by traditional machine learning approaches, frequently outperforming initial attempts to apply deep learning and neural networks to the task. Independent of the specific approach used, three general methodologies have been followed to learn discourse trees from small datasets, such as RST-DT <ref type="bibr" target="#b2">(Carlson et al., 2002)</ref> or Instructional-DT <ref type="bibr" target="#b35">(Subba and Di Eugenio, 2009</ref>): (1) Top-down discourse parsers, splitting the document into non-overlapping text-constituents starting from the representation of the complete discourse down to individual EDUs, assigning the two resulting sub-spans a nuclearity attribute and predicting the relation holding between the sub-trees . (2) Bottom-up parsing, starting from the discourse-segmented list of EDUs and aggregating two adjacent units in every step. This approach is mostly realized using the CKY dynamic programming strategy to obtain optimal trees as in <ref type="bibr" target="#b15">Joty et al. (2015)</ref> and <ref type="bibr" target="#b17">Li et al. (2016)</ref> or using a greedy method <ref type="bibr" target="#b10">(Hernault et al., 2010)</ref>. (3) A frequently used and more locally inspired approach of bottom-up discourse parsing using the linear shift-reduce framework, adopted from previous work in syntactic parsing. While the current traditional state-of-the-art discourse parser by <ref type="bibr" target="#b38">Wang et al. (2017)</ref> uses the bottom-up shift-reduce method predicted by two separate Support Vector Machines (SVMs) for structure/nuclearity prediction and relation estimation, neural models <ref type="bibr" target="#b40">(Yu et al., 2018;</ref><ref type="bibr" target="#b1">Braud et al., 2017)</ref> utilize multi-layer perceptrons (MLP) for classifying possible actions. In our work we also follow the bottom-up shift-reduce strategy, with the detailed description of our system provided in the following section.</p><p>Besides the active research area on discourse parsing, a second line of work has emerged recently, trying to generate large-scale discourse treebanks through automated annotations from downstream tasks, such as sentiment analysis <ref type="bibr" target="#b12">(Huber and Carenini, 2020)</ref>, text classification <ref type="bibr" target="#b21">(Liu and Lapata, 2018)</ref>, summarization <ref type="bibr" target="#b22">(Liu et al., 2019a)</ref> and fake news detection <ref type="bibr" target="#b16">(Karimi and Tang, 2019)</ref>. The majority of these approaches follows the intuition that discourse trees can be inferred from downstream tasks by predicting latent representations during the learning process of the task itself <ref type="bibr" target="#b21">(Liu and Lapata, 2018;</ref><ref type="bibr" target="#b16">Karimi and Tang, 2019;</ref><ref type="bibr" target="#b22">Liu et al., 2019a)</ref> in an end-to-end manner. However, recent work by <ref type="bibr" target="#b7">Ferracane et al. (2019)</ref> has shown that the trees resulting from this approach are not only poorly aligned with general discourse structures, but furthermore are oftentimes too shallow. A rather different strategy has been employed by <ref type="bibr" target="#b12">Huber and Carenini (2020)</ref>, trying to explicitly generate a discourse augmented treebank through distant supervision from sentiment annotations in combination with Multiple-Instance Learning (MIL) and a CKY bottom-up tree generation approach. While the resulting MEGA-DT dataset has only been proposed and released recently, the authors show promising results in their work, reaching the best inter-domain performance when comparing their dataset against RST-DT and the Instruction-DT. This leads us to believe that their treebank does not only learn sentiment-related information, but can also be used to infer general discourse structures on a large scale. We will further evaluate this in section 4.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Discourse Parsing</head><p>We follow the well-established bottom-up shift-reduce aggregation principle, as previously shown effective for traditional discourse parsers such as <ref type="bibr" target="#b13">(Ji and Eisenstein, 2014;</ref><ref type="bibr" target="#b38">Wang et al., 2017)</ref> as well as neural approaches <ref type="bibr" target="#b1">(Braud et al., 2017;</ref><ref type="bibr" target="#b40">Yu et al., 2018)</ref>. In this section, we will first introduce the general principle of shift-reduce parsing and define the necessary data structures and actions available to our system. Based on the general description, we will subsequently describe the approach taken to execute a single step in the linear-time model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">General Shift-Reduce Architecture</head><p>The transition-based shift-reduce parsing architecture traditionally consists of two data structures (a queue and a stack), interacting through a set of possible actions categorized into shift and reduce actions. This architecture is illustrated in <ref type="figure" target="#fig_0">Figure 1(a, b)</ref>.</p><p>The Queue initially contains the EDUs of the complete document in the natural, sequential order, obtained either from manual annotation or off-the-shelf discourse segmenters (e.g. <ref type="bibr" target="#b18">(Li et al., 2018)</ref>). Depending on the action performed by the parser, the top element on the queue is either read or moved to the stack. At the end of the parsing process, the queue must be empty.</p><p>The Stack represents the previously processed part of the document (also in natural order). At the beginning of the process, the stack is empty and is subsequently filled and aggregated according to the system's actions. After the parsing process is completed, the stack contains the complete, aggregated document as a single discourse tree.</p><p>The Shift operation delays aggregations of sub-trees at the beginning of the document by popping the top input node (EDU) off the queue and pushing it onto the stack. The shift-reduce algorithm needs to set hard constraints to only allow shift operations if the queue still contains unprocessed nodes.</p><p>The Reduce-X operation is used to aggregate the top two partial trees (S 1 , S 2 ) on the stack into a single representation (S 1,2 ). For complete RST-style discourse trees, each reduce action needs to further define a nuclearity assignment X N ? {NN, NS, SN} to the sub-tree covering S 1,2 and a relation X R ? {Elaboration, Contrast, ...} holding between them. In this work, we limit the scope of the reduce action to solely predict the nuclearity assigment X N , as the MEGA-DT treebank currently only provides partial discourse trees, not incorporating the relation assignment.</p><p>While the specific implementation of the stack and queue components are mostly fixed, the selection of shift-and reduce-actions can be realized with rule-based approaches <ref type="bibr" target="#b27">(Marcu, 2000)</ref> or a variety of machine learning models, such as Support Vector Machines (SVMs) <ref type="bibr" target="#b13">(Ji and Eisenstein, 2014)</ref> or neural classifiers <ref type="bibr" target="#b40">(Yu et al., 2018)</ref>. The shift-reduce action selection classifier used in this work is explained below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Shift-Reduce Action Classifier</head><p>In this work, we take advantage of recent success of the BERT-inspired models on the task of language modeling, employing the distilled version  of the large-scale RoBERTa <ref type="bibr" target="#b23">(Liu et al., 2019b)</ref> language model as the base for our action classifier. At each step in the shift-reduce framework, we predict the next action by encoding the top two elements of the Stack (S 1 and S 2 ) and the top element of the Queue (Q 1 ) <ref type="bibr" target="#b38">(Wang et al., 2017</ref>) using the RoBERTa model as well as additional structural features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RoBERTa-based semantic features:</head><p>To align with the input format requirements of the RoBERTa model, we first encode the joint representation of the three components under consideration (namely S 1 , S 2 and Q 1 ) into a single string</p><formula xml:id="formula_0">s = [CLS] S 2 [SEP ] S 1 [SEP ] Q 1 [SEP ]</formula><p>with denoting the concatenation and following the standard RoBERTa syntax, [CLS] and [SEP ] representing the sequence-classification and end-of-sequence tokens respectively. Please note that S 1 , S 2 and Q 1 are not the typically used dense representations of sub-trees or EDUs, but solely contain the textual representation of a sub-tree or EDU as a flat sequence of words.</p><p>Since the input sequence-length of the RoBERTa language model is by default bound by 512 tokens and the elements on the stack (S 1 , S 2 ) represent increasingly large sub-trees (and therefore text-spans) with every additional reduce operation executed, we restrict the length of S 1 and S 2 to a maximum of 240 words each. Specifically, if one of the constituents exceeds 240 tokens, the concatenation of the 120 leading and trailing words is chosen as the span's representation. The decision to retain the leading and trailing parts of a span comes from the observation that those parts often contain important cue words which signal explicit discourse relations <ref type="bibr" target="#b33">(Prasad et al., 2008)</ref> 1 . Furthermore, if the length of S 1 or S 2 is below the maximum number of 240 tokens, it is padded with [MASK] tokens to preserve absolute positions in the RoBERTa input, which is important since RoBERTa uses absolute positional embeddings. The top element on the queue (Q 1 ) is truncated or extended to the leftover capacity of 28 tokens in the same way, which clearly suffices for Queue-elements, only containing single EDUs which are 13 words on average for the RST-DT corpus and 7 words for MEGA-DT. Hence, the remaining space of 480 tokens is split equally among the stack elements, as described above. The embedding c for the current stack and queue configuration is computed as follows:</p><formula xml:id="formula_1">v = RoBERTa(s)<label>(1)</label></formula><p>with c = v[0] ? R 768 encoding the full span representation at the index of the [CLS]-token.</p><p>Structural features: Previous successful approaches to traditional discourse parsing <ref type="bibr" target="#b15">(Joty et al., 2015;</ref><ref type="bibr" target="#b13">Ji and Eisenstein, 2014)</ref> have shown that the structural organization of a document into sentences and paragraphs plays a crucial role when predicting discourse, with <ref type="bibr" target="#b15">Joty et al. (2015)</ref> giving strong intuition for their usefulness by showing that less than 5% of discourse subtrees violate sentence boundaries in the RST-DT corpus. To explicitly model these structural features, we use the same organizational features as <ref type="bibr" target="#b38">Wang et al. (2017)</ref> to determine where a span is positioned within the document as well as relative to adjacent constituents. More specifically, for all three spans S 1 , S 2 and Q 1 , we extract two sets of features:</p><p>(1) Whether the span is at the beginning/end of a sentence/paragraph/document and (2) For each pair of adjacent spans ((S 2 , S 1 ) and (S 1 , Q 1 )) we compute the features indicating whether the pair is within a single sentence/paragraph. For the three constituents under consideration, this results in an ordered sequence o of 28 values. Adding a distinct embedding layer with 10 neurons u i = emb(o i ) on top of each value o i in the sequence results in a concatenated dense representation u = u 1 , ..., u 28 ? R 280 for the structural features. Whenever a feature cannot be computed (for example when the Queue is empty or the Stack contains a single element), we represent it as a zero-vector.</p><p>Action classification: To predict the next shift-reduce action during the tree-generation process, the semantic and structural features described above are concatenated (see <ref type="figure" target="#fig_0">Figure 1(c)</ref>) and fed into a twolayer MLP with an intermediate GeLU <ref type="bibr" target="#b9">(Hendrycks and Gimpel, 2016)</ref> activation and a final softmax layer (see eq. 2, 3) for each of the four possible actions (Shift, Reduce NN , Reduce NS , Reduce SN ) l = MLP(c u) (2) p = SoftMax(l)</p><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Neural Shift-Reduce Training Procedure</head><p>We minimize the weighted cross-entropy loss in every parsing step individually, allowing for (1) more fine-grained optimization and (2) more parallelizable training. The training loss is thereby computed between the unnormalized prediction l (see eq. 2) and the respective gold-label y ? {Shift, Reduce N N , Reduce N S , Reduce SN }. Since there is only a single shift but three reduce actions, we weight the four output classes by factors [ 3 6 , 1 6 , 1 6 , 1 6 ] to equally penalize an incorrect shift/reduce action. At test time, the document tree structure is constructed greedily by selecting the action with the highest probability (see eq. 3) at each parsing step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We first introduce the three datasets we used to train and evaluate our model against strong baselines (section 4.1). Further, we describe how to effectively combine diverse treebanks in a neural manner using pretraining and fine-tuning, now possible with our neural discourse parser (section 4.2). Our model hyperparameters and the respective search spaces on the development set are presented in section 4.3, followed by the baseline models in section 4.4. The experiments section will then introduce the metrics used in this paper (section 4.5), discuss insights gathered in preliminary evaluations (section 4.6) and finally present results aggregated in section 4.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>This work relies on three distinct RST-style discourse treebanks for the English language.</p><p>RST-DT is the largest human-annotated RST-style discourse parsing corpus <ref type="bibr" target="#b2">(Carlson et al., 2002)</ref>, consisting of news articles from the Wall Street Journal. The treebank contains 347 documents in the training-and 38 in the test-set. We further split the training portion into 90% training data and 10% development data to perform hyper-parameter and architecture optimization.</p><p>Instructional Dataset is the second human-annotated RST-style corpus used in this work, containing 176 documents. The Instructional Dataset (short: Instr-DT) is used to train and evaluate discourse parsers in the home-repair instructions domain <ref type="bibr" target="#b35">(Subba and Di Eugenio, 2009</ref>). During preprocessing, we combine multi-rooted documents using a sequence of right-branching decisions with an N-N nuclearity assignment. We randomly separate the data into a 90% training-and a 10% test-portion. Please note that our training/test split is consistent across all models except the CODRA model described below, where we report the original results published by the authors using 10-fold validation.</p><p>MEGA-DT is the first successful automatically generated "silver-standard" discourse treebank obtained by applying distant supervision on the large-scale Yelp'13 sentiment dataset <ref type="bibr" target="#b36">(Tang et al., 2015)</ref>. Recently published by <ref type="bibr" target="#b12">Huber and Carenini (2020)</ref>, the treebank contains ?250,000 documents with full RST-style discourse trees encompassing structure and nuclearity attributes. The MEGA-DT corpus has been shown to achieve superior performance when compared to human-annotated datasets (including RST-DT) on the discourse domain-transfer task. Due to computational limitations we pretrain our model on a 52.5k subset of MEGA-DT, with 50k trees used for training and 2.5k datapoints left out for the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Combining Treebanks</head><p>In a first stage of our experiments, we will verify the effectiveness of our proposed architecture by training and evaluating the parser on individual datasets. Additionally, we will pretrain the parser on MEGA-DT until it converges on the development portion and then fine-tune on RST-DT and Instr-DT to evaluate the usefulness of pretraining on silver-standard discourse trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hyperparameters and Training Setup</head><p>The hyperparameters in our model are heavily influenced by previous findings. For the RoBERTa model <ref type="bibr" target="#b23">(Liu et al., 2019b)</ref>, we use the pre-trained distilled version proposed in  with 6 layers containing 12 attention heads and a hidden size of 768, as implemented by . The structural features used as inputs for the classification module are encoded as 10-dimensional embeddings for each of the 28 organizational features. During training we use the AdamW optimizer <ref type="bibr" target="#b24">(Loshchilov and Hutter, 2019)</ref> with a learning rate of 0.001 and a weight decay value of 0.01 for both pretraining and fine-tuning. We further apply gradient norm clipping at 0.2 <ref type="bibr" target="#b31">(Pascanu et al., 2013)</ref>. The learning rate was scheduled as in <ref type="bibr" target="#b37">Vaswani et al. (2017)</ref>, using 4000 warm-up steps. Due to the variable size trees in the training data, we aggregate documents with identical number of EDUs into batches of size 20 during pretraining and 5 for fine-tuning. All model configurations are trained by early stopping if the performance of neither structure nor nuclearity improves over 3 consecutive epochs on the development dataset. Our models are trained using PyTorch <ref type="bibr" target="#b32">(Paszke et al., 2019)</ref> on a GTX 1080 Ti GPU with 11GB of memory. Our code and model-checkpoints will be made publicly available with the publication of this paper 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Baselines</head><p>To evaluate the performance of our model in the context of RST-style discourse parsing, we compare it against a variety of previously proposed, competitive baselines: The DPLP parser <ref type="bibr" target="#b13">(Ji and Eisenstein, 2014</ref>) is a traditional discourse parser utilizing an SVM-classifier within the shift-reduce framework solely based on linear projections of lexical features. The CODRA model <ref type="bibr" target="#b15">(Joty et al., 2015)</ref> uses an optimal CKY-based chart parser in combination with Dynamic Conditional Random Fields (CRF), separated on sentence-level. The gCRF model (Feng and Hirst, 2014) follows a similar approach but utilizes a greedy strategy. The Two-Stage parser proposed by <ref type="bibr" target="#b38">Wang et al. (2017)</ref> is the current SOTA system on the RST-DT structure prediction. The model uses two separate linear SVM classifiers. We use the public codebase 3 provided by <ref type="bibr" target="#b38">Wang et al. (2017)</ref> and remove the relation classification module for our experiments. Transition-Syntax: the parser by <ref type="bibr" target="#b40">Yu et al. (2018)</ref> is a neural shift-reduce parser utilizing LSTMs to generate EDU embeddings. In addition, they apply a neural dependency parser for extracting syntactic features. Cross-Lingual is the neural shift-reduce approach by <ref type="bibr" target="#b1">Braud et al. (2017)</ref>, utilizing RST treebanks from multiple languages and the Top-Down-Generative parser by <ref type="bibr" target="#b25">Mabona et al. (2019)</ref> is a recent top-down transition-based neural generative parser employing Tree-LSTM for encoding subtrees on the stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Metrics</head><p>As suggested in a recent literature analysis by <ref type="bibr" target="#b28">Morey et al. (2017)</ref>, we use the original Parseval measure to compare the micro-average F1-scores of our model with our selected baselines. To further allow additional comparisons, we also report the results with respect to RST-Parseval, currently still more commonly used in recent literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Preliminary Evaluation</head><p>In our preliminary experiments, we evaluate a set of modelling decisions on the held-out development set, influencing the design of our final model. We obtained three useful insights during this phase:</p><p>(1) Adding padding to the three classifier input strings used in the RoBERTa model, extending each of them to the maximum defined length of 240 words for the two stack elements S 1 and S 2 and padding the top element on the queue (Q 1 ) to 28 words substantially enhanced the performance of the component. We believe this is likely to be the case because RoBERTa internally uses absolute positional embeddings.</p><p>(2) Following the intuition that an incorrect shift-and reduce-action should be penalized similarly (independent of the nuclearity label), we found that weighting the loss function as described in section 3.3 boosts the model's performance on both, the structure and nuclearity metric.  (3) We experimented with different ways for summarizing the outputs of RoBERTa (see <ref type="figure" target="#fig_0">Figure 1(c)</ref>) into a single vector. After experiments with simple and attention-based averaging of RoBERTa outputs, we found these approaches to produce sligtly worse results compared to simply using the output vector corresponding to the [CLS] token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Results</head><p>The final results of our evaluation are presented in Tables 1 and 2. The two tables contain slightly different subsets of competitive discourse parsers from previous work, depending on the metric on which the original authors evaluate their models. The reported scores were either taken from the original paper or the literature survey by <ref type="bibr">Morey et al. (2017) 4</ref> . The left-most column in the first sub-table contains the models in the comparison, showing previously proposed baselines along with two versions of our new neural discourse parser, with and without pretraining. The center column contains the evaluation results on the structure prediction task for the two test datasets (RST-DT and Instr-DT). The right-most column shows the performance for each of the models on the nuclearity prediction task, again subdivided for the two evaluation datasets. The second sub-table contains the results for various ablations of our model and the bottom sub-table shows human results on the tasks. For all of our models we report the average performance as well as the standard deviation on each metric over five independent runs. We compare the two versions of our neural discourse parser against the best-performing, previously proposed model for each of the four prediction tasks, meaning we compare the performance of our model, for example on the RST-DT dataset, against the current SOTA model on RST-DT structure-prediction by <ref type="bibr" target="#b38">Wang et al. (2017)</ref> and for nuclearity against <ref type="bibr" target="#b40">Yu et al. (2018)</ref>. The best performing baseline on the Instr-DT dataset is the CODRA model <ref type="bibr" target="#b15">(Joty et al., 2015)</ref>. Please note again the different evaluation procedure on Instr-DT that was used for CODRA model <ref type="bibr" target="#b15">(Joty et al., 2015)</ref>. For a more direct comparison, please see our Instr-DT results against the Two-Stage parser <ref type="bibr" target="#b38">(Wang et al., 2017)</ref>, which utilizes the same split as ours.</p><p>When examining our final evaluations shown in Tables 1 and 2 it becomes clear that our newly proposed neural discourse parser reaches the highest performance on all measures except the structure prediction on the Instr-DT dataset. We observe that our model strongly outperforms the SOTA approach on the RST-DT structure prediction by <ref type="bibr" target="#b38">Wang et al. (2017)</ref>. Furthermore, pretraining on the MEGA-DT treebank leads to further improvement with respect to the mean scores over independent runs.</p><p>On the Instr-DT dataset, our parser achieves a result similar to the model of <ref type="bibr" target="#b15">Joty et al. (2015)</ref> on the structure prediction task and substantially outperforms the SOTA baseline on the nuclearity measure when pretraining is applied. Nonetheless, a particularly important result is that our system produces consistently strong performance across multiple domains, which neither of the top-performing traditional systems <ref type="bibr" target="#b38">(Wang et al., 2017;</ref><ref type="bibr" target="#b15">Joty et al., 2015)</ref> managed to demonstrate. This serves as an indication that employing large-scale language models alleviates the need for extensive manual feature engineering employed by these systems for RST discourse parsing.</p><p>In addition, we perform an ablation study of our system to analyze the importance of each individual component of our parser. The first row in the second sub-tables illustrates the results when only organizational features are used, while the second row shows the impact of removing the features and only using RoBERTa for the action classification. Finally, the third row contains the performance of our system with organizational features and a randomly initialized RoBERTa model component.</p><p>We obverse that removing the organizational features results in a noticeable drop in performance, implying the importance of encoding the document structure explicitly. Unsurprisingly, removing the RoBERTa feature extractor leads to a large performance drop, far below the competitive baselines, since this version of our system does not take discourse connective words into account. Finally, we demonstrate the importance of LM pretraining and pretrained word embeddings in the last row of the ablation sub-table. While this system performs on par with traditional systems in respect to structure prediction, most likely because of the organizational features, it demonstrates inferior performance on the nuclearity prediction task, which (even in the easiest scenarios) requires knowledge of more high-level concepts, such as sentence coordination and subordination. In more advanced cases, it plausibly requires knowledge about the author's communicative goal. The overall difficulty of this task is reflected in the relatively low human evaluation scores shown in the last row. Our results can be summarized as follows:</p><p>(1) Our proposed approach achieves state of the art performance on both the RST-DT and the Instr-DT datasets.</p><p>(2) Applying large-scale language models leads to stronger results and higher domain adaptivity in RST discourse parsing. (3) Pretraining the discourse parser on the large-scale "silver-standard" MEGA-DT treebank enhances the performance and supports the ability of the neural parser to generalize across multiple datasets and domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we proposed a rather simple, yet highly effective discourse parser, utilizing recent neural BERT-based language models in combination with structural features. The integration of those inputfeatures within a standard shift-reduce framework as well as an unprecedented use of recent large-scale "silver-standard" discourse parsing datasets for pretraining reaches a new state-of-the-art performance on both, the RST-DT and Instr-DT treebanks. We show that our new, neural discourse parser already achieves better or similar performance when trained and evaluated on the RST-DT and Instr-DT datasets, however, the consistent and significant SOTA result is reached when incorporating pretraining on the MEGA-DT corpus. The presented pretraining approach on the silver-standard MEGA-DT dataset also further validates the usefulness of additional supervision for this task and calls for more work in that area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Work</head><p>As directions for future work, we plan to run experiments with larger language models, as our lightweight RoBERTa model only contains 82M parameters, while top-performing language models such as BERT-Large utilize an order of magnitude more parameters. We also want to explore neural parsing strategies besides the shift-reduce framework trained on the large-scale MEGA-DT treebank, comparable to the recently proposed top-down neural discourse parsing architecture by <ref type="bibr" target="#b25">Mabona et al. (2019)</ref>. Further, we plan to extend our framework to also predict relations, generating complete discourse trees. Another line of future work is to evaluate the effectiveness of pretraining on other, more shallow discourse analysis frameworks and datasets such as PDTB, only containing flat discourse trees, which might potentially require new approaches for silver-standard annotation. Lastly, in the short term we plan to overcome our computational limitations and pretrain our model on the full MEGA-DT. After that, we also want to generally venture into even larger pretrained treebanks generated according to <ref type="bibr" target="#b12">Huber and Carenini (2020)</ref>, taking into account more diverse sentiment datasets (such as IMDB <ref type="bibr" target="#b5">(Diao et al., 2014)</ref> and Amazon reviews <ref type="bibr" target="#b41">(Zhang et al., 2015)</ref>) to extend the size and generality of the pretraining approach and eventually enhance the overall performance of our parser.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Example Shift action -Top element of the Queue gets moved to the top of the Stack. (b) Example Reduce action -Top 2 elements of a stack are assembled into a subtree. (c) Example of input to our classifier, consisting of the RoBERTa string-encoding and numerical, structural features. Note that since EDUs 1 and 2 form a subtree, their spans are concatenated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>?0.27 72.43 ?1.12 64.55 ?0.56 61.38 ?1.37 44.41 Our Parser + Pretraining ?0.95 72.94 ?0.61 65.41 ?0.90 61.86 ?1.11 46.59 Our Parser (-RoBERTa) ?0.18 59.89 ?0.30 54.68 ?0.55 33.28 ?1.83 28.36 Our Parser (-Features) ?0.72 70.61 ?1.74 63.32 ?0.65 58.37 ?1.83 44.41 Our Parser (-LM Pretraining) ?0.42 65.78 ?2.97 53.50 ?0.58 48.93 ?2.69 32.82</figDesc><table><row><cell>Model</cell><cell cols="2">Structure RST-DT Instr-DT</cell><cell cols="2">Nuclearity RST-DT Instr-DT</cell></row><row><cell>DPLP(2014)</cell><cell>64.10</cell><cell>-</cell><cell>54.20</cell><cell>-</cell></row><row><cell>gCRF(2014)</cell><cell>68.60</cell><cell>-</cell><cell>55.90</cell><cell>-</cell></row><row><cell>CODRA(2015)</cell><cell>65.10</cell><cell>-</cell><cell>55.50</cell><cell>-</cell></row><row><cell>Cross-Lingual (2017)</cell><cell>62.70</cell><cell>-</cell><cell>54.50</cell><cell>-</cell></row><row><cell>Two-Stage(2017)</cell><cell>70.97</cell><cell>58.86</cell><cell>57.97</cell><cell>40.00</cell></row><row><cell>Top-Down-Generative(2019)</cell><cell>67.10</cell><cell>-</cell><cell>57.40</cell><cell>-</cell></row><row><cell>Our Parser</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Human (2017)</cell><cell>78.7</cell><cell>-</cell><cell>66.8</cell><cell>-</cell></row><row><cell cols="5">Table 1: Micro-averaged F1-scores for structure and nuclearity prediction using the original Parseval</cell></row><row><cell cols="5">measure as proposed in Morey et al. (2017), evaluated on the RST-DT and Instr-DT corpora. Best</cell></row><row><cell cols="5">performance per column is bold. (subscripts on results indicate standard deviation, -non-published</cell></row><row><cell>values)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>?0.13 86.22 ?0.56 82.27 ?0.50 73.03 ?1.11 65.82 Our Parser + Pretraining ?0.48 86.47 ?0.30 82.71 ?0.70 73.53 ?1.01 66.59 Our Parser (-RoBERTa) ?0.09 79.95 ?0.15 77.34 ?0.27 48.72 ?1.07 59.52 Our Parser (-Features) ?0.36 85.30 ?0.87 81.66 ?0.46 71.04 ?0.59 65.98 Our Parser (-LM Pretraining) ?0.21 82.89 ?1.49 76.75 ?0.24 63.22 ?0.85 59.18</figDesc><table><row><cell>Model</cell><cell cols="2">Structure RST-DT Instr-DT</cell><cell cols="2">Nuclearity RST-DT Instr-DT</cell></row><row><cell>DPLP(2014)</cell><cell>82.00</cell><cell>-</cell><cell>68.20</cell><cell>-</cell></row><row><cell>gCRF(2014)</cell><cell>84.30</cell><cell>-</cell><cell>69.40</cell><cell>-</cell></row><row><cell>CODRA(2015)</cell><cell>82.60</cell><cell>82.88</cell><cell>68.30</cell><cell>64.13</cell></row><row><cell>Cross-Lingual (2017)</cell><cell>81.30</cell><cell>-</cell><cell>68.10</cell><cell>-</cell></row><row><cell>Transition-Syntax(2018)</cell><cell>85.50</cell><cell>-</cell><cell>73.10</cell><cell>-</cell></row><row><cell>Two-Stage(2017)</cell><cell>85.98</cell><cell>79.43</cell><cell>72.40</cell><cell>62.39</cell></row><row><cell>Our Parser</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Human (2017)</cell><cell>88.30</cell><cell>-</cell><cell>77.30</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Micro-averaged F1-scores for structure and nuclearity prediction using RST-Parseval, evaluated on the RST-DT and Instr-DT corpora. Best performance per column is bold. (subscripts on results indicate standard deviation, -non-published values)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">As opposed to implicit discourse relations, which can only be inferred from the complete semantics of spans.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/ Software.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/yizhongw/StageDP/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Even though the authors of the Two-Stage parser only report RST-Parseval scores on RST-DT, we also evaluate their approach on Instr-DT and with respect to the original Parseval metric.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers and the UBC-NLP group for their insightful comments and suggestions. This research was partially supported by the Language &amp; Speech Innovation Lab of Cloud BU, Huawei Technologies Co., Ltd.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Better document-level sentiment analysis from RST discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parminder</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2212" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-lingual RST discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chlo?</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>S?gaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="292" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">RST discourse treebank. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ampersand: Argument mining for persuasive online discussions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuhin</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hidey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smaranda</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyssa</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2926" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A linear-time bottom-up discourse parser with constraints and postediting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Wei Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ferracane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><forename type="middle">Jessy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01472</idno>
		<title level="m">Evaluating discourse in structured text representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Abstractive summarization of product reviews using discourse structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shima</forename><surname>Gerani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bita</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nejat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1602" to="1613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hilda: A discourse parser using support vector machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Hernault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Prendinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using rhetorical structure in sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hogenboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavius</forename><surname>Frasincar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franciska</forename><forename type="middle">De</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uzay</forename><surname>Kaymak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="69" to="77" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mega RST discourse treebanks with structure and nuclearity from scalable distant sentiment supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Representation learning for text-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural discourse structure for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="996" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Codra: A novel discriminative framework for rhetorical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning hierarchical discourse-level structure for fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07389</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discourse parsing with attention-based hierarchical neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="362" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Segbot: A generic neural text segmentation model with pointer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence and the 23rd</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence and the 23rd</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<title level="m">European Conference on Artificial Intelligence, IJCAI-ECAI-2018, pages xx -xx</title>
		<meeting><address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A unified linear-time framework for sentence-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prathyusha</forename><surname>Jwalapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saiful</forename><surname>Bari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05682</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning structured text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="63" to="75" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Single document summarization as tree induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1745" to="1755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural generative rhetorical structure parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amandla</forename><surname>Mabona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="2284" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rhetorical structure theory: Toward a functional theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text-Interdisciplinary Journal for the Study of Discourse</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The Theory and Practice of Discourse Parsing and Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How much progress have we made on RST discourse parsing? a replication study of recent results on the RST-DT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="1319" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A dependency perspective on RST discourse parsing and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="197" to="235" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring joint neural model for sentence level discourse parsing and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bita</forename><surname>Nejat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="289" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
	<note>Junjie Bai, and Soumith Chintala</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>The penn discourse treebank 2.0. LREC</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An effective discourse parser that uses rich linguistic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Subba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">Di</forename><surname>Eugenio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="566" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
		<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A two-stage parsing method for text-level discourse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="184" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Huggingface&apos;s transformers: State-of-theart natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R&amp;apos;emi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transition-based neural RST parsing with implicit syntax features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="559" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
