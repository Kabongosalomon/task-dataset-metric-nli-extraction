<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">S3CNet: A Sparse Semantic Scene Completion Network for LiDAR Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Agia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhai</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Bingbing</surname></persName>
						</author>
						<title level="a" type="main">S3CNet: A Sparse Semantic Scene Completion Network for LiDAR Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sparse Convolution</term>
					<term>Semantic Scene Completion</term>
					<term>Autonomous Driv- ing</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the increasing reliance of self-driving and similar robotic systems on robust 3D vision, the processing of LiDAR scans with deep convolutional neural networks has become a trend in academia and industry alike. Prior attempts on the challenging Semantic Scene Completion task -which entails the inference of dense 3D structure and associated semantic labels from "sparse" representations -have been, to a degree, successful in small indoor scenes when provided with dense point clouds or dense depth maps often fused with semantic segmentation maps from RGB images. However, the performance of these systems drop drastically when applied to large outdoor scenes characterized by dynamic and exponentially sparser conditions. Likewise, processing of the entire sparse volume becomes infeasible due to memory limitations and workarounds introduce computational inefficiency as practitioners are forced to divide the overall volume into multiple equal segments and infer on each individually, rendering real-time performance impossible. In this work, we formulate a method that subsumes the sparsity of large-scale environments and present S3CNet, a sparse convolution based neural network that predicts the semantically completed scene from a single, unified LiDAR point cloud. We show that our proposed method outperforms all counterparts on the 3D task, achieving state-of-the art results on the SemanticKITTI benchmark <ref type="bibr" target="#b0">[1]</ref>. Furthermore, we propose a 2D variant of S3CNet with a multiview fusion strategy to complement our 3D network, providing robustness to occlusions and extreme sparsity in distant regions. We conduct experiments for the 2D semantic scene completion task and compare the results of our sparse 2D network against several leading LiDAR segmentation models adapted for bird's eye view segmentation on two open-source datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Scene understanding is a challenging component of the autonomous driving problem, and is considered by many as a foundational building block of a complete self-driving system. The construction of maps, the process of locating static and dynamic objects, and the response to the environment during self-driving are inseparable from the agent's understanding of the 3D scene. In practice, scene understanding is typically approached by semantic segmentation. When working with sparse Li-DAR scans, scene understanding is not only reflected in the semantic segmentation of the 3D point cloud but also includes the prediction and completion of certain regions, that is, semantic scene completion. Scene completion is the premise of 3D semantic map construction and is a hot topic in current research. However, as semantic segmentation based on 2D image content has reached very mature levels, methods that infer complete structure and semantics from 3D point cloud scenes, which is of significant import to robust perception and autonomous driving, are only at preliminary development and exploration stages. Limited by the sparsity of point cloud data and the lack of features, it becomes very difficult to extract useful semantic information in 3D scenes. Therefore, the understanding of large scale scenes based on 3D point clouds has become an industry endeavor.  Semantic segmentation and scene completion of 3D point clouds are usually studied separately <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, but with the emergence of large-scale datasets such as ScanNet <ref type="bibr" target="#b3">[4]</ref> and SemanticKITTI <ref type="bibr" target="#b0">[1]</ref>, researchers have discovered a deep intertwining of an object's semantics with its underlying geometry, and since, have begun exploiting this with the joint learning of semantic segmentation and scene completion to boost model performance <ref type="bibr" target="#b4">[5]</ref>. For instance, speculating that an object occluded by vehicles and surrounded by leaves is a trunk simplifies the task of inferring it's shape. Conversely, inferring the shape of a pole-like object forms a prior on it's semantic class being a trunk rather than a wall. While previous semantic scene completion methods built on dense 2D or 3D convolutional layers have done well in small-scale indoor environments, they have struggled to maintain their accuracy and efficiency in outdoor environments for several reasons. For one, dense 2D convolutional methods that thrived in the feature rich 2D image space are no longer sufficient when tackling large and sparse LiDAR scans that contain far fewer geometric and semantic descriptors. Furthermore, the dense 3D convolution becomes extremely wasteful in terms of computation and memory since the majority of the 3D volume of interest is in fact empty. Thereby, our main contributions are listed as the following: (a) a sparse tensor based neural network architecture that efficiently learns features from sparse 3D point cloud data and jointly solves the coupled scene completion and semantic segmentation problem; (b) a novel geometric-aware 3D tensor segmentation loss; (c) a multi-view fusion and semantic post-processing strategy addressing the challenges of distant or occluded regions and small-sized objects. Given a single sparse point cloud frame, our model predicts a dense 3D occupancy cuboid with semantic labels assigned to each voxel cell (as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>), generating rich information of the 3D environment that is not contained in the original input such as gaps between LiDAR scans, occluded regions and future scenes.</p><p>In order to effectively complete occluded voxel regions from LiDAR scans, we focus on exploiting the geometrical relationship of the 3D points both locally and globally. In this work, we utilize point-wise normal vectors as a geometrical feature encoding to guide our model in filling the gaps according to the object's local surface convexity. We also leverage a LiDAR-based flipped Truncated Signed Distance Function (fTSDF <ref type="bibr" target="#b4">[5]</ref>) computed from a spherical range image as a spatial encoding to differentiate free, occupied and occluded space of a scene. As for future scenes, because these regions are far from the vehicle and are primarily road or other forms of terrain, we propose a 2D variant of the sparse semantic scene completion network to support the construction of the 3D scene via multi-view fusion with Bird's Eye View (BEV) semantic map predictions. To tackle sparsity, we leveraged the Minkowski Engine <ref type="bibr">[6]</ref>, an auto-differentiation library for sparse tensors to build our 2D and 3D semantic scene completion network. We have also adopted a combined geometric inspired semantic segmentation loss to improve the accuracy of semantic label predictions. Since our network is trained in a complex real-world autonomous driving dataset with 20 classes of dynamic and static objects, and the input data is simply a voxelized LiDAR point cloud appended with geometrical and spatial feature encodings, our model can be deployed on-the-go with various LiDAR sensors. We demonstrate this by applying our method to unseen real-world voxel data, which yields reasonable qualitative results. Our experiments show that our model outperforms all baseline methods by a large margin, with exceptional performance in the prediction of small, under-represented class categories such as bicycles, pedestrians, traffic signs and more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>We review the related works across four major areas: volume reconstruction, point cloud segmentation, semantic scene completion, and multi-view fusion.</p><p>Volume Reconstruction. There are several approaches to inferring complete volumetric occupancy of shapes and scenes from partial or sparse geometric data. Efficient methods based on object symmetry <ref type="bibr">[7,</ref><ref type="bibr" target="#b7">8]</ref> and plane fitting <ref type="bibr" target="#b8">[9]</ref> apply for small non-complex completion tasks. In larger scenes with irregular objects, the former are supplanted by methods that fit 3D mesh models to object instances based on the local scene geometry <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12</ref>]. Yet, a lack of diversity within the 3D model library often leads to incomplete reconstruction, and expanding the library slows down retrieval. Attempts to simplify the process to 3D bounding box fitting neglects the local geometry of objects <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Other studies process grid-octree data with CNNs to predict high resolution outputs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, but are tailored to reconstructing individual objects rather than entire scenes.  Point Cloud Segmentation. A variety of methods segment range images constructed by spherical projection of a point cloud with deep networks, and backproject the predicted semantic classes onto the corresponding points in 3D space <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. The small sized range image tensor that these methods operate on leads to unparalleled speed, but ? 25% of the original point cloud is unrecoverable after the initial spherical projection. Alternative approaches project point clouds onto a bird's eye view perspective, computing pillar features (voxels) from 3D points to construct a BEV map and process it with deep CNNs <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Another emergent stream aims to segment point clouds by hierarchically extracting features from the 3D points directly to capture local and global context of the scan <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. While these methods have had reasonable success, they are typically an order of magnitude slower than their non-direct counterparts.</p><p>Semantic Scene Completion. Most mainstream methods are built on small indoor scenes with dense depth maps or dense point clouds, which effectively reduces the impact of sparseness on scene completion. The representative methods are SSCNet <ref type="bibr" target="#b4">[5]</ref> and ScanNet <ref type="bibr" target="#b3">[4]</ref>. Their approach transforms dense depth maps into a volumetric TSDF signal and passes it through a dense 3D network. Extensions are made in TS3D <ref type="bibr" target="#b23">[24]</ref> by incorporating semantic segmentation from RGB images into the construction of the input TSDF volume. In outdoor scenes, these methods are limited by the vast increase in sparsity and their network architectures that predict at reduced voxel resolutions. Improvements are made in SATNet <ref type="bibr" target="#b24">[25]</ref>, which maintains high output resolution with a series of dilated convolutional modules (TNet) to capture global context, and propose configurations for early and late fusion of semantic information. Behley et al. <ref type="bibr" target="#b0">[1]</ref> produced a state-of-the-art system by integrating semantic priors from color images and LiDAR segmentation with the TSDF volume, and adopting a TNet network backbone. However, their memory intensive design requires users to infer on 6 equal parts of the input before fusing the semantic completed partial scenes, deterring potential real-time usage. Furthermore, the dependence on color images introduces instability in the overall system under low-light and poor weather conditions. Therefore, the design of a semantic scene completion method based solely on 3D point cloud data is motivated by the need for faster inference speeds, a smaller memory footprint, and robustness to extreme conditions.</p><p>Multi-view Fusion. Fusing semantic and geometric features across various view-points and dimensions has been explored for a variety of tasks. Several of the aforementioned semantic scene completion methods demonstrate the lifting of image-space semantic labels into 3D space <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Similarly, Hane et al. <ref type="bibr" target="#b25">[26]</ref> proposed a joint optimization strategy for semantic image segmentation and scene reconstruction, using the completed 3D scene as a geometric prior on the corresponding image pixels to enforce spatially consistent segmentations. Extensions were made to support cityscale reconstruction at reasonable memory costs with an adaptive multi-resolution model <ref type="bibr" target="#b26">[27]</ref>. Dai and Nie?ner <ref type="bibr" target="#b27">[28]</ref> designed an 2D-3D network that fuses semantic features from RGB-D images with a differential backprojection layer, achieving multiple view-point reconstruction in indoor settings. Meanwhile, SLAM-based approaches <ref type="bibr" target="#b28">[29]</ref> aim to produce temporally consistent reconstructions by tracking motion states over sequential frames and mapping predicted semantics into 3D space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We describe our methods for LiDAR-based semantic scene completion in large outdoor driving scenes. After a brief system overview, we present our unique procedure for computing key spatial features from an input LiDAR scan, the detailed design of our networks, fusion module and refinement module, and a novel loss function incorporating a geometric-aware 3D segmentation loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Pipeline</head><p>The entire system pipeline is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. From a single LiDAR scan, we construct two sparse tensors that encapsulate the scene into memory efficient 2D and 3D representations. Each tensor is passed through their corresponding semantic scene completion network, 2D S3CNet or 3D S3CNet, to semantically complete the scene in the respective dimension. We propose a dynamic voxel fusion method to further densify the reconstructed scene with the predicted 2D semantic BEV map (detailed discussion in Section 3.3). This offsets the significant memory demands on the 3D network -exponential sparsity growth in 3D space makes it difficult to complete classes at range. Using a sparse tensor spatial propagation network <ref type="bibr" target="#b29">[30]</ref>, we refine the semantic labels in noisy regions of the fused 2D-3D predictions. Remission</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatial Feature Engineering</head><formula xml:id="formula_0">0 1 2 Sparse Tensor ?2 ?2 ?2 ?2 ?2 ?2 ?2 ?1 ?1 ?1 ?1 ?1 ?1 ?1 2 0 1 3</formula><p>Pointwise Normal Vector Sparse 2D Feature. We define the sparse 2D tensor as the set of non-empty pillars approximating the point distribution along the x-y plane (BEV). Each pillar, p i,j , is a 7-dimensional vector encoding the mean, minimum, and maximum heights and intensities of points within the voxel, and the point density; all values are normalized.</p><p>Sparse 3D Feature. The sparsity of the raw point cloud makes it difficult to extract spatial features, and thus, we transform it into a range image via spherical projection. As the quality of extracted spatial features are sensitive to noise, we retrieve a smooth range image by performing dynamic depth completion using the dilation method of Ku et al. <ref type="bibr" target="#b30">[31]</ref>. This enables robust extraction of a 3-dimensional normal surface feature for each pixel that is reversely assigned to the points in 3D space. Further, we maintain a memory efficient sparse 3D tensor by modifying the sign-flipped TSDF approach of Song et al. <ref type="bibr" target="#b4">[5]</ref>, and compute TSDF values from the smooth range image, storing only the coordinates within the truncation range of existing LiDAR points. All other features for TSDF generated coordinates are zero-padded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network Architecture</head><p>We adopt the Minkowski Engine <ref type="bibr">[6]</ref> as our sparse tensor auto-differentiation framework to build the entire system. A sparse tensor can be defined as a hash-table of coordinates and their corresponding features:</p><formula xml:id="formula_1">x = [C n?d , F n?m ].</formula><p>Here, n are the number of non-empty voxels, d and m are the dimension of coordinates and features, respectively. The sparse convolutional layer is thus:</p><formula xml:id="formula_2">x u = i?N D (uu,K,C in ) W i x u+i for u ? C out<label>(1)</label></formula><p>Where K is the kernel size and N D (u, K, C in ) are the set of offsets that are at most 1 2 (K ? 1) away from u, the current coordinate. Unlike the conventional convolution, this generalized convolution (introduced by Choy et al. <ref type="bibr">[6]</ref> et al) suits generic input and output coordinates, and arbitrary kernel shapes. It allows extending a sparse tensor network to extremely high-dimensional spaces and dynamically generating coordinates for generative tasks. The arbitrary input shape and multidimensional support enables us deploy the same network layout in different dimensional spaces. Hence, our 2D and 3D networks share the same set of network components (built off sparse convolution, transposed convolution and pooling layers) with differing coordinate dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>target_s2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coordinates Features</head><p>Input Sparse Tensor 0,0,? 1, 0,0,? 1,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encode_block_1 Encode_block_2 Encode_block_3</head><p>Coordinates Classes</p><p>Output Sparse Tensor 0,0,? 1, 0,0,? 1,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decode_block_1 Decode_block_2 Decode_block_3</head><p>ME_Conv <ref type="formula">(</ref>   As illustrated in <ref type="figure" target="#fig_5">Fig. 4</ref>, our 3D network is assembled from four primary building blocks: encode, decode, dilation, and spatial propagation. We implement a squeeze re-weight (SR) <ref type="bibr" target="#b17">[18]</ref> layer to model inter-channel dependencies and improve generalization. Each encoder block contains a context aggregation module (CAM) <ref type="bibr" target="#b16">[17]</ref> which captures context in a large receptive field, improving robustness to dropout noise. We adapt these modules for sparse tensor support, and observe improvements to both the segmentation and completion tasks. The decode blocks utilize sparse transposed convolutions capable of generating new coordinates with the outer product of the weight kernel and the input coordinates. As new coordinates in 3D are generated by the cube of the kernel size, we preserve memory with pruning modules that remove redundant coordinates throughout scene completion -training supervision is provided by ground truth filter masks ( <ref type="figure" target="#fig_1">Fig. 4, target s2-16</ref>). The dilation block features a sparse atrous spatial pyramid pooling (ASPP) module to trade-off accurate localization (small fieldof-view) with context assimilation (large field-of-view), our dilation rates are <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. The spatial propagation block contains two parts, 3D spatial propagation module and guidance convolution network. The guidance network produces affinity matrices used to guide the spatial propagation network to deform the sparse tensor into a desired 3D shape.</p><p>Loss function. When training our 2D network, we accomplish healthy BEV completion and balanced learning of under-represented class categories by combining pixel-wise focal loss, weighted cross entropy loss. Per-voxel binary cross entropy loss is used to train the pruning modules.</p><formula xml:id="formula_3">L 2D (p, y) = ?(? C c?C w c y c log(p c ) + ? C c?C y c (1 ? p c ) ? log(p c )) + ?L BCE (p, y)<label>(2)</label></formula><p>Here, p is the predicted label and y is the ground truth label. The weighting factors ?, ?, and ? are empirically set to 0.5, 0.5, and 1, respectively. We define a novel loss function to train our 3D network. It consists of a completion term (voxelized binary cross entropy) and a geometric-aware 3D tensor segmentation loss. The two terms are balanced by an ? constant empirically set to 0.35.</p><formula xml:id="formula_4">L 3D (p, y) = ?L completion (p, y) + (1 ? ?)L GA (p, y)<label>(3)</label></formula><p>The completion loss is expressed below, where L BCE is the binary cross entropy loss over volumetric occupancy. Note that this is applied to train the pruning modules at various scale spaces.</p><formula xml:id="formula_5">L completion (p, y) = i,j,k L BCE (p ijk , y ijk )<label>(4)</label></formula><p>Below is the geometric-aware 3D tensor segmentation loss computed over the final output tensor.</p><formula xml:id="formula_6">L GA (p, y) = ? 1 N i,j,k C c=1 (? + ?M LGA )y ijk,c log(p ijk,c )<label>(5)</label></formula><p>2D Prediction 3D Prediction <ref type="figure">Figure 6</ref>: Birds eye view 2D results compared with 3D predictions.</p><p>The terms M LGA , ? and ? describe signals computed from the same local cubic neighborhood of a given coordinate (As shown in <ref type="figure" target="#fig_6">Fig. 5</ref>).</p><p>The Local Geometric Anisotropy defined by Li et al. <ref type="bibr" target="#b31">[32]</ref>,</p><formula xml:id="formula_7">M LGA = K i=1 (c p ? c qi )</formula><p>, is a discrete smoothness promoting signal that penalizes the prediction of many classes within a local neighborhood. Although it ensures locally consistent predictions in homogeneous regions (i.e. road center, middle of a wall), it may incorrectly divert the model from inferring boundaries between separate objects. We thus introduce ?, which accounts for the local arrangement of classes based on the volumetric gradient, and downscales M LGA when the neighborhood contains structured predictions of different classes. To smoothen-out the loss manifold, we include a continuous entropy signal ? = ? c?C P (c)log(P (c)), where P (c) is the distribution of class c amongst all classes C in the local neighborhood.</p><p>Eq. (5) thus decomposes into two multiplicative factors with the crossentropy loss which explicitly models the relationship between a predicted voxel and it's local neighborhood. Intuitively, the smooth local entropy term, ?, down-scales the loss in easily classified homogeneous regions, enabling the network to attend to non-uniform regions (e.g. class boundaries) as learning progresses. However, a measure of non-uniformity alone is insufficient in that non-uniform regions should be more heavily penalized if the predicted neighborhood lacks structure. This motivates the inclusion of ?M LGA which also considers spacial arrangement of classes and down-scales the loss in structured cubic regions. In combination, we acquire a smooth loss manifold when the local prediction is close to the ground truth as well as uncluttered, with sharp increases when the local cubic neighbourhood is noisy and far off from the ground truth. This speeds up convergence while reducing the chance of stabilizing in local optimums.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-view Fusion and Spatial Propagation.</head><p>The task of 2D semantic scene completion is far less complex than it's 3D counterpart because it does not account for the local structure of objects along the z dimension, relegating the task to semantic filling in the bird's eye view plane. This allows our sparse 2D network to more easily understand the global distribution pattern of semantic classes even at far distances, yielding accurate predictions of roads, terrain, and side walks. When confronted with heavy noise or occlusions, such as <ref type="figure">Fig. 6</ref>, our 3D model may lack the confidence to complete certain regions. To remedy this, we express the predicted 3D tensor as a stack of BEV images, and attempt to fill in each empty voxel with the corresponding prediction in the 2D network. The lifting algorithm operates as follows: (1) split the 3D volume into layers along the z-dimension (32 layers in this application); (2) locate the slice with the maximum occurrence of the desired class; (3) fill each voxel with the associated 2D pixel prediction if there exists such a class in the voxel's n ? n (n = 3) neighborhood, otherwise attempt to fill in the above voxel until the highest layer is exceeded; (4) repeat for all classes. Hence, by lifting the 2D voxel-wise semantic completion into 3D space, we improve both completion and segmentation metrics of the 3D task at minimal cost. To mitigate any additional post-fusion noise, we apply a spatial propagation network that refines the segmentation results in 3D space.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our system on the SemanticKITTI dataset <ref type="bibr" target="#b0">[1]</ref>, which contains 8,728 voxelized LiDAR scans and the corresponding ground truth labels. We use the public train/val/test split defined in the SemanticKITTI API, and provide qualitative visualizations of our predictions. For evaluation metrics, we follow <ref type="bibr" target="#b4">[5]</ref> using mean IoU over positive classes and binary per-voxel completion IoU. Experiments are deployed on Nvidia GP100 GPUs (16GB Graphic RAM); we use two GPUs per model and train for 50 epochs each. The average inference time for 3D S3CNet is 0.5s (frame with 40,000?500 points), and training requires around 120 hours. For 2D S3CNet, the average inference time is 0.05s resulting in 50 training hours. The training scheme for the best performing 2D and 3D models are: (3D) Adam optimizer, 0.0025 learning rate, 0.0005 weight decay, and (2D) SGD optimizer, 0.001 learning rate, and 0.0005 weight decay. Both experiments also incorporate an exponential learning rate scheduler with a decay rate of 0.9 every 10 epochs. Qualitative Results. <ref type="figure">Fig. 9</ref> depicts the inferences of our system on three SemanticKITTI test set samples and the corresponding RGB images. We notice that our method correctly captures most classes with exceptional detail and consistency, including small object categories like people and traffic signs in challenging scenes. The middle column highlights an open intersection scene with heavy front-facing occlusion, complicating the task of inferring distant mid-intersection vehicles. For 2D S3CNet, we demonstrate the prediction and ground truth BEV map in static scenes and dynamic scenes with moving vehicles (as shown in <ref type="figure" target="#fig_8">Fig. 8</ref>).   <ref type="bibr" target="#b19">[20]</ref> 0.1486 0.2777 PointSeg <ref type="bibr" target="#b17">[18]</ref> 0.1204 0.2317 SSv2 (w/o CRF) <ref type="bibr" target="#b16">[17]</ref> 0.1294 0.1718 DBLiDARNet <ref type="bibr" target="#b18">[19]</ref> 0.1682 0.2382 SalsaNext <ref type="bibr" target="#b32">[33]</ref> 0.1780 0.2715 Ours 0.27003 0.3032 Quantitative results. Our primary results are shown in <ref type="table">Table.</ref> 1, where we compare our overall system to several state-of-theart methods in the SemanticKITTI test set benchmark. Note the approaches without citations are non-published works. At the time of writing our proposed S3CNet considerably outperforms all others, achieving a mean IoU score of 29.5% (a +23.9% improvement on the previous leading method) and a +66.6% over the baseline method <ref type="bibr" target="#b0">[1]</ref>. For 2D S3CNet, we conduct extra experiments on the nuScenes dataset <ref type="bibr" target="#b33">[34]</ref>. Ground truth 2D semantic scenes are created from object bounding boxes and cropped HD map data, and aligned with LiDAR frames at an identical spatial extent and voxel resolution to SemanticKITTI. We produced 1,166,187 frames of valid LiDAR scan and semantic scene label pairs and we split the train, validation, test into a 14:3:3 ratio. The total training time of our 2D S3CNet for 50 epochs on the nuScene dataset is ?350 hours. Ablation study. We investigate the individual contribution of all components in our system. As shown in <ref type="table">Table.</ref> 3, we conduct various experiments on the 2D and 3D SSC task by modifying or removing core components of the system and track the resulting effect on mean IoU and completion IoU scores on the SemanticKITTI validation set. For both the 2D and 3D networks, we observe a mean IoU drop after removing the CAM and SR modules. Since the decode blocks are mainly responsible for completion, removing SR modules results in a more severe drop in completion IoU compared to CAM which are only present in the encoder. Eliminating spatial features decreases overall performance by a large amount; the system losses geometric priors that guide completion and spatial priors that distinguish free from occluded space. Adopting Lovasz-softmax achieves the highest mean IoU increase, since it directly optimizes for the mean IoU metric (Jaccard index).</p><p>In our experiments, focal loss combined with binary cross entropy loss provides no performative advantage over the baseline weighted cross entropy loss for both the 2D and 3D networks.  <ref type="table">Table 3</ref>: Ablation study on 2D and 3D S3CNet models and core system components.</p><p>Data augmentation. To increase model robustness, we integrate a series of data augmentation techniques into the training of our 2D and 3D networks. For both 2D and 3D tasks we apply uniform random cropping and dropout to the LiDAR point cloud, as well as uniform random translation of ?0.1m in all three dimensions. On the 2D datasets, random rotations of ?45 ? are applied only on the yaw angle, however, we apply random rotation of ?10 ? on any two of the three Euler angles (roll, pitch, yaw) at a time when training the 3D model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented a Sparse Semantic Scene Completion Network, S3CNet, capable of efficiently reconstructing large outdoor scenes and predicting semantic voxel-wise labels from a single LiDAR scan. To complement S3CNet, we designed a novel geometric-aware sparse tensor segmentation loss that promotes class-consistent predictions in homogeneous regions while encouraging locally structured multi-class segmentations along object boundaries. In combination with Multi-View Fusion and Spatial Propagation post-processing modules, our method achieves state-of-the-art results on the SemanticKITTI test set benchmark by a large margin. We also adapt several leading LiDAR segmentation networks as baselines for 2D semantic scene completion, and demonstrate that the 2D variant of S3CNet outperforms these baselines on two large-scale datasets. Furthermore, we include a detailed ablation study highlighting the contribution of each individual component to the overall system. Future work holds the extension of our sparse tensor method to real-time speeds with improved performance across the board, additional investigation on useful spatial feature encodings, and a learning-based multi-view fusion technique to enable end-to-end learning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function Optimizer Scheduler Rates</head><p>HD-UNet <ref type="bibr" target="#b19">[20]</ref> Weighted CE SGD Step lr = 0.01-0.02 dr = 0.1 m=0.9 wd=0</p><p>PointSeg <ref type="bibr" target="#b17">[18]</ref> Weighted CE Adagrad</p><p>Step lr = 0.005 dr = 0.1 m=0.9 wd=0.0005</p><p>SSv2 <ref type="bibr" target="#b16">[17]</ref> Focal Loss SGD Step lr = 0.01 dr = 0.1 m=0.9 wd=0.0001</p><p>DBLiDARNet <ref type="bibr" target="#b18">[19]</ref> Weighted CE Adam None lr = 0.0001 wd=0.0005</p><p>SalsaNext <ref type="bibr" target="#b32">[33]</ref> Weighted CE + Lovasz Softmax SGD Exp lr=0.05 dr=0.01 m=0.9 wd=0.0001</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Focal Loss + Weighted CE + BCE SGD Exp lr=0.001 dr=0.01 m=0.9 wd=0.0005 lr: learning rate dr: decay rate m: momentum wd: weight decay AICNet <ref type="bibr" target="#b34">[35]</ref> Weighted CE SGD Step lr = 0.015 0.215 dr = 0.1 m=0.9 wd=0.0005</p><p>DDRNet <ref type="bibr" target="#b35">[36]</ref> Weighted CE SGD Exp lr = 0.02 0.193 dr = 0.1 m=0.9 wd=0.0001</p><p>PALNet <ref type="bibr" target="#b31">[32]</ref> Position Importance Aware Loss SGD Exp lr = 0.02 0.255 dr = 0.05 m=0.9 wd=0.0001</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Geo-Aware Loss + Binary Cross Entropy SGD Exp lr = 0.025 0.303 dr = 0.1 m=0.9 wd=0.0001 lr: learning rate dr: decay rate m: momentum wd: weight decay </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Semantic Scene Completion on SemanticKITTI Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Full system pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Sparse 3D tensor features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>S3CNet network architecture. Example target masks are visualized as target s2-16.After extracting the features as described in Section 3.2, we create the sparse 3D tensor by voxelizing the point cloud and extracting the coordinates and features of non-empty or TSDF generated points. If multiple points occupy the same voxel, their features are averaged. The voxel resolution is 0.2m in each dimension; this applies to the sparse 2D tensor as well. The spatial extent of our predictions in the x-y-z directions are [0m, 51.2m], [-25.6m,<ref type="bibr" target="#b24">25</ref>.6m],[- 2m, 4.4m], respectively. Discretizing the volume yields a [256, 256, 32] sized tensor, upon which our sparse 3D network will predict the set of occupied coordinates and a probability distribution over the 20 possible semantic categories. Local geometric anisotropy of a sparse tensor voxel cp in the predicted 3D semantic scene completion label. Gradient of tensor based on anistropy operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Diagrams that illustrate how to calculate the local geometric anisotropy and gradient for a given prediction label voxel grid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results of predictions on SemanticKITTI test sequences. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>2D qualitative results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>45 (1) 0.16 (1) 0.39 (5) 0.34 (1) 0.21 (6) 0.45 (1) 0.35 (1) 0.16 (1) 0.31 (1) 0.31 (1) 0.24 (1) JS3C-Net 0.238 (2) 0.56 (1) 0.64 (1) 0.39 (1) 0.34 (1) 0.14 (1) 0.394 (2) 0.33 (1) 0.072 (1) 0.14 (2) 0.08 (2) 0.12 (2) 0.43 (1) 0.19 (5) 0.40 (1) 0.08 (2) 0.05 (2) 0.00 (2) 0.30 (2) 0.18 (304 (3) 0.09 (3) 0.35 (6) 0.20 (4) 0.28 (5) 0.02 (4) 0.07 (3) 0.00 (4) 0.23 (6) 0.16 (5) 0.16 (2) Source: https://competitions.codalab.org/competitions/22037</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Qualitative results of S3CNet in SemanticKITTI dataset. 2D (BEV map) Semantic scene completion qualitative results in NuScenes dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>SemanticKITTI Test Set Benchmark. ( * ) is the prediction IoU rank.</figDesc><table><row><cell>Model</cell><cell>SK</cell><cell>mIoU</cell><cell>NS</cell></row><row><cell>HDUNet</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: 2D S3CNet quantitative results and com-</cell></row><row><cell>parison to state-of-the-art on SemanticKITTI vali-</cell></row><row><cell>dation set (SK) and nuScenes test set (NS). [34])</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table.2 shows that our 2D S3CNet outmatches several well-known LiDAR segmentation baselines (adapted for BEV predictions) on the segmentation component of the SSC task, with comparable results on the completion component for both the SemanticKITTI and nuScenes datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Postprocessing modules like Multi-view Fusion and Spatial Propagation Network demonstrate very high contribution to the final results -without MVF the system performance degrades to that of the focal loss baseline. A key distinction between MVF and SPN is the comparative impact of MVF on completion IoU to the SPN on segmentation mean IoU, respectively.</figDesc><table><row><cell>Dimension</cell><cell>Model</cell><cell cols="7">Features Normal TSDF LGA (Ours) WeightedCE focal Lovasz BinaryCE Losses</cell><cell cols="5">MVF SPN Data Aug. mIoU (val) Completion IoU (val)</cell></row><row><cell>3D 2D</cell><cell>Full Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>0.3308 0.2789</cell><cell>0.5712 0.7032</cell></row><row><cell>3D 2D</cell><cell>w/o CAM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>0.2974 0.2803</cell><cell>0.5833 0.6880</cell></row><row><cell>3D 2D</cell><cell>w/o SR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>0.3005 0.2604</cell><cell>0.5455 0.6097</cell></row><row><cell>3D 2D</cell><cell>w/o Feature</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>0.2795 0.2518</cell><cell>0.4345 0.6597</cell></row><row><cell>3D 2D</cell><cell>Lovasz-softmax</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>0.3012 0.2633</cell><cell>0.5934 0.6089</cell></row><row><cell>3D 2D</cell><cell>Focal Loss (baseline)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>0.2853 0.2403</cell><cell>0.5319 0.7136</cell></row><row><cell>3D 2D</cell><cell>w/o MVF</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.2780 -</cell><cell>0.4430 -</cell></row><row><cell>3D 2D</cell><cell>w/o SPN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>0.2493 0.2303</cell><cell>0.4902 0.6736</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Training configurations for 2D Semantic Scene Completion models on SemanticKITTI and NuScenes datasets.</figDesc><table><row><cell>Model</cell><cell>Loss Function</cell><cell cols="2">Optimizer Scheduler</cell><cell>Rates</cell><cell>Best mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>lr = 0.01</cell><cell></cell></row><row><cell>SSCNet [5]</cell><cell>Weighted CE</cell><cell>SGD</cell><cell>Step</cell><cell>dr = 0.1 m=0.9</cell><cell>0.182</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>wd=0.0005</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Training configurations for 3D Semantic Scene Completion models in SemanticKITTI dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Professor John K. Tsotsos for reviewing this work, as well as the anonymous reviewers for their valuable suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9297" to="9307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structured prediction of unobserved voxels from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5431" to="5440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scannet: Richlyannotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1746" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Acquiring 3d indoor environments with variability and repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<idno>138:1-138:11</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Heuristic 3d object shape completion based on symmetry and scene context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schiebener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vahrenkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rapter: rebuilding man-made scenes with regular arrangements of planes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Monszpart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mellado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="103" to="104" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Aligning 3d models to rgb-d images of cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4731" to="4740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint 3d object and layout inference from a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="183" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Database-assisted object retrieval for real-time 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="435" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A linear approach to matching cuboids in rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2171" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="808" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Octnetfusion: Learning depth fusion from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2088" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pointseg: Real-time semantic segmentation based on 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06288</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deeptemporalseg: Temporally consistent semantic segmentation of 3d lidar scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06962</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hdnet: Exploiting hd maps for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Salsanet: Fast road and vehicle segmentation in lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cavdar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08291</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11108" to="11117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two stream 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sawatzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">See and think: Disentangling semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="263" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint 3d scene reconstruction and class segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Angst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large-scale semantic 3d reconstruction: an adaptive multi-resolution model for multi-class volumetric labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3176" to="3184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint 3d-multi-view prediction for 3d semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="452" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">When 2.5 d is not enough: Simultaneous reconstruction, segmentation and recognition on dense slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2295" to="2302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning affinity via spatial propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">In defense of classical image processing: Fast depth completion on the cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 15th Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="16" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Depth based semantic scene completion with position importance aware loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="219" to="226" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Salsanext: Fast semantic segmentation of lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cortinhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzelepis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Aksoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03653</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11621" to="11631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Anisotropic convolutional networks for 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3351" to="3359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rgbd based dimensional decomposition residual network for 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7693" to="7702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">our model well captures both completion and semantic segmentation characteristics of different scenes. Because a single ground truth label was constructed from LiDAR scans across several time steps (in order to densify the scene), dynamic objects were filtered out to avoid labelling noise. For instance, in the bottom-right most sample, while the bus was filtered out of the ground-truth scene, our well engineered features enabled our model to detect the object with the correct class label. Another example of this is visible in the top-right most sample, where our model detects a moving bicyclist in front of a vehicle. These object labels are learned from static scenes</title>
	</analytic>
	<monogr>
		<title level="m">Appendix A: Extra Qualitative Results We present additional qualitative results on the SemanticKITTI training set</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>As we can see in Fig. but are successfully inferred in dynamic scenes which reflects positively on our model&apos;s generalization ability</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">NuScenes 2D qualitative results: we project the 2D semantic BEV map back to 3D lidar points according the respective x and y coordinates, and overlay the semantic point cloud data on the rgb images to show the model&apos;s qualitative results. As we can see from Fig. 10, our predictions cover most of the road surface and precisely detects vehicles</title>
		<imprint/>
	</monogr>
	<note>pole-like objects and pedestrians</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Experiment Configurations We provide the training scheme for all 2D experiments in Table. 4, which includes the adapted LiDAR segmentation baselines that have undergone multiple optimization iterations to achieve the stated results on the SemanticKITTI and NuScenes datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Appendix</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Note that all experiments use identical input data and augmentation configurations; changes only occur to the model, the loss function, optimizer, scheduler, and supporting hyperparameters -much of. which are based on the training scheme proposed by the original works</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">We also list our experiment configurations including the loss function, hyperparameters, scheduler, and provide the best validation mean IoU for all 3D model experiments (see Table. 5). We implemented a list of state-of-the-art methods</title>
		<imprint/>
	</monogr>
	<note>not currently on the SemanticKITTI benchmark</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Amongst the competitors, our model achieved the best mean IoU on the validation split</title>
		<imprint/>
	</monogr>
	<note>and trained them on the SemanticKITTI dataset for 50 epochs. sequence 08</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
