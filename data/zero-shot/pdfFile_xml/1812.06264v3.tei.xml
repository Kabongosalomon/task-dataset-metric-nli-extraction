<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Discrete Distribution Decomposition for Match Density Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename><surname>Fisher</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Discrete Distribution Decomposition for Match Density Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Explicit representations of the global match distributions of pixel-wise correspondences between pairs of images are desirable for uncertainty estimation and downstream applications. However, the computation of the match density for each pixel may be prohibitively expensive due to the large number of candidates. In this paper, we propose Hierarchical Discrete Distribution Decomposition (HD 3 ), a framework suitable for learning probabilistic pixel correspondences in both optical flow and stereo matching. We decompose the full match density into multiple scales hierarchically, and estimate the local matching distributions at each scale conditioned on the matching and warping at coarser scales. The local distributions can then be composed together to form the global match density. Despite its simplicity, our probabilistic method achieves state-ofthe-art results for both optical flow and stereo matching on established benchmarks. We also find the estimated uncertainty is a good indication of the reliability of the predicted correspondences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Finding dense pixel correspondences between two images, typically for stereo matching or optical flow, is one of the earliest problems studied in the computer vision literature. Dense correspondences have wide application including for activity recognition <ref type="bibr" target="#b35">[36]</ref>, video interpolation <ref type="bibr" target="#b22">[23]</ref>, scene geometry perception <ref type="bibr" target="#b12">[13]</ref>, and many others. Challenges when solving this problem include texture ambiguity, complex object motion, illumination change, and entangled occlusion estimation.</p><p>Classic approaches jointly optimize local texture matching and neighbor affinity on images <ref type="bibr" target="#b16">[17]</ref> possibly in a coarse-to-fine fashion <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42]</ref>. While these methods can achieve impressive correspondence accuracy, the optimization step may be too slow for downstream applications. Recent works using deep convolutional networks (ConvNets) have achieved similar or even better matching results without an optimization step <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40]</ref>. Pixel features learned directly from correspondence supervision can capture both local appearance and global context information due to the large network receptive fields. With GPU acceleration, it is possible to use these networks to regress the pixel displacements in real time <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>However, the estimation uncertainty inherent in correspondence estimation is neglected by displacement regression approaches. Though post-hoc confidence measures <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref> can recover the uncertainty to some degree, they are independent of model training; uncertainty is ignored in the training process. Recognizing the missing uncertainty measures in optical flow methods, some works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43]</ref> propose probabilistic frameworks for joint correspondence and uncertainty estimation. Due to constraints on computation and parameter number, they rely on the local Gaussian noise assumption to represent the match distribution. Consequently, they cannot model complicated distributions on a large image area. Early works in stereo matching show that we can build a complete match cost volume as a proxy to estimate the match density, but were not applicable for high-resolution stereo matching nor general optical flow due to the excessive amount of computation needed for the complete cost volume.</p><p>In this work, we propose Hierarchical Discrete Distribution Decomposition (HD 3 ), a general probabilistic framework for match density estimation. We aim to find the discrete distribution of possible correspondences with a large support defined on the image grids for each pixel. We adopt a general model to represent pixel-level match probability without any parametric distribution assumption. The model-inherent uncertainty measures can be naturally derived from our estimated match densities. HD 3 decomposes the full match density into multiple levels of local distributions similar to quadtrees. To extract discriminative features for matching, we use networks with Deep Layer Aggregation (DLA) <ref type="bibr" target="#b49">[50]</ref> to build the multiscale feature pyramid. The DLA framework provides us with feature networks of different computation-accuracy trade-offs, which can be easily integrated with other recognition tasks in complex applications. We estimate the match density of the residual motion in each scale, conditioned on match densities at coarser scales. We can propagate the conditional information from previous levels to the prediction at the current level through iterative feature warping and Reference Frame Target Frame <ref type="figure">Figure 1</ref>: Illustration of HD 3 . We aim to estimate discrete match distribution in this work. For reducing the infeasible computation cost, the overall distribution is decomposed into multiple scales hierarchically at learning time. The full match information can be recovered by composing predictions from all levels. Please refer to Sec. 3.2 for more details. density bypass connections. The multi-scale match densities can then be used to recover the complete match density. We can easily convert between point estimates and match densities to train our models on existing datasets with annotations in the form of motion vectors.</p><p>We evaluate our framework extensively in two applications: stereo matching and optical flow. Our method achieves state-of-the-art results on both the synthetic dataset MPI Sintel <ref type="bibr" target="#b6">[7]</ref> and the real dataset KITTI <ref type="bibr" target="#b12">[13]</ref>. Our method not only surpasses all two-frame based optical flow methods by large margins but also beats some competitive scene flow methods on both KITTI 2012 &amp; 2015. We also evaluate our uncertainty estimation and demonstrate the error-awareness of our method in its predictions. Our code is available at https://github.com/ucbdrive/hd3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Great efforts have been devoted to the problems of finding dense correspondences in the past four decades. For a thorough review, we refer to popular benchmarks including Middlebury <ref type="bibr" target="#b3">[4]</ref>, MPI Sintel <ref type="bibr" target="#b6">[7]</ref>, and KITTI <ref type="bibr" target="#b31">[32]</ref> benchmarks for both the classical methods and the latest advances in these areas. We will discuss the most related ideas in this section.</p><p>Correspondence Estimation. Classical stereo matching usually involves local correspondence extraction and semiglobal regularization <ref type="bibr" target="#b15">[16]</ref>. On the other hand, optical flow methods typically adopt MRFs <ref type="bibr" target="#b27">[28]</ref> to jointly reason about the displacements, occlusions, and symmetries <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b45">46]</ref> for tackling the more unconstrained and challenging 2D correspondence problem. Despite the distinct differences between the search space dimensions, stereo matching and optical flow share similar assumptions such as brightness constancy and edge-preserving continuity <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>With the success of deep learning, end-to-end models have been designed for these dense prediction tasks.</p><p>Benefiting from pretraining on a large corpus of synthetic data <ref type="bibr" target="#b29">[30]</ref>, these methods achieve impressive results on par with classical methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>. Furthermore, recent advances emphasize the incorporation of classical principles into network designs, such as pyramid matching, feature warping, and contextual regularizer <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b39">40]</ref>. These improvements contribute to the superior performance of deep learning models, allowing them to surpass classical methods. However, such learning methods neglect the modelinherent uncertainty estimation, i.e. they are agnostic of the prediction failure, which is quite important for applications such as autonomous driving and medical imaging. In contrast, our work focuses on the probabilistic correspondence estimation, which can naturally convey the confidence of the predictions.</p><p>Uncertainty Measures. Various uncertainty measures have been proposed for classical optical flow estimation. Barron et al. <ref type="bibr" target="#b4">[5]</ref> proposed a simple method based on the input data characteristics while ignoring the estimated optical flow itself. Kondermann et al. <ref type="bibr" target="#b26">[27]</ref> learned a probabilistic flow model and obtained uncertainty estimation through hypothesis testing. Mac Aodha et al. <ref type="bibr" target="#b28">[29]</ref> trained a classifier to assess the prediction quality in terms of end-point-error. These methods either leverage only part of the input information, such as images or predicted flow, for uncertainty estimation, or require post-processing steps independent of model inference itself.</p><p>Recently, Gast et al. <ref type="bibr" target="#b11">[12]</ref> recognized the importance of model-inherent uncertainty measures for deep networks. They proposed probabilistic output layers and employed assumed density filtering to propagate activation uncertainties through the network. For computational tractability, they assumed Gaussian noise and adopted a parametric distribution. Though it can be easily adapted for use with existing regression networks, their performance is only competitive with the deterministic counterparts. Our method provides inherent uncertainty estimation as well as new state-of-the- art results for both optical flow and stereo matching.</p><p>Coarse-to-Fine. Because of the complexity of finding 2D correspondences for each pixel in optical flow, it is natural to match the pixels from coarse to fine resolutions in an image or feature pyramid. This method can be used effectively in the optimization methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b37">38]</ref> as well as patch matching <ref type="bibr" target="#b17">[18]</ref>. Its effectiveness is also verified by recent deep learning approaches such as SpyNet <ref type="bibr" target="#b32">[33]</ref>, PWC-Net <ref type="bibr" target="#b39">[40]</ref>, and LiteFlowNet <ref type="bibr" target="#b18">[19]</ref>. We also estimate our hierarchically decomposed match densities based on the feature pyramid representation <ref type="bibr" target="#b49">[50]</ref>. Our contribution lies in the decomposition of the discrete probability distribution instead of the feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we discuss our probabilistic framework for match density estimation. Without loss of generality, we focus on solving 2D correspondences for optical flow in this section, which can be easily adapted to the 1D case for stereo matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary</head><p>We first introduce the notations and basic concepts used. Given a pair of images I = {I 1 , I 2 }, we denote the motion field as f = {f ij } where f ij = (u ij , v ij ) T for pixels (i, j), i = 1, . . . , n, j = 1, . . . , m. In contrast to Wannenwetsch et al. <ref type="bibr" target="#b42">[43]</ref> where {f ij } are continuous, we treat them as discrete random variables. We call their density functions match densities. We use p(f |I) to denote the joint probability distribution of {f ij }. For brevity, we omit the conditional I in the following discussion when there is no ambiguity. Finally, we introduce a ?2 upsampling operator ? and an opposite downsampling operator ? ?1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Match Density Decomposition</head><p>The main challenge of estimating the full match density is the prohibitive computational cost. Assume we have an image with size 1000 ? 1000 and displacement range [?50 <ref type="bibr">, 50]</ref>. The cardinality of {f ij } would be 10 6 and the support size of each f ij could be 10 4 . In this case, the entire distribution volume would have 10 billion cells, which is intractable to generate.</p><p>Our key observation is that the full match density can be decomposed hierarchically into multiple levels of distributions. <ref type="figure">Fig. 1</ref> provides an intuitive illustration. Let us consider multi-scale motion fields {f l } (l = 0, . . . , L), where higher level f l has half of the resolution of the lower level f l+1 and f L is identical as f . We introduce a transformation g l = f l ? ?(f l?1 ) to shift the absolute multi-scale motion fields to residual ones. We can recover the original motion field f from {g l } (l = 0, . . . , L) via</p><formula xml:id="formula_0">f = L l=0 ? L?l (g l ).</formula><p>(1) Naturally, we have the decomposition of p(f ) as</p><formula xml:id="formula_1">p(f ) = {g l }?F L l=0 p(g l |G l?1 ),<label>(2)</label></formula><p>where G l = {g s } l s=0 , and F is the set of all possible {g l } that satisfies Eq. 1. Therefore, we can in turn estimate the decomposed match densities p(g l |G l?1 ) and recover full match density p(f ) through Eq. 2 afterward. The benefit of adopting such decomposition lies in that match density p(g l |G l?1 ) actually has quite low variance, i.e. probabilities concentrate on a small subset R g l of the entire support R g l . Without loss of much information, we can focus on solving p(g l |G l?1 ) with g s ? R g s for s = 0, . . . , l ? 1.</p><p>Consequently, for maximizing the posterior distribution p(f ), we achieve satisfactory approximation through maximizing each of the decomposed match densities. We will discuss our selection of support subsets in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning Decomposed Match Density</head><p>Our objective becomes estimating multi-scale decomposed match densities p(g l |G l?1 ). We propose to learn such information through multiple levels of ConvNets. At each level, a ConvNet is designed to estimate the decomposed match density. Note that g l is conditioned on G l?1 , while theoretically we can sample g s ? G l?1 according to predicted densities at coarser levels.</p><p>In this section, we first discuss how to transform point estimate into match density, which is adopted for generating our distribution supervision for each level. Let us consider a general motion vector f ij ? f and its density function p(f ij ). As stated in Sec. 3.2, we prefer p(f ij ) to possess a low variance, which would greatly reduce the computation cost through our decomposition. We observe that realvalued f ij uniquely falls into a 2 ? 2 window W ij in the image grid. This inspires us to splat the bilinear weights of f ij w.r.t. coordinates in W ij to p(f ij ). Concretely, for any d ? Z 2 , we have</p><formula xml:id="formula_2">P(f ij = d) = 0 d / ? W ij ?(f ij ?d) d ? W ij ,<label>(3)</label></formula><p>where ?(?) means the product of elements in the vector, and d is the diagonal opposite coordinate of d in W ij . We call such conversion as V2D (see <ref type="figure" target="#fig_1">Fig. 3</ref>), which depicts our assumption for the ground-truth match density. As seen from Eq. 3, the support of p(f ij ) is indeed W ij which has a maximum size of 4. Ideally, we can sample g s ? G l?1 in a quadtree fashion during estimating the match density of g l . However, such computation is still heavy for both training and evaluation. For trade-off, we can discard samplings with minor probabilities. A trivial practice is always taking arg max at each level. As a substitution, we propose local expectation to further reduce the loss of information. Specifically, for any general match density p(f ij ), we define W * ij as the 2 ? 2 window over which the integral of p(f ij ) maximizes among all candidate windows. We only retain the probabilities of p(f ij ) in W * ij and normalize it into p * (f ij ). The local expectation is defined as</p><formula xml:id="formula_3">E[f ij ] w.r.t. p * (f ij ).</formula><p>In the following, we use expectation to denote local expectation by default. We call this conversion D2V (see <ref type="figure" target="#fig_1">Fig. 3</ref>). Therefore, at each level, instead of exhaustive sampling we always take the max posterior of g l as E[g l ], and we only estimate p(g l |G (l?1) * ) (p l res for short in the following) in each level, where G (l?1) * = {E[g s ]} l?1 s=0 . This enables us to get rid of expensive training and test time sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network Architecture Design</head><p>Finally, we discuss the network architecture design for estimating the decomposed match densities. We achieve this objective via stacking multiple levels of ConvNets, which we call density decoders {D l }. D l infers the match density p l res in its respective level l. A single level of the entire network is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. In the following, we discuss the details of our subnetworks.</p><p>The architecture design of our density decoder D l is motivated by the close relationship between the targeted output p l res and the similarity information between image pairs, or their embedded representations. Our match density estimation operates on multi-scale feature embeddings {F l 1 , F l 2 }, which are extracted via a DLA <ref type="bibr" target="#b49">[50]</ref> network over {I 1 , I 2 }. Affinity information can be obtained through the correlation <ref type="bibr" target="#b9">[10]</ref> of feature embeddings between different frames. For performing long-range correlation and imposing conditional priors from previous levels, we always warp the feature F l 2 according to ?(E[f l?1 ]) before correlation. The cost volume is concatenated with F l 1 , ?(E[f l?1 ]), and the upsampled density embedding E l?1 up from the previous level, then fed into our density decoder D l . The decoder D l produces the density embedding, from which we obtain the match density via a classifier. Also, we upsample the den-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>HD 3 provides hierarchically decomposed match densities. It can be used for different tasks, such as stereo matching and optical flow. The probability of point estimates can be used as uncertainty estimation. It is hard to evaluate the quality of the learned distribution directly, but we can investigate its performance being applied to these specific tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Network Variants. We can apply our models to stereo matching and optical flow. The networks are called HD 3 S and HD 3 F. The two variants differ slightly: we adopt 1D correlation for HD 3 S and 2D correlation for HD 3 F. The correlation range is always 4 for both tasks at different levels, which is consistent with the size of match density support. Since we treat stereo matching as 1D flow estimation, we clip the positive values in converted point estimates at each level for HD 3 S. The pyramid level is set to 5 for HD 3 F and 6 for HD 3 S based on experiment results.</p><p>Module Details. We select DLA-34-Up <ref type="bibr" target="#b49">[50]</ref> as our pyramid feature extractor, because it can achieve competitive se-mantic segmentation accuracy on small datasets with much less computation than the deeper alternatives. The features at the coarsest level are ?64 downsampled. The density decoder D l consists of two residual blocks plus one aggregation node <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b49">50]</ref>, except for the last level when it is fulfilled via a dilated convolutional network <ref type="bibr" target="#b48">[49]</ref> as a context module. We adopt batch normalization <ref type="bibr">[22]</ref> in all of our models to stabilize the training. Predictions are upsampled from the lowest level with highest output resolution to full resolution during evaluation.</p><p>Training Details. We train our models on 8 GPUs without synchronized batch normalization. The weights of pyramid feature extractor are initialized from the ImageNet pretrained model. The network is optimized by Adam <ref type="bibr" target="#b24">[25]</ref>, where ? 1 = 0.9, ? 2 = 0.999. For all of our pretraining experiments on synthetic datasets, models are trained for 200 epochs, and the learning rate is decayed by 0.5 every 30 epochs after 70 epochs for 4 times in total. As for data augmentation, besides random cropping, we adopt random resizing and color perturbation <ref type="bibr" target="#b30">[31]</ref> during the fine-tuning stage, and introduce random flipping for optical flow experiments. The dense and sparse annotations, as supervision at different scales, are bilinearly downsampled and average pooled from the ground-truth map respectively. In this section, unless otherwise stated, confidence maps are obtained through aggregating the probabilities within W * i of the last level match density, and uncertainty maps are the opposite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Stereo Matching</head><p>To evaluate the performance of our HD 3 S model, we benchmark our result on the KITTI stereo dataset <ref type="bibr" target="#b12">[13]</ref>. Due to the limited amount of training data in KITTI, we pretrain our model on the FlyingThings3D dataset <ref type="bibr" target="#b29">[30]</ref>.</p><p>FlyingThings3D. We use the FlyingThings3D dataset as  training data. Following the training protocol of the original FlowNet2 model <ref type="bibr" target="#b20">[21]</ref>, we use a subset of the dataset which omits some extremely hard samples. We train our model with a batch size of 32 and an initial learning rate of 2 ? 10 ?4 . The image crop size is 320 ? 896. Qualitative examples, as well as the confidence maps, are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. We find low confidence correlates well with prediction errors, which generally occurs at boundaries and occlusions.  <ref type="figure">Figure 6</ref>: Example stereo error maps on KITTI 2015 test set. We contrast our method with GC-Net <ref type="bibr" target="#b23">[24]</ref>. Orange corresponds to erroneous prediction. This figure is best viewed in color. and image crop size 320 ? 896. The initial learning rate is 1 ? 10 ?5 and decayed by 0.5 at the 1000th and the 1500th epoch. As shown in Tab. 1, our method achieves the lowest percentages of disparity outliers in both non-occluded (Out-Noc) and total regions (Out-All), background (D1-bg) and foreground regions (D1-fg), among all of the competitive baselines on both datasets. We also hold the lowest inference time for processing a standard KITTI stereo pair. Note that we do not leverage the entire Scene Flow dataset <ref type="bibr" target="#b29">[30]</ref> for training as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref>, nor do we utilize additional semantic or edge cues as in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b46">47]</ref>  shown in <ref type="figure">Fig. 6</ref>. Our method exhibits better performance in regions with complex and ambiguous textures. This indicates the effectiveness of hierarchical match density learning based on pyramid feature representations, which exhibits robustness to local noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Optical Flow</head><p>We pretrain our HD 3 F on synthetic data from Fly-ingChairs <ref type="bibr" target="#b9">[10]</ref> and FlyingThings3D <ref type="bibr" target="#b20">[21]</ref>, then investigate the effectiveness of our model on established optical flow benchmarks including MPI Sintel <ref type="bibr" target="#b6">[7]</ref> and KITTI <ref type="bibr" target="#b12">[13]</ref>.</p><p>FlyingChairs. We train our network on FlyingChairs with batch size 64 and initial learning rate 4 ? 10 ?4 . Images are randomly resized and cropped to 384 ? 512 patches. We find larger crop size can improve the network performance.</p><p>FlyingThings3D. We further fine-tune the model on the FlyingThings3D data, the same subset in our stereo matching experiments, with batch size 32, learning rate 4 ? 10 ?5 and image crop size 384 ? 832. We visualize examples of multi-scale predictions in <ref type="figure" target="#fig_3">Fig. 5</ref>. The results indicate that our model is able to progressively refine the prediction from coarse to fine scales. Though we adopt the discrete distribution, our model can still capture very detailed displacements.</p><p>MPI Sintel. Finally, we fine-tune our model on MPI Sintel <ref type="bibr" target="#b6">[7]</ref> for 1200 epochs with batch size 32 and image crop size 384 ? 768. The initial learning rate is 2 ? 10 ?5 and   <ref type="figure">Figure 7</ref>: Example flow error maps on KITTI 2015 test set. We compare our method with PWC-Net <ref type="bibr" target="#b39">[40]</ref>. Orange corresponds to erroneous prediction. This figure is best viewed in color. decayed by 0.5 at the 600th and the 900th epoch. Though the dataset provides training data of different subsets (clean &amp; final passes), we only adopt the final pass as training data rendered with motion blur, defocus blur, and atmospheric effects. As shown in Tab. 2, we can obtain the lowest average EPE in the final pass, and compelling results on the clean pass, though our model does not see the clean pass data during training. In the model generalization experiment, our pretrained HD 3 F estimates the flow accurately near the occlusion boundary, resulting in the lowest out-Input Image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence Map</head><p>Error Map <ref type="figure">Figure 8</ref>: Example confidence maps of our predictions and error maps w.r.t. ground-truth. In confidence maps, white colors mean confident predictions while dark colors denote uncertain ones. In the error maps, warmer colors indicate inaccurate predictions. lier percentage on KITTI (see the "HD 3 F (Ours)" entry in Tab. 3). The metric of EPE emphasizes large motion error. This influence makes our pretrained HD 3 F achieve higher EPE on MPI Sintel (see the "HD 3 F (Ours)" entry in Tab. 2).</p><p>KITTI. Alternatively, we can finetune our pretrained model on KITTI dataset. We follow the configurations of our stereo experiment. Tab. 3 summarize the results. We can obtain the lowest F1-Noc on KITTI 2012 test set and the lowest F1-all on KITTI 2015 test set. At the time of writing, HD 3 F outperforms all two-frame optical flow methods by large margins on both KITTI 2012 &amp; 2015. It even surpasses some competitive scene flow methods such as PRSM <ref type="bibr" target="#b41">[42]</ref>, which use additional stereo data. This reveals the suitability of our probabilistic method in challenging real-world cases. We show qualitative comparisons against PWC-Net in <ref type="figure">Fig. 7</ref>. Our method appear to have advantages in estimating many thin structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Uncertainty Estimation</head><p>We also conduct quantitative analysis of uncertainty estimation. We compute the log likelihoods of our network predictions and compare HD 3 F with probabilistic flow networks <ref type="bibr" target="#b11">[12]</ref>. FlowNetDropOut uses variational Gaussian dropout layers <ref type="bibr" target="#b25">[26]</ref>. While FlowNetProbOut replaces deterministic outputs with probabilistic output layers. FlowNe-tADF propagates uncertainty through the entire network using ADF. During the evaluation, we recover the full match density through composing the multi-scale match densities. This can be achieved through iteratively sampling from coarse to fine, and we assume a discrete non-uniform distribution for sampling outside W * i (see Sec. 3.3). As shown in Tab. 4, HD 3 F achieves the best average log likelihoods against all of the baselines.</p><p>Furthermore, we measure the reliability of network prediction based on uncertainty. We treat predictions with uncertainty greater than a certain threshold (? = 0.3) as out-   liers and compare such criterion with the forward-backward consistency check <ref type="bibr" target="#b47">[48]</ref> which is popularly adopted for point estimate. Both methods use the same HD 3 F model for inference. As shown in Tab. 5, our uncertainty estimation gives the highest mean IoU and mean accuracy in both nonoccluded and overall regions. <ref type="figure">Fig. 8</ref> presents visualization of the confidence and error maps. We can observe the positive correlation between our estimated uncertainty and prediction error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed Hierarchical Discrete Distribution Decomposition (HD 3 ) for estimating the match density. Our approach decomposed the match density into multiple scales and learned the decomposed match densities in an end-toend manner. The predicted match densities can be converted into point estimates, while providing model-inherent uncertainty measures at the same time. Our experiments demonstrated the advantages of our method on established benchmarks.</p><p>In the future, we hope to integrate more information into our framework such as the pixel assignment probabilities from segmentation. Currently, we do not consider relationships between match densities of adjacent pixels, but this may help remove match uncertainty in challenging cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our architecture. The submodule at the lth level is presented here. F l andF l denotes the lth level of original and warped pyramid features of image pair I. E l up denotes upsampled density embeddings between different levels as density bypass connections. f l and g l denote motion vectors and p l corresponds to match density. Their conversion is fulfilled by our D2V and V2D modules. For details please refer to our method part. This figure is best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Conversion between motion vectors and match densities. The support is taken as 3 ? 3 here for illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualized stereo results on the validation set of FlyingThings3D. Cold colors in the error map denote correct predictions while warm colors mean the contrary. Our network gives accurate results in most regions, while errors tend to occur at boundaries and occlusions. sity embedding to E l up and feed it to the next level as density bypass connections. Both of the pyramid feature extractor and the density decoder are jointly trained in an end-to-end manner.At inference time, we calculate E[g l ] from each predicted p l res and compose them via Eq. 1 to produce the point estimate of f . While during training time, we downsample the ground-truth motion field into f l gt . The residual motion w.r.t. ?(E[f l?1 ]) is converted into p l gt . The entire training loss comes in the form of Kullback?Leibler divergence L = l g?R g l p l gt (g)(log p l gt (g) ? log p l res (g)). (4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative multi-scale flow result on the validation set of FlyingThings3D dataset. Bilinearly downsampled raw images, coarser level flows, error maps and confidence maps are enlarged via nearest neighbor upsampling for visualization purpose. Our network gives precise predictions in most regions, while occasionally presents confusion in occluded regions and disappearing parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Average EPE results on MPI Sintel dataset. "-ft" means finetuning on the Sintel training set and numbers in the parenthesis are results on data the method has been trained on.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Optical flow results on KITTI dataset. "-ft" means finetuning on the KITTI training set. Numbers in parenthesis are results on data the network has been trained on.</figDesc><table><row><cell>PWC</cell></row><row><cell>Ours</cell></row><row><cell>PWC</cell></row><row><cell>Ours</cell></row><row><cell>PWC</cell></row><row><cell>Ours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Average log likelihoods of probabilistic flow methods on MPI Sintel training set and FlyingChairs test set.</figDesc><table><row><cell>Classes</cell><cell>Methods</cell><cell>IoU</cell><cell>Noc</cell><cell>Acc</cell><cell>IoU</cell><cell>All</cell><cell>Acc</cell></row><row><cell>Outlier</cell><cell>Consistency Ours</cell><cell>17.5 37.6</cell><cell></cell><cell>64.9 57.8</cell><cell>23.3 44.1</cell><cell></cell><cell>81.9 76.4</cell></row><row><cell>Inlier</cell><cell>Consistency Ours</cell><cell>84.2 96.1</cell><cell></cell><cell>85.8 97.8</cell><cell>75.6 91.8</cell><cell></cell><cell>76.9 93.7</cell></row><row><cell>Mean</cell><cell>Consistency Ours</cell><cell>50.9 66.9</cell><cell></cell><cell>75.4 77.8</cell><cell>49.5 68.0</cell><cell></cell><cell>79.4 85.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Classification result of inlier and outlier predictions on KITTI 2015 training set. Noc denotes evaluation only in the nonoccluded area, while All denotes evaluation in the overall region.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work was supported by Berkeley AI Research and Berkeley DeepDrive.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A computational framework and an algorithm for the measurement of visual motion. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Flow fields: Dense correspondence fields for highly accurate large displacement optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cnn-based patch matching for optical flow with thresholded hinge embedding loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Performance of optical flow techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Beauchemin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bounding boxes, segmentations and object coordinates: How important is recognition for 3d scene flow estimation in autonomous driving scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hosseini Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Full flow: Optical flow estimation by global optimization over regular grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Patchbatch: a batch augmented loss for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gadot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lightweight probabilistic deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Vision meets robotics: The kitti dataset. IJRR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Displets: Resolving stereo ambiguities using object knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluation of stereo matching costs on images with radiometric differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient coarse-to-fine patchmatch for large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">LiteFlowNet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mirrorflow: Exploiting symmetries in joint optical flow and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">FlowNet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A statistical confidence measure for optical flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kondermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Markov random field models in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Brostow. Learning a confidence measure for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">UnFlow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sgm-nets: Semi-global matching with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the integration of optical flow and action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>G?ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved stereo matching with constant highway networks and reflective confidence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Probability distributions of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Edgestereo: A context integrated residual pyramid network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Practical deep stereo (pds): Toward applications-friendly deep stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">3d scene flow estimation with a piecewise rigid scene model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Probflow: Joint optical flow and uncertainty estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Wannenwetsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Optical flow in mostly rigid scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient joint segmentation, occlusion labeling, stereo and flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Segstereo: Exploiting semantic information for disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
