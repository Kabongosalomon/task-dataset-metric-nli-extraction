<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GAITMIXER: SKELETON-BASED GAIT REPRESENTATION LEARNING VIA WIDE-SPECTRUM MULTI-AXIAL MIXER</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekkasit</forename><surname>Pinyoanuntapong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Charlotte</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayman</forename><surname>Ali</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Charlotte</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pu</forename><surname>Wang</surname></persName>
							<email>pu.wang@uncc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Charlotte</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Lee</surname></persName>
							<email>minwoo.lee@uncc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Charlotte</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
							<email>chen.chen@crcv.ucf.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GAITMIXER: SKELETON-BASED GAIT REPRESENTATION LEARNING VIA WIDE-SPECTRUM MULTI-AXIAL MIXER</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Gait Recognition</term>
					<term>Self-Attention</term>
					<term>Large- kernel Convolution</term>
					<term>Multi-axial Mixer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing gait recognition methods are appearance-based, which rely on the silhouettes extracted from the video data of human walking activities. The less-investigated skeletonbased gait recognition methods directly learn the gait dynamics from 2D/3D human skeleton sequences, which are theoretically more robust solutions in the presence of appearance changes caused by clothes, hairstyles, and carrying objects. However, the performance of skeleton-based solutions is still largely behind the appearance-based ones. This paper aims to close such performance gap by proposing a novel network model, GaitMixer, to learn more discriminative gait representation from skeleton sequence data. In particular, GaitMixer follows a heterogeneous multi-axial mixer architecture, which exploits the spatial self-attention mixer followed by the temporal large-kernel convolution mixer to learn rich multi-frequency signals in the gait feature maps. Experiments on the widely used gait database, CASIA-B, demonstrate that GaitMixer outperforms the previous SOTA skeleton-based methods by a large margin while achieving a competitive performance compared with the representative appearance-based solutions. Code will be available at https://github.com/exitudio/gaitmixer</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Unlike short-distance biometrics (e.g., fingerprints, facial, iris, palm, and finger vein patterns), gait can be recognized from a distance without the subject's cooperation or interference. Such long-distance biometrics has a huge potential to extend its applications to forensic identification, access control, and social security. The gait recognition methods are generally either appearance-based or skeletonbased. Appearance-based approaches <ref type="bibr" target="#b8">[1]</ref> <ref type="bibr" target="#b9">[2]</ref>[3] <ref type="bibr" target="#b11">[4]</ref> utilize background subtraction to obtain silhouettes from a video sequence, which are further analyzed using carefully-designed network models for gait representation learning. On the other hand, skeleton-based approaches <ref type="bibr" target="#b12">[5]</ref> <ref type="bibr" target="#b13">[6]</ref> <ref type="bibr" target="#b14">[7]</ref> utilize the skeleton sequences extracted from 2D/3D pose estimators as the <ref type="bibr">(a)</ref> (b) (c) (d) <ref type="figure">Fig. 1.</ref> (a) Global self-attention token mixing <ref type="bibr" target="#b15">[8]</ref>. (b) Selfattention mixing along H and W axes <ref type="bibr" target="#b16">[9]</ref>. (c) Convolutionmixing along H and W axes <ref type="bibr" target="#b13">[6]</ref> (d) Heterogeneous multi-axial mixer (ours).</p><p>inputs to learn effective gait representations. Theoretically, skeleton-based methods are more robust to appearance variations caused by hairstyles, carrying objects, and clothes. However, the skeleton-based approaches, which still do not receive sufficient attention, yield a large performance gap compared with the appearance-based counterparts.</p><p>To close this gap, this paper tries to exploit more effective gait feature encoders by proposing the multi-axial mixer, which is a generic transformer-like architecture that mixes the feature patches (i.e., tokens) along each axis of the feature space, respectively, e.g., width-wise, height-wise, and channel-wise axes in image feature space. Many recent highperformance network backbones can be considered as the special cases of multi-axial mixer based on what types of mixing functions are applied, which mainly include convolution <ref type="bibr" target="#b13">[6]</ref>[10] <ref type="bibr" target="#b18">[11]</ref>, self-attention <ref type="bibr" target="#b19">[12]</ref> <ref type="bibr" target="#b20">[13]</ref>, and multi-layer perceptrons (MLP) <ref type="bibr" target="#b21">[14]</ref> [15] as shown in <ref type="figure">Fig. 1</ref>. Multi-axial mixers have been demonstrated to achieve SOTA performance in image classification and video recognition tasks, while significantly reducing computation complexities compared with other competitive network models, such as vision transformers. Despite their promising features, current multi-axial mixers generally exploit the homogeneous architecture design, where the same type of mixing functions (e.g., either convolution, self-attention, or MLP) is applied along each feature space axis. Such design, however, has limited capacity to learn multi-frequency features. In particular, it has been established that convolutions focus more on local information and therefore are good learners for high-frequency features <ref type="bibr" target="#b23">[16]</ref>. Self-attentions, on the contrary, are designed to model  long-range interactions and are more capable to capture lowfrequency signals (global information) in feature map <ref type="bibr" target="#b24">[17]</ref>.</p><p>In this paper, we propose GaitMixer, a novel heterogeneous spatial-temporal axial mixer, which can effectively learn the discriminative gait representation by capturing both high-frequency and low-frequency features. In particular, GaitMixer consists of a spatial self-attention mixer and a temporal large-kernel axial mixer <ref type="figure" target="#fig_0">(Fig. 2)</ref>. The spatial axial mixer learns interactions among the joints within each skeleton frame. The temporal axial mixer models the interactions among the temporal tokens of each single joint at different time indices. Experiments on the widely used gait database, CASIA-B, demonstrate that GaitMixer outperforms the previous SOTA skeleton-based <ref type="bibr" target="#b13">[6]</ref> methods by 12% on average, while achieving a competitive performance compared with the representative appearance-based solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Appearance-based approaches extract binary images of a human silhouette from the source images by subtracting static background <ref type="bibr" target="#b25">[18]</ref>. GaitNet <ref type="bibr" target="#b8">[1]</ref> integrates silhouette extraction into the model as an end-to-end network for gait recognition. GaitSet <ref type="bibr" target="#b9">[2]</ref> decouples the temporal continuous sequence by learning identity information from the set of independent frames to be immune to permutation of frames and be able to integrate frames from different videos. While the majority of methods <ref type="bibr" target="#b8">[1]</ref>[2][4] take the entire shape as input, more recent approaches GaitPart <ref type="bibr" target="#b10">[3]</ref> focus on each part of the body individually assuming that each part of human body needs its own spatial-temporal learning by separating silhouette into several parts horizontally. Skeleton-based approaches (i.e., modelbased approaches) use skeleton data as the model inputs. In the early work, pose-based temporal-spatial network (PTSN) <ref type="bibr" target="#b12">[5]</ref> utilizes a long-short term memory (LSTM) to capture the dynamic information and CNN to learn static information of a gait sequence in parallel. PoseGait <ref type="bibr" target="#b26">[19]</ref> utilizes 3D pose es-timated from images in order to be invariant to view changes, along with hand-crafted features including joint angle, limb length, and joint motion. The most recent methods, Gait-Graph <ref type="bibr" target="#b13">[6]</ref> and GaitGraph2 <ref type="bibr" target="#b14">[7]</ref>, adopt graph convolution neural networks (GCNs) for gait recognition, inspired by the successes of GCNs in action recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GAITMIXER: HETEROGENEOUS WIDE-SPECTRUM SPATIAL-TEMPORAL MIXER</head><p>To effectively learn both high-frequency and low-frequency gait features, we introduce the GaitMixer, a heterogeneous spatial-temporal axial mixer architecture. As shown in <ref type="figure" target="#fig_0">Fig.  2</ref>, GaitMixer consists of a spatial self-attention mixer and a temporal large-kernel axial mixer <ref type="figure" target="#fig_0">(Fig. 2)</ref>. The spatial axial mixer only learns interactions among the joints within each skeleton frame. The d y -dimensional spatial representation y t ? R |J|?dy for each skeleton frame t with |J| joints is learned after B S self-attention blocks. Then, the representations of T skeleton frames within a gait sequence are concatenated into z ? R |J|?T ?dy , which is then forwarded to a temporal axial mixer to capture the interactions among the tokens of each single joint at different temporal indices. The temporal axial mixer consists of B T one-dimensional largekernel convolution blocks. Moreover, to simplify our Gait-Mixer architecture, both spatial and temporal mixers adopt the isotropic design, which does not perform feature downsampling and maintains the same feature resolutions at all layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spatial Mixer with Axial Self-attention</head><p>The spatial mixer module aims to learn a high dimensional representation embedding from each skeleton frame. Although self-attention tends to capture low-frequency (or global) features, our experiments demonstrate that selfattention is sufficient to learn both high-frequency and low-frequency signals in the feature map along spatial axis. This also indicates that self-attention can effectively model both short-range and long-range inter-joint dependencies. Given a 2D skeleton with joints J, we consider each joint (i.e., x and y coordinates) as a spatial token (with 2 channels) and perform the feature extraction among all |J| spatial tokens by following the isotropic transformer pipeline. Specifically, the spatial taken x i ? R 2 is passed through a trainable linear projection, which maps each token to a high dimension embedding x i ? R dx . Then, the spatial token embeddings of each skeleton frame x = (x 1 , x 2 , . . . , x |J| ) are mixed by inter-token dot product attentions to generate an output sequence y = (y 1 , y 2 , . . . , y |j| ) where y i ? R dy . Running h self-attentions in parallel leads to the multihead self-attention with h heads, where the outputs of the attention heads are concatenated and projected into the expected dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal Mixer with Large-kernel Convolution</head><p>The essence of a walking sequence is composed of multiple short repeated cycles. In the temporal axis, self-attention may not be able to capture wide-band multi-frequency features, considering that the global receptive field of self-attention is much easier to capture low-frequency features. It demands a large amount of data for self-attention to establish the desirable locality inductive bias that is the key to learn highfrequency features. To learn both high-frequency and lowfrequency temporal data, we utilize large kernel depth-wise separable convolution in the temporal mixer as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. In general, convolution neural networks tend to capture high-frequency (local) features, however, the large kernel allows the model to also learn low-frequency features. 60 frames are used in the temporal model which covers around 4 cycles of walking. A large one-dimension kernel with size of 31 ? 1 is used to capture mid-range information (around 2 walking cycles). A reverse padding with size of 30 is applied to keep the temporal dimension the same. The temporal mixer only communicates with all frames along the temporal axis of the same joints. A temporal mixer is composed of two types of axial mixers. First, a token mixer is implemented by a depth-wise convolution that learns all embeddings only in the same channel. Next, a channel mixer is a 1 ? 1 convolution that learns only specific embedding along all channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Representation head and Loss Function</head><p>We apply average pooling along spatial and temporal dimensions to reduce the output from x temp ? R F ?(J?c) to x hidden ? R c , where F and J are number of frames and joints respectively. The number of channels c is set to 256. Finally, layer norm, fully connected layer, and l 2 -norm are applied respectively and return the feature embedding in 128 dimensions. To learn the discriminative gait representation, we apply the triplet loss with multi-similarity miner. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>CASIA-B <ref type="bibr" target="#b27">[20]</ref> has been widely adopted as a multi-view, RGB, and silhouette gait dataset. The data acquisition is performed by 124 individuals from 11 viewing angles ranging from 0 to 180 with 18 angle differences. To mimic typical daily walking conditions, each subject performs six sequences of normal walking (NM), two sequences of walking with a coat (CL), and two sequences of walking with carrying a bag (BG). For each individual, ten sequences are captured from each view angle. This paper follows a widely-used test protocol <ref type="bibr">[</ref>  <ref type="bibr" target="#b26">[19]</ref>, which uses the data of the first 74 subjects' sequences for the training and the remaining 50 subjects' sequences for testing. Furthermore, the test dataset is divided into gallery and probe sets. The gallery set includes the first four sequences of the normal walking condition. The probe set consists of the last two sequences of normal walking, two walking with a coat on, and walking with carrying a bag. Finally, the results are reported for all viewing angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Training Details HRNet <ref type="bibr" target="#b28">[21]</ref> is used as a 2D human pose estimator. We follow data augmentations from GaitGraph <ref type="bibr" target="#b13">[6]</ref> and add normalization of the joint position in (x, y)-coordinates by dividing 320 which is the width of the original videos to input data while keeping the aspect ratio. Adam optimizer is used with 6e?3 learning rate with 1-cycle learning rate and 1e?5 weight decay. We are using a balanced batch sampler to sample the number of walking data per person equally. The batch size is (74, 4), denoting 74 people and 4 walking samples per person. Testing. Each gait testing sample contains 60 frames selected from the middle of the sequence data. The test set is separated into probe and gallery. Both are fed into the model to obtain the feature representations. The ID of the gallery representation that has the smallest cosine distance from the probe will be the predicted ID of the probe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the SOTA Methods</head><p>To demonstrate the superior performance of GaitMixer as a heterogeneous multi-axial mixer model, we also build GaitFormer, which is a homogeneous multi-axial mixer that  adopts self-attention for both spatial and temporal axes. In <ref type="figure" target="#fig_1">Fig. 3</ref>, we visualize the frequency magnitude of the output feature maps from GaitMixer and GaitFormer, respectively. It can be observed that GaitMixer concentrates on both highfrequency and low-frequency components along both temporal and spatial axes in feature maps. This confirms the superior capacity of GaitMixer to capture features in widespectrum bands. GaitFormer, however, cannot effectively model the high-frequency feature components. The performance comparisons between our approaches and the SOTA skeleton-based methods are shown in <ref type="table" target="#tab_2">Table 1</ref>. It is shown that our multi-axial mixer models outperform the existing solutions by a large margin in both cross-view and crosswalking-condition cases. Moreover, GaitMixer achieves better recognition accuracy than GaitFormer because GaitMixer can jointly exploit the heterogeneous mixing at different feature space dimensions. <ref type="table" target="#tab_3">Table 2</ref> shows a competitive performance of our skeleton-based methods, compared with the representative appearance-based methods. GaitMixer achieves much higher accuracy than all appearance-based approaches in wearing coat condition. It is due to the inherent robustness of skeleton data against large appearance changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization</head><p>We use class activation map (Grad-CAM) <ref type="bibr" target="#b29">[22]</ref> to show which parts of the input gait sequence contribute most to the final recognition result. As shown in <ref type="figure">Fig. 4 (bottom)</ref>, GaitMixer focuses on continuous joint sequences with a variety of differ- <ref type="figure">Fig. 4</ref>. Grad-CAM <ref type="bibr" target="#b29">[22]</ref> visualizations. Top-left: GaitGraph <ref type="bibr" target="#b13">[6]</ref>. Top-right: GaitFormer. Bottom: GaitMixer. X-axis represents frames 1 to 60 and Y-axis represents 17 joints. Features with higher contributions have higher heat temperatures. ent temporal windows, thus capturing short-, mid, and-longrange temporal feature interactions. Moreover, GaitMixer also pays attentions to a diverse set of joints except ears, eyes, and nose, which is also as expected because the landmarks on face are not relevant to the gait dynamics. As shown in <ref type="figure">Fig. 4</ref> (top-left), GaitGraph tends to focus on some specific joints over a large temporal window and it also exploits the features from face landmarks for gait recognition. Both limitations could degrade the performance of GaitGraph. GaitFormer <ref type="figure">(Fig. 4 (top-right)</ref>) pays more attention to certain skeleton frameworks without capturing rich spatial-temporal feature interactions. This can be the key contributing factor that affects its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we present GaitMixer model, a novel heterogeneous multi-axial architecture combining a spatial selfattention mixer and a large kernel temporal convolution mixer to capture both high-frequency and low-frequency dynamics of gait data. Our approach achieves the best accuracy on the well-known CASIA-B gait dataset for all conditions when compared to previous skeleton-based methods and is superior to appearance-based approaches with coats conditions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>(top) GaitMixer consists of a spatial mixer followed by a temporal mixer. (bottom) The detailed network architecture of the spatial self-attention mixer and large-kernel convolution mixer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>2D FFT of GaitMixer and GaitFormer feature maps of 4 channels. Higher temperature indicates larger magnitude. Pixels closer to the center represent lower frequencies 4. EXPERIMENTS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Joint to Embedding Spatial Mixer 32x60x17 32x60x17 64x60x17 128x60x17 256x60x17 Spatial Mixer Spatial Mixer Spatial Mixer Layer Norm Temporal Mixer Temporal Mixer Temporal Mixer Temporal Mixer AVG Pooling Layer Norm FC 32x60x17 32x60x17 32x60x17 LayerNorm Mult-Head Self Attetion Mult-Head Self Attetion Mult-Head Self Attetion LayerNorm 1x1 Convy17) ... Spatial Mixer Temporal Mixer Representation Head Spatial Mixer Embed 60 Embed 1 Embed 2 Embed 3 ... Temporal Mixer</head><label></label><figDesc></figDesc><table><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>17</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(x1,y1)</cell><cell>(x2,y2)</cell><cell>(x3,y3)</cell><cell>(x17,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>arXiv:2210.15491v2 [cs.CV] 29 Oct 2022</cell><cell>GELU</cell><cell>1x1 Conv</cell><cell>31x1 Conv</cell><cell>1x1 Conv</cell><cell>GELU</cell><cell>BatchNorm</cell><cell>Token Mixer Channel Mixer</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Averaged Rank-1 accuracies on CASIA-B per probe angle excluding identical-view cases.</figDesc><table><row><cell>Gallery NM#1-4</cell><cell>0?-180?mean</cell></row><row><cell>Probe</cell><cell>0?18?36?54?72?90?108?126?144?162?180?N</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Averaged Rank-1 accuracies on CASIA-B comparison with both appearance-based and skeleton-based methods</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="2">Probe NM BG CL Mean</cell></row><row><cell>Appearance based</cell><cell>GaitNet [1] GaitSet [2] GaitPart [3]</cell><cell>91.6 85.7 58.9 95.0 87.2 70.4 96.2 91.5 78.7</cell><cell>78.7 84.2 88.8</cell></row><row><cell></cell><cell>PoseGait</cell><cell>68.7 44.5 36.0</cell><cell>49.7</cell></row><row><cell>Skeleton</cell><cell>GaitGraph</cell><cell>87.7 74.8 66.3</cell><cell>76.3</cell></row><row><cell>based</cell><cell>GaitGraph2</cell><cell>82.0 73.2 63.6</cell><cell>72.9</cell></row><row><cell></cell><cell cols="2">GaitFormer (ours) 91.5 81.4 77.2</cell><cell>83.4</cell></row><row><cell></cell><cell>GaitMixer (ours)</cell><cell>94.9 85.6 84.5</cell><cell>88.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaitgraph</surname></persName>
		</author>
		<idno>6] 85.3 88.5 91.0 92.5 87.2 86.5 88.4 89.2 87.9 85.9 81.9 87.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaitformer</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">90</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaitmixer</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ours) 94.4 94.9 94.6 96.3 95.3 96.3 95.3 94.7 95.3 94.7 92.2 94.9</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaitgraph</surname></persName>
		</author>
		<idno>6] 75.8 76.7 75.9 76.1 71.4 73.9 78.0 74.7 75.4 75.4 69.2 74.8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaitformer</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">82</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaitmixer</surname></persName>
		</author>
		<idno>ours) 83.5 85.6 88.1 89.7 85.2 87.4 84.0 84.7 84.6 87.0 81.4 85.6</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Posegait</surname></persName>
		</author>
		<idno>19] 24.3 29.7 41.3 38.8 38.2 38.5 41.6 44.9 42.2 33.4 22.5 36.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaitmixer</surname></persName>
		</author>
		<idno>ours) 81.2 83.6 82.3 83.5 84.5 84.8 86.9 88.9 87.0 85.7 81.6 84.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gaitnet: An end-to-end network for gait based human identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">106988</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Gaitset: Cross-view gait recognition through utilizing gait as a deep set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gaitpart: Temporal partbased model for gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14213" to="14221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A comprehensive study on cross-view gait based human identification with deep cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pose-based temporal-spatial network (ptsn) for gait recognition with carrying and clothing variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rijun</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunshui</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Edel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometric Recognition. 2017</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gaitgraph: Graph convolutional network for skeleton-based gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torben</forename><surname>Teepe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Gilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>H?rmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="2314" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Towards a deeper understanding of skeleton-based gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torben</forename><surname>Teepe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Gilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>H?rmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6816" to="6826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stronger, faster and more explainable: A graph convolutional baseline for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifeng</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial temporal transformer network for skeletonbased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiara</forename><surname>Plizzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cannici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Matteucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition. ICPR International Workshops and Challenges</title>
		<imprint>
			<biblScope unit="page" from="694" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">3d human pose estimation with spatial and temporal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mendieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Morphmlp: A selfattention free, mlp-like backbone for image and video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Shou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12527</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Scaling up your kernels to 31x31: Revisiting large kernel design in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fast vision transformers with hilo attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Silhouette analysis-based gait recognition for human identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhong</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1505" to="1518" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A model-based gait recognition method with body pose and human prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rijun</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhen</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">107069</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A framework for evaluating the effect of view angle, clothing and carrying condition on gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoliang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Pattern Recognition (ICPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="441" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="336" to="359" />
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
