<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Affordance Transfer Learning for Human-Object Interaction Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
							<email>baosheng.yu@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
							<email>pengxiaojiang@sztu.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">Shenzhen Technology University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Affordance Transfer Learning for Human-Object Interaction Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reasoning the human-object interactions (HOI) is essential for deeper scene understanding, while object affordances (or functionalities) are of great importance for human to discover unseen HOIs with novel objects. Inspired by this, we introduce an affordance transfer learning approach to jointly detect HOIs with novel objects and recognize affordances. Specifically, HOI representations can be decoupled into a combination of affordance and object representations, making it possible to compose novel interactions by combining affordance representations and novel object representations from additional images, i.e. transferring the affordance to novel objects. With the proposed affordance transfer learning, the model is also capable of inferring the affordances of novel objects from known affordance representations. The proposed method can thus be used to 1) improve the performance of HOI detection, especially for the HOIs with unseen objects; and 2) infer the affordances of novel objects. Experimental results on two datasets, HICO-DET and HOI-COCO (from V-COCO), demonstrate significant improvements over recent state-ofthe-art methods for HOI detection and object affordance detection. Code is available at https://github.com/ zhihou7/HOI-CL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human-object interaction (HOI) detection aims to localize the human and objects in a given image, and recognize the interactions between the human and objects <ref type="bibr" target="#b2">[3]</ref>. Considering the combinatorial nature of HOIs, there are always a variety of rare or unseen interactions with novel objects (e.g., "ride tiger"), which remains a great challenge for the HOI detection model to detect unseen interactions with those novel objects.  <ref type="figure">Figure 1</ref>. An intuitive example to demonstrate affordance transfer learning for jointly exploring human interactions with novel objects (e.g., "tiger"), and recognizing the affordance of novel objects. The proposed method is able to learn from the unseen interaction samples (e.g., "ride tiger") that are composed from affordance representations and novel object representations, which meanwhile transfers the affordance to novel objects and enables the object affordance recognition.</p><p>The interactions between the human and object, human, verb, object , can be captured by either a humancentric (actions) or object-centric (affordance) manner. Specifically, each HOI can be disentangled into a verb and an object, in which the verb also indicates one of the possible affordances (or functionalities) of the object <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>, i.e. what actions can be applied to a particular object <ref type="bibr" target="#b9">[10]</ref>. Therefore, we are able to jointly learn the affordances of object from the HOI samples, making it possible to com-pose new HOIs by combining the affordance representations in existing HOIs with novel object representations. Meanwhile, the composition of object representations and the corresponding affordance representations transfers the affordance representation (verb) to novel objects, which we term as affordance transfer learning or ATL. The affordance transfer learning empowers the shared affordance representation learning among different objects, and further facilitates the detection on HOIs with novel objects. For example, with the shared affordance representation (e.g., "rideable") between "tiger" and "horse" as illustrated in <ref type="figure">Figure 1</ref>, we are able to compose new HOIs (i.e., "ride tiger"), and thus enable the detection of unseen HOIs. The proposed affordance transfer learning framework further generalizes the compositional learning for HOI detection <ref type="bibr" target="#b16">[17]</ref> with the ability to detect HOI with additional unseen objects, rather than only the objects from existing HOI samples.</p><p>The proposed affordance transfer learning also empowers the HOI detection model to learn object affordance in a weakly supervised manner. Recent HOI detection approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b6">7]</ref> usually fail to explore the possibility of object affordance recognition with the HOI detection model, and previous affordance learning methods <ref type="bibr" target="#b14">[15]</ref> largely ignore transferring shared affordance representations from existing HOIs to novel objects by HOI detection model. By composing new HOI samples from the affordance representations and novel object representations, affordance transfer learning enables the HOI model to distinguish whether a novel object representation can be combined or not with an affordance representation (i.e., verb). We thus recognize object affordances with HOI model as follows: 1) we maintain a feature bank of decoupled affordance representations from the HOI detection dataset; 2) we extract object representations from additional object detection datasets using the same HOI backbone network; and 3) we combine the object representations with all affordance representations in the feature bank as the input of the HOI classifier. Finally, we are able to obtain a set of HOI predictions, which are further used to infer the object affordances. Overall, the main contribution of this paper can be summarized as follows,</p><p>? We introduce an affordance transfer learning framework to exploit a broader source of data for HOI detection, especially for human interactions with novel objects.</p><p>? We incorporate HOI detection network with decomposed affordances to infer the affordance of novel objects.</p><p>? The proposed method not only improves recent stateof-the-art HOI detection methods but also facilitates the recognition of object affordance at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">HOI Understanding</head><p>Human-object interaction (HOI) detection are receiving increasing attention from the community <ref type="bibr" target="#b2">[3]</ref>. HOI detection aims to not only detect object and human in an image, but also reason the relationships between human and objects. Since Gputa et al. <ref type="bibr" target="#b11">[12]</ref> presented a Human-Object Interaction approach, massive traditional methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> were introduced for HOI recognition using spatial relation <ref type="bibr" target="#b11">[12]</ref>, pose <ref type="bibr" target="#b39">[40]</ref>, human part <ref type="bibr" target="#b40">[41]</ref> in the early. Recently, Chao et al. <ref type="bibr" target="#b3">[4]</ref> introduced a large HOI recognition data HICO <ref type="bibr" target="#b3">[4]</ref> and a challenging HOI detection dataset HICO-DET <ref type="bibr" target="#b2">[3]</ref> for HOI understanding. Meanwhile, Gupta et al. <ref type="bibr" target="#b12">[13]</ref> introduced the V-COCO dataset, which mainly focuses on the grounding of verbs and their semantic roles.</p><p>Currently, there are a large number of HOI detection approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b18">19]</ref> improving the HOI benchmark <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17]</ref>. According to the target, current methods can be categoried into twostage HOI detection, which contains common HOI detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b24">25]</ref> and few-and zero-shot HOI detection <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b25">26]</ref>, and one-stage HOI detection <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b17">18]</ref>. Recently, Hou et al. <ref type="bibr" target="#b16">[17]</ref> propose a visual compositional learning framework to compose novel HOI samples between pair-wise HOI images for low-and zeroshot HOI detection. However, VCL <ref type="bibr" target="#b16">[17]</ref> can not compose human-novel-object interactions and ignores the possibility of affordance recognition with HOI model. We introduce a novel framework, Affordance Transfer Learning, to transfer the affordance representation to novel objects via composing affordance and novel object representations, and thus enable the detection of HOIs with novel objects. James J. Gibson defined the word affordance in <ref type="bibr" target="#b9">[10]</ref>. Object affordances are those action possibilities that are perceiveable by an actor <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref>, that is also the possibilities of Human-Object Interactions. In the early, Kjellstr et al. <ref type="bibr" target="#b19">[20]</ref> investigated learning the affordances of objects from human demonstration. Yao &amp; Li et al. <ref type="bibr" target="#b41">[42]</ref> presented a weak supervised approach to discover object funtionalities from HOI data in the environment where the person is interacting with musical instruments. Fouhey et al. <ref type="bibr" target="#b5">[6]</ref> introduced an approach to estimate functional surfaces by observing human actions. Recently, Fang et al. <ref type="bibr" target="#b4">[5]</ref> introduces Demo2Vec to learn interaction region and action label from online video. Differently, we demonstrate to learn a shared affordance representation with HOI model, and recognize object affordance via classifying the composite HOIs of shared affordance representations from existing HOIs and the target object representation. <ref type="bibr">Figure</ref> 2. An overview of affordance transfer learning or ATL for HOI detection. We first extract the human, object, and affordance features via the ROI-Pooling from the feature pyramids <ref type="bibr" target="#b28">[29]</ref>, respectively. Meanwhile, we also extract new object features from an additional object datasets using the same backbone network. After that, we concatenate the affordance and the object features (from HOI datasets) as the real HOIs. We also compose new HOIs using the affordance features and the object features extracted from additional object datasets, which transfer the affordance to novel objects. Both the composite HOIs and real HOIs share the same HOI classifier. In addition, human features and spatial pattern features are combined to construct the spatial HOI branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first give an overview of the proposed method, and then introduce the affordance transfer learning for HOI detection. Lastly, we describe the recognition of object affordances with the proposed HOI detection model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The motivation of affordance transfer learning is to transfer the affordance to novel objects for exploring unseen HOIs, i.e., combining the affordance representations and novel object representations. The affordance transfer learning meanwhile enables HOI network to recognize object affordance. Similar to <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref>, we utilize the popular two-stage HOI detection framework: 1) we first detect the objects in a given image using a common object detector (e.g., Faster-R-CNN <ref type="bibr" target="#b28">[29]</ref>); and 2) we then construct HOIs from object and affordance representations to perform HOI classification. The main framework of our affordance transfer learning is illustrated in <ref type="figure">Figure 2</ref>, which consists of three branches, spatial HOI, real HOI, and composite HOI. Inspired by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17]</ref>, we utilize the spatial HOI branch to further improve the HOI detection performance. Specifically, the spatial pattern representation consists of two 64 ? 64 binary feature maps to indicate hu-man and object relative positions, i.e., the pixels within the human (or object) bounding box are assigned the value 1. Both real HOIs and composite HOIs are constructed from object/affordance features and share the same HOI classifier <ref type="bibr" target="#b29">[30]</ref>, while the difference between them is that the objects in the composite HOIs are extracted from additional object image datasets. Furthermore, by transferring the shared affordance representations extracted from HOI samples to novel objects, we are also able to use the HOI detection model to recognize the affordance of novel objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Affordance Transfer Learning</head><p>The proposed affordance transfer learning first composes novel HOI samples between object representations from additional object images (e.g., images from COCO dataset <ref type="bibr" target="#b23">[24]</ref>) and decoupled affordance representation as illustrated in <ref type="figure">Figure 2</ref>, and then generalize the affordance representation to novel objects via jointly optimizing the network with the composite HOIs. With the additional objects, the affordance transfer learning effectively decouples the affordance representation from the scenes, and then enables the composition of affordance and novel objects to recognize the affordance of novel objects. In this subsection, we introduce how to efficiently compose new HOIs and remove invalid HOIs for affordance transfer learning.</p><p>Efficient HOI Composition. The label assignment for the composite HOIs can be integrated from the verb label (or the affordance label) and the object label. The object label l o is provided by the object datasets, and we obtain the verb label l v by decoupling the HOI label. Similar to <ref type="bibr" target="#b16">[17]</ref>, we decouple the HOI label space into a verb-HOI co-occurrence matrix A v ? R Nv?C a the object-HOI cooccurrence matrix A o ? R No?C , where N v , N o , and C indicate the numbers of verbs, objects and HOI categories, respectively. Here, both A v and A o are binary matrix. Given the one-hot HOI label y ? R C , we then obtain the verb label l v from yA T v . To compose a new HOI by the object l o and verb l v , we assign the label to the composite HOI as follows,?</p><formula xml:id="formula_0">= ( l o A o )&amp;(l v A v ),<label>(1)</label></formula><p>where &amp; indicates the element-wise "and" logical operation. Both l o A o and l v A v are binary vectors (i.e. HOI labels), which represent all possible HOIs corresponding to l o and l v , respectively. Therefore, the intersection between l o A o and l v A v indicates the label of the verb-object pair l v , l o , i.e., the label of the composite HOI. Invalid HOI Elimination. With additional object datasets, we can compose a large number of new types of HOI samples by combining object and affordance features. Considering a variety of object categories, there is nevertheless some invalid HOIs (e.g., "ride orange"), i.e., the invalid HOIs are out the space of ground truth HOI labels. Furthermore, the same verb might have different meanings in different scenes <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b42">43]</ref>, while the verbs in current HOI dataset (e.g., HICO-DET) mainly represents action (affordance) and are usually not ambiguous <ref type="bibr" target="#b8">[9]</ref>. Meanwhile, a variety of recent HOI detection methods do not distinguish the same affordance among different HOIs <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b27">28]</ref>. To this end, we also equally treat the same affordance from different HOIs for the evaluation of the transfer affordance learning. Following <ref type="bibr" target="#b16">[17]</ref>, we simply remove those HOIs, which is out of the HOI label space (e.g., "ride dog" in HICO-DET) as illustrated in the right part of <ref type="figure">Figure 2</ref>. In addition, the one-hot labels of those invalid HOIs are all zeros according to <ref type="bibr">Equation 1</ref>. That is, we can easily remove those composite HOIs according to the one-hot labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Object Affordance Recognition</head><p>In this subsection, we introduce how to infer the object affordance during the testing phase. Considering that we jointly optimize the decoupled components (i.e., object features and affordance features from object and HOI images) in HOI samples and novel object samples with affordance transfer learning, the proposed method thus is able to distinguish whether a novel object is combinable or not with a specific affordance (i.e., valid HOIs). Therefore, we design  <ref type="figure">Figure 3</ref>. An illustration of object affordance recognition with HOI network. Here, we use verb to represent affordance. We first construct an affordance feature bank from the decoupled affordance representations. For any object (e.g. strawberry), we extract the object feature by the Feature Extractor according to bounding box. Then, the object feature is combined with all affordances in the bank to input into HOI classifier for obtaining predicted interactions. The interactions are further converted into affordances (e.g. eatable).</p><p>a simple yet effective object affordance recognition method using the HOI detection model. Specifically, we first build an affordance feature bank as follows.</p><p>Affordance Feature Bank. We construct the affordance feature bank from HOI datasets (e.g. HICO-DET and HOI-COCO). In order to reduce storage space and computation, we randomly choose a maximun of M instances for each affordance in HICO-DET. In our experiment, M is 100. Then, we extract the features of those affordances to construct an off-the-shelf affordance feature bank.</p><p>Given an object feature extracted from the object image, we combine it with all affordances in the feature bank to obtain a set of HOIs. As illustrated in <ref type="figure">Figure 3</ref>, we obtain all HOI predictions from the HOI classifier. After that, we are able to convert all HOI predictions to affordance predictions according to the HOI-verb co-occurrence matrix A v . Specifically, we remove the predicted affordances whose label is not the same as the corresponding affordance labels in the feature bank. As a result, we obtain a list of affordances with many repeated elements. Let F i denotes the frequency (count) of the affordance i and S i indicates the number of affordance (or verb) i in the feature bank, we evaluate the probability of the affordance i as Fi Si .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Optimization and Inference</head><p>During the training stage, we train the proposed method with an unique loss L AT L for transferring the affordance to novel objects. Meanwhile, similar to <ref type="bibr" target="#b7">[8]</ref>, we also incorporate spatial pattern loss L sp to optimize the learning of the HOI spatial pattern representation. In addition, an HOI loss L hoi is used for HOI classification. Lastly, the overall training loss function is defined as follows,</p><formula xml:id="formula_1">L = L hoi sp + ? 1 L hoi + ? 2 L AT L ,<label>(2)</label></formula><p>where ? 1 and ? 2 are two hyper-parameters to balance different losses. Both the feature extractors and the HOI detection modules are jointly trained in an end-to-end manner. L hoi sp , L hoi , L AT L are binary cross entropy losses. During the testing stage, affordance transfer learning module is not necessary. Similar to <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17]</ref>, we predict the final HOIs with spatial HOI predictions and verb-object HOI predictions. Formally, given a human-object bounding box pair (b h , b o ), we predict the score S c h,o as s h ?s o ?s c hoi ?s c sp for each HOI category c ? 1, ..., C, where C denotes the total number of possible HOI types, s h and s o are the human and object detection scores respectively, s c sp represents the spatial HOI prediction score and s c hoi is the verb-object HOI prediction score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct a number of experiments to evaluate the our method for HOI detection using two HOI datasets: HICO-DET <ref type="bibr" target="#b2">[3]</ref> and HOI-COCO (built from V-COCO <ref type="bibr" target="#b12">[13]</ref>). Furthermore, we also evaluate the HOI detection model with affordance transfer learning for object affordance recognition on COCO dataset <ref type="bibr" target="#b23">[24]</ref>, HICO-DET dataset <ref type="bibr" target="#b2">[3]</ref>, and COCO classes and non-COCO classes in Object365 <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>HICO-DET <ref type="bibr" target="#b2">[3]</ref> dataset consists of 38,118 images in the training set and 9,658 test images over 600 types of interactions (80 object categories in COCO dataset and 117 unique verbs) with over 90,000 HOI instances.</p><p>HOI-COCO is built from the V-COCO dataset <ref type="bibr" target="#b12">[13]</ref>, which contains 10,346 images with 16,199 person instances. Each annotated person in V-COCO has binary labels for 26 different actions. V-COCO mainly focuses on verb recognition, and has limited object categories (only two). Thus we construct a new benchmark HOI-COCO for the evaluation of verb-object pairs as follows. We use 21 actions from all 26 actions in V-COCO (i.e., five non-interaction actions, "walk", "run, "smile", "stand" and "point ", are removed). As a result, we build HOI-COCO benchmark with 222 HOI categories over 21 verbs and 80 objects. Meanwhile, we use the same train/val split in V-COCO for HOI-COCO. Similar to HICO-DET <ref type="bibr" target="#b2">[3]</ref>, we evaluate the performance on HOI-COCO under three different settings: Full (222 types), Rare (97 types), and NonRare (115 types). The HOI type in Rare category contains less than 10 training instances, and the distribution of HOI categories is long-tailed.</p><p>COCO <ref type="bibr" target="#b23">[24]</ref> dataset is a widely-used benchmark for common object detection with 80 different object classes. Considering that both HICO-DET <ref type="bibr" target="#b2">[3]</ref> and HOI-COCO consist of the same object label sets to COCO, we thus directly incorporate the COCO dataset as the additional object dataset in our experiments.</p><p>Object365 <ref type="bibr" target="#b30">[31]</ref> is a recently proposed large-scale common object detection dataset with 365 object categories. The domain of Object365 is different from COCO <ref type="bibr" target="#b23">[24]</ref>. In detail, we select objects that are labeled as COCO classes from Object365 validation dataset to evaluate the affordance recognition of objects on new domain. Meanwhile, we choose 12 new types of objects and label manually the affordance of those objects according to the HICO-DET and HOI-COCO, respectively. Those objects are used to evaluate affordance recognition on new types of objects. See more details in supplementary materials.</p><p>Evaluation Metrics. We follow the standard evaluation metric <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39]</ref> and report mean average precision for HICO-DET dataset <ref type="bibr" target="#b2">[3]</ref> and HOI-COCO. A prediction is a true positive only when the detected human and object bounding boxes have IoUs larger than 0.5 with reference to ground truth, and the HOI category is accurately predicted. Object affordance recognition is a multiple label classification problem (i.e. an object usually has multiple affordances). Thus, we compare Precision, Recall and F1-Score for evaluating object affordance recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>For HICO-DET, similar to recent methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b16">17]</ref>, we use the object detector fine-tuned on HICO-DET, i.e. the detector provided in <ref type="bibr" target="#b16">[17]</ref>. For HOI-COCO, we directly use the object detector pre-trained on COCO. Besides, all HOI classifiers consist of two fully-connected layers with 1024 hidden units. To compare with recent methods on HICO-DET, we use two object images in each minibatch. On HOI-COCO, we only use one object image for evaluation. Besides, we also use an auxiliary verb loss <ref type="bibr" target="#b15">[16]</ref> to improve our baseline. During training, following <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17]</ref>, we augment the ground truth boxes via random crop and random shift. During inference, we keep human and objects with the score larger than 0.3 and 0.1 on HICO-DET respectively. Following <ref type="bibr" target="#b16">[17]</ref>, we set ? 1 = 2, ? 2 = 0.5 on HICO-DET, and ? 1 = 0.5, ? 2 = 0.5 on HOI-COCO, respectively. To prevent composite interactions from dominating the training of the model, we keep the number of composite interactions not more than the number of objects in each mini-batch by randomly sampling composite HOIs. We train the model for 1.2M iterations on HICO-DET dataset and 300K iterations on HOI-COCO with an initial learning rare of 0.01. For object affordance recognition, we use the actions of each HOI dataset as affordances and remove the "no interaction" categories on HICO-DET <ref type="table">Table 1</ref>. Comparison to recent state-of-the-art methods with finetuned detector on HICO-DET dataset <ref type="bibr" target="#b2">[3]</ref>. The content in brackets indicates the source of the object images. The last two rows are one-stage HOI detection results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">HOI Detection</head><p>HICO-DET. We report the performance on three different settings: Full (600 categories), Rare (138 categories) and NonRare (462 categories) in "Default" and "Known" modes on HICO-DET. As shown in <ref type="table">Table 1</ref>, the proposed method outperforms recent state-of-the-art methods among all categories. Furthermore, with better object detection results provided in <ref type="bibr" target="#b6">[7]</ref>, the performance of ATL dramatically increases to 28.53%. Meanwhile, we find ATL is more effective on Rare category. Specifically, when using the objects from the training set of HICO-DET, the proposed method is similar to VCL <ref type="bibr" target="#b16">[17]</ref> as shown in <ref type="table">Table 1</ref>. ATL also improves the baseline effectively based on One-Stage method. Here, the baseline is the model without compositional learning. Details of One-Stage method is provided in supplementary materials.</p><p>HOI-COCO. We find the proposed method has similar performance to VCL when using HOI-COCO as the source of object images in <ref type="table">Table 2</ref>. Here, we evaluate the performance of VCL on HOI-COCO dataset using the official code from <ref type="bibr" target="#b16">[17]</ref>. When using the COCO object dataset, the proposed method significantly improves the performance, especially on Rare categories, e.g., over 1.5% than VCL and 2.9% than the baseline, respectively. Meanwhile, the proposed method also gives a larger improvement than baseline in NonRare category comparing with VCL, suggesting that ATL also increases the diversity of HOIs via composing new samples. Furthermore, when using both HICO-DET and COCO to provide object images, we further improve the performance to 25.29%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Zero-Shot HOI detection</head><p>The proposed affordance transfer learning enables the detection of HOIs with novel objects due to the mecha- nism of composing HOI samples of unseen classes. Therefore, we evaluate the proposed method for zero-shot HOI detection on HICO-DET <ref type="bibr" target="#b2">[3]</ref>. We report the performance on two settings: 1) Unseen Composition and 2) Novel Object. Specifically, Unseen Composition means there are unseen HOIs in the test but the verbs and objects of the unseen HOIs exist in training data, while the objects of unseen HOIs in novel object HOI detection do not exist in training data. For compositional zero-shot learning, we follow <ref type="bibr" target="#b16">[17]</ref> to evaluate on rare-first unseen HOIs (firstly select tail HOIs in HICO-DET as unseen data) and non-rare first unseen HOIs (firstly select head HOIs in HICO-DET as unseen data). We evaluate zero-shot HOI detection on three categories: Unseen (120 categories), Seen (480 categories) and Full (600 categories). For novel object HOI detection, similar to <ref type="bibr" target="#b1">[2]</ref>, we choose 100 unseen categories (includes 12 unseen objects) and 500 seen categories. We choose the object detector provided in <ref type="bibr" target="#b16">[17]</ref> to compare fairly with <ref type="bibr" target="#b16">[17]</ref>.</p><p>Compositional Zero-Shot HOI Detection. In <ref type="table">Table 3</ref>, we find our approach effectively improves the non-rare first zero-shot HOI detection. Meanwhile, our approach achieves better result on seen category in rare first zeroshot HOI detection. Particularly, the affordances in tail part of HOIs are usually rare, the composite samples of tail HOIs with additional objects are much less than that of head <ref type="table">Table 4</ref>. Comparison of object affordance recognition with HOI network among different datasets (based on Mean average Precision). Val2017 is the validation 2017 of COCO <ref type="bibr" target="#b23">[24]</ref>. Subset of Object365 is the validation of Object365 <ref type="bibr" target="#b30">[31]</ref> with only COCO labels. Novel classes are selected from Object365 with non-COCO labels. Object means what object dataset we use. ATL ZS means novel object zeroshot HOI detection model in <ref type="table">Table 3</ref>  HOIs. Therefore, our approach achieves even worse result on unseen category. Novel Object HOI Detection. <ref type="table">Table 3</ref> demonstrates that transferring affordance representation to novel objects effectively facilitates the detection of unseen HOIs with novel objects. Here we use the network without affordance transfer learning as our baseline. We find using HICO-DET (remove HOIs with unseen objects) as object images even degrades the performance on unseen categories compared to the baseline because we compose massive seen HOI samples but not unseen HOI samples with HICO-DET. Besides, similar to <ref type="bibr" target="#b1">[2]</ref>, we use an generic object detector to enable HOI detection with novel object, which provides a strong baseline. While we only use the boxes of the detector (not use the object label predicted by detector), the performances of baseline and ATL (HICO-DET) on unseen category decrease to 0. However, ATL (COCO) still achieves 5.05% on unseen category. <ref type="table">Table 4</ref> shows ATL significantly improves the baseline by over 40 % in F1 score among all datasets with COCO categories, and by over 30% on novel object classes on HOI-COCO. On HICO-DET, ATL improves the baseline by nearly 10% among all categories in datasets with COCO categories, and by around 5% on novel object classes. Those experiments indicate that the affordance transfer learning via composing novel HOIs effectively disentangles the affordance and object representations from the scenes and endows the HOI network with the ability of affordance recognition. Noticeably, ATL ZS (COCO), that composing interactions from affordances and novel objects, largely improves the baseline model ATL ZS (HICO) on the 12 novel classes among all evaluation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Object Affordance Recognition</head><p>With the object images from the HOI dataset, our method </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Studies</head><p>The number of object images in each batch. <ref type="table">Table 5</ref> shows ATL achieves best performance with 2 object images. We think more object images increase the diversity of object features and balance the object distribution. However, too much object images also hampers the performance.</p><p>Object detector. Due to the domain shift between  HICO-DET and COCO, COCO detector usually achieves worse result. we thus use the same fine-tuned object detector as <ref type="bibr" target="#b16">[17]</ref>.  <ref type="table">Table 4</ref> illustrates the scene generalization of affordance and object representations. Noticeably, worse object detector largely hampers HOI detection in two-stage method. Thus, it is necessary to utilize better object detector for evaluating HOI detection, and ATL further improves HOI detection effectively with better object detector. Domain difference. From the large performance gap between different object detectors in <ref type="table" target="#tab_5">Table 6</ref>, we find the HICO-DET dataset has a different domain to COCO. <ref type="table" target="#tab_4">Table 7</ref> shows with the same number of object instances, COCO dataset improves the performance larger than HICO-DET dataset due to the domain difference on HOI-COCO. There is a similar trend in <ref type="table">Table 1 and Table 2</ref>. With the same COCO dataset, our method facilitates HOI detection on HOI-COCO dataset better than that on HICO-DET.  Affordance comparison with object detection results. Our method can also be applied to detected boxes of an object detector. For a robust comparison, we directly compare ATL with the object affordance result converted from object detection results according to the object affordance annotation (i.e. the ground truth affordances of an object category) on HICO-DET test set. Here we use the detected box of a COCO pretrained Faster-RCNN. We train our model on HOI-COCO dataset and COCO (2014) dataset, which has a same training set to COCO pretrained Faster-RCNN. <ref type="figure" target="#fig_2">Figure 4</ref> illustrates ATL achieves better affordance recognition results among different confidences. Meanwhile, ATL has better performance than object affordance detection when the confidence of detected box is lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Qualitative Results</head><p>We demonstrate the result of exploring unseen HOIs with novel objects in <ref type="figure" target="#fig_3">Figure 5</ref>. We find the baseline can not recognize the object at all, while the proposed method effectively detects the HOI with unseen objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduce a novel approach, affordance transfer learning or ATL, to transfer the affordance to novel objects via composing objects (from object images) and affordances (from HOI images) for HOI detection. ATL effectively facilitates HOI detection in long-tailed settings, especially for HOIs with novel objects. In addition, we devise a simple yet effective method to incorporate HOI detection model for object affordance recognition and ATL significantly improves the performance of the HOI detection model for object affordance recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>In this paper, we present an affordance transfer learning approach for Human-Object Interaction understanding and Object understanding in an unified way. More details and illustrations are introduced in appendix. We provide more examples between HOI and affordance in Section B. ATL for One-Stage HOI detection is illustrated in Section C. Section D contains more details. Section E shows more  comparison between object affordance recognition and the ablation study of the number of verbs on affordance recognition. We illustrate the Non-COCO classes that we select from Object365 on section F. Section G provides additional affordance results (mAP) and additional illustration of recent HOI approaches. Lastly, we compare prior approaches (i.e. VCL <ref type="bibr" target="#b16">[17]</ref>, FCL <ref type="bibr" target="#b15">[16]</ref>) and ATL in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Examples of HOI and Object Affordance</head><p>Images labeled with HOI annotations simultaneously show the affordance of the objects. Therefore, we can not only learn to detect HOIs, but also learn to recognize the affordance of the objects. By combining the affordance representation with various kinds of its corresponding objects, we enable the HOI model to recognize the affordance of novel objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Affordance Transfer Learning for One-Stage HOI detection</head><p>Current HOI approaches mainly include one-stage methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b34">35]</ref> and two-stage methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>. In main paper, we simultaneously evaluate ATL on both one-stage method and two-stage method. We implement ATL based on the code of <ref type="bibr" target="#b34">[35]</ref>, which implement HOI detection based on Faster-RCNN <ref type="bibr" target="#b28">[29]</ref>. In details, we use 2 object images and 4 HOI images for each batch with 2 GPUs. Here, we regard the concatenation of features extracted from union and human boxes with RoI Align separately as verb feature. We regard the feature extracted from object boxes as object feature. We compose novel HOIs from object features and verb features between HOI images and object images. Different from two-stage method, we also com- <ref type="table">Table 8</ref>. Additional Ablation study of object affordance recognition with HOI network among different number of object images on HICO-DET. Val2017 is the validation 2017 of COCO <ref type="bibr" target="#b23">[24]</ref>. Subset of Object365 is the validation of Object365 <ref type="bibr" target="#b30">[31]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Supplementary Description</head><p>. In the <ref type="table">Table 4</ref> (object affordance recognition) in paper, we illustrate the affordance recognition of novel classes in zero-shot HOI detection on HICO-DET. All objects in HICO-DET, Val2017, Object365 with COCO classes are from the 12 novel classes (unseen objects). In the Novel Classes category, those objects are still Non-COCO classes. Besides, we can use both COCO images and HOI images as object images in our experiments. But we do not find any improvement on HICO-DET. Thus in <ref type="table">Table 1</ref>, we do not include the result when we use two datasets as object images. It might be because there are nearly 900,000 object instances in COCO while HICO has only around 100, 000 object instances. For novel object zero-shot, there are too much many composite HOIs for seen HOIs, we thus remove some COCO object images for balancing the data. Besides, similar to <ref type="bibr" target="#b15">[16]</ref>, we also fuse HOI detection result to object detection result to improve the baseline. In addition, in our experiment, the baseline (ATL with only HICO data as object images) converges faster because the number of training data is much less than ATL (COCO). For simplicity, we directly fine-tune ATL (HICO) and ATL (COCO) models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Ablation Study</head><p>The effect of different number of object images on affordance recognition. <ref type="table">Table 8</ref> illustrates the comparison of object affordance recognition among different number of object images in the minibatch on HICO-DET dataset. We find ATL with two images in each batch apparently improves the performance of object affordance recognition compared to one image and three images among COCO categories. Moreover, we find with more object images in each batch, ATL further improves the affordance recognition performance on Non-COCO classes. This means with multiple object images, ATL has better generalization of affordance recognition to novel classes.</p><p>Union verb representation. Following <ref type="bibr" target="#b16">[17]</ref>, we extract verb representation from the union box in our experiments. The effect of union verb representation <ref type="bibr" target="#b16">[17]</ref> on HOI detection is relatively small. However, we notice the union verb representation has a significant effect on object affordance recognition. <ref type="table">Table 9</ref> illustrates the results of ATL on affordance recognition drops by over 5% on COCO, Object365 and HICO-DET dataset when extracting verb representation from human box. However, for the baseline model, the performance of affordance recognition improves when we extract verb representation from human box. This might because the union box also includes some noise information (the region out of human and object box), while the compositional approach facilitates the verb representation learning from union box, i.e. extract useful information for verb representation from the union box. Noticeably, the performances of the two verb representations on HOI detection are not very different. We think object affordance recognition also provides a benchmark for the evaluate The effect of the number of verbs on affordance recognition. In affordance recognition, we randomly choose M instances for those affordances with more than M instances in dataset and all instances for other affordances. We ablate M in <ref type="table">Table 10</ref> under the ATL model with COCO objects and our baseline. The baseline is the model without compositional learning. Besides, when we use different M , we also update S i . If we keep S i same as the number when M = 100, all results will be very small when M &lt; 100. <ref type="table">Table 10</ref> shows the number goes stable after 20. This means we do not need to store a large number of templates of affordance representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Non-COCO classes</head><p>For evaluating ATL on affordance recognition of unseen classes, we manually select 12 non-coco classes from ob-ject365: glove, microphone, american football, strawberry, flashlight, tape, baozi, durian, boots, ship, flower, basketball. The actions that we can act on those objects (i.e. affordance) on HOI-COCO and HICO-DET are list on <ref type="table">Table 11</ref> and <ref type="table" target="#tab_9">Table 12</ref> respectively.</p><p>We further provide some visual examples of the Non-COCO classes in <ref type="figure" target="#fig_6">Figure 7</ref>. ATL can recognize the affordance of those objects without being interacted by combining the affordance representation and those object features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Additional Results and Comparision</head><p>We find the metrics (Recall, Precision, F1) the paper (first version) uses is not much robust. F1 is sensitive to the confidence. Thus, we further evaluate the affordance recognition in <ref type="table">Table 13</ref> by Mean average Precision (mAP) (%). <ref type="table">Table 13</ref> shows the compositional learning approach consistently improves the baseline among all categories.</p><p>Due to the limitation of space in main paper. Other recent HOI detection methods are provided in <ref type="table" target="#tab_5">Table 16</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Discussion with Prior Approaches</head><p>ATL extends VCL <ref type="bibr" target="#b16">[17]</ref> by composing verbs and objects from object detection datasets which do not have HOI annotations. ATL presents a way to explore a broader source of data for HOI detection. Meanwhile, ATL finds that the HOI network trained with compositional learning can be simultaneously applied to affordance recognition. Meanwhile, ATL shows with more data, ATL can improve the generalization of affordance recognition on new dataset.</p><p>Prior to ATL, Fabricated Compositional Learning <ref type="bibr" target="#b15">[16]</ref> was presented to fabricated objects to ease the open longtailed issue for HOI detection. FCL <ref type="bibr" target="#b15">[16]</ref> inspires our to compose novel HOIs from verb features from HOI images and object features from external object datasets. Compared to VCL <ref type="bibr" target="#b16">[17]</ref> and ATL <ref type="bibr" target="#b16">[17]</ref>, FCL <ref type="bibr" target="#b16">[17]</ref> is more flexible to generate balanced objects for each verb, and thus achieves better performance on some zero-shot settings. However, FCL also has some limitations. Although FCL achieves similar <ref type="table">Table 13</ref>. Comparison of object affordance recognition with HOI network among different datasets. Val2017 is the validation 2017 of COCO <ref type="bibr" target="#b23">[24]</ref>. Object365 is the validation of Object365 <ref type="bibr" target="#b30">[31]</ref> with only COCO labels. Novel classes are selected from Object365 with non-COCO labels. Object means what object dataset we use. ATL ZS means novel object zero-shot HOI detection model in <ref type="table">Table 3</ref>  even better performance to ATL in HOI detection, <ref type="table">Table 13</ref> shows the model of FCL in fact is unable to recognize affordance. Besides, <ref type="table">Table 14</ref> further shows although FCL <ref type="bibr" target="#b16">[17]</ref> achieves also good results on Novel Object HOI detection  with a generic object detector, the results of FCL <ref type="bibr" target="#b16">[17]</ref> on Unseen category drop to 0 without generic object detector. We further illustrates the complementary between FCL and VCL in <ref type="table">Table 15</ref>. Here, we fuse the prediction results of the two model to evaluate the complementary. We find this can largely improves the result.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Comparison of object affordance recognition (F1) between ATL and the conversion from object detection results on HICO-DET. Confidence is the object detection confidence for choosing object boxes. Red is our method and Blue is the conversion from object detection results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of unseen object zero-shot detection result (top 5) between the proposed method and Baseline. The correct results are highlighted in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Examples about HOI and Affordance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Examples of Non-COCO classes and its affordances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Comparison to recent state-of-the-art methods on HOI-COCO dataset. Comparison of Zero Shot Detection results of our proposed method. UC means unseen composition HOI detection. NO means novel object HOI detection. * means we only use the boxes of the detection results. Here, the baseline means we do not use affordance transfer learning (i.e. without LAT L).</figDesc><table><row><cell>Method</cell><cell>object data</cell><cell></cell><cell cols="3">Full Rare NonRare</cell></row><row><cell>Baseline</cell><cell>-</cell><cell></cell><cell cols="2">22.86 6.87</cell><cell>35.27</cell></row><row><cell>VCL [17]</cell><cell cols="2">HOI-COCO</cell><cell cols="2">23.53 8.29</cell><cell>35.36</cell></row><row><cell>ATL</cell><cell cols="2">HOI-COCO</cell><cell cols="2">23.40 8.01</cell><cell>35.34</cell></row><row><cell>ATL</cell><cell>COCO</cell><cell></cell><cell cols="2">24.84 9.79</cell><cell>36.51</cell></row><row><cell>ATL</cell><cell cols="4">COCO, HICO-DET 25.29 9.85</cell><cell>37.27</cell></row><row><cell>Method</cell><cell cols="5">Type Unseen Seen Full</cell></row><row><cell cols="2">Shen et al. [32]</cell><cell>UC</cell><cell>5.62</cell><cell>-</cell><cell>6.26</cell></row><row><cell>FG [2]</cell><cell></cell><cell>UC</cell><cell cols="3">10.93 12.60 12.26</cell></row><row><cell cols="2">VCL [17] (rare first)</cell><cell>UC</cell><cell cols="3">10.06 24.28 21.43</cell></row><row><cell cols="2">ATL (rare first)</cell><cell>UC</cell><cell>9.18</cell><cell cols="2">24.67 21.57</cell></row><row><cell cols="3">VCL [17] (non-rare first) UC</cell><cell cols="3">16.22 18.52 18.06</cell></row><row><cell cols="2">ATL (non-rare first)</cell><cell>UC</cell><cell cols="3">18.25 18.78 18.67</cell></row><row><cell>FG [2]</cell><cell></cell><cell>NO</cell><cell cols="3">11.22 14.36 13.84</cell></row><row><cell>Baseline</cell><cell></cell><cell>NO</cell><cell cols="3">12.84 20.63 19.33</cell></row><row><cell cols="2">ATL (HICO-DET)</cell><cell>NO</cell><cell cols="3">11.35 20.96 19.36</cell></row><row><cell cols="2">ATL (COCO)</cell><cell>NO</cell><cell cols="3">15.11 21.54 20.47</cell></row><row><cell>Baseline*</cell><cell></cell><cell>NO</cell><cell>0.00</cell><cell cols="2">14.13 11.77</cell></row><row><cell cols="2">ATL (HICO-DET)*</cell><cell>NO</cell><cell>0.00</cell><cell cols="2">13.67 11.39</cell></row><row><cell cols="2">ATL (COCO)*</cell><cell>NO</cell><cell>5.05</cell><cell cols="2">14.69 13.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>on HICO-DET. For ATL ZS , we show the results of the 12 classes of novel objects in Val2017, Subset of Object365 and HICO-DET. 71.79 72.15 68.60 67.52 65.82 87.98 82.59 83.84 54.75 35.85 40.43 ATL HOI HOI 80.71 72.79 74.44 71.76 67.34 67.13 90.29 83.21 85.30 58.73 37.75 42.75 ATL HOI COCO 90.94 87.33 87.65 82.95 82.13 80.80 93.35 90.77 91.02 53.65 40.94 43.57</figDesc><table><row><cell>Method</cell><cell cols="2">HOI Data Object</cell><cell>Val2017 of COCO Rec Prec F1</cell><cell>Subset of Object365 Rec Prec F1</cell><cell>HICO-DET Rec Prec</cell><cell>F1</cell><cell>Novel classes Rec Prec</cell><cell>F1</cell></row><row><cell>Baseline</cell><cell>HOI</cell><cell>-</cell><cell cols="6">28.62 32.34 27.08 21.75 22.20 19.83 36.64 49.83 37.67 12.39 8.63 9.62</cell></row><row><cell cols="9">VCL [17] HOI 76.93 Baseline HOI HICO -8.11 29.21 11.81 6.77 26.1 9.97 8.11 29.21 15.55 8.12 15.87 8.78</cell></row><row><cell>VCL [17]</cell><cell>HICO</cell><cell cols="7">HICO 9.63 43.62 14.89 10.66 38.69 15.77 10.76 53.54 16.82 7.81 22.63 11.02</cell></row><row><cell>ATL</cell><cell>HICO</cell><cell cols="7">HICO 14.01 46.45 20.24 17.71 50.92 24.61 15.54 52.25 22.54 12.78 28.8 16.78</cell></row><row><cell>ATL</cell><cell>HICO</cell><cell cols="7">COCO 33.69 79.54 44.32 28.25 63.56 35.24 30.27 73.53 40.31 12.41 14.56 12.86</cell></row><row><cell>ATL ZS</cell><cell>HICO</cell><cell cols="3">HICO 4.28 22.96 6.98 3.54 19.35 5.8</cell><cell cols="4">6.02 32.22 9.93 5.02 11.63 6.79</cell></row><row><cell>ATL ZS</cell><cell>HICO</cell><cell cols="7">COCO 19.41 66.70 29.01 15.57 55.58 23.49 19.36 67.55 28.81 14.00 28.60 18.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Illustration of the number of object images in each batch on HICO-DET dataset. Illustration of the effect of different object detectors on HOI detection in HICO-DET. Fine-tuned detector is provided in<ref type="bibr" target="#b16">[17]</ref>. GT means ground truth boxes. The last column is the detection mAP on HICO-DET test dataset. DET, the proposed method has a better performance than VCL<ref type="bibr" target="#b16">[17]</ref>, while the two methods have similar performance on HOI detection, as shown inTable 1. We find Recall, Precision and F1-Score are sensitive to the threshold. Thus, we further illustrates the Mean average Precision results of all models in Supplementary Materials.</figDesc><table><row><cell></cell><cell cols="5">#Images Full Rare NonRare</cell></row><row><cell></cell><cell>1</cell><cell cols="2">24.07 18.17</cell><cell cols="2">25.83</cell></row><row><cell></cell><cell>2</cell><cell cols="2">24.50 18.53</cell><cell cols="2">26.28</cell></row><row><cell></cell><cell>3</cell><cell cols="2">24.19 17.33</cell><cell cols="2">26.24</cell></row><row><cell>Model</cell><cell>Detector</cell><cell>Full</cell><cell cols="3">Rare NonRare mAP</cell></row><row><cell>Baseline</cell><cell>COCO</cell><cell cols="3">21.07 16.79</cell><cell>22.35</cell><cell>20.82</cell></row><row><cell>ATL</cell><cell>COCO</cell><cell cols="3">20.08 15.57</cell><cell>21.43</cell><cell>20.82</cell></row><row><cell cols="5">Baseline Fine-tuned 23.44 16.80</cell><cell>25.43</cell><cell>30.79</cell></row><row><cell>ATL</cell><cell cols="4">Fine-tuned 24.50 18.53</cell><cell>26.28</cell><cell>30.79</cell></row><row><cell>Baseline</cell><cell>GT</cell><cell cols="3">43.32 33.84</cell><cell>46.15</cell><cell>100</cell></row><row><cell>ATL</cell><cell>GT</cell><cell cols="3">44.27 35.52</cell><cell>46.89</cell><cell>100</cell></row><row><cell cols="6">is similar to VCL [17] on HOI-COCO dataset, because both</cell></row><row><cell cols="6">two methods compose HOI samples between two images.</cell></row><row><cell>On HICO-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Illustration of effect of domain shift on ATL between object images and HOI images on HOI-COCO dataset. Sub-COCO is a subset of COCO images that we randomly choose the same number of object instances to the objects of HICO-DET from COCO dataset.</figDesc><table><row><cell cols="3">Method Object images Full Rare NonRare</cell></row><row><cell>ATL</cell><cell>HICO-DET 24.21 9.52</cell><cell>35.61</cell></row><row><cell>ATL</cell><cell>Sub-COCO 24.74 9.60</cell><cell>36.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>illustrates better detected object boxes improves the performance largely. Meanwhile, we find ATL is apparently sensitive to worse boxes. Under worse object detector (i.e. COCO detector), ATL does not improve the result. It might be because composing affordance features and object features from additional images results in poor generalization to worse boxes. When we transfer affordance representation to objects from a large number of additional images via composing novel HOI samples, we improve the scene generalization (i.e. the model generalizes to novel scenes) of the affordance representation learning, while degrading the generalization to worse object boxes on HICO-DET test set. The object affordance recognition in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>with only COCO labels. Novel classes are selected from Object365 with non-COCO labels. Object means what object dataset we use. The content in parentheses indicates the number of images in each batch. 79.54 44.32 28.25 63.56 35.24 30.27 73.53 40.31 12.41 14.56 12.86 ATL (3) HICO COCO 27.36 78.21 38.12 21.84 53.57 28.25 13.85 57.42 20.94 12.15 26.07 15.56</figDesc><table><row><cell cols="3">Method HOI Data Object</cell><cell cols="3">Val2017 of COCO Rec Prec F1</cell><cell cols="2">Subset of Object365 Rec Prec F1</cell><cell>HICO-DET Rec Prec</cell><cell>F1</cell><cell>Novel classes Rec Prec</cell><cell>F1</cell></row><row><cell>ATL (1)</cell><cell>HICO</cell><cell cols="6">COCO 16.63 62.91 24.73 12.47 39.92 17.45 12.47 52.45 18.66 7.30 18.44 9.73</cell></row><row><cell cols="8">ATL (2) COCO 33.69 Table 9. Illustration of union verb representation on object affordance recognition. w/o union verb means we extract verb representation HICO</cell></row><row><cell cols="8">from the human box. Val2017 is the validation 2017 of COCO [24]. Object365 is the validation of Object365 [31] with only COCO labels.</cell></row><row><cell cols="8">Novel classes are selected from Object365 with non-COCO labels. Object means what object dataset we use. All results are reported by</cell></row><row><cell cols="3">Mean average Precision (mAP)(%).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell cols="4">HOI Data Object Val2017 Object365 HICO-DET Novel classes</cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell></cell><cell>HICO</cell><cell>-</cell><cell></cell><cell>19.71</cell><cell>17.86</cell><cell>23.18</cell><cell>6.80</cell></row><row><cell></cell><cell cols="3">Baseline w/o union verb</cell><cell>HICO</cell><cell>-</cell><cell></cell><cell>28.16</cell><cell>25.14</cell><cell>37.88</cell><cell>8.38</cell></row><row><cell></cell><cell>ATL</cell><cell></cell><cell></cell><cell>HICO</cell><cell cols="2">HICO</cell><cell>52.01</cell><cell>50.94</cell><cell>59.44</cell><cell>15.64</cell></row><row><cell></cell><cell cols="3">ATL w/o union verb</cell><cell>HICO</cell><cell cols="2">HICO</cell><cell>44.47</cell><cell>45.35</cell><cell>52.76</cell><cell>15.09</cell></row><row><cell cols="7">pose object features and verb features between HOI images</cell></row><row><cell cols="7">(i.e. VCL [17]). Meanwhile, in one-stage method, we di-</cell></row><row><cell cols="7">rectly predict 117 verbs, which are further combined with</cell></row><row><cell cols="7">object detection result to construct HOI prediction. Be-</cell></row><row><cell cols="7">sides, during optimization, we keep the object detection op-</cell></row><row><cell cols="7">timization for object images. Baseline is the model without</cell></row><row><cell cols="7">compositional learning loss. Code is available at https:</cell></row><row><cell cols="6">//github.com/zhihou7/HOI-CL-OneStage.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 .Table 11</head><label>1011</label><figDesc>The effect of different number of verbs in affordance feature bank. Mean average Precision (mAP) (%) is reported. Dataset means the evaluation object dataset. HICO-DET means the test set of HICO-DET. Val2017 means the validation set of COCO2017. 15.90 17.69 18.74 19.25 19.67 19.71 ATL (COCO) Val2017 52.98 53.74 55.40 55.19 54.88 55.77 56.05 Baseline HICO-DET 14.77 18.30 20.22 21.70 22.21 23.00 23.18 ATL (COCO) HICO-DET 56.04 58.03 59.14 57.84 56.61 57.23 57.41</figDesc><table><row><cell></cell><cell>#M</cell><cell>Dataset</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>40</cell><cell>80</cell><cell>100</cell></row><row><cell cols="4">Baseline 13.39 . Affordances of Non-COCO classes from Object365 on Val2017</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HOI-COCO.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>name</cell><cell></cell><cell>verbs/affordances</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>glove</cell><cell></cell><cell>carry, throw, hold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>microphone</cell><cell cols="3">talk on phone, carry, throw, look, hold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>american football</cell><cell cols="2">kick, carry, throw, look, hit, hold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>strawberry</cell><cell cols="2">cut, eat, carry, throw, hold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>flashlight</cell><cell></cell><cell>carry, throw, hold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tape</cell><cell></cell><cell>carry, throw, hold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>baozi</cell><cell></cell><cell>eat, carry, look, hold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>durian</cell><cell></cell><cell>eat, carry, hold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>boots</cell><cell></cell><cell>carry, hold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ship</cell><cell></cell><cell>ride, sit, lay, look</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>flower</cell><cell></cell><cell>look, hold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>basketball</cell><cell></cell><cell>throw, hold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">of verb (action) representation learning.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 12 .</head><label>12</label><figDesc>Affordances of Non-COCO classes from Object365 on HICO-DET.</figDesc><table><row><cell>name</cell><cell>verbs/affordances</cell></row><row><cell>glove</cell><cell>buy, carry, hold, lift, pick up, wear</cell></row><row><cell>microphone</cell><cell>carry, hold, lift, pick up</cell></row><row><cell cols="2">american football block, carry, catch, hold, kick, lift, pick up, throw</cell></row><row><cell>strawberry</cell><cell>buy, eat, hold, lift, move</cell></row><row><cell>flashlight</cell><cell>buy, hold, lift, pick up</cell></row><row><cell>tape</cell><cell>buy, hold, lift, pick up</cell></row><row><cell>baozi</cell><cell>buy, eat, hold, lift, pick up</cell></row><row><cell>durian</cell><cell>buy, hold, lift, pick up</cell></row><row><cell>boots</cell><cell>buy, hold, lift, pick up, wear</cell></row><row><cell>ship</cell><cell>adjust, board</cell></row><row><cell>flower</cell><cell>buy, hold, hose, lift, pick up</cell></row><row><cell>basketball</cell><cell>block, hold, kick, lift, pick up, throw</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 14 .Table 15 .Table 16 .</head><label>141516</label><figDesc>on HICO-DET. For ATL ZS , we show the results of the 12 classes of novel objects in Val2017, Subset of Object365 and HICO-DET. All results are reported by Mean average Precision (mAP)(%). Comparison of Zero Shot Detection results of between FCL<ref type="bibr" target="#b15">[16]</ref> and ATL. NO means novel object HOI detection. * means we only use the boxes of the detection results. Evaluation of the complementary between ATL and FCL. We use the released model of FCL<ref type="bibr" target="#b15">[16]</ref>. Additional Illustration of recent HOI detection approaches.RN  [21] 21.34 18.53 22.18 23.69 20.64 24.60 IP-Net [37] 19.56 12.79 21.58 22.05 15.77 23.92 PPDM [23] 21.73 13.78 24.10 24.58 16.65 26.84 Kim et al. [18] 17.58 11.72 19.33 19.76 14.68 21.27 ACP [19] 20.59 15.92 21.98 ---PD-Net [43] 20.81 15.90 22.28 24.78 18.88 26.54 FCMNet [25] 20.41 17.34 21.56 22.04 18.97 23.12 VCL [17] 23.63 17.21 25.55 25.98 19.12 28.03 DRG [7] 24.53 19.47 26.04 27.98 23.11 29.43 ATL (COCO) V CL 24.50 18.53 26.28 27.23 21.27 29.00 ATL (COCO) DRG 28.53 21.64 30.59 31.18 24.15 33.29</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Method</cell><cell cols="4">HOI Data Object Val2017 Object365 HICO-DET Novel classes</cell></row><row><cell></cell><cell></cell><cell cols="2">Baseline</cell><cell></cell><cell>HOI</cell><cell>-</cell><cell>31.91</cell><cell>26.16</cell><cell>44.00</cell><cell>14.27</cell></row><row><cell></cell><cell></cell><cell cols="2">FCL [16]</cell><cell></cell><cell>HOI</cell><cell>-</cell><cell>41.89</cell><cell>32.20</cell><cell>55.95</cell><cell>18.84</cell></row><row><cell></cell><cell></cell><cell cols="2">VCL [17]</cell><cell></cell><cell>HOI</cell><cell>HOI</cell><cell>76.43</cell><cell>69.04</cell><cell>86.89</cell><cell>32.36</cell></row><row><cell></cell><cell></cell><cell>ATL</cell><cell></cell><cell></cell><cell>HOI</cell><cell>HOI</cell><cell>76.52</cell><cell>69.27</cell><cell>87.20</cell><cell>34.20</cell></row><row><cell></cell><cell></cell><cell>ATL</cell><cell></cell><cell></cell><cell>HOI</cell><cell cols="2">COCO 90.84</cell><cell>85.83</cell><cell>92.79</cell><cell>36.28</cell></row><row><cell></cell><cell></cell><cell cols="2">Baseline</cell><cell cols="2">HICO</cell><cell>-</cell><cell>19.71</cell><cell>17.86</cell><cell>23.18</cell><cell>6.80</cell></row><row><cell></cell><cell></cell><cell cols="2">FCL [16]</cell><cell cols="2">HICO</cell><cell>-</cell><cell>25.11</cell><cell>25.21</cell><cell>37.32</cell><cell>6.80</cell></row><row><cell></cell><cell></cell><cell cols="2">VCL [17]</cell><cell cols="2">HICO</cell><cell>HICO</cell><cell>36.74</cell><cell>35.73</cell><cell>43.15</cell><cell>12.05</cell></row><row><cell></cell><cell></cell><cell>ATL</cell><cell></cell><cell cols="2">HICO</cell><cell>HICO</cell><cell>52.01</cell><cell>50.94</cell><cell>59.44</cell><cell>15.64</cell></row><row><cell></cell><cell></cell><cell>ATL</cell><cell></cell><cell cols="2">HICO</cell><cell cols="2">COCO 56.05</cell><cell>40.83</cell><cell>57.41</cell><cell>8.52</cell></row><row><cell></cell><cell></cell><cell>ATL ZS</cell><cell></cell><cell cols="2">HICO</cell><cell>HICO</cell><cell>24.21</cell><cell>20.88</cell><cell>28.56</cell><cell>12.26</cell></row><row><cell></cell><cell></cell><cell>ATL ZS</cell><cell></cell><cell cols="2">HICO</cell><cell cols="2">COCO 35.55</cell><cell>31.77</cell><cell>39.45</cell><cell>13.25</cell></row><row><cell>Method</cell><cell></cell><cell cols="5">Type Unseen Seen Full</cell></row><row><cell>FCL [16]</cell><cell></cell><cell>NO</cell><cell cols="4">15.38 21.30 20.32</cell></row><row><cell cols="2">ATL (COCO)</cell><cell>NO</cell><cell cols="4">15.11 21.54 20.47</cell></row><row><cell>FCL [16]</cell><cell></cell><cell>NO</cell><cell cols="2">0.00</cell><cell cols="2">13.71 11.43</cell></row><row><cell cols="3">ATL (COCO)* NO</cell><cell cols="2">5.05</cell><cell cols="2">14.69 13.08</cell></row><row><cell>Method</cell><cell></cell><cell cols="5">Full Rare Non-Rare</cell></row><row><cell cols="2">FCL [16]</cell><cell cols="3">24.68 20.03</cell><cell cols="2">26.07</cell></row><row><cell cols="5">ATL (COCO) 24.50 18.53</cell><cell cols="2">26.28</cell></row><row><cell cols="2">FCL + ATL</cell><cell cols="3">25.63 21.18</cell><cell cols="2">26.95</cell></row><row><cell>Method</cell><cell cols="7">Default Full Rare NonRare Full Rare NonRare Known Object</cell></row><row><cell>FG [2]</cell><cell cols="4">21.96 16.43 23.62</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VSGNet [33]</cell><cell cols="4">19.80 16.05 20.91</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DJ-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th symposium on operating systems design and implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Detecting human-object interactions via functional generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Sai Saketh Rambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ieee winter conference on applications of computer vision (wacv)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1017" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Demo2vec: Reasoning object affordances from online videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">People watching: Human actions as a cue for single view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vincent Delaitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="259" to="274" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Drg: Dual relation graph for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ican: Instancecentric attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Disambiguating visual verbs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spandana</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The ecological approach to visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8359" to="8367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Observing human-object interactions: Using spatial and functional compatibility for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1775" to="1789" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nofrills human-object interaction detection: Factorization, layout encodings, and training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Visual affordance and function understanding: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Hassanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Tahtali</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detecting human-object interaction via fabricated compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Baosheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021. 5</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual compositional learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Uniondet: Union-level detector towards real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Detecting human-object interactions with action co-occurrence priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Visual object-action recognition: Inferring object affordances from human demonstration. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Kjellstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danica</forename><surname>Kragi?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detailed 2d-3d joint representation for human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10166" to="10175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transferable interactiveness prior for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ppdm: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Amplifying key cues for human-object-interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Consnet: Learning consistency graph for zero-shot human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The Design of Everyday Things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Basic Books, Inc., USA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detecting unseen visual relations using analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recognition using visual phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Amin</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1745" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scaling human-object interaction recognition through zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vsgnet: Spatial attention network for detecting human object interactions using graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oytun</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pose-aware multi-level feature network for human object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9469" to="9478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discovering human interactions with novel objects via zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim-Hui</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yap-Peng</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep contextual attention for humanobject interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Haris</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorma</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laaksonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5694" to="5702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning human-object interaction detection using interaction points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Interact as you intend: Intention-driven human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions with knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recognizing human-object interactions in still images by modeling the mutual context of objects and human poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1691" to="1703" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><forename type="middle">Lai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International conference on computer vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1331" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Discovering object functionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Polysemy deciphering network for robust human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
