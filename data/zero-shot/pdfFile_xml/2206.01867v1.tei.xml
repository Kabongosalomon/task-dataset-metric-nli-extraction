<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPGNet: Spatial Projection Guided 3D Human Pose Estimation in Low Dimensional Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Wang</surname></persName>
							<email>zihan6@ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimin</forename><surname>Chen</surname></persName>
							<email>ruimin6@ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanfang</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anup</forename><surname>Basu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SPGNet: Spatial Projection Guided 3D Human Pose Estimation in Low Dimensional Space</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Machine Learning for Multimedia ? Pattern processing</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a method SPGNet for 3D human pose estimation that mixes multi-dimensional re-projection into supervised learning. In this method, the 2D-to-3D-lifting network predicts the global position and coordinates of the 3D human pose. Then, we re-project the estimated 3D pose back to the 2D key points along with spatial adjustments. The loss functions compare the estimated 3D pose with the 3D pose ground truth, and re-projected 2D pose with the input 2D pose. In addition, we propose a kinematic constraint to restrict the predicted target with constant human bone length. Based on the estimation results for the dataset Human3.6M, our approach outperforms many state-of-the-art methods both qualitatively and quantitatively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D human shape and posture estimation from a single image or video is a fundamental topic in computer vision. Unfortunately, it is not easy to estimate 3D body shape and posture directly from monocular images without any 3D information. The problem of 3D human shape and posture estimation can be defined as giving images as the input, and generating a 3D skeleton as the output. A typical 3D skeleton consists of 3D points for 17 joints. Mathematically it can be written as a mapping function f (M ) = x, where M is a fixed-sized matrix representing the image input and x is a matrix representing the 3D joints (size is <ref type="bibr" target="#b16">(17,</ref><ref type="bibr" target="#b2">3)</ref> in our case). Generally, f is applied to each frame of the video.</p><p>Before deep learning was widely used, massive 3D labeled data and 3D parameters with prior knowledge were necessary to deal with this problem <ref type="bibr" target="#b0">[1]</ref>. After introducing the deep learning strategy, some approaches extract the 3D human pose based on the input image directly without any intermediate stage. Some of the existing strategies rely on convolutional neural networks to learn visual representations successfully from a very large dataset <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b18">19]</ref>. However, recent research has shifted to two-stage approaches. The 2D key points detection algorithm is first used to acquire the 2D poses from images. Then, 2D-to-3D pose lifting is applied as the second stage <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15]</ref>. Existing methods have focused on optimizing the loss functions or neural network structures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>. arXiv:2206.01867v1 [cs.CV] 4 Jun 2022 <ref type="figure">Fig. 1</ref>: General workflow of our SPGNet, take 2D key points as input, and estimate the 3D pose with its global position. The 2D re-projection combines both estimations and loss to optimize the model. Inspired by the cycle consistency in unsupervised learning, some approaches re-map the predicted 3D poses to 2D poses in a semi-supervised learning framework <ref type="bibr" target="#b19">[20]</ref>. Like following the CycleGAN Loss, the network is designed incorporating two components. One is the mapping from 2D to 3D. Another one is remapping 3D to 2D and comparing the re-projected 2D poses with the 2D input. However, the results for unlabeled videos are relatively unremarkable and the training process is relatively slow. In our work, we also extract the intermediate 2D pose. However, we do not intuitively deploy the CycleGAN semi-supervised learning framework, which leads us to avoid the abovementioned drawbacks.</p><p>We design SPGNet, a novel neural network architecture that combines traditional supervised learning with a spatial projection that re-projects the predicted 3D poses to 2D poses. Instead of traditional loss functions based on the ground truth and 3D pose output, we introduce the 2D MPJPE loss (defined as Equation 3). As shown in <ref type="figure">Fig. 1</ref>, SPGNet computes the 2D projection of 3D poses output with the estimated global position. The 2D MPJPE loss minimizes the re-projected 2D poses with the 2D poses input, which reuses the 2D poses input during the learning process. This approach increases the accuracy of 3D pose prediction. Overall, our main contributions are:</p><p>1. Introducing an adaptive supervised training framework for 3D human pose estimation under the category of 2D-to-3D lifting approach. 2. Exploiting the 2D pose input efficiently and improving robustness by presenting a re-projection loss, which is based on global pose estimation from 2D poses and the 2D poses themself.</p><p>Our model achieves 45.3 millimeters accuracy in Protocol 1 and 35.7 millimeters in Protocol 2 in the Human3.6M dataset, which are 0.6% and 1.4% relative improvements over previous approaches <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous research on 3D Human Pose Estimation can be classified into two main categories. The first category extracts the 3D human pose based on the input image directly without any intermediate stage. Under this category, some recent approaches rely on convolutional neural networks to learn visual representations successfully from very large datasets. The predicted accuracy has increased considerably during the past few years <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b18">19]</ref>. The second category consists of two stages, namely 2D key points detection from video and 2D-to-3D pose lifting <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">2D Key Joint detection</head><p>Before estimating 3D human pose, many methods require labeled key joints. Inaccurate key joint labels may cause pose prediction to fail. Initially, 2D human pose was estimated based on Deep Neural Networks (DNNs), later Convolutional Neural Networks (CNNs) have shown more advantages in 2D key joint detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b24">25]</ref>. CNN-based models have outstanding performances in extreme test conditions (body occlusion or low-resolution images). A heatmap is generated to show the possibilities of a specific joint shown in an image and the estimate is refined to improve localization. Reusing the hidden layer in the CNN-based model, Tompson et al. proposed a method that uses heatmap, works as a regularizer to find their output, and increases the accuracy <ref type="bibr" target="#b23">[24]</ref>. A similar method proposed by Yang et al. uses an end-to-end mixture of parts model <ref type="bibr" target="#b28">[29]</ref>. In their method, the probability is calculated by the softmax function and a Max-sum algorithm is applied between pose parts. The pose machine uses multi-stage differentiable iterations to the joint on the heat-map to finally converge to one solution <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b15">16]</ref>. The pose machine combines the previous output and the updated prediction for the same input image in each subsequent stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image to 3D pose</head><p>Without estimating the 2D human pose, the 3D pose can be constructed directly through an image. This method minimizes the effects of error prediction during the 2D pose estimation, and in general, it can be more robust <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b8">9]</ref>. Additionally, it eliminates the limitation of the unlabeled image used for training <ref type="bibr" target="#b29">[30]</ref>. One of the methods uses Pose Orientation Net (PONet) and generates heat maps: limb confidence maps and 3D orientation maps. With these heat maps, the model uses a fixed-length skeleton to match with the 3D orientation maps and complete the missing limbs in a 3D pose using sub-networks <ref type="bibr" target="#b25">[26]</ref>. For a more specific pose, Ruiz et al. used three loss functions for each angle's rotation, with classification and regression <ref type="bibr" target="#b20">[21]</ref>. Another approach proposed by Yang et al. uses the Generative Adversarial Networks (GANs) to directly predict the 3D pose using unlabeled images in outdoor scenarios <ref type="bibr" target="#b29">[30]</ref>. Therefore, skipping 2D pose detection provides a solution for unpaired 2D-to-3D data training <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">2D-to-3D pose lifting</head><p>3D pose estimation can be lifted from 2D human pose joints <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18]</ref>. During the training, some approaches directly using 2D human pose ground truth as input <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b4">5]</ref>. This approach ensures the accuracy of the inputs and distraction is minimized. Moreover, 2D label on the image is easier to obtain. Some datasets, like Human3.6M and HumanEval, contain multiple 2D views, and EpipolarPose with another method estimate 3D pose from different directions at the same frame. The mixture of the multi-views provides consistency loss and recovers the pose via triangulation <ref type="bibr" target="#b10">[11]</ref>. The model can be optimized by blending multiple 2D views into the same 3D pose. Depth prediction is important for some 2D-to-3D approaches. The simpler method uses binary ordinal depth relation prediction, while other have explicit depth prediction on each joint <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this section, we briefly explain the details of the proposed SPGNet architecture, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>To better illustrate our method, we first formulate the 3D pose estimation problem as a 2D-to-3D lifting pipeline. Then, we assume the dataset defined as</p><formula xml:id="formula_0">D = {(x i , y i )} N i=1</formula><p>consisting of N data, where each data x i is associated with a corresponding label y i . Here, x i ? R M ?2 represents the one frame input of 2D pose key points. Similarly, y i ? R M ?3 represent estimated 3D pose key points labels, where M denoted as number of joints for each human pose. In order to take advantage of temporal information between frames, we define the sequence of input for one frame as vector {x k ? R 1?M ?2 |k = 1, 2, 3...J}, where J represents the number of frames in the input sequence. The goal is to optimize the prediction function (estimated 3D pose key points in three dimensions) f from the training data,</p><formula xml:id="formula_1">f (x; W ) = arg max y?D F (x, y; W )<label>(1)</label></formula><p>where W is the weight of the neural network and F is the optimizing function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatial Projection Guided Approach</head><p>In the training process, SPGNet contains three components: an encoder, a projector and loss functions. First, the encoder processes the input 2D key points, aiming to transform the data into precise 3D coordinates that represent the estimated human pose. Then, the projector transforms the estimated 3D human pose into a re-projected 2D pose for computing the similarity with the original input. Finally, multiple loss functions guide the backpropagation of the neural network in order to learn helpful representations in the latent space.</p><p>The Encoder First, we encode our input data through an encoder and obtain the predicted 3D pose key points in three dimensions. Then, we decompose our outputt i for a certain frame intot k i andt p i , wheret k i is the predicted 3D pose key points coordinates in a certain 2D plane andt p i is the spatial information to confirm the global position of pose. In this case,t k i ? R M ?2 , which is the (x, y) coordinates of each joint of a 3D pose in three dimensions. Similarly,t p i ? R M is just the (z) coordinate as predicted by the position. We do same decomposition for label y i and gain y k i , y p i for further loss computation.</p><p>The Projector A projector is utilized to map the estimated 3D human pose into the re-projected 2D pose. In order to minimize the impact of lens distortion on projection, we chose the nonlinear projector. The schematic diagram is shown in <ref type="figure" target="#fig_1">Figure 3</ref>. This schematic diagram shows the forward projection onto the image plane that maps (x, y, z) coordinate into the (x, y) plane. The center is the camera center, and the focal length is the distance between the camera center and the image plane along the principal axis perpendicular to the image plane. These two parameters represent the spatial geometry relationship of the 3D pose in low dimensions. Another essential aspect to consider is the camera distortion. It is a kind of optical aberration that causes a straight line in the scene to not remain straight in an image. There are two common camera distortions. First, the radial distortion causes the magnification of the image to decrease or increase with distance from the principal axis. Second, tangential distortion occurs because the lens assembly is not centered and parallel to the image plane. Fixing lens distortion during projection can increase projection accuracy. Consequently, the projector has several intrinsic camera parameters as input to ensure the projection is at a right angle, specified by the particular dataset. Here, we define focal length, center, radial distortion, and tangential distortion as f c , c e , d r , and d t , respectively. The pseudocode of nonlinear projection is summarized in Algorithm 1.</p><p>Algorithm 1: Nonlinear projector mapping estimated 3D pose to reprojected 2D pose.</p><p>1 Projector (t k i ,t p i , fc, ce, dr, dt); Input : Estimated 3D poset k i , estimated global positiont p i , intrinsic camera parameters (fc, ce, dr, dt) Output: Re-projected 2D pose pose 2d 2 pose depth =t k i /t p i ; 3 posecons = Clamp (pose depth ) with (min = ?1, max = 1) ; 4 r = Sum (posecons) value between (x, y) coordinates; 5 concat = Concatenates the given sequence (r, r 2 , r 3 ) ; 6 R = Sum (1 + (dr ? concat)) value between (x, y, z) coordinates; 7 T = Sum (dt ? posecons) value between (x, y) coordinates; 8 posetrans = posecons ? (R + T ) + dt ? r ; 9 pose 2d = fc ? posetrans + ce ; 10 return pose 2d</p><p>Loss functions Usually, deep learning models predict the target without restraint. Thus, we propose the kinematic constraint loss as a penalty to maintain consistency. This is under the assumption that the length of human bones is constant from beginning to end. We construct the output 3D pose pairs (?,?), where? is the previous predicted framet, and? is current predicted frame. Then, we define kinematic constraint loss as follows:</p><formula xml:id="formula_2">L kc (?;?) = 1 2M M i=1 P j=1 abs( ? i ?? j 2 ? ? i ?? j 2 )<label>(2)</label></formula><p>Where,? j is the parent joint of the current joint? i . The 3D human pose can be regarded as a tree-like structure. Each joint has at least one connection with the other joint (parent), and some of them have multiple connections. Thus, for a particular joint, the number of connections is determined by the P denoted as parents. The dummy variable P depends on a specific dataset that consists of an indeterminate number of joints. The constant 0.5 is a multiplier for model stability because the repeated bone length would be calculated twice. Note that the first predicted frame does not have?. Thus, we omit the computation of the loss function for the first predicted frame. This loss can be regarded as a penalty between consecutive frames and effectively maintaining the length of the bone, as shown in Section 4.5.</p><p>The loss function for estimated 3D human pose is simply the mean per joint position error (MPJPE), which is the Protocol 1 used in many existing work:</p><formula xml:id="formula_3">L mpjpe (t k i ; y k ) = 1 M M i=1 t k i ? y k i 2<label>(3)</label></formula><p>The MPJPE loss will calculate the euclidean distance between all the joints of the predicted 3D pose and the ground truth. During the backpropagation, the loss gradient provides information for optimizing the degree of the key point's accuracy. Notably, the ground truth is in the camera space, transformed by using the intrinsic and extrinsic camera parameters. Therefore, for the global position y p , MPJPE cannot hold the depth information of the 3D pose. We use the weighted MPJPE loss function for estimated global position to retain the maximum spatial feature:</p><formula xml:id="formula_4">L wmpjpe (t p i ; y p ) = 1 M M i=1 1 y k i t p i ? y k i 2<label>(4)</label></formula><p>The inverse term 1/y k i is the regularization term compared with MPJPE loss to force the predicted 3D pose centered around the trim area. This is assuming that the pose object cannot move very far away from the camera position. The model learns the 3D pose characteristics of centralization in terms of results. In addition, the weighted MPJPE loss significantly increases the accuracy of the projected 2D pose and reduces the error caused by the abnormal predicted global position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Design of Encoder</head><p>The architecture of the 2D-to-3D lifting neural network we designed is a temporal dilated convolutional model inspired by previous lifting approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref>. The neural network is fully implemented with the residual connections <ref type="bibr" target="#b5">[6]</ref> in order to transform the sequence of input x defined in 3.1 through temporal convolutional layers. In detail, each residual blocks can be defined as z = D(?(N (C(x)))),</p><p>where z is the extracted feature, C is the convolutional layer with 1024 input sizes except the input layer and 1024 output sizes, B is the batch normalization layer, ? is the ReLU activation layer, and finally, D is the dropout layer. Two residual blocks form a residual connection shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The number of residual blocks depends on the number of input frames. For example, input x with J = 243 needs 8 residual blocks for residual connections. Reminder, the input convolutional layer has 2 ? M input size to adapt the 2D key points, where M is defined in Section 3.1. Finally, the output layer is a convolutional layer with output size 3 ? M , fitting with size of estimated 3D pose, defined as:</p><formula xml:id="formula_6">t i = C out (z),<label>(6)</label></formula><p>wheret i is the output of encoder for one frame defined in Section 3.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Metrics</head><p>We evaluate our method on the public dataset Human3.6M, widely used in other work <ref type="bibr" target="#b7">[8]</ref>. The Human3.6M dataset is collected by a motion capture system. As one of the largest 3D human pose estimation datasets, 11 professional actors performed 15 scenarios consisting of 3.6 million video frames. There are four digital video cameras, one time-of-flight sensor, and ten motion cameras to capture the human pose. Our experiments follow the previous work <ref type="bibr" target="#b19">[20]</ref> to adopt a standard 17-joints skeleton and split the dataset into a training set (S1, S5, S6, S7, S8) and a testing set (S9, S11). We take two widely used protocols to evaluate our model: Protocol 1 is the mean per joint position error (MPJPE) defined in Equation 3. MPJPE calculates the Euclidean distance between the estimated positions of the joints and ground truth in millimeters. Protocol 2 is the Procrustes mean per joint position error (P-MPJPE), which calculates the error after aligning the estimated 3D pose to the ground truth in a rigid transformation, such as translation, rotation, and scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We train our model with input in camera space for consistency of other work through quaternion transformation. Here, we set our temporal convolutional model with 243 frames to benefit from the consecutive video stream. We choose Adam <ref type="bibr" target="#b9">[10]</ref> as an optimizer and train our model with 100 epochs. The learning rate starts from 0.001 and decays exponentially every epoch. We adopt finetuned 2D pose detection key points through the Cascaded Pyramid Network <ref type="bibr" target="#b3">[4]</ref> and ground truth 2D as our input. We apply the data augmentation method, pose flipping horizontally in the training set, with settings similar to <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-of-the-art Methods</head><p>This section reports our model's performance on 15 actions belonging to S9 and S11. First, we use the CPN network as the 2D pose detector to obtain the 2D key points as our input data. We compare them using Protocol 1 and Protocol 2, shown in <ref type="table" target="#tab_0">Table 1</ref> and 2. Our model has a lower average error than all other approaches under both protocols and does not rely on additional data as many other approaches. Under Protocol 1 <ref type="table" target="#tab_0">(Table 1)</ref>, our model slightly outperforms the previous best result <ref type="bibr" target="#b27">[28]</ref> by 0.3 mm on the average, corresponding to a 0.6% error reduction. To be more specific, we got the best 7 out of 15 actions and 7 second best actions. This indicates that our model's architecture has a better learning ability to extract features in the latent space in order to keep the spatial and temporal information. For Protocol 2, our model achieves the best results in terms of average P-MJPJE, with a 1.4% error reduction. However, for individual actions, most actions predicted by our model only achieve the second-best results. Yang's <ref type="bibr" target="#b29">[30]</ref> work reports a significant improvement in the actions with complex spatial relationships, such as sitting or direction, achieving the seven best results. Compared to results in <ref type="table" target="#tab_0">Table 1</ref>, their method uses the feature of GANs but makes it hard to detect the global position of the 3D human pose, leading the better performance in Protocol 1.</p><p>To further study our method, we utilize the ground truth of 2D key points as our input to evaluate our model, with results shown in <ref type="table" target="#tab_2">Table 3</ref> under Protocol 1 and Protocol 2. By using 2D ground truth, the models generally get better performance than <ref type="table" target="#tab_1">Table 2</ref>. Compared to the previous best result <ref type="bibr" target="#b13">[14]</ref>, our model outperforms by 1.3 mm on the average. Our method highlights the reuse of the 2D key points for the 2D MPJPE loss, so the model depends more on the 2D key points than other models that only use those data at the input stage. This is evident from the higher error deduction comparing our model to previously implemented methods between <ref type="table" target="#tab_1">Table 2 and Table 3</ref>. More discussion about the improvement in performance in this case is presented in a later section. We also report our model's performance in Protocol 2, resulting in better performance of 25.3 mm on the average, outperforming the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>Analysis of re-projected 2D pose loss We further analyze the MPJPE loss between the re-projected 2D pose and the original input. We choose the encoder only consisting of MPJPE loss for the estimated 3D human pose as our encoder and add kinematic constraint as Baseline* to compare with SPGNet, as shown in <ref type="figure" target="#fig_2">Figure 4a</ref>. The histogram indicates that our model dramatically benefits from the MPJPE loss of the re-projected 2D pose, leading to the reprojected 2D pose being closer to the ground truth 2D human pose. Furthermore, the kinematic constraint slightly improves the model's accuracy, by adding a bone length constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of size of input frames</head><p>The size of the input frames has a significant impact on the pose estimation performance. For example, the chart in <ref type="figure" target="#fig_3">Figure 4b</ref> shows that the error in Protocol 1 decreases by 0.9 millimeters and 1.8 millimeters while the size of the input frames increases from 27 to 81 and 243, respectively. A similar error decrease trend is also reflected in Protocol 2.</p><p>Consequently, we conclude that the larger size of sequential input provides more temporal information to allow our model to capture the movement of the 3D human pose between frames, similar to <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of kinematic constraints</head><p>In this section, we compare the sixteen bone lengths measured in different frames in one particular walking scenario, shown in <ref type="figure">Figure 5</ref>. We can see that the difference in most small bones is almost negligible. Some large bones, such as leg   <ref type="figure" target="#fig_2">Figure 4a</ref>, Baseline is the model only learned from MPJPE loss, and Baseline* represents the Baseline adding kinematic constraints. In the chart 4b, J represents the number of frames in the input sequence.</p><p>bone or spine, have some relatively large variance compared to the small bone. However, the errors are all within 0.065 meters, which is acceptable and may be caused by movement of the frame. As a consequence, this line chart indicates that our kinematic constraint is effective. <ref type="figure">Fig. 5</ref>: Line chart of the measured bone lengths in seven different frames extracted from S11 walking scenarios. The different colors of lines represent the sixteen bones of a 3D human pose (tree-like structure).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Shielding problem</head><p>We further analyze the limitation of our method from visualization results, shown in <ref type="figure" target="#fig_4">Figure 6</ref>. We found that in the first row, which represents the action of sitting, the human limbs barely overlap in the camera's view. Thus, our results cannot be distinguished by the human eye if the consideration of global position is ignored. However, in the third row, the red arm of the human overlaps with the human body, which forms a shield masking the inner limbs. Consequently, the spatial relationship is not perfectly represented for both human pose in the third row, the variance of body tilt, and the difference of the arm's spatial positions, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed SPGNet, a fully convolutional network based on supervised learning for human 3D pose estimation. To utilize our 2D-to-3d-lifting network, we used 2D key points in both input and re-projection stages and introduced kinematic constraints of human bone length and the corresponding loss function. Our model achieved more reliable estimates than state-of-the-art methods. SPGNet utilizes 2D labels in a more effective way, so the performance is expected to increase using image-to-2D methods with higher accuracies. Furthermore, besides the popular Human3.6M dataset, more datasets need to be tested for better analysis of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Supervised training framework with a 3D pose model with predicted 2D pose sequence input. In addition to the kinematic constraint and MPJPE loss, we concatenate the global position and key points of the 3D poses to perform the projection. Finally, we compare the re-projected 2D pose and original input 2D poses to compute a 2D MPJPE loss and perform backpropagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Schematic diagram of the spatial dimension projection for SPGNet. The projector simulates the real camera to maintain consistency. The 3D estimated pose is projected to a 2D image with the same principle point, focal length, radial distortion and tangential distortion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( a )</head><label>a</label><figDesc>Comparisons of different settings (b) Comparisons of size of input frames</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Clustered column charts (a) and (b) illustrate the comparisons of different sittings and input frames, respectively. The error shows a decreasing trend as the model is applied from the Baseline to our SPGNet or the size of input frames increases. Notably, in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Visualization of qualitative results on video clips.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Detailed results under Protocol 1 (MPJPE) The table reports the result with CPN 2D detection pose key points as input. The last line is the average of all 15 action results in millimeter. Best results in bold, second best underlined.</figDesc><table><row><cell>Methods</cell><cell>Martinez</cell><cell>Sun</cell><cell>Yang</cell><cell>Lee</cell><cell>Pavllo</cell><cell>Cai</cell><cell>Xu</cell><cell>Our</cell></row><row><cell></cell><cell>[15]</cell><cell>[22]</cell><cell>[30]</cell><cell>[13]</cell><cell>[20]</cell><cell>[2]</cell><cell>[28]</cell><cell></cell></row><row><cell>Direct.</cell><cell>51.8</cell><cell>52.8</cell><cell>51.5</cell><cell>40.2</cell><cell>45.1</cell><cell>44.6</cell><cell>37.4</cell><cell>37.5</cell></row><row><cell>Discuss.</cell><cell>56.2</cell><cell>54.8</cell><cell>58.9</cell><cell>49.2</cell><cell>47.4</cell><cell>47.4</cell><cell>43.5</cell><cell>44.7</cell></row><row><cell>Eat</cell><cell>58.1</cell><cell>54.2</cell><cell>50.4</cell><cell>47.8</cell><cell>42.0</cell><cell>45.6</cell><cell>42.7</cell><cell>41.8</cell></row><row><cell>Greet</cell><cell>59.0</cell><cell>54.3</cell><cell>57.0</cell><cell>52.6</cell><cell>46.0</cell><cell>48.8</cell><cell>42.7</cell><cell>42.1</cell></row><row><cell>Phone</cell><cell>69.5</cell><cell>61.8</cell><cell>62.1</cell><cell>50.1</cell><cell>49.1</cell><cell>50.8</cell><cell>46.6</cell><cell>45.5</cell></row><row><cell>Photo</cell><cell>78.4</cell><cell>67.2</cell><cell>65.4</cell><cell>75.0</cell><cell>56.7</cell><cell>59.0</cell><cell>59.7</cell><cell>58.9</cell></row><row><cell>Pose</cell><cell>55.2</cell><cell>53.1</cell><cell>49.8</cell><cell>50.2</cell><cell>44.5</cell><cell>47.2</cell><cell>41.3</cell><cell>42.0</cell></row><row><cell>Purch.</cell><cell>58.1</cell><cell>53.6</cell><cell>52.7</cell><cell>43.0</cell><cell>44.4</cell><cell>43.9</cell><cell>45.1</cell><cell>46.7</cell></row><row><cell>Sit</cell><cell>74.0</cell><cell>71.7</cell><cell>69.2</cell><cell>55.8</cell><cell>57.2</cell><cell>57.9</cell><cell>52.7</cell><cell>52.8</cell></row><row><cell>SitD</cell><cell>94.6</cell><cell>86.7</cell><cell>85.2</cell><cell>73.9</cell><cell>66.1</cell><cell>61.9</cell><cell>60.2</cell><cell>59.4</cell></row><row><cell>Smoke</cell><cell>62.3</cell><cell>61.5</cell><cell>57.4</cell><cell>54.1</cell><cell>47.5</cell><cell>49.7</cell><cell>45.8</cell><cell>46.7</cell></row><row><cell>Wait</cell><cell>59.1</cell><cell>53.4</cell><cell>58.4</cell><cell>55.6</cell><cell>44.8</cell><cell>46.6</cell><cell>43.1</cell><cell>42.8</cell></row><row><cell>WalkD</cell><cell>65.1</cell><cell>61.6</cell><cell>43.6</cell><cell>58.2</cell><cell>49.2</cell><cell>51.3</cell><cell>47.7</cell><cell>46.6</cell></row><row><cell>Walk</cell><cell>49.5</cell><cell>47.1</cell><cell>60.1</cell><cell>43.3</cell><cell>32.6</cell><cell>37.1</cell><cell>33.7</cell><cell>34.7</cell></row><row><cell>WalkT</cell><cell>52.4</cell><cell>53.4</cell><cell>47.7</cell><cell>43.3</cell><cell>34.0</cell><cell>39.4</cell><cell>37.1</cell><cell>36.8</cell></row><row><cell>Avg</cell><cell>62.9</cell><cell>59.1</cell><cell>58.6</cell><cell>52.8</cell><cell>47.1</cell><cell>48.8</cell><cell>45.6</cell><cell>45.3</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Detailed results under Protocol 2 (P-MPJPE)</figDesc><table><row><cell>Methods</cell><cell>Martinez</cell><cell>Sun</cell><cell>Yang</cell><cell>Lee</cell><cell>Pavllo</cell><cell>Liu</cell><cell>Xu</cell><cell>Our</cell></row><row><cell></cell><cell>[15]</cell><cell>[22]</cell><cell>[30]</cell><cell>[13]</cell><cell>[20]</cell><cell>[14]</cell><cell>[28]</cell><cell></cell></row><row><cell>Direct.</cell><cell>39.5</cell><cell>42.1</cell><cell>26.9</cell><cell>34.9</cell><cell>34.2</cell><cell>32.5</cell><cell>31.0</cell><cell>30.8</cell></row><row><cell>Discuss.</cell><cell>43.2</cell><cell>44.3</cell><cell>30.9</cell><cell>35.2</cell><cell>36.8</cell><cell>35.3</cell><cell>36.8</cell><cell>34.6</cell></row><row><cell>Eat</cell><cell>46.4</cell><cell>45.0</cell><cell>36.3</cell><cell>43.2</cell><cell>33.9</cell><cell>34.3</cell><cell>34.7</cell><cell>34.1</cell></row><row><cell>Greet</cell><cell>47.0</cell><cell>45.4</cell><cell>39.9</cell><cell>42.6</cell><cell>37.5</cell><cell>36.2</cell><cell>34.4</cell><cell>35.8</cell></row><row><cell>Phone</cell><cell>51.0</cell><cell>51.5</cell><cell>43.9</cell><cell>46.2</cell><cell>37.1</cell><cell>37.8</cell><cell>36.2</cell><cell>35.3</cell></row><row><cell>Photo</cell><cell>56.0</cell><cell>53.0</cell><cell>47.4</cell><cell>55.0</cell><cell>43.2</cell><cell>43.0</cell><cell>43.9</cell><cell>43.2</cell></row><row><cell>Pose</cell><cell>41.4</cell><cell>43.2</cell><cell>28.8</cell><cell>37.6</cell><cell>34.4</cell><cell>33.0</cell><cell>31.6</cell><cell>31.9</cell></row><row><cell>Purch.</cell><cell>40.6</cell><cell>41.3</cell><cell>29.4</cell><cell>38.8</cell><cell>33.5</cell><cell>32.2</cell><cell>33.5</cell><cell>32.5</cell></row><row><cell>Sit</cell><cell>56.5</cell><cell>59.3</cell><cell>36.9</cell><cell>50.9</cell><cell>45.3</cell><cell>45.7</cell><cell>42.3</cell><cell>42.1</cell></row><row><cell>SitD</cell><cell>69.4</cell><cell>73.3</cell><cell>58.4</cell><cell>67.3</cell><cell>52.7</cell><cell>51.8</cell><cell>49.0</cell><cell>49.9</cell></row><row><cell>Smoke</cell><cell>49.2</cell><cell>51.0</cell><cell>41.5</cell><cell>48.9</cell><cell>37.7</cell><cell>38.4</cell><cell>37.1</cell><cell>39.0</cell></row><row><cell>Wait</cell><cell>45.0</cell><cell>44.0</cell><cell>30.5</cell><cell>35.2</cell><cell>34.1</cell><cell>32.8</cell><cell>33.0</cell><cell>32.6</cell></row><row><cell>WalkD</cell><cell>49.5</cell><cell>48.0</cell><cell>29.5</cell><cell>50.7</cell><cell>38.0</cell><cell>37.5</cell><cell>39.1</cell><cell>37.2</cell></row><row><cell>Walk</cell><cell>38.0</cell><cell>38.3</cell><cell>42.5</cell><cell>31.0</cell><cell>25.8</cell><cell>25.8</cell><cell>26.9</cell><cell>26.9</cell></row><row><cell>WalkT</cell><cell>43.1</cell><cell>44.8</cell><cell>32.2</cell><cell>34.6</cell><cell>27.7</cell><cell>28.9</cell><cell>31.9</cell><cell>29.7</cell></row><row><cell>Avg</cell><cell>47.7</cell><cell>48.3</cell><cell>37.7</cell><cell>43.4</cell><cell>36.8</cell><cell>36.8</cell><cell>36.2</cell><cell>35.7</cell></row></table><note>*The table reports the result with CPN 2D detection pose key points as input. The last line is the average of all 15 action results in millimeter. Best in bold, second best underlined.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Detailed results based on ground truth of 2D human pose.</figDesc><table><row><cell></cell><cell></cell><cell>Protocol 1</cell><cell></cell><cell></cell><cell>Protocol 2</cell></row><row><cell>Methods</cell><cell>Hossain</cell><cell>Lee</cell><cell>Liu</cell><cell>Our</cell><cell>Our</cell></row><row><cell></cell><cell>[7]</cell><cell>[13]</cell><cell>[14]</cell><cell></cell><cell></cell></row><row><cell>Direct.</cell><cell>35.2</cell><cell>32.1</cell><cell>34.5</cell><cell>29.5</cell><cell>21.1</cell></row><row><cell>Discuss.</cell><cell>40.8</cell><cell>36.6</cell><cell>37.1</cell><cell>33.6</cell><cell>23.9</cell></row><row><cell>Eat</cell><cell>37.2</cell><cell>34.3</cell><cell>33.6</cell><cell>32.8</cell><cell>24.5</cell></row><row><cell>Greet</cell><cell>37.4</cell><cell>37.8</cell><cell>34.2</cell><cell>32.5</cell><cell>24.6</cell></row><row><cell>Phone</cell><cell>43.2</cell><cell>44.5</cell><cell>32.9</cell><cell>32.5</cell><cell>25.3</cell></row><row><cell>Photo</cell><cell>44.0</cell><cell>49.9</cell><cell>37.1</cell><cell>33.6</cell><cell>29.9</cell></row><row><cell>Pose</cell><cell>38.9</cell><cell>40.9</cell><cell>39.6</cell><cell>37.5</cell><cell>21.1</cell></row><row><cell>Purch.</cell><cell>35.6</cell><cell>36.2</cell><cell>35.8</cell><cell>29.8</cell><cell>23.7</cell></row><row><cell>Sit</cell><cell>42.3</cell><cell>44.1</cell><cell>40.7</cell><cell>30.7</cell><cell>30.4</cell></row><row><cell>SitD</cell><cell>44.6</cell><cell>45.6</cell><cell>41.4</cell><cell>38.1</cell><cell>38.9</cell></row><row><cell>Smoke</cell><cell>39.7</cell><cell>35.3</cell><cell>33.0</cell><cell>46.4</cell><cell>26.9</cell></row><row><cell>Wait</cell><cell>39.7</cell><cell>35.9</cell><cell>33.8</cell><cell>35.9</cell><cell>23.1</cell></row><row><cell>WalkD</cell><cell>40.2</cell><cell>30.3</cell><cell>33.0</cell><cell>31.8</cell><cell>27.7</cell></row><row><cell>Walk</cell><cell>32.8</cell><cell>37.6</cell><cell>26.6</cell><cell>25.5</cell><cell>18.8</cell></row><row><cell>WalkT</cell><cell>35.5</cell><cell>35.5</cell><cell>26.9</cell><cell>28.5</cell><cell>19.6</cell></row><row><cell>Avg</cell><cell>39.2</cell><cell>38.4</cell><cell>34.7</cell><cell>33.4</cell><cell>25.3</cell></row></table><note>*Utilize the ground truth 2D human pose to predict the target. The table reports results of both Protocal 1 (left side) and Protocol 2 (right side). We label the best in bold.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Monocular 3d pose estimation and tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2010.5540156</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2010.5540156" />
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="623" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00236</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00236" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">3d human pose estimation = 2d pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno>abs/1612.06524</idno>
		<ptr target="http://arxiv.org/abs/1612.06524" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1711.07319</idno>
		<ptr target="http://arxiv.org/abs/1711.07319" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Can 3d pose be learned from 2d projections alone?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Huynh</surname></persName>
		</author>
		<idno>CoRR abs/1808.07182</idno>
		<ptr target="http://arxiv.org/abs/1808.07182" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<ptr target="http://arxiv.org/abs/1512.03385" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Exploiting temporal information for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<idno>abs/1711.08585</idno>
		<ptr target="http://arxiv.org/abs/1711.08585" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>abs/1712.06584</idno>
		<ptr target="http://arxiv.org/abs/1712.06584" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<ptr target="http://dblp.uni-trier.de/db/journals/corr/corr1412.html" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<idno>abs/1903.02330</idno>
		<ptr target="http://arxiv.org/abs/1903.02330" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<idno>abs/1701.02468</idno>
		<ptr target="http://arxiv.org/abs/1701.02468" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Asari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<idno>abs/1705.03098</idno>
		<ptr target="http://arxiv.org/abs/1705.03098" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno>abs/1603.06937</idno>
		<ptr target="http://arxiv.org/abs/1603.06937" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3467" to="3475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00763</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00763" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno>abs/1611.07828</idno>
		<ptr target="http://arxiv.org/abs/1611.07828" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno>abs/1811.11742</idno>
		<ptr target="http://arxiv.org/abs/1811.11742" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fine-grained head pose estimation without keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno>abs/1704.00159</idno>
		<ptr target="http://arxiv.org/abs/1704.00159" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno>abs/1711.08229</idno>
		<ptr target="http://arxiv.org/abs/1711.08229" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>CoRR abs/1312.4659</idno>
		<ptr target="http://arxiv.org/abs/1312.4659" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Ponet: Robust 3d human pose estimation via learning orientations only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno>abs/2112.11153</idno>
		<ptr target="https://arxiv.org/abs/2112.11153" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.335</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.335" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3073" to="3082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1803.09722</idno>
		<ptr target="http://arxiv.org/abs/1803.09722" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
