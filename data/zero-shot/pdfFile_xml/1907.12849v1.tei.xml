<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Orientation-aware Semantic Segmentation on Icosahedron Spheres</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toshiba Research Europe Limited</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Liwicki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toshiba Research Europe Limited</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Smith</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of York</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toshiba Research Europe Limited</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Orientation-aware Semantic Segmentation on Icosahedron Spheres</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address semantic segmentation on omnidirectional images, to leverage a holistic understanding of the surrounding scene for applications like autonomous driving systems. For the spherical domain, several methods recently adopt an icosahedron mesh, but systems are typically rotation invariant or require significant memory and parameters, thus enabling execution only at very low resolutions. In our work, we propose an orientation-aware CNN framework for the icosahedron mesh. Our representation allows for fast network operations, as our design simplifies to standard network operations of classical CNNs, but under consideration of north-aligned kernel convolutions for features on the sphere. We implement our representation and demonstrate its memory efficiency up-to a level-8 resolution mesh (equivalent to 640?1024 equirectangular images). Finally, since our kernels operate on the tangent of the sphere, standard feature weights, pretrained on perspective data, can be directly transferred with only small need for weight refinement. In our evaluation our orientation-aware CNN becomes a new state of the art for the recent 2D3DS dataset, and our Omni-SYNTHIA version of SYNTHIA. Rotation invariant classification and segmentation tasks are additionally presented for comparison to prior art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We address the problem of spherical semantic segmentation on omnidirectional images. Accurate semantic segmentation is useful for many applications including scene understanding, robotics, and medical image processing. It is also a key component for autonomous driving technology. Deep convolutional neutral networks (CNNs) have pushed the performance on a wide array of high-level tasks, including image classification, object detection and semantic segmentation. In particular, most research on CNNs for semantic segmentation <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b3">4]</ref>   <ref type="figure">Figure 1</ref>. Given spherical input, we convert it to an unfolded icosahedron mesh. Hexagonal filters are then applied under consideration of north alignment, as we efficiently interpolate vertices. Our approach is suited to most classical CNN architectures, e.g. U-Net <ref type="bibr" target="#b22">[22]</ref>. Since we work with spherical data, final segmentation results provide a holistic labeling of the environment. spective images. In our work, we focus on omnidirectional images, as such data provides a holistic understanding of the surrounding scene with a large field of view. The complete receptive field is especially important for autonomous driving systems. Furthermore, recent popularity in omnidirectional capturing devices and the increasing number of datasets with omnidirectional signals make omnidirectional processing very relevant for modern technology. While spherical input could be represented as planar equirectangular images where standard CNNs are directly applied, such choice is inferior due to latitude dependent distortions and boundaries. In <ref type="bibr" target="#b26">[25]</ref> a perspective network is distilled to work on equirectangular input. The main drawback is that weight sharing is only enabled in horizontal direction. Therefore, the model requires more parameters than a perspective one. SphereNet <ref type="bibr" target="#b9">[9]</ref> projects equirectan-gular input onto a latitude-longitude grid. A constant grid kernel is convolved with each vertex on the sphere by sampling on the tangent plane. However, it is not straightforward to implement pooling and up-sampling for dense prediction tasks.</p><p>In 3D shape analysis, one of the challenges in applying CNNs is how to define a natural convolution operator on non-euclidean surfaces. Several works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b1">2]</ref> have focused on networks for manifolds or graphs. Unlike general 3D shapes, omnidirectional images are orientable with the existence of north and south poles. Therefore, the lack for shift-invariance on surfaces or graphs could be overcome with an orientation-aware representation.</p><p>Most recently, several works propose to use an icosahedron mesh as the underlying spherical data representation. The base icosahedron is the most regular polyhedron, consisting of 12 vertices and 20 faces. It also provides a simple way of resolution increase via subdivision. In <ref type="bibr" target="#b14">[14]</ref>, UGSCNN is proposed to use the linear combinations of differential operators weighted by learnable parameters. Since the operaters are precomputed, the number of parameters is reduced to 4 per kernel. The main issue of this approach, as observed in our experiments, is that it requires a lot of memory for their mesh convolution if the resolution is lifted for better input/output quality. Similar to our method in the use of icosahedron, <ref type="bibr" target="#b7">[7]</ref> proposes a gauge equivariant CNN. Here, filter weights are shared across multiple orientations. While rotation covariance and invariance is essential in applications such as 3D shape classification and climate pattern prediction, it might be undesired in the semantic segmentation task which we consider here. On the contrary, we argue that the orientation information from cameras attached to vehicles or drones is very important a cue and should be exploited.</p><p>Therefore, we propose and investigate a novel framework for the application of CNNs to omnidirectional input, targeting semantic segmentation. We take advantage of both, the icosahedron representation for efficiency and orientation information to improve accuracy in orientationaware tasks ( <ref type="figure">Fig. 1</ref>). Our hypothesis is that aligning all learnable filters to the north pole is essential for omnidirectional semantic segmentation. We also argue that high resolution meshes (i.e. a level-8 icosahedron mesh) are needed for detailed segmentation. Due to memory restrictions, CNN operations need to be implemented efficiently to reach such high resolution.</p><p>In our work, we first map the spherical data to an icosahedron mesh, which we unfold along the equator, similarly to cube maps <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b4">5]</ref> and <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b7">7]</ref>. In the icosahedron, vertices have at most 6 neighbors. Therefore, we propose to use a hexagonal filter that is applied to each vertex's neighborhood. After simple manipulation of the unfolded mesh, standard planar CNN operations compute our hexagonal convolutions, pooling and up-sampling layers. Finally we emphasize, since our filters are similar to standard 3 ? 3 kernels applied to the tangent of the sphere, weight transfer from pretrained perspective CNNs is possible.</p><p>To validate our approach we use the omnidirectional 2D3DS dataset <ref type="bibr" target="#b0">[1]</ref> and additionally prepare our Omni-SYNTHIA dataset, which is produced from SYNTHIA data <ref type="bibr" target="#b23">[23]</ref>. Qualitative as well as quantitative results demonstrate that our method outperforms previous state-of-theart approaches in both scenarios. Performance on spherical MNIST classification <ref type="bibr" target="#b6">[6]</ref> and climate pattern segmentation <ref type="bibr" target="#b21">[21]</ref> is also shown in comparison with previous methods in literature. In summary, our contributions are:</p><p>1. We propose and implement a memory efficient icosahedron-based CNN framework for spherical data.</p><p>2. We introduce fast interpolation for orientation-aware filter convolutions on the sphere.</p><p>3. We present weight transfer from kernels learned through classical CNNs, applied to perspective data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>We evaluate our method on both non-orientation-aware and orientation-aware, publicly available datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>CNNs on Equirectangular Images Although classical CNNs are not designed for omnidirectional data, they could still be used for spherical input if the data are converted to equirectangular form. Conversion from spherical coordinates to equirectangular images is a linear one-to-one mapping, but spherical inputs are distorted drastically especially in polar regions. Another artifact is that north and south poles are stretched to lines. Lai et al. <ref type="bibr" target="#b15">[15]</ref> apply this method in the application of converting panoramic video to normal perspective. Another method along this line is to project spherical data onto multiple faces of a convex polygons, such as a cube. In <ref type="bibr" target="#b19">[19]</ref>, omnidirectional images are mapped to 6 faces of a cube, and then trained with normal CNNs. However, distortions still exist and discontinuities between faces have to be carefully handled.</p><p>Spherical CNNs In order to generalize convolution from planar images to spherical signals, the most natural idea is to replace shifts of the plane by rotations of the sphere. Cohen et al. <ref type="bibr" target="#b6">[6]</ref> propose a spherical CNN which is invariant in the SO(3) group. Esteves et al. <ref type="bibr" target="#b11">[11]</ref> use spherical harmonic basis to achieve similar results. Zhou et al. <ref type="bibr" target="#b32">[31]</ref> propose to extend normal CNNs to extract rotation-dependent features by including an additional orientation channel.</p><p>CNNs with Deformable Kernels Some works <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b13">13]</ref> consider adapting the sampling locations of convolutional kernels. Dai et al. <ref type="bibr" target="#b10">[10]</ref> propose to learn the deformable convolution which samples the input features through learned offsets. An Active Convolutional Unit is introduced in <ref type="bibr" target="#b13">[13]</ref> to provide more freedom to a conventional convolution by using position parameters. These methods requires additional model parameters and training steps to learn the sampling locations. In our work, we adapt the kernel shape to fit icosahedron geometry. Unlike deformable methods, our sampling locations can be precomputed and reused without the need of training.</p><p>CNNs with Grid Kernels Another line of works aim to adapt the regular grid kernel to work on omnidirectional images. Su and Grauman <ref type="bibr" target="#b26">[25]</ref> propose to process equirectangular images as perspective ones by adapting the weights according to the elevation angles. Weight sharing is only enabled along the longitudes. To reduce the computational cost and degradation in accuracy, a Kernel Transformer Network <ref type="bibr" target="#b27">[26]</ref> is applied to transfer convolution kernels from perspective images to equirectangular inputs. Coors et al. <ref type="bibr" target="#b9">[9]</ref> present SphereNet to minimize the distortions introduced by applying grid kernels on equirectangular images.</p><p>Here, a kernel of fixed shape is used to sample on the tangent plane according to the location on the sphere. Wrapping the kernel around the sphere avoids cuts and discontinuities.</p><p>CNNs with Reparameterized Kernels For the efficiency of CNNs, several works are proposed to use parameterized convolution kernels. Boscani et al. <ref type="bibr" target="#b1">[2]</ref> introduce oriented anisotropic diffusion kernels to estimate dense shape correspondence. Cohen and Welling <ref type="bibr" target="#b8">[8]</ref> employ a linear combination of filters to achieve equivariant convolution filters. In <ref type="bibr" target="#b29">[28]</ref>, 3D steerable CNNs using linear combination of filter banks are developed. Recently, Jiang et al. <ref type="bibr" target="#b14">[14]</ref> utilized parameterized differential operators as spherical convolution for unstructured grid data. Here, a convolution operation is a linear combination of four differential operators with learnable weights. However, these methods are limited to the chosen kernel types and are not maximally flexible.</p><p>CNNs on Icosahedron Related to our approach in discrete representation, several works utilize an icosahedron for spherical image representation. As the most uniform and accurate discretization of the sphere, the icosahedron is the regular convex polyhedron with the most faces. A spherical mesh can be generated by progressively subdividing each face into four equal triangles and reprojecting each node to unit length. Lee et al. <ref type="bibr" target="#b16">[16]</ref> is one of the first to suggest the use of icosahedrons for CNNs on omnidirectional images. Here, convolution filters are defined in terms of triangle faces. In <ref type="bibr" target="#b14">[14]</ref>, UGSCNN is proposed to efficiently train a convolutional network with spherical data . Similar to cubes <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b4">5]</ref>, we unfold our mesh (c) and align its 5 components to the standard image grid (d) for efficient computation of convolution, pooling and up-sampling.</p><p>mapped to an icosahedron mesh. Liu et al. <ref type="bibr" target="#b17">[17]</ref> uses the icosahedron based spherical grid as the discrete representation of the spherical images and proposes an azimuth-zenith anisotropic CNN for 3D shape analysis. Cohen et al. <ref type="bibr" target="#b7">[7]</ref> employ an icosahedron mesh to present a gauge equivariant CNN. Equivariance is ensured by enforcing filter weight sharing across multiple orientations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Spherical Representation</head><p>We represent the spherical input through vertices on an icosahedron mesh <ref type="figure" target="#fig_0">(Fig. 2)</ref>. The mapping is based on the vertices' azimuth and zenith angles -e.g. the input color is obtained from an equirectangular input through interpolation. Similar to cube maps <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b4">5]</ref>, the icosahedron simplifies the sphere into a set of planar regions. While the cube represents the sphere only with 6 planar regions, the icosahedral representation is the convex geodesic grid with the largest number of regular faces. In total, our gird consists of 20 faces and 12 vertices at the lowest resolution, and f r = 20 * 4 r faces and n r = 2+10 * 4 r vertices at resolution level r ? 0. Note, a resolution increase is achieved by subdivision of the triangular faces at r = 0 into 4 r equal regular triangular parts. In the following, we present an efficient orientation-aware implementation of convolutions in ?3.1, and our down-and up-sampling techniques in ?3.2. Finally, weight transfer from trained kernels of standard perspective CNNs is discussed in ?3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Orientation-aware Convolutions</head><p>If a camera is attached to a vehicle, the orientation and location of objects such as sky, buildings, sidewalks or roads are likely similar across the dataset. Therefore, we believe  an orientation-aware system can be beneficial, while tasks with arbitrary rotations may benefit from rotation invariance <ref type="bibr" target="#b6">[6]</ref> or weight sharing across rotated filters <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b7">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficient Convolutions through Padding</head><p>We first define the north and south pole as any two vertices that have maximum distance on the icosahedron mesh. Similar to <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b7">7]</ref>, the mesh is then converted to a planar representation by unfolding it along the equator <ref type="figure" target="#fig_0">(Fig. 2</ref>). Finally, we split the surface into five components, and align the vertices with a regular image grid through a simple affine transformation.</p><p>Notice, vertices have a neighborhood of either 5 or 6 points. Hence we employ hexagonal filters in our work, instead of regular 3 ? 3 kernels. Let us ignore the vertices at the poles (e.g. through reasoning of dropout), and adjust the neighborhood cardinality to 6 for all vertices with 5 neighbors through simple repetition. Now, our planar representation of the icosahedron simplifies the convolution with hexagonal filters to standard 2D convolution with a masked kernel, after padding as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>North-alignment through Interpolation In its natural implementation, our filters are aligned to the icosahedron mesh. Consequently, the filter orientation is inconsistent, since the surfaces near the north and south poles are stitched. We reduce the effect of such distortions by aligning filters vertically through interpolation <ref type="figure" target="#fig_3">(Fig. 4)</ref>.</p><p>The na?ve convolution with weights {w j } 7 j=1 at vertex v i and its neighbors {v n i j } 6 j=1 , is computed as 6 j=1 w j v n i j + w 7 v i , where n i j holds the neighborhood indices of v i . Instead, we north-align the neighborhood with interpolations using arc-based weights {? i j } 6 j=1 as follows: Since the hexagonal neighborhood is approximately symmetric, we further simplify (1) by introducing a unified weight ? i , such that {? i ? ? i j } 6 j=1 holds. Hence we write</p><formula xml:id="formula_0">6 j=2 w j (? i j v n i j + (1 ? ? i j )v n i j?1 ) + w 1 (? i 1 v n i 1 + (1 ? ? i 1 )v n i 6 ) + w 7 v i .<label>(1)</label></formula><formula xml:id="formula_1">(a) North-alignment ? i ? i +? i * ? i ? i +? i * (b) Interpolated filters</formula><formula xml:id="formula_2">? i ? ? 6 j=1 w j v n i j + w 7 v i ? ? + (1 ? ? i ) ? ? 6 j=2 w j v n i j?1 + w 1 v n i 6 + w 7 v i ? ? .<label>(2)</label></formula><p>Thus, north-aligned filters can be achieved through 2 standard convolutions, which are then weighted based on the vertices' interpolations ? i . The arc-interpolation ? i is based on the angle distance between the direction towards the first and sixth neighbors (i.e. v n i 1 and v n i 6 respectively) and the north-south axis when projected onto the surface of the sphere. In particular, we first find the projective plane of the north-south axis a = 0 1 0 T towards vector v i as the plane with normal n i = vi?a |vi?a| . Since the spherical surface is approximated by the plane of vectors v i ? v n i 1 and v i ? v n i 6 , we only require the angles between these vectors and the plane given by n i , to find interpolation ? i = ?i ?i+?i with</p><formula xml:id="formula_3">? i = arccos (v i ? v n i 1 ) T (I ? n i n T i )(v i ? v n i 1 ) (v i ? v n i 1 ) (I ? n i n T i )(v i ? v n i 1 ) ? i = arccos (v i ? v n i 6 ) T (I ? n i n T i )(v i ? v n i 6 ) (v i ? v n i 6 ) (I ? n i n T i )(v i ? v n i 6 )</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pooling and Up-sampling</head><p>Down-sampling through pooling and bi-linear upsampling are important building blocks of CNNs, and are frequently employed in the encoder-decoder framework of semantic segmentation (e.g. <ref type="bibr" target="#b22">[22]</ref>). Pooling is aimed at summarising the neighborhood of features to introduce robustness towards image translations and omissions. Typically, a <ref type="figure">Figure 5</ref>. The weights of conventional 3 ? 3 kernels trained on perspective data can be transferred to our model via simple interpolation as our filters operate on the sphere's tangent planes.</p><formula xml:id="formula_5">w 1 = p 2 w 2 = sin ? 3 p1+p4 2 + (1 ? sin ? 3 ) p2+p5 2 w 3 = sin ? 3 p4+p7 2 + (1 ? sin ? 3 ) p5+p8 2 w 4 = p 8 w 5 = sin ? 3 p6+p9 2 + (1 ? sin ? 3 ) p5+p8 2 w 6 = sin ? 3 p3+p6 2 + (1 ? sin ? 3 ) p2+p5 2 w 7 = p 5</formula><p>very small and non-overlapping neighborhood of 2 ? 2 pixels is considered in standard images, to balance detail and redundancy. Bi-linear up-sampling is used in the decoder to increase sub-sampled feature-maps to larger resolutions. We note, in our icosahedron mesh the number of vertices increases by a factor of 4 for each resolution (excluding poles). Therefore during down-sampling from resolution r to r ? 1, we summarize a neighborhood of 4 at r with 1 vertex at r ? 1. A natural choice is to pool over</p><formula xml:id="formula_6">{v i , v n i 1 , v n i 2 , v n i 3 }</formula><p>for vertices v i that are represented in both resolutions. Thus, we apply a simple standard 2 ? 2 strided pooling with kernel 2 ? 2 on each icosahedron part.</p><p>Analogously, bi-linear up-sampling or transposed convolutions are applied by padding the icosahedron parts at left and top followed by up-sampling by a factor of 2 in height and width <ref type="figure" target="#fig_2">(Fig. 3</ref>). Due to padding, this results in a 1-pixel border at each size which we simply remove to provide the expected up-sampling result.</p><p>Finally we emphasize, methods like pyramid pooling <ref type="bibr" target="#b31">[30]</ref> can be computed by combining our pooling and upsampling techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Weight Transfer from Perspective Networks</head><p>Similar to SphereNet <ref type="bibr" target="#b9">[9]</ref>, our network applies an oriented filter at the local tangent plane of each vertex on the sphere. Consequently, the transfer of pretrained perspective network weights is naturally possible in our setup. Since we apply hexagonal filters with 7 weights, we interpolate from the standard 3 ? 3 kernels as shown in <ref type="figure">Fig. 5</ref>. Specifically, we align north and south of the hexagon with the second and eighth weight of the standard convolution kernel respectively. Bi-linear interpolation provides the remaining values for our filter. After transfer, weight refinement is necessary, but can be computed on a much smaller dataset (as done in <ref type="bibr" target="#b9">[9]</ref>), or reduced learning iterations. Alternatively, but left for future work, it should be possible to learn hexagonal filter weights directly on perspective datasets <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b12">12]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>The main focus of this paper is omnidirectional semantic segmentation. Both synthetic urban scene and real indoor environments are evaluated. For completeness, we also include our model in comparison with previous state-of-theart methods on spherical MNIST classification in ?4.1 and a climate pattern prediction task in ?4.2. In ?4.3 and ?4.4, performance on omnidirectional semantic segmentation tasks are summarized and analysed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Spherical MNIST</head><p>We follow <ref type="bibr" target="#b6">[6]</ref> in the preparation of the spherical MNIST dataset, as we prepare non-rotated training and testing (N/N), non-rotated training with rotated testing (N/R) and rotated training and testing (R/R) tasks. Both non-rotated and rotated versions are generated using public source code provided by UGSCNN <ref type="bibr" target="#b14">[14]</ref>. <ref type="bibr" target="#b0">1</ref> Training set and test set include 60,000 and 10,000 digits, respectively. Input signals for this experiment are on a level-4 mesh (i.e. r = 4). The residual U-Net architecture of <ref type="bibr" target="#b14">[14]</ref>, including the necessary modifications to adapt to the classification task, is used in our experiments. We call this network "HexRUNet-C".</p><p>As shown in <ref type="table">Table 1</ref>, our method outperforms previous methods for N/N, achieving 99.45% accuracy. In R/R, our method performs better than competing Spherical CNN and UGSCNN. Gauge Net benefits from weight sharing across differently oriented filters, and achieves best accuracy for this task amongst all approaches. Similar to <ref type="bibr" target="#b14">[14]</ref>, our method is orientation-aware by design and thus not rotationinvariant. Therefore, it is expected to not generalize well to randomly rotated test data in the N/R setting, while Spherical CNN performs best in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Climate Pattern Segmentation</head><p>We further evaluate our method on the task of climate pattern segmentation. The task is first proposed by Mudigonda et al. <ref type="bibr" target="#b21">[21]</ref>, and the goal is to predict extreme weather events, i.e. Tropical Cyclones (TC) and Atomospheric Rivers (AT), from simulated global climate data. The training set consists of 43,916 patterns, and 6,274 samples are used for validation. Evaluation results on the val-TC AR BG <ref type="figure">Figure 6</ref>. Semantic segmentation results of HexRUNet-32 on climate pattern (right) in comparison to ground truth (left). idation set are shown in <ref type="table">Table 2</ref> and <ref type="figure">Fig. 6</ref> use the same residual U-Net architecture as UGSCNN <ref type="bibr" target="#b14">[14]</ref>. We include two variants using different numbers of parameters: HexRUNet-8 and HexRUNet-32 use 8 and 32 as output channels for the first convolution layer, respectively. As is shown, both versions outperform UGSCNN in terms of mean accuracy. With 32 features, HexRUNet-32's mean accuracy is similar to best performing Gauge Net. However, our method does not match Gauge Net in terms of mean average precision (mAP). We attribute this to the fact that there is no direct orientation information to exploit in this climate data. In contrast, Gauge Net shows its advantage of weight sharing across orientations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Stanford 2D3DS</head><p>For our first omnidirectional semantic segmentation experiment, we evaluate our method on the 2D3DS dataset <ref type="bibr" target="#b0">[1]</ref>, which consists of 1413 equirectangular RGB-D images. The groundtruth attributes each pixel to one of 13 classes. Following <ref type="bibr" target="#b14">[14]</ref>, we convert the depth data to be in meter unit and clip to between 0 and 4 meters. RGB data is converted to be in the range of [0, 1] by dividing 255. Finally, all data is mean subtracted and standard deviation normalized. The preprocessed signals are sampled on a level-5 mesh (r = 5) using bi-linear interpolation for images and nearest-neighbors for labels. Class-wise weighted cross-entropy loss is used to balance the class examples.</p><p>Using our proposed network operators, we employ the residual U-Net architecture of <ref type="bibr" target="#b14">[14]</ref>, which we call HexRUNet (see Sup. Mat. for details). We evaluate our method following the 3-fold splits, and report the mean intersection over union (mIoU) and class accuracy (mAcc) in <ref type="table" target="#tab_4">Table 3</ref> and 4, respectively. Our method outperforms orientation-aware method UGSCNN <ref type="bibr" target="#b14">[14]</ref>, rotationequivariant method Gauge Net <ref type="bibr" target="#b7">[7]</ref>  line <ref type="bibr" target="#b22">[22]</ref> on equirectangular images that have been subsampled to mach level-5 mesh resolution. As for per-class evaluations, our method achieves best performance in most classes. This demonstrates that semantic segmentation indeed benefits from our orientation-aware network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Omni-SYNTHIA</head><p>To further validate our method on omnidirectional semantic segmentation, we create an omnidirectional version from a subset of the SYNTHIA datset <ref type="bibr" target="#b23">[23]</ref>. The SYN-THIA dataset consists of multi-viewpoint photo-realistic frames rendered from a virtual city and comes with pixellevel semantic annotations for 13 classes. We refer the readers to <ref type="bibr" target="#b23">[23]</ref> for details. We select the "Summer" sequences of all five places (2?New York-like, 2?Highway and 1?European-like) to create our own omnidirectional dataset. We split the dataset into a training set of 1818 images (from New York-like and Highway sequences) and use 451 images of the European-like sequence for validation. Only RGB channels are used in our experiments. The icosahedron mesh is populated with data from equirectangular images using interpolation for RGB data and nearest neighbor for labels. Again, we report mIoU and mAcc. Here we use the standard U-Net architecture <ref type="bibr" target="#b22">[22]</ref> to facilitate weight transfer from perspective U-Net in one of our experiments. We call this network "HexUNet". For an ablation study, we also evaluate our method without north-alignment described in ?3.1, denoted as "HexUNet-nI".</p><p>Comparison with State-of-the-art We compare our method to UGSCNN <ref type="bibr" target="#b14">[14]</ref> using data sampled at mesh level-   6 (r = 6). We also include planar U-Net <ref type="bibr" target="#b22">[22]</ref> using original perspective images, which have been sub-sampled to match the icosahedron resolution (see Sup. Mat. for details). Table 5 and 6 report mIoU and mAcc respectively, while <ref type="figure" target="#fig_5">Fig. 8</ref> shows qualitative results. HexUNet outperforms previous state-of-the-art with significant margin across most classes. The performance on small objects, e.g. "pedestrian" and "sign", is poor, while all methods fail for "cyclist". We attribute this to an unbalanced dataset. Note here, class-wise weighted cross-entropy loss is not used. Finally we emphasize, HexUNet performs slightly better than HexUNet-nI, thus verifying the importance of orientation-aware filters in semantic segmentation.</p><p>Evaluation at Different Resolutions Most previous methods limit their mesh resolution to level r = 5 which consists of merely 2,562 vertices to represent omnidirectional input. In contrast, an icosahedron mesh at level r = 8</p><p>is required to match the pixel number of 640?1024 images, with 655, 362 ? 655, 360. Since we believe high resolution input/output is beneficial for the semantic segmentation task, we evaluate our method at different resolutions (r = {6, 7, 8}), shown in <ref type="table">Table 7</ref>. Our method achieves best performance at r = 7, while r = 7 and r = 8 perform similar. Since we use a standard U-Net structure consisting of only 4 encoder (and decoder) layers, perception of context is reduced at r = 8. This is further illustrated by the bottom-rightmost result in <ref type="figure" target="#fig_6">Fig. 9</ref>, where a car's wheel is misclassified as road-markings. Resolution r = 6 and r = 7 are able to adequately label this. Finally, network training times are shown in <ref type="table" target="#tab_6">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of Perspective Weights Transfer</head><p>As shown in ?3.3, our method utilizes an orientation-aware hexagon convolution kernel which allows direct weight transfer from perspective networks. Initialized with the learned filters (3 ? 3 kernels) from perspective U-Net, we perform weight refinement of only 10 epochs (in contrast to up-to 500 epochs otherwise), and report results as "HexUNet-T" in <ref type="table">Table 5</ref>, 6 and 7. The proposed filter transfer obtains competitive results, especially at resolution level r = 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduced a novel method to perform CNN operations on spherical images, represented on an icosahedron mesh. Our method exploits orientation information, as we introduce an efficient interpolation of kernel convolutions, based on north-alignment. The proposed framework is simple to implement, and memory efficient execution is demonstrated for input meshes of level r = 8 (equivalent to a 640 ? 1024 equirectangular image). In our evaluation on 2D3DS data <ref type="bibr" target="#b0">[1]</ref> and our Omni-SYNTHIA version of SYN-THIA <ref type="bibr" target="#b23">[23]</ref>, our method becomes the new state of the art for the omnidirectional semantic segmentation task. Furthermore, weight transfer from pretrained standard perspective CNNs was illustrated in our work.</p><p>One limitation of the proposed approach is the poor seg-  <ref type="table">Table 7</ref>. Evaluation at different resolution on Omni-SYNTHIA dataset. (The current implementation of <ref type="bibr" target="#b14">[14]</ref> could not fit data with resolution higher than r = 6 to our 11Gb GPU memory, thus no results are available for UGSCNN at r = {7, 8}.) r = 6 r = 7 r = 8 UGSCNN 1055s 7817s -HexUNet 238s 406s 978s  <ref type="bibr" target="#b14">[14]</ref> is used for the comparison.) mentation accuracy for small objects (e.g. "pedestrian" and "cyclist") which we attribute to unbalanced dataset. Future work will incorporate better architectures such as <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b24">24]</ref> for improved segmentation of small objects. Finally, we plan to exploit our framework for further orientation-aware learning tasks, such as localization and mapping. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Image-grid-aligned representation of spherical data Spherical input data (a) is represented by an icosahedronbased geodesic grid (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Convolution with our hexagonal filters (a) and upsampling (b) reduce to standard CNN operations after padding the sphere component with features from neighboring sphere parts. Pooling is computed with a standard 2x2 kernel with stride 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Given arc-based interpolation of the neighborhood for north-alignment (a), our convolution is computed with 2 weighted filters (b). The weights are precomputed for all vertices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>and the U-Net base-Qualitative segmentation results on 2D3DS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Segmentation results on Omni-SYNTHIA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>GTFigure 9 .</head><label>9</label><figDesc>Unfolded visualization of semantic segmentation results at different resolutions on Omni-SYNTHIA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>thus far has focused on per- * Equal contribution: {chao.zhang, stephan.liwicki}@crl.toshiba.co.uk</figDesc><table><row><cell>beam</cell><cell>board</cell></row><row><cell>bookcase</cell><cell>ceiling</cell></row><row><cell>chair</cell><cell>clutter</cell></row><row><cell>column</cell><cell>door</cell></row><row><cell>floor</cell><cell>sofa</cell></row><row><cell>table</cell><cell>wall</cell></row><row><cell>window</cell><cell>unknown</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Method mIoU beam board bookcase ceiling chair clutter column door floor sofa table wall window</figDesc><table><row><cell>UNet</cell><cell>35.9</cell><cell>8.5</cell><cell>27.2</cell><cell>30.7</cell><cell>78.6</cell><cell>35.3</cell><cell>28.8</cell><cell>4.9</cell><cell cols="2">33.8 89.1</cell><cell>8.2</cell><cell cols="2">38.5 58.8</cell><cell>23.9</cell></row><row><cell>Gauge Net</cell><cell>39.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UGSCNN</cell><cell>38.3</cell><cell>8.7</cell><cell>32.7</cell><cell>33.4</cell><cell>82.2</cell><cell>42.0</cell><cell>25.6</cell><cell>10.1</cell><cell cols="2">41.6 87.0</cell><cell>7.6</cell><cell cols="2">41.7 61.7</cell><cell>23.5</cell></row><row><cell cols="2">HexRUNet 43.3</cell><cell>10.9</cell><cell>39.7</cell><cell>37.2</cell><cell>84.8</cell><cell>50.5</cell><cell>29.2</cell><cell>11.5</cell><cell cols="5">45.3 92.9 19.1 49.1 63.8</cell><cell>29.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Mean intersection over union (IoU) comparison on 2D3DS dataset. Per-class IoU is shown when available.</figDesc><table><row><cell cols="2">Method</cell><cell cols="14">mAcc beam board bookcase ceiling chair clutter column door floor sofa table wall window</cell></row><row><cell>UNet</cell><cell></cell><cell>50.8</cell><cell>17.8</cell><cell>40.4</cell><cell>59.1</cell><cell>91.8</cell><cell>50.9</cell><cell>46.0</cell><cell>8.7</cell><cell cols="5">44.0 94.8 26.2 68.6 77.2</cell><cell>34.8</cell></row><row><cell cols="2">Gauge Net</cell><cell>55.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">UGSCNN</cell><cell>54.7</cell><cell>19.6</cell><cell>48.6</cell><cell>49.6</cell><cell>93.6</cell><cell>63.8</cell><cell>43.1</cell><cell>28.0</cell><cell cols="5">63.2 96.4 21.0 70.0 74.6</cell><cell>39.0</cell></row><row><cell cols="2">HexRUNet</cell><cell>58.6</cell><cell>23.2</cell><cell>56.5</cell><cell>62.1</cell><cell>94.6</cell><cell>66.7</cell><cell>41.5</cell><cell>18.3</cell><cell cols="5">64.5 96.2 41.1 79.7 77.2</cell><cell>41.1</cell></row><row><cell cols="16">Table 4. Mean class accuracy (mAcc) comparison on 2D3DS dataset. Per-class accuracy is shown when available.</cell></row><row><cell>RGB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UGSCNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HexUNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>building</cell><cell>car</cell><cell></cell><cell>cyclist</cell><cell></cell><cell>fence</cell><cell>marking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>misc</cell><cell cols="2">pedestrian</cell><cell>pole</cell><cell></cell><cell>road</cell><cell>sidewalk</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sign</cell><cell>sky</cell><cell></cell><cell cols="2">vegetation</cell><cell>invalid</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Method mIoU building car cyclist fence marking misc pedestrian pole road sidewalk sign sky vegetation Mean IoU comparison at r = 6 on Omni-SYNTHIA dataset. Per-class accuracy comparison at r = 6 on Omni-SYNTHIA dataset.</figDesc><table><row><cell>UNet</cell><cell>38.8</cell><cell>80.8</cell><cell>59.4</cell><cell>0.0</cell><cell>0.3</cell><cell>54.3</cell><cell>12.1</cell><cell>4.8</cell><cell>16.4 74.3</cell><cell>58.2</cell><cell>0.2 90.4</cell><cell>49.6</cell></row><row><cell>UGSCNN</cell><cell>36.9</cell><cell>63.3</cell><cell>33.3</cell><cell>0.0</cell><cell>0.1</cell><cell>73.7</cell><cell>1.2</cell><cell>2.3</cell><cell>10.0 79.9</cell><cell>69.3</cell><cell>1.0 89.1</cell><cell>56.3</cell></row><row><cell>HexUNet-T</cell><cell>36.7</cell><cell>71.9</cell><cell>53.1</cell><cell>0.0</cell><cell>1.1</cell><cell>69.0</cell><cell>4.9</cell><cell>0.4</cell><cell>11.1 72.2</cell><cell>52.9</cell><cell>0.0 92.3</cell><cell>48.4</cell></row><row><cell cols="2">HexUNet-nI 42.4</cell><cell>77.1</cell><cell>64.8</cell><cell>0.0</cell><cell>2.4</cell><cell>74.3</cell><cell>10.4</cell><cell>2.0</cell><cell>23.6 84.7</cell><cell>68.6</cell><cell>1.0 93.1</cell><cell>48.7</cell></row><row><cell>HexUNet</cell><cell>43.6</cell><cell>81.0</cell><cell>66.9</cell><cell>0.0</cell><cell>2.9</cell><cell>71.0</cell><cell>13.7</cell><cell>5.6</cell><cell>30.4 83.1</cell><cell>67.0</cell><cell>1.5 93.3</cell><cell>50.2</cell></row><row><cell>Method</cell><cell cols="12">mAcc building car cyclist fence marking misc pedestrian pole road sidewalk sign sky vegetation</cell></row><row><cell>UNet</cell><cell>45.1</cell><cell>91.9</cell><cell>63.6</cell><cell>0.0</cell><cell>4.5</cell><cell>57.1</cell><cell>17.9</cell><cell>5.0</cell><cell>19.7 88.8</cell><cell>73.9</cell><cell>0.2 94.8</cell><cell>69.3</cell></row><row><cell>UGSCNN</cell><cell>50.7</cell><cell>93.2</cell><cell>81.4</cell><cell>0.0</cell><cell>5.3</cell><cell>83.2</cell><cell>33.7</cell><cell>2.5</cell><cell>14.9 90.8</cell><cell>82.7</cell><cell>1.3 96.1</cell><cell>74.0</cell></row><row><cell>HexUNet-T</cell><cell>44.8</cell><cell>80.0</cell><cell>60.9</cell><cell>0.0</cell><cell>1.6</cell><cell>74.7</cell><cell>26.9</cell><cell>0.4</cell><cell>13.0 80.0</cell><cell>75.2</cell><cell>0.0 96.2</cell><cell>73.4</cell></row><row><cell>HexUNet-nI</cell><cell>50.6</cell><cell>83.9</cell><cell>69.6</cell><cell>0.0</cell><cell>2.5</cell><cell>82.9</cell><cell>39.1</cell><cell>2.0</cell><cell>30.7 91.8</cell><cell>83.6</cell><cell>1.1 94.8</cell><cell>76.5</cell></row><row><cell>HexUNet</cell><cell>52.2</cell><cell>88.7</cell><cell>72.7</cell><cell>0.0</cell><cell>3.3</cell><cell>85.9</cell><cell>36.6</cell><cell>6.2</cell><cell>42.5 89.6</cell><cell>83.7</cell><cell>1.6 95.6</cell><cell>71.6</cell></row><row><cell>Method</cell><cell cols="6">r = 6 mIoU mAcc mIoU mAcc mIoU mAcc r = 7 r = 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UNet</cell><cell>38.8</cell><cell>45.1</cell><cell>44.6</cell><cell>52.6</cell><cell>43.8</cell><cell>52.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UGSCNN</cell><cell>36.9</cell><cell>50.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HexUNet-T</cell><cell>36.7</cell><cell>44.8</cell><cell>38.0</cell><cell>47.2</cell><cell>45.3</cell><cell>52.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">HexUNet-nI 42.4</cell><cell>50.6</cell><cell>45.1</cell><cell>53.4</cell><cell>45.4</cell><cell>53.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HexUNet</cell><cell>43.6</cell><cell>52.2</cell><cell>48.3</cell><cell>57.1</cell><cell>47.1</cell><cell>55.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Comparison of average training time per epoch. Evaluations are performed on one Nvidia 1080Ti GPU with 11Gb memory. (Our method is implemented and benchmarked in Tensorflow, while PyTorch implementation of</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/maxjiang93/ugscnn</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodol?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;16</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3189" to="3197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cube padding for weakly-supervised salience prediction in 360 ? videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;19</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Spherical CNNs. In ICLR&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kicanaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04615</idno>
		<title level="m">Gauge equivariant convolutional networks and the icosahedral CNN</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08498</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Steerable CNNs. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SphereNet: Learning spherical representations for detection and classification in omnidirectional images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Condurache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="518" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;17</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning so (3) equivariant representations with spherical cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="54" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hexaconv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02108</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Active convolution: Learning the shape of convolution for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;17</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4201" to="4209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spherical CNNs on unstructured grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR&apos;19</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic-driven generation of hyperlapse from 360 degree video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2610" to="2621" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>June</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08196</idno>
		<title level="m">Spherephd: Applying cnns on a spherical polyhedron representation of 360 degree images</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning 3d shapes using alt-az anisotropic 2-sphere convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR&apos;19</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Salnet360: Saliency maps for omni-directional images with cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chalasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Communication</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;17</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Segmenting and tracking extreme climate events using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mudigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Obrien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prabhat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning for Physical Sciences Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>held with NIPS&apos;17</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIC-CAI&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;16</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning spherical convolution for fast features from 360 imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;17</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Kernel transformer networks for compact spherical convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03115</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Design of kernels in convolutional neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;16</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="51" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cohen. 3d steerable cnns: Learning rotationally equivariant features in volumetric data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10402" to="10413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04458</idno>
		<title level="m">Cubenet: Equivariance to 3d rotation and translation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;17</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Oriented response networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;17</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4961" to="4970" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
