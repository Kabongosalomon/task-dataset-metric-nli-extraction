<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Categorical Depth Distribution Network for Monocular 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Reading</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto Robotics Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
							<email>ali.harakeh@utoronto.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto Robotics Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Chae</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto Robotics Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
							<email>stevenw@utias.utoronto.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto Robotics Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Categorical Depth Distribution Network for Monocular 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monocular 3D object detection is a key problem for autonomous vehicles, as it provides a solution with simple configuration compared to typical multi-sensor systems. The main challenge in monocular 3D detection lies in accurately predicting object depth, which must be inferred from object and scene cues due to the lack of direct range measurement. Many methods attempt to directly estimate depth to assist in 3D detection, but show limited performance as a result of depth inaccuracy. Our proposed solution, Categorical Depth Distribution Network (CaDDN), uses a predicted categorical depth distribution for each pixel to project rich contextual feature information to the appropriate depth interval in 3D space. We then use the computationally efficient bird's-eye-view projection and single-stage detector to produce the final output detections. We design CaDDN as a fully differentiable end-to-end approach for joint depth estimation and object detection. We validate our approach on the KITTI 3D object detection benchmark, where we rank 1 st among published monocular methods. We also provide the first monocular 3D detection results on the newly released Waymo Open Dataset. We provide a code release for CaDDN which is made available here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Perception in 3D space is a key component in fields such as autonomous vehicles and robotics, enabling systems to understand their environment and react accordingly. Li-DAR <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b49">51]</ref> and stereo <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b9">11]</ref> sensors have a long history of use for 3D perception tasks, showing excellent results on 3D object detection benchmarks such as the KITTI 3D object detection benchmark <ref type="bibr" target="#b14">[16]</ref> due to their ability to generate precise 3D measurements.</p><p>Monocular based 3D perception has been pursued simultaneously, motivated by the potential for a low-cost, easyto-deploy solution with a single camera <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b20">22]</ref>. Performance on the same 3D object detection benchmarks lags significantly relative to LiDAR and stereo methods, due to the loss of depth information when scene information is projected onto the image plane. To combat this effect, monocular object detection methods <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b57">59]</ref> often learn depth explicitly, by training a monocular depth estimation network in a separate stage. However, depth estimates are consumed directly in the 3D object detection stage without an understanding of depth confidence, leading to networks that tend to be overconfident in depth predictions. Over-confidence in depth is particularly an issue at long range <ref type="bibr" target="#b57">[59]</ref>, leading to poor localization. Further, depth estimation is separated from 3D detection during the training phase, preventing depth map estimates from being optimized for the detection task.</p><p>Depth information in image data can also be learned implicitly, by directly transforming features from images to 3D space and finally to bird's-eye-view (BEV) grids <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b42">44]</ref>. Implicit methods, however, tend to suffer from feature smearing, wherein similar image features can exist at multiple locations in the projected space. Feature smearing increases the difficulty of localizing objects in the scene.</p><p>To resolve the identified issues, we propose a monocular 3D object detection method, CaDDN, that enables accurate 3D detection by learning categorical depth distributions. By leveraging probabilistic depth estimation, CaDDN is able to generate high quality bird's-eye-view feature representations from images in an end-to-end fashion. We summarize our approach with three contributions.</p><p>(1) Categorical Depth Distributions. In order to perform 3D detection, we predict pixel-wise categorical depth distributions to accurately locate image information in 3D space. Each predicted distribution describes the probabilities that a pixel belongs to a set of predefined depth bins. We encourage our distributions to be as sharp as possible around the correct depth bins, in order to encourage our network to focus more on image information where depth estimation is both accurate and confident <ref type="bibr" target="#b21">[23]</ref>. By doing so, our network is able to produce sharper and more accurate features that are useful for 3D detection (see <ref type="figure" target="#fig_0">Figure 1</ref>). On the other hand, our network retains the ability to produce less sharp distributions when depth estimation confidence is low. Using categorical distributions allows our feature encoding to capture the inherent depth estimation uncertainty to reduce the impact of erroneous depth estimates, a property shown to be key to CaDDN's improved performance in Section 4.3. Sharpness in our predicted depth distributions is encouraged through supervision with one-hot encodings of the correct depth bin, which can be generated by projecting LiDAR depth data into the camera frame.</p><p>(2) End-To-End Depth Reasoning. We learn depth distributions in an end-to-end fashion, jointly optimizing for accurate depth prediction as well as accurate 3D object detection. We argue that joint depth estimation and 3D detection reasoning encourages depth estimates to be optimized for the 3D detection task, leading to increased performance as shown in Section 4.3.</p><p>(3) BEV Scene Representation. We introduce a novel method to generate high quality bird's-eye-view scene representations from single images using categorical depth distributions and projective geometry. We select the bird'seye-view representation due to its ability to produce excellent 3D detection performance with high computational efficiency <ref type="bibr" target="#b24">[26]</ref>. The generated bird's-eye-view representation is used as input to a bird's-eye-view based detector to produce the final output.</p><p>CaDDN is shown to rank first among all previously published monocular methods on the Car and Pedestrian categories of the KITTI 3D object detection test benchmark [1], with margins of 1.69% and 1.46% AP| R40 respectively. We are the first to report monocular 3D object detection results on the Waymo Open Dataset <ref type="bibr" target="#b54">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Monocular Depth Estimation. Monocular depth estimation is performed by generating a single depth value for every pixel in an image. As such, many monocular depth estimation methods are based on architectures used in well-studied pixel-to-pixel mapping problems such as semantic segmentation. As an example, fully convolutional networks (FCNs) <ref type="bibr" target="#b32">[34]</ref> were introduced for semantic segmentation, and were subsequently adopted for monocular depth estimation <ref type="bibr" target="#b23">[25]</ref>. The atrous spatial pyramid pooling (ASPP) module was also first proposed for semantic segmentation in DeepLab <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b4">6]</ref> and subsequently used for depth estimation in DORN <ref type="bibr" target="#b13">[15]</ref> and BTS <ref type="bibr" target="#b25">[27]</ref>. Further, many methods jointly perform depth estimation and segmentation <ref type="bibr" target="#b61">[63,</ref><ref type="bibr" target="#b64">66,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b12">14]</ref> in an end-to-end manner. We follow the design of the semantic segmentation network DeepLabV3 <ref type="bibr" target="#b4">[6]</ref> for estimating categorical depth distributions for each pixel in the image. BEV Semantic Segmentation. BEV segmentation methods <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b47">49]</ref> attempt to predict BEV semantic maps of 3D scenes from images. Images can be used to either directly estimate BEV semantic maps <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b58">60]</ref> or to estimate a BEV feature representation <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b39">41]</ref> as an intermediate step for the segmentation task. In particular, Lift, Splat, Shoot <ref type="bibr" target="#b42">[44]</ref> predicts categorical depth distributions in an unsupervised manner, in order to generate intermediate BEV representations. In this work, we predict categorical depth distributions via supervision with ground truth one-hot encodings to generate more accurate depth distributions for object detection. Monocular 3D Detection. Monocular 3D object detection methods often generate intermediate representations to assist in the 3D detection task. Based on these representations, monocular detection can be divided into three categories: direct, depth-based, and grid-based methods. Direct Methods. Direct methods <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b30">32]</ref> estimate 3D detections directly from images without predicting an intermediate 3D scene representation. Rather, direct methods <ref type="bibr" target="#b51">[53,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b1">3]</ref> can incorporate the geometric relationship between the 2D image plane and 3D space to assist with detections. For example, object keypoints can be estimated on the image plane, in order to assist in 3D box construction using known geometry <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b27">29]</ref>. M3D-RPN <ref type="bibr" target="#b1">[3]</ref> introduces depth-aware convolutions that divides the input row-wise and learns non-shared kernels for each region, to learn location specific features that correlate to regions in 3D space. Shape estimation can be performed for objects in the scene to create an understanding of 3D object geometry. Shape estimates can be supervised from labeled vertices of 3D CAD models <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b22">24]</ref>, from LiDAR scans <ref type="bibr" target="#b20">[22]</ref>, or directly from input data in a self-supervised manner <ref type="bibr" target="#b0">[2]</ref>. A drawback for direct methods is that detections are generated directly from 2D images, without access to explicit depth information, usually resulting in reduced performance in localization relative to other methods. Depth-Based Methods. Depth-based methods perform the 3D detection task using pixel-wise depth maps as an additional input, where the depth maps are precomputed using monocular depth estimation architectures <ref type="bibr" target="#b13">[15]</ref>. Estimated depth maps can be used in combination with images to perform the 3D detection task <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b62">64,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b11">13]</ref>. Alternatively, depth maps can be converted to 3D point clouds, commonly known as Pseudo-LiDAR <ref type="bibr" target="#b57">[59]</ref>, which are either used directly <ref type="bibr" target="#b59">[61,</ref><ref type="bibr" target="#b63">65]</ref> or combined with image information <ref type="bibr" target="#b60">[62,</ref><ref type="bibr" target="#b35">37]</ref> to generate 3D object detection results. Depthbased methods separate depth estimation from 3D object detection during the training stage, leading to the learning of sub-optimal depth maps used for the 3D detection task. Accurate depth should be prioritized for pixels belonging to objects of interest, and is less important for background pixels, a property that is not captured if depth estimation and object detection are trained independently. Grid-Based Methods. Grid-based methods avoid estimating raw depth values by predicting a BEV grid <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b53">55]</ref> representation, to be used as input for 3D detection architectures. Specifically, OFT <ref type="bibr" target="#b46">[48]</ref> populates a voxel grid by projecting voxels into the image plane and sampling image features, to be transformed into a BEV representation. Multiple voxels can be projected to the same image feature, leading to repeated features along the projection ray and reduced detection accuracy.</p><p>CaDDN addresses all identified issues by jointly performing depth estimation and 3D object detection in an endto-end manner, and leverages the depth estimates to generate meaningful bird's-eye-view representations with accu-rate and localized features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>CaDDN learns to generate BEV representations from images by projecting image features into 3D space. 3D object detection is then performed with the rich BEV representation using an efficient BEV detection network. An overview of CaDDN's architecture is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D Representation Learning</head><p>Our network learns to produce BEV representations that are well-suited for the task of 3D object detection. Taking an image as input, we construct a frustum feature grid using the estimated categorical depth distributions. The frustum feature grid is transformed into a voxel grid using known camera calibration parameters, and then collapsed to a bird's-eye-view feature grid. Frustum Feature Network. The purpose of the frustum feature network is to project image information into 3D space, by associating image features to estimated depths. Specifically, the input to the frustum feature network is an image I ? R W I ?H I ?3 , where W I , H I are the width and height of the image. The output is a frustum feature grid G ? R W F ?H F ?D?C , where W F , H F , are the width and height of the image feature representation, D is the number of discretized depth bins, and C is the number of feature channels. We note that the structure of the frustum grid is similar to the plane-sweep volume used in the stereo 3D de- tection method DSGN <ref type="bibr" target="#b9">[11]</ref>.</p><p>A ResNet-101 <ref type="bibr" target="#b15">[17]</ref> backbone is used to extract image</p><formula xml:id="formula_0">featuresF ? R W F ?H F ?C (see Image Backbone in Fig- ure 2)</formula><p>. In our implementation, we extract the image features from Block1 of the ResNet-101 backbone in order to maintain a high spatial resolution. A high spatial resolution is necessary for an effective frustum to voxel grid transformation, such that the frustum grid can be finely sampled without repeated features.</p><p>The image featuresF are used to estimate pixel-wise categorical depth distributions D ? R W F ?H F ?D , where the categories are the D discretized depth bins. Specifically, we predict D probabilities for each pixel in the image features F, where each probability indicates the network's confidence that depth value belongs to a specified depth bin. The definition of the depth bins relies on the depth discretization method as discussed in Section 3.3.</p><p>We follow the design of the semantic segmentation network DeepLabV3 <ref type="bibr" target="#b4">[6]</ref> to estimate the categorical depth distributions from image featuresF (Depth Distribution Network in <ref type="figure" target="#fig_1">Figure 2</ref>), where we modify the network to produce pixel-wise probability scores of belonging to depth bins rather than semantic classes with a downsample-upsample architecture. Image featuresF are downsampled with the remaining components of the ResNet-101 <ref type="bibr" target="#b15">[17]</ref> backbone (Block2, Block3, and Block4). An atrous spatial pyramid pooling <ref type="bibr" target="#b4">[6]</ref> (ASPP) module is applied to capture multi-scale information, where the number of output channels is set as D. The output of the ASPP module is upsampled to the original feature size with bilinear interpolation to produce the categorical depth distributions D ? R W F ?H F ?D . A softmax function is applied for each pixel to normalize the D logits into probabilities between 0 and 1.</p><p>In parallel to estimating depth distributions, we perform channel reduction (Image Channel Reduce in <ref type="figure" target="#fig_1">Figure 2</ref>) on image featuresF to generate the final image features F, using a 1x1 convolution + BatchNorm + ReLU layer to reduce the number of channels from C = 256 to C = 64. Channel reduction is required to reduce the high memory footprint of ResNet-101 features that will be populated in the 3D frustum grid.</p><p>Let </p><formula xml:id="formula_1">G(u, v) = D(u, v) ? F(u, v)<label>(1)</label></formula><p>where D(u, v) is the predicted depth distribution and G(u, v) is an output matrix of size D ? C. The outer product in Equation 1 is computed for each pixel to form frustum</p><formula xml:id="formula_2">features G ? R W F ?H F ?D?C .</formula><p>Frustum to Voxel Transformation. The frustum features G ? R W F ?H F ?D?C are transformed to a voxel representation V ? R X?Y ?Z?C leveraging known camera calibration and differentiable sampling, shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Voxel sampling points s v k = [x, y, z] T k are generated at the center of each voxel and transformed to the frustum grid to form frustum sampling pointss</p><formula xml:id="formula_3">f k = [u, v, d c ] T k ,</formula><p>where d c is the continuous depth value along the frustum depth axis d i . The transformation is performed using the camera calibration matrix P ? R 3?4 . Each continuous depth value d c is converted to a discrete depth bin index d i using the depth discretization method outlined in Section 3.3. Frustum features in G are sampled using sampling points</p><formula xml:id="formula_4">s f k = [u, v, d i ] T k</formula><p>with trilinear interpolation (shown in blue in <ref type="figure" target="#fig_3">Figure 4</ref>) to populate voxel features in V.</p><p>The spatial resolution of the frustum grid G and the voxel grid V should be similar for an effective transformation. A high resolution voxel grid V leads to a high density of sampling points that will oversample a low resolution frustum grid, resulting in a large amount of similar voxel features. Therefore, we extract the featuresF from Block1 of the ResNet-101 backbone to ensure our frustum grid G is of high spatial resolution. Voxel Collapse to BEV. The voxel features V ? R X?Y ?Z?C are collapsed to a single height plane to generate bird's-eye-view features B ? R X?Y ?C . BEV grids greatly reduce the computational overhead while offering similar detection performance to 3D voxel grids <ref type="bibr" target="#b24">[26]</ref>, motivating their use in our network. We concatenate the vertical axis z of the voxel grid V along the channel dimension c to form a BEV gridB ? R X?Y ?Z * C . The number of channels are reduced using a 1x1 convolution + BatchNorm + ReLU layer (see BEV Channel Reduce in <ref type="figure" target="#fig_1">Figure 2)</ref>, which retrieves the original number of channels C while learning the relative importance of each height slice, resulting in a BEV grid B ? R X?Y ?C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">BEV 3D Object Detection</head><p>To perform 3D object detection on the BEV feature grid, we adopt the backbone and detection head of the wellestablished BEV 3D object detector PointPillars <ref type="bibr" target="#b24">[26]</ref>, as it has been shown to provide accurate 3D detection results with a low computational overhead. For the BEV backbone, we increase the number of 3x3 convolution + BatchNorm + ReLU layers in the downsample blocks from (4, 6, 6) used in the original PointPillars <ref type="bibr" target="#b24">[26]</ref> to <ref type="bibr" target="#b8">(10,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b8">10)</ref> for Block1, Block2, and Block3 respectively. Increasing the number of convolutional layers expands the learning capacity in our BEV network, important for learning from lower quality features produced by images compared to higher quality features originally produced by LiDAR point clouds. We use the same detection head as PointPillars <ref type="bibr" target="#b24">[26]</ref> to generate our final detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Depth Discretization</head><p>The continuous depth space is discretized in order to define the set of D bins used in the depth distributions D. Depth discretization can be performed with uniform discretization (UD) with a fixed bin size, spacing-increasing discretization (SID) <ref type="bibr" target="#b13">[15]</ref> with increasing bin sizes in log space, or linear-increasing discretization (LID) <ref type="bibr" target="#b55">[57]</ref> with linearly increasing bin sizes. Depth discretization techniques are visualized in <ref type="figure" target="#fig_5">Figure 5</ref>. We adopt LID as our depth discretization as it provides balanced depth estimation for all depths <ref type="bibr" target="#b55">[57]</ref>. LID is defined as:</p><formula xml:id="formula_5">d c = d min + d max ? d min D(D + 1) ? d i (d i + 1)<label>(2)</label></formula><p>where d c is the continuous depth value, [d min , d max ] is the full depth range to be discretized, D is the number of depth bins, and d i is the depth bin index. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Depth Distribution Label Generation</head><p>We require depth distribution labelsD in order to supervise our predicted depth distributions. Depth distribution labels are generated by projecting LiDAR point clouds into the image frame to create sparse dense maps. Depth completion <ref type="bibr" target="#b18">[20]</ref> is performed to generate depth values at each pixel in the image. We require depth information at each image feature pixel, so we downsample the depth maps of size W I ? H I to the image feature size W F ? H F . The depth maps are converted to bin indices using the LID discretization method described in Section 3.3, followed by a conversion into a one-hot encoding to generate the depth distribution labels. A one-hot encoding ensures the depth distribution labels are sharp, essential to encourage sharpness in our depth distribution predictions via supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training Losses</head><p>Generally, classification is performed by predicting categorical distributions, and encouraging sharpness in the distribution in order to select the correct class <ref type="bibr" target="#b17">[19]</ref>. We leverage classification to encourage a single correct depth bin when supervising the depth distribution network, using the focal loss <ref type="bibr" target="#b28">[30]</ref>:</p><formula xml:id="formula_6">L depth = 1 W F ? H F W F u=1 H F v=1 FL(D(u, v),D(u, v)) (3)</formula><p>where D is the depth distribution predictions andD is the depth distribution labels. We found that autonomous driving datasets contain images with fewer object pixels than background pixels, leading to loss functions that prioritize background pixels when all pixel losses are weighted evenly. We set the focal loss <ref type="bibr" target="#b28">[30]</ref> weighting factor ? as ? fg = 3.25 for foreground object pixels and ? bg = 0.25 for background pixels. Foreground object pixels are deter-Car (IOU = 0.7)</p><p>Pedestrian (IOU = 0.5) Cyclist (IOU = 0. mined as all pixels that lie within 2D object bounding box labels, and background pixels are all remaining pixels. We set the focal loss <ref type="bibr" target="#b28">[30]</ref> focusing parameter ? = 2.0. We use the classification loss L cls , regression loss L reg , and direction classification loss L dir from PointPillars <ref type="bibr" target="#b24">[26]</ref> for 3D object detection. The total loss of our network is the combination of the depth and 3D detection losses:</p><formula xml:id="formula_7">L = ? depth L depth + ? cls L cls + ? reg L reg + ? dir L dir (4)</formula><p>where ? depth , ? cls , ? reg , ? dir are fixed loss weighting factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>To demonstrate the effectiveness of CaDDN we present results on both the KITTI 3D object detection benchmark <ref type="bibr" target="#b14">[16]</ref> and the Waymo Open Dataset <ref type="bibr" target="#b54">[56]</ref>.</p><p>The KITTI 3D object detection benchmark <ref type="bibr" target="#b14">[16]</ref> is divided into 7,481 training samples and 7,518 testing samples. The training samples are commonly divided into a train set (3,712 samples) and a val set (3,769 samples) following <ref type="bibr" target="#b8">[10]</ref>, which is also adopted here. We compare CaDDN with existing methods on the test set by training our model on both the train and val sets. We evaluate on the val set for ablation by training our model on only the train set.</p><p>The Waymo Open Dataset <ref type="bibr" target="#b54">[56]</ref> is a more recently released autonomous driving dataset, which consists of 798 training sequences and 202 validation sequences. The dataset also includes 150 test sequences without ground truth data. The dataset provides object labels in the full 360 ? field of view with a multi-camera rig. We only use the front camera and only consider object labels in the frontcamera's field of view (50.4 ? ) for the task of monocular object detection, and provide results on the validation sequences. We sample every 3 rd frame from the training sequences to form our training set <ref type="bibr" target="#b49">(51,</ref><ref type="bibr">564</ref>   <ref type="bibr" target="#b41">[43]</ref>. The network is trained on a NVIDIA Tesla V100 (32G) GPU. The Adam <ref type="bibr" target="#b16">[18]</ref> optimizer is used with an initial learning rate of 0.001 and is modified using the one-cycle learning rate policy <ref type="bibr" target="#b52">[54]</ref>. We train the model for 80 epochs on the KITTI dataset <ref type="bibr" target="#b14">[16]</ref> and 10 epochs on the Waymo Open Dataset <ref type="bibr" target="#b54">[56]</ref>. We use a batch size of 4 for KITTI <ref type="bibr" target="#b14">[16]</ref> and a batch size of 2 for Waymo. The values ? depth = 3.0, ? cls = 1.0, ? reg = 2.0, ? dir = 0.2 are used for the loss weighting factors in Equation <ref type="bibr" target="#b2">4</ref>. We employ horizontal flip as our data augmentation and train one model for all classes. During inference, we filter boxes with a score threshold of 0.1 and apply non-maximum suppression (NMS) with an IoU threshold of 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">KITTI Dataset Results</head><p>Results on the KITTI dataset <ref type="bibr" target="#b14">[16]</ref> are evaluated using average precision (AP| R40 ). The evaluation is separated by difficulty settings (Easy, Moderate, and Hard) and by object class (Car, Pedestrian, and Cyclist). The Car class has an IoU criteria of 0.7 while the Pedestrian and Cyclist classes have an IoU criteria of 0.5, where IoU criteria is a threshold to be considered a true positive detection. <ref type="table">Table 1</ref> shows the results of CaDDN on the KITTI <ref type="bibr" target="#b14">[16]</ref> test set compared to state-of-the-art published monocular methods, listed in rank order of performance on the Car class at the Moderate difficulty setting. We note that our method outperforms previous single frame methods by large margins on AP| R40 of +2.40%, +1.69%, and +1.29% on the Car class on the Easy, Moderate, and Hard difficulties respectively. Additionally, CaDDN ranks higher than the multi-frame method Kinematic3D <ref type="bibr" target="#b2">[4]</ref>. Our method also outperforms the previous state-of-the art method on the Pedestrian class MonoPair <ref type="bibr" target="#b10">[12]</ref> with margins on AP| R40 of +2.85%, +1.46%, and +1.23%. Our method achieves second place on the Cyclist class with margins on AP| R40 of -1.37%, -1.33%, and -0.38% relative to MonoPSR <ref type="bibr" target="#b20">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Waymo Dataset Results</head><p>We adopt the officially released evaluation to calculate the mean average precision (mAP) and the mean average precision weighted by heading (mAPH) on the Waymo Open Dataset <ref type="bibr" target="#b54">[56]</ref>. The evaluation is separated by difficulty setting (LEVEL 1, LEVEL 2) and distance to the sensor (0 -30m, 30 -50m, and 50m -?). We evaluate on the Vehicle class with an IoU criteria of 0.7 and 0.5.</p><p>To the best of our knowledge, no monocular methods have reported results on Waymo. In order to provide a baseline, we extend the official implementation of M3D- RPN <ref type="bibr" target="#b1">[3]</ref> to support the Waymo Open Dataset <ref type="bibr" target="#b54">[56]</ref>. <ref type="table" target="#tab_2">Table 2</ref> shows the results of both the M3D-RPN <ref type="bibr" target="#b1">[3]</ref> baseline and CaDDN on the Waymo validation set. Our method significantly outperforms M3D-RPN <ref type="bibr" target="#b1">[3]</ref> with margins on AP/APH of +4.69%/+4.65% and +4.15%/+4.12% on the LEVEL 1 and LEVEL 2 difficulties respectively for an IoU criteria of 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>We provide ablation studies on our network to validate our design choices. The results are shown in <ref type="table">Tables 3 and 4</ref>. Sharpness in Depth Distributions. Experiment 1 in <ref type="table">Ta-ble 3</ref> shows the detection performance when frustum features G are populated by repeating image features F along depth axis d i . Experiment 2 adds depth distribution predictions D to separately weigh image features F, which improves performance on AP| R40 by +1.50%, +0.77%, and +0.46% on the Car class on the Easy, Moderate, and Hard difficulties respectively. Performance is greatly increased (+10.40%, +7.60%, +6.54%) once depth distribution supervision is added in Experiment 3 validating its inclusion. The addition of depth distribution supervision encourages sharp and accurate categorical depth distributions, that encourages image information to be located in 3D space where depth estimation is both accurate and confident. Encouraging sharpness around correct depth bins results in object features that are uniquely located and easily distinguished (see <ref type="figure" target="#fig_0">Figure 1</ref>) in the BEV projection. Object Weighting for Depth Distribution Estimation. Experiments 1, 2, and 3 in <ref type="table">Table 3</ref> use a fixed loss weighting factor ? = 0.25 for all pixels in the depth loss function L depth . Experiment 4 shows an improvement (+0.67%, +1.07%, +0.91%) after depth loss weights ? fg = 3.25/? bg = 0.25 are set seperately for foreground object and background pixels (see Section 3.5). Setting a larger foreground object weighting factor ? fg encourages depth estimation to be prioritized for object pixels, leading to more accurate depth estimation and localization for objects. Linear Increasing Discretization. Experiment 5 in <ref type="table">Table 3</ref> shows the detection performance improvement (+3.17%, +1.21%, +1.09%) when LID (see Section 3.3) is used rather than uniform discretization. We attribute the performance increase to the accurate depth estimation LID provides across all depths <ref type="bibr" target="#b55">[57]</ref>. Joint Depth Understanding. Experiments 1, 2 and 3 in <ref type="table">Table 4</ref> show the detection performance with separate depth estimation from BTS <ref type="bibr" target="#b25">[27]</ref>, DORN <ref type="bibr" target="#b13">[15]</ref>, and CaDDN respectively. The depth maps from BTS <ref type="bibr" target="#b25">[27]</ref> and DORN <ref type="bibr" target="#b13">[15]</ref> are converted to depth bin indices using LID discretization as outlined in Section 3.3, and converted to a one-hot encoding to generate the depth distributions D. The one-hot encoding places the image feature at a single depth bin indicated by the input depth map when generating frustum features G. We constuct an equivalent version of CaDDN that selects a single depth bin for each pixel, by selecting the bin with highest probability for each distribution in D. Experiment 4 shows improved performance (+2.97%, +1.45%, +1.86%) when depth estimation and object detection are performed jointly, which we attribute to the well-known benefits of end-to-end learning for 3D detection. Categorical Depth Distributions. Experiment 5 in <ref type="table">Table 4</ref> uses the full depth distribution D in the frustum features computation G = D ? F, leading to a clear increase in performance (+2.96%, 2.60%, 1.88%). We attribute the perfor- <ref type="figure">Figure 6</ref>. We plot the entropy of the estimated depth distributions D against depth. We show both the mean (solid line) and 95% confidence interval (shaded region) at each ground truth depth bin. mance increase to the additional depth uncertainty information embedded in the feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Depth Distribution Uncertainty</head><p>To validate that our depth distributions contain meaningful uncertainty information, we compute the Shanon entropy for each estimated categorical depth distribution in D. We label each distribution with its associated ground truth depth bin and foreground/background classification. For each group, we compute the entropy statistics which are shown in <ref type="figure">Figure 6</ref>. We observe that entropy generally increases as a function of depth, where depth estimates are challenging, indicating our distributions describe meaningful uncertainty information. Our network produces the lowest distribution entropy at pixels with ground truth depth of around 6 meters. We attribute the high entropy at depths closer than 6 meters to the small number of pixels at shorter ranges in the training set. Finally, we note that the foreground depth distribution estimates have slightly higher entropy than background pixels, a phenomenon that can also be attributed to training set imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented CaDDN, a novel monocular 3D object detection method that estimates accurate categorical depth distributions for each pixel. The depth distributions are combined with the image features to generate bird's-eye-view representations that retain depth confidence, to be exploited for 3D object detection. We have shown that estimating sharp categorical distributions centered around the correct depth value, and jointly performing depth estimation and object detection is vital for 3D object detection performance, leading to a 1 st place ranking on the KITTI dataset [1] among all published methods at the time of submission.   <ref type="table">Table 5</ref> shows the results of CaDDN on the KITTI <ref type="bibr" target="#b14">[16]</ref> test set for BEV detection. Our method outpeforms previous single frame methods by large margins on AP| R40 of +2.91%, +1.59%, and +2.22% on the Car class on the Easy, Moderate, and Hard difficulties respectively. Our method outperforms the previous state-of-the art method on the Pedestrian class MonoPair <ref type="bibr" target="#b10">[12]</ref> with margins on AP| R40 of +3.73%, +2.37%, and +1.88%. Our method achieves first or second place on the Cyclist class with margins on AP| R40 of -1.37%, -1.33%, and +0.18% relative to MonoPSR <ref type="bibr" target="#b20">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. KITTI Dataset Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Ablation Studies</head><p>Depth Discretization. <ref type="table">Table 6</ref> shows the detection peformance of CaDDN with each of the depth discretization methods outlined in Section 3.3. We observe that LID offers the highest performance, leading to its use for our method. Feature Resolution. <ref type="table" target="#tab_6">Table 7</ref> shows the detection peformance of CaDDN when modifying the image feature extraction layer in the Image Backbone (See <ref type="figure" target="#fig_1">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Details</head><p>Depth Distributions. We add an depth bin to our depth distributions D that represents any depth outside the range [d min , d max ]. The added bin is included in the depth distribution loss L depth , but is removed when generating frustum features G. Training Details. We initialize the Image Backbone and Depth Distribution Network (See <ref type="figure" target="#fig_1">Figure 2</ref>) using the DeepLabV3 <ref type="bibr" target="#b4">[6]</ref> model pretrained on the MS-COCO dataset <ref type="bibr" target="#b29">[31]</ref>. All other components are randomly initialized.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Input image. (b) Without depth distribution supervision, BEV features from CaDDN suffer from smearing effects. (c) Depth distribution supervision encourages BEV features from CaDDN to encode meaningful depth confidence, in which objects can be accurately detected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>CaDDN Architecture. The network is composed of three modules to generate 3D feature representations and one to perform 3D detection. Frustum features G are generated from an image I using estimated depth distributions D, which are transformed into voxel features V. The voxel features are collapsed to bird's-eye-view features B to be used for 3D object detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Each feature pixel F(u, v) is weighted by its depth distribution probabilities D(u, v) of belonging to D discrete depth bins to generate frustum features G(u, v).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Sampling points in each voxel are projected into the frustum grid. Frustum features are sampled using trilinear interpolation (shown as blue in G) to populate voxels in V.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(u, v, c) represent a coordinate in image features F and (u, v, d i ) represent a coordinate in categorical depth distributions D, where (u, v) are the feature pixel location, c is the channel index, and d i is the depth bin index. To generate a frustum feature grid G, each feature pixel F(u, v) is weighted by its associated depth bin probabilities in D(u, v) to populate the depth axis d i , visualized in Figure 3. Feature pixels can be weighted by depth probability using the outer product, defined as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Depth Discretization Methods. Depth dc is discretized over a depth range [dmin, dmax] into D discrete bins. Commonly used methods include uniform (UD), spacing-increasing (SID), and linear-increasing (LID) discretization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>samples) due to the large dataset size and high frame rate. Input Parameters. The voxel grid is defined by a range and voxel size in 3D space. On KITTI [16], we use [2, 46.8] ? [?30.08, 30.08] ? [?3, 1] (m) for the range and [0.16, 0.16, 0.16] (m) for the voxel size for the x, y, and z axes respectively. On Waymo, we use [2, 55.76] ? [?25.6, 25.6] ? [?4, 4] (m) for the range and [0.16, 0.16, 0.16] (m) for the voxel size. Additionally, we downsample Waymo images to 1248 ? 832. Training and Inference Details. Our method is implemented in PyTorch</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results on the Waymo Open Dataset Validation Set on the Vehicle class. We evaluate M3D-RPN<ref type="bibr" target="#b1">[3]</ref> as a baseline for comparison.</figDesc><table><row><cell>Difficulty</cell><cell>Method</cell><cell cols="8">3D mAP Overall 0 -30m 30 -50m 50m -? Overall 0 -30m 30 -50m 50m -? 3D mAPH</cell></row><row><cell>LEVEL 1 (IOU = 0.7)</cell><cell>M3D-RPN [3] CaDNN (Ours) Improvement</cell><cell>0.35 5.03 +4.69</cell><cell>1.12 14.54 +13.43</cell><cell>0.18 1.47 +1.28</cell><cell>0.02 0.10 +0.08</cell><cell>0.34 4.99 +4.65</cell><cell>1.10 14.43 +13.33</cell><cell>0.18 1.45 +1.28</cell><cell>0.02 0.10 +0.08</cell></row><row><cell>LEVEL 2 (IOU = 0.7)</cell><cell>M3D-RPN [3] CaDNN (Ours) Improvement</cell><cell>0.33 4.49 +4.15</cell><cell>1.12 14.50 +13.38</cell><cell>0.18 1.42 +1.24</cell><cell>0.02 0.09 +0.07</cell><cell>0.33 4.45 +4.12</cell><cell>1.10 14.38 +13.28</cell><cell>0.17 1.41 +1.24</cell><cell>0.02 0.09 +0.07</cell></row><row><cell>LEVEL 1 (IOU = 0.5)</cell><cell>M3D-RPN [3] CaDNN (Ours) Improvement</cell><cell cols="2">3.79 17.54 +13.76 +33.86 11.14 45.00</cell><cell>2.16 9.24 +7.08</cell><cell>0.26 0.64 +0.39</cell><cell cols="2">3.63 17.31 +13.69 +33.77 10.70 44.46</cell><cell>2.09 9.11 +7.02</cell><cell>0.21 0.62 +0.41</cell></row><row><cell>LEVEL 2 (IOU = 0.5)</cell><cell>M3D-RPN [3] CaDNN (Ours) Improvement</cell><cell cols="2">3.61 16.51 +12.89 +33.75 11.12 44.87</cell><cell>2.12 8.99 +6.87</cell><cell>0.24 0.58 +0.34</cell><cell cols="2">3.46 16.28 +12.82 +33.66 10.67 44.33</cell><cell>2.04 8.86 +6.81</cell><cell>0.20 0.55 +0.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>CaDDN Ablation Experiments on the KITTI val set using AP| R 40 . D indicates depth distribution prediction, L depth indicates depth distribution supervision. ? fg indicates separate setting of loss weighting factor for foreground object pixels in the depth loss function L depth . LID indicates the LID discretization method. CaDDN Depth Estimation Ablation on the KITTI val set using AP| R 40 . D indicates the source of the depth estimates used to generate depth distributions. L depth indicates if depth estimation and object detection are seperately or jointly optimized. ? indicates if full distributions are used to generate frustum features G.</figDesc><table><row><cell cols="3">Exp. D L depth ? fg LID</cell><cell cols="3">Car (IOU = 0.7) Easy Mod. Hard</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell>7.83</cell><cell>5.66</cell><cell>4.84</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell>9.33</cell><cell>6.43</cell><cell>5.30</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell cols="3">19.73 14.03 11.84</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell cols="3">20.40 15.10 12.75</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell cols="3">23.57 16.31 13.84</cell></row><row><cell>Exp.</cell><cell>D</cell><cell>L depth ?</cell><cell cols="3">Car (IOU = 0.7) Easy Mod. Hard</cell></row><row><cell>1</cell><cell>BTS [27]</cell><cell>Sep.</cell><cell cols="2">16.69 10.18</cell><cell>8.63</cell></row><row><cell>2</cell><cell>DORN [15]</cell><cell>Sep.</cell><cell cols="2">16.43 11.04</cell><cell>9.65</cell></row><row><cell>3</cell><cell>CaDDN</cell><cell>Sep.</cell><cell cols="3">17.64 12.26 10.10</cell></row><row><cell>4</cell><cell>CaDDN</cell><cell>Joint</cell><cell cols="3">20.61 13.71 11.96</cell></row><row><cell>5</cell><cell>CaDDN</cell><cell>Joint</cell><cell cols="3">23.57 16.31 13.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>BEV detection results on the KITTI [16] test set. Results are shown using the AP|R 40 metric only for results that are readily available. We indicate the highest result with red and the second highest with blue. Full results for CaDDN can be accessed here. CaDDN Depth Discretization Ablation Experiments on the KITTI val set using AP| R 40 . See Section 3.3 for description of depth discretization methods.</figDesc><table><row><cell cols="2">Exp. Disc. Method</cell><cell>Car (IOU = 0.7) Easy Mod. Hard</cell></row><row><cell>1</cell><cell>UD</cell><cell>20.40 15.10 12.75</cell></row><row><cell>2</cell><cell>SID</cell><cell>22.00 15.65 13.26</cell></row><row><cell>3</cell><cell>LID</cell><cell>23.57 16.31 13.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>CaDDN Feature Resolution Ablation Experiments on the KITTI val set using AP| R 40 . Block indicates the Image Backbone block where image featuresF are extracted from (See Figure 2). WF ? HF indicates the width and height of image featuresF. periment 1 shows the performance when image features are extracted from Block1. Experiments 2, 3, and 4 shows reduced performance when smaller resolution image features are extracted. Smaller spatial resolutions in the image features cause oversampling in the frustum to voxel grid transformation, leading to many voxel features V with similar features (See Section 3.1).</figDesc><table><row><cell>). Ex-</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Monocular differentiable rendering for selfsupervised 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Beker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Mihai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiro</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Matsuoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">M3D-RPN: monocular 3D region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Kinematic 3d object detection in monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep MANTA: A coarse-to-fine many-task network for joint 2d and 3D vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaonary</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Teuli?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Chateau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Huimin Ma1, Sanja Fidler, and Raquel Urtasun</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dsgn: Deep stereo geometry network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Monopair: Monocular 3d object detection using pairwise spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supervised machine learning: A review of classification techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kotsiantis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Conference on Emerging Artificial Intelligence Applications in Computer Engineering: Real Word AI Systems with Applications in EHealth, HCI, Information Retrieval and Pervasive Technologies, NLD</title>
		<meeting>the 2007 Conference on Emerging Artificial Intelligence Applications in Computer Engineering: Real Word AI Systems with Applications in EHealth, HCI, Information Retrieval and Pervasive Technologies, NLD</meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">Lake</forename><surname>Waslander</surname></persName>
		</author>
		<title level="m">defense of classical image processing: Fast depth completion on the CPU. CRV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint 3D proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">Lake</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Monocular 3D object detection leveraging accurate proposals and shape reconstruction. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Accurate uncertainties for deep learning using calibrated regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Fenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00263</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3D-RCNN: Instance-level 3D object reconstruction via render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Vasileios Belagiannis, Federico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PointPillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Kyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Wook</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il Hong</forename><surname>Suh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Confidence guided stereo 3d object detection with split depth estimation. IROS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaici</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feidao</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft COCO: common objects in context. ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reinforced axial refinement network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chufan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Smoke: Singlestage monocular 3d object detection via keypoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Toth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPRW</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Monocular semantic occupancy grid mapping with convolutional variational auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gijs</forename><surname>Dubbelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinus Jacobus Gerardus</forename><surname>Van De Molengraft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>Rethinking pseudo-lidar representation. ECCV, 2020. 1, 3, 6</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Accurate monocular 3D object detection via color-embedded 3D reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ROI-10D: monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Monolayout: Amodal scene layout from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustubh</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapnil</forename><surname>Daga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhika</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Shankar Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madhava</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Murthy</forename><surname>Jatavallabhula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WACV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
		<title level="m">3d bounding box estimation using deep learning and geometry. CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Bev-seg: Bird&apos;s eye view semantic segmentation using geometry and semantic point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaahan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Radia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ionel</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gonzalez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cross-view semantic segmentation for sensing surroundings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankai</forename><surname>Bowen Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Philion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Object-centric stereo matching for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">End-to-end pseudo-lidar for image-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<idno>2020. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Predicting semantic map representations from images using pyramid occupancy networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Orthographic feature transform for monocular 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to look around objects for topview representations of outdoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menghua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">PV-RCNN: Pointvoxel feature set abstraction for 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno>2020. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">PointR-CNN: 3D object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>L?pez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards generalization across depth for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1 -learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Learning 2d to 3d lifting for object detection in 3d for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Cen-ter3d: Center-based monocular 3d object detection with joint depth understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Dorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiragkumar</forename><surname>Savani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sdc-depth: Semantic divide-and-conquer network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pseudo-LiDAR from visual depth estimation: Bridging the gap in 3D object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A parametric top-view representation of complex road scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection with pseudo-lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3D object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pad-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Distancenormalized unified representation for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun Kim Xuepeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection via feature domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Joint task-recursive learning for semantic segmentation and depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
