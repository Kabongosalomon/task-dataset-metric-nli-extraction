<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long-Tailed Recognition via Weight Balancing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaden</forename><surname>Alshammari</surname></persName>
							<email>shaden@mit.eduyxw@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT UIUC ?</orgName>
								<orgName type="institution">Argo AI CMU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT UIUC ?</orgName>
								<orgName type="institution">Argo AI CMU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT UIUC ?</orgName>
								<orgName type="institution">Argo AI CMU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
							<email>shuk@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT UIUC ?</orgName>
								<orgName type="institution">Argo AI CMU</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Long-Tailed Recognition via Weight Balancing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Clearly, classifiers of common classes have "artificially" larger norms than rare ones, because they are trained with more data. This can result in over-predictions of common classes, or alternatively, under-predictions of rare classes. This observation motivates us to balance norms via parameter regularization. To do so, we explore simple weight balancing techniques including L2-normalization, weight decay, and the MaxNorm constraint. We find that applying the latter two results in class weights to be far more balanced ((b)-right), allowing rare classes to have a "fighting chance" when competing with common classes. Our model boosts overall accuracy to 53.35% (blue bars in (a)), significantly higher than the naive model (38.38%) and prior art, e.g., RIDE (49.1%) <ref type="bibr" target="#b72">[73]</ref>, ACE (49.6%) <ref type="bibr" target="#b9">[10]</ref>, and PaCo (52.0%) <ref type="bibr" target="#b16">[17]</ref>. Results are from experiments <ref type="table">(Table 1</ref>) on CIFAR100-LT with an imbalance factor 100 <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In the real open world, data tends to follow long-tailed class distributions, motivating the well-studied long-tailed recognition (LTR) problem. Naive training produces models that are biased toward common classes in terms of higher accuracy. The key to addressing LTR is to balance various aspects including data distribution, training losses, and gradients in learning. We explore an orthogonal direction, weight balancing, motivated by the empirical observation that the naively trained classifier has "artificially" larger weights in norm for common classes (because there exists abundant data to train them, unlike the rare classes). We investigate three techniques to balance weights, L2normalization, weight decay, and MaxNorm. We first point out that L2-normalization "perfectly" balances per-class weights to be unit norm, but such a hard constraint might prevent classes from learning better classifiers. In contrast, weight decay penalizes larger weights more heavily and so learns small balanced weights; the MaxNorm constraint encourages growing small weights within a norm ball but caps all the weights by the radius. Our extensive study shows that both help learn balanced weights and greatly improve the LTR accuracy. Surprisingly, weight decay, although underexplored in LTR, significantly improves over prior work. Therefore, we adopt a two-stage training paradigm and propose a simple approach to LTR: (1) learning features using the cross-entropy loss by tuning weight decay, and (2) learning classifiers using class-balanced loss by tuning weight decay and MaxNorm. Our approach achieves the state-ofthe-art accuracy on five standard benchmarks, serving as a future baseline for long-tailed recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the real open world, data tends to follow long-tailed distributions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b84">85]</ref>. Through the lens of classification, this means that the number of per-class data, or class cardinality, is heavily imbalanced <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b71">72]</ref>. Numerous applications emphasize the rare classes. For example, autonomous vehicles should recognize not only common objects such as cars and pedestrians, but also rare ones like strollers and animals for driving safety <ref type="bibr" target="#b40">[41]</ref>. A bioimage analysis system should recognize both commonlyand rarely-seen species for ecological research <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b71">72]</ref>. This motivates the well-studied problem of long-tailed recognition (LTR), which trains on class-imbalanced data and aims to achieve high accuracy averaged across all the classes <ref type="bibr" target="#b83">[84]</ref>. LTR has attracted increasing attention especially using deep neural networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b77">78]</ref>.</p><p>Status quo. Because common classes have significantly more training data than rare classes, they dominate the training loss, contribute the most of gradients, and obtain high accuracy <ref type="bibr" target="#b83">[84]</ref>. Consequently, a naively trained model performs well on them but significantly worse on the rare classes ( <ref type="figure" target="#fig_1">Fig. 1a</ref>). The key to addressing LTR is to balance various aspects. Many methods propose to balance per-class data distributions during training by upsampling rare classes or downsampling common classes <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Some others balance the losses or gradients during training <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b70">71]</ref>. Some approaches adopt transfer learning that learn features on common classes and use the features to learn rare-class classifiers <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b86">87]</ref>. It shows that decoupling feature learning and classifier learning leads to significant improvement over models that train them jointly <ref type="bibr" target="#b37">[38]</ref>. From benchmarking results, the state-of-theart accuracy is achieved by either ensembling expert models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b75">76]</ref> or the adoption of self-supervised pretraining with aggressive data augmentation techniques <ref type="bibr" target="#b16">[17]</ref>.</p><p>Motivation. We observe that a naively trained model on long-tailed class distributed data has "artificially" large weights for common classes <ref type="figure" target="#fig_1">(Fig 1b)</ref>. Prior work also notes this observation <ref type="bibr" target="#b37">[38]</ref>. Intuitively, this is because common classes have more training data that significantly grows classifier weights ( <ref type="figure" target="#fig_2">Fig. 2a</ref>). This motivates our work to balance network weights across classes for long-tailed recognition. In contrast to existing methods (as exhaustively reviewed in a recent survey paper <ref type="bibr" target="#b83">[84]</ref>), our work explores an orthogonal direction of weight balancing.</p><p>Contribution. To balance network weights in norm, we study three simple techniques. We first point out that L2normalization perfectly balances classifier weights to have unit norm ( <ref type="figure" target="#fig_2">Fig. 2b</ref>). However, L2-normalization might be too strict to learn flexible parameters for better classifiers. We then study weight decay <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b43">44]</ref> and the MaxNorm constraint <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b65">66]</ref>. Weight decay penalizes larger weights more heavily and so learns small balanced weights <ref type="figure" target="#fig_2">(Fig. 2c</ref>); MaxNorm encourages growing small weights within a norm ball and caps all the weights by the radius <ref type="figure" target="#fig_2">(Fig. 2d</ref>). We find that both effectively learn balanced weights and boost LTR performance, although these well-known regularizers are underexplored in the LTR literature. Please refer to <ref type="figure" target="#fig_1">Fig. 1</ref> for a nutshell of our work.</p><p>Key Findings. We show how simple regularizers boost LTR performance. Without inventing new losses or adopting aggressive augmentation techniques or designing new network modules, we follow the simple two-stage training paradigm <ref type="bibr" target="#b37">[38]</ref> and derive a simple approach that rivals or outperforms the state-of-the-art methods: (1) train a backbone using the standard cross-entropy loss by properly tuning weight decay, and (2) train the classifier using a classbalanced loss by tuning weight decay and MaxNorm. It is important to note how our simple approach challenges the increasingly complicated LTR models, and hence serves as a strong future baseline for LTR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Long-Tailed Recognition (LTR). Real-world data tends to follow long-tailed class distributions, i.e., a few classes are commonly seen that have significantly more data than many classes that are infrequently / rarely seen. As a result, a model naively trained on such data performs significantly worse on rare classes than common classes. LTR requires training on such data to achieve high accuracy averaged across all classes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b77">78]</ref>. For LTR, numerous methods emphasize the accuracy on rare-classes. Data rebalancing techniques resample the training data to achieve a more balanced data distribution across classes <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b66">67]</ref>, such as over-sampling rare-classes <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref> and undersampling common-classes <ref type="bibr" target="#b20">[21]</ref>. Class-balanced loss reweighting assigns weights to the classes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b82">83]</ref>, or even training examples <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b67">68]</ref>, aiming to modify their gradients to make the class-imbalanced data contribute properly to training. Transfer learning methods transfer feature representations learned on the common-classes to the rare-classes <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b78">79]</ref>. Recent work examines the training procedure and finds LTR to be better addressed by decoupling feature learning and classifier learning, rather than training them jointly <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b87">88]</ref>. It is found that the SGD momentum causes issues in LTR that prevent further improvement <ref type="bibr" target="#b70">[71]</ref> . Other sophisticated methods exploit selfsupervised pretraining with more aggressive data augmentation techniques <ref type="bibr" target="#b16">[17]</ref>, or ensemble expert models trained on different data regimes <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b72">73]</ref>. For a comprehensive review of the LTR literature, we refer the reader to the recent survey paper <ref type="bibr" target="#b83">[84]</ref>. Different from all the existing methods, we explore an orthogonal direction of parameter regularization, leading to a much simpler approach to LTR.</p><p>Parameter Regularization adds extra information to solve an ill-posed problem, improving generalizability and preventing overfitting <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b56">57]</ref>. Regularization plays a crucial role in deep learning <ref type="bibr" target="#b42">[43]</ref>. One well-known regularization is weight decay, which often applies L2-norm penalty on network weights <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52]</ref>. There exist many more regularizations <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">45]</ref>, such as weight normalization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b63">64]</ref>, MaxNorm constraints <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b65">66]</ref>, data augmentation <ref type="bibr" target="#b81">[82]</ref>, and dropout <ref type="bibr" target="#b34">[35]</ref>. In this work, we particularly examine the well-known yet underexplored regularizers in the LTR literature: L2-normalization, weight decay, and the MaxNorm constraint <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b65">66]</ref>.</p><p>Stage-wise Training turns to be effective in training deep networks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b85">86]</ref>. This can date back to stagewise layer pretraining <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34]</ref>. Recently, Kang et al. convincingly demonstrate that stage-wise training is important to LTR <ref type="bibr" target="#b37">[38]</ref>. Concretely, Kang et al. propose to decouple feature learning and classifier learning into two independent stages <ref type="bibr" target="#b37">[38]</ref>: (1) feature learning using the standard cross-entropy loss, and (2) classifier learning over the learned feature using a class-balancing loss. While they did not explain why a single one-stage training with the classbalancing loss performed poorly, intuitively, this is because a class-balancing loss artificially scales up gradients computed from rare-class training data, which hurts the feature representation learning and hence the final LTR performance. Follow-up work indirectly demonstrates this intuition with improved performance by stabilizing gradients during training <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b70">71]</ref>. In our paper, we adopt this twostage training procedure, but focus on how to balance network weights for LTR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Weight Balancing for Long-Tailed Learning</head><p>Preliminaries. Long-tailed recognition (LTR) aims to train over a training set D={( LTR focuses on learning a K-way classification network f (?; ?) parameterized by ? = {? l,j }, where ? l,j is the j th filter weights at layer-l. In a conv-layer, ? l,j is a 3D kernel that convolves the input (activation). For brevity, we denote ? k as the classifier filter corresponding to classk. Given a data example x i , the network predicts a label y i = f (x i ; ?). We measure the prediction error between y i and the ground-truth y i using a cost function (y i , y i ), e.g., a cross-entropy (CE) loss <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b55">56]</ref> or a class-balanced loss (CB) <ref type="bibr" target="#b18">[19]</ref>. To train the network f (?; ?), we optimize ? by minimizing (y i , y i ) over the whole training set D:</p><formula xml:id="formula_0">x i , y i )} N i=1 , where data ex- ample x i is labeled as y i ? [1, . . . , K].</formula><formula xml:id="formula_1">? * = arg min ? F (?; D) ? N i=1 f (x i ; ?), y i .<label>(1)</label></formula><p>Naively solving (1) produces a classifier (i.e., the last layer) that has artificially large weights in norm for common classes ( <ref type="figure" target="#fig_1">Fig. 1b</ref>-left, <ref type="figure" target="#fig_2">Fig. 2a</ref>). Therefore, we are motivated to learn a balanced classifier by regularizing classifier weights, denoted by ? k for k = 1, . . . , K. Intermediate layers also have imbalanced filter weights ( <ref type="figure">Fig. 3</ref>) even though a filter tends to fire on multiple classes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b80">81]</ref>. Generally, one can also balance the weights at intermediate layers, and our study shows that doing so boosts performance. Nevertheless, to simplify presentation in the following, we focus on regularization on the classifier weights ? k 's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Weight Balancing Techniques</head><p>We examine the following three techniques to balance weights with respect to norms.</p><p>L2-normalization. A "perfect" way to balance the classifier weights ? k 's is to L2-normalize the classifier weights:</p><formula xml:id="formula_2">? * = arg min ? F (?; D), s.t. ? k 2 2 = 1, ? k. (2)</formula><p>As L2-normalization forces weights to be unit-length, the classifier weights will have unit norm constant during training ( <ref type="figure" target="#fig_2">Fig. 2b</ref>). Inspired by <ref type="bibr" target="#b37">[38]</ref>, we also post-hoc L2normalize a trained classifier, i.e., ? k =? k / ? k 2 . We find that post-hoc L2-normalization oftentimes improves LTR performance, favoring rare-classes yet sacrificing commonclass accuracy. But it can also significantly decrease overall performance, e.g., on iNaturalist in <ref type="table">Table 3</ref>. Post-hoc L2normalization is similar to the ? -normalization <ref type="bibr" target="#b37">[38]</ref>, which allows varied per-class weight norms (rather than forcing them to be the same) and achieves better LTR performance. This suggests that L2-normalization is too strict to strike a balance among the long-tailed distributed classes. Importantly, our exploration finds that, while training with an L2-normalization constraint on the classifier improves over naive training, it underperforms the other two regularizers described below.</p><p>Weight Decay is a well-studied technique <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b54">55]</ref> used to constrain a network by limiting the growth of the network weights. It decreases the complexity of the network, effectively mitigating overfitting and improving generalization. Weight decay typically applies an L2-norm penalty to the network weights (we focus on the classifier ? k 's for now):</p><formula xml:id="formula_3">? * = arg min ? F (?; D) + ? k ? k 2 2 ,<label>(3)</label></formula><p>where ? is a hyperparameter to control the impact of weight decay. The weight decay term in (3) penalizes more heavily on large weights, preventing them from growing too large ( <ref type="figure" target="#fig_2">Fig. 2c</ref>) <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b54">55]</ref>. That said, weight decay encourages learning small balanced weights, as demonstrated by <ref type="figure" target="#fig_2">Fig. 2</ref>. Somewhat surprisingly, weight decay is underexplored in the literature of long-tailed recognition. To the best of our knowledge, existing methods did not properly tune weight decay <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b70">71]</ref> (cf. code <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b69">70]</ref>) aside from their technical innovations. This makes it unclear whether their improved LTR performance is due to better regularization inherent in these methods. Importantly, our exploration demonstrates that, by simply tuning weight decay, we outperform most of the state-of-the-art methods on long-tailed benchmarks ( <ref type="table" target="#tab_4">Tables 2 and 3</ref>)! MaxNorm Constraint. The third regularizer we explore is the MaxNorm constraint <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b65">66]</ref>. MaxNorm caps weight norms within an L2-norm ball with radius ?:</p><formula xml:id="formula_4">? * = arg min ? F (?; D), s.t. ? k 2 2 ? ? 2 , ? k,<label>(4)</label></formula><p>where the hyperparameter ? is the radius of the norm-ball. Solving (4) can be efficiently done through Projected Gradient Descent (PGD), which projects big weights that are outside the L2-norm ball onto the constraint set <ref type="bibr" target="#b65">[66]</ref>. It simply applies a renormalization step after each batch update. Specifically, at each iteration, PGD first computes an updated ? k and then projects it onto the norm ball:  <ref type="figure">Figure 3</ref>. Weight decay helps learn balanced weights at hidden layers. We compare the norm distribution at each layer (which has 512 filters) from the naive model (orange) and the one trained with weight decay (blue). For each layer of a model, we sort the filter weights of each layer from high to low, compute their mean (the centerline) and variance (the shadow). While individual filters in the hidden layers are not class-specific by design, recent work demonstrates that certain filters tend to fire on certain classes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b80">81]</ref>; we still find them to be "imbalanced" in norms from the naive model. Weight decay encourages learning small and balanced filters, cf. its flat centerline and small variance.</p><formula xml:id="formula_5">? k ? min 1, ?/ ? k 2 * ? k .<label>(5)</label></formula><p>Different from L2-normalization that strictly sets the norm value for all the filter weights as 1, MaxNorm relaxes this constraint that allows the weights to move within the normball during training, as visualized in <ref type="figure" target="#fig_2">Fig. 2d</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Further Discussion</head><p>To better understand how and why the aforementioned regularizers work for long-tailed recognition, we discuss the following aspects.</p><p>Weight Decay and MaxNorm. Both regularizers balance weight norms dynamically during training, as opposed to L2-normalization which simply forces per-filter weights to be unit-length in norm. Weight decay encourages learning small weights, and MaxNorm encourages weights to grow within a norm ball but cap them when their norms exceed the radius. Weight decay pulls all weights to the origin. As a result, when ? increases in (3), the weight decay penalty prevails F (?; D), making training unstable <ref type="bibr" target="#b5">[6]</ref> ( <ref type="figure">Fig. 4)</ref>. In contrast, MaxNorm does not pull weights towards the origin but simply caps the weight norms, and so has better numerical stability.</p><p>Although weight decay and MaxNorm appear to be quite different, they are related that weight decay can be thought of as an immediate step when solving MaxNorm. Let's rewrite the MaxNorm constrained objective function (4) by constructing a Lagrangian function:</p><formula xml:id="formula_6">? * = arg min ? max ??0 F (?; D) + k ?( ? k 2 2 ? ?),<label>(6)</label></formula><p>where ? is the Karush-Kuhn-Tucker (KKT) multiplier. Suppose that we could solve (6) using the coordinate descent method, i.e., iteratively optimizing over ? and ? <ref type="bibr" target="#b74">[75]</ref>. When fixing ?, we have the same loss as (3) which is  <ref type="figure">Figure 4</ref>. Tuning weight decay drastically improves long-tailed recognition performance. We do not use any class-balancing techniques but simply use CE loss and tune weight decay ? to regularize all network weight. For example, tuning ? yields 46.1% accuracy on CIFAR100-LT (IF=100), outperforming many state-ofthe-art methods such as DiVE (45.4%) <ref type="bibr" target="#b31">[32]</ref> and SSD (46.0%) <ref type="bibr" target="#b45">[46]</ref>. By checking the publicly available code, we find that existing methods do not tune weight decay, e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref> set ? =2e-4 (according to their code <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>), leading to poor accuracy 38.32%. constrained by weight decay, and ? becomes the hyperparameter ? to control weight decay. That said, solving the weight decay constrained problem (3) is a step of solving MaxNorm <ref type="bibr" target="#b3">(4)</ref>. Interestingly, we find that applying weight decay and MaxNorm jointly yields better performance than using each of them independently. This is probably because of their complementary advantages: (1) weight decay on the small weights still improves their generalization and reduces overfitting, and (2) MaxNorm prevents the large weights from dominating the training.</p><p>Extreme cases. When ??? in MaxNorm, (4) boils down to the naive training (1). On the other hand, a sufficiently small ? encourages all the weights to be close to the surface of the norm-ball. This is still different from the L2-normalization which strictly requires the weights to be on the surface. Compared to L2-normalization ( <ref type="figure" target="#fig_2">Fig. 2b)</ref>, MaxNorm offers freespace within the norm ball to let weights grow <ref type="figure" target="#fig_2">(Fig. 2d</ref>). This intuitively explains why MaxNorm performs better than L2-normalization.</p><p>Weight decay can easily balance all network weights. We point out that weight decay regularizes classifier weights without the need to separate per-class filters. This offers convenience in training, differently from MaxNorm which must separate each filter and scaling it w.r.t its norms. Because of such a convenience, weight decay can be easily used to balance all network weights <ref type="figure">(Fig. 3)</ref>. In principle, MaxNorm can also be applied to all layers, but we find it non-trivial to do so, as this seems to require setting perlayer thresholds in (4) (tuning which is time-consuming). While weight decay is widely used in network training, we find that properly tuning it drastically improves long-tailed recognition accuracy <ref type="table">(Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Pipeline</head><p>Because the aforementioned weight balancing techniques are not exclusive to each other, in principle, one  <ref type="figure">Figure 5</ref>. Frequency distributions w.r.t class cardinality of five benchmarks. Left: We modify CIFAR100 by downsampling examples per class with different imbalance factors (IF) varying from 10 to 100. Right: We use two large-scale datasets: ImageNet-LT <ref type="bibr" target="#b48">[49]</ref> that downsamples per-class images from ImageNet <ref type="bibr" target="#b19">[20]</ref>, and iNaturalist <ref type="bibr" target="#b71">[72]</ref> which is a real-world dataset with IF=500.</p><p>can use a single technique or multiple ones together. Recall that we follow the two-stage training paradigm <ref type="bibr" target="#b37">[38]</ref> in our work, which first trains a network for feature representation and then trains the classifier atop the learned features. This raises a question how to apply the weight balancing techniques effectively. Among extensive exploration, we find that tuning ? for weight decay in <ref type="formula" target="#formula_3">(3)</ref> is sufficient to learn a generalizable feature representation as the first-stage training. In contrast, applying MaxNorm is nontrivial because we find that it requires setting per-layer thresholds in (4). This tuning process is time-consuming. In the second-stage training (i.e., training the classifier), we find that tuning either/both weight decay and MaxNorm remarkably improves LTR accuracy. Because the classifier training simply involves only one layer (or two layers if we think of the top two as a non-linear classifier), tuning hyperparameters of the regularizers is quite efficient. To tune them, one can use random search <ref type="bibr" target="#b4">[5]</ref> or Bayesian Optimization <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b68">69]</ref>. We use the latter in this work. In summary, our simple training pipeline consists of the following two stages:</p><p>1. Feature learning: train a network by using the crossentropy loss and tuning weight decay. 2. Classifier learning: train a classifier over the learned features using a class-balanced loss <ref type="bibr" target="#b18">[19]</ref>, weight decay, and MaxNorm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We carry out extensive experiments to demonstrate how balancing network weights boosts long-tailed recognition performance. First, we ablate the design choices in our pipeline as suggested in Section 3.3. Then, we benchmark our methods on five established long-tailed datasets, showing that they rival or outperform existing LTR methods. We start with the experiment setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup</head><p>Datasets. We use five long-tailed benchmarks. Following <ref type="bibr" target="#b12">[13]</ref>, we modify the CIFAR100 dataset <ref type="bibr" target="#b41">[42]</ref>   <ref type="figure">Figure 6</ref>. Per-class weight norms (top row) and marginal likelihood (bottom row) in the classifier vs. class ID sorted by class cardinality in decreasing order. The plots are on CIFAR100-LT (IF100) val-set which has class-balanced data. According to <ref type="bibr" target="#b60">[61]</ref>, the ideal marginal likelihood should follow a uniform distribution. Interestingly, L2-normalization that "perfectly" balances weight norms does not produce "uniform" marginal likelihood. Weight decay (WD) slightly mitigates norm imbalance and marginal likelihood imbalance, but MaxNorm dramatically helps both. The final model that incorporates MaxNorm, weight decay, and class-balanced loss yields nearly "perfect" marginal likelihood and balanced weights, which have a small bias towards rare-classes, presumably to emphasize their accuracy. ponential decay functions, resulting in a long-tailed version, named CIFAR100-LT. CIFAR100-LT still has 100 classes and a balanced validation set for evaluation. By varying an imbalance factor (IF) ? [100, 50, 10], we create three longtailed training sets ( <ref type="figure">Fig. 5-left)</ref>. ImageNet-LT is introduced in <ref type="bibr" target="#b47">[48]</ref> by artificially truncating the balanced version Ima-geNet <ref type="bibr" target="#b19">[20]</ref>. ImageNet-LT has 1,000 classes, and the number of per-class training data ranges from 5 to 1280. iNat-uralist2018 <ref type="bibr" target="#b71">[72]</ref> is a real-world dataset that has 8,142 naturally long-tailed classes. <ref type="figure">Fig. 5</ref> summarizes the class frequency distributions of these datasets. ImageNet and iNat-uralist2018 are publicly available for non-commercial research and educational purposes; CIFAR100 is released under the MIT license. We note that ImageNet and CIFAR100 have a "people" class or contain images that captured human faces and person signatures. This is a concern related to fairness and privacy. Therefore, we cautiously proceed our research and release our code under the MIT License without re-distributing the data. Network architectures. For a fair comparison to prior art, we follow <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b77">78]</ref> to use specific network architectures on each dataset. We use ResNet32 <ref type="bibr" target="#b30">[31]</ref> on CIFAR100-LT, ResNeXt50 <ref type="bibr" target="#b76">[77]</ref> on ImageNet-LT, and ResNet50 [31] on iNaturalist2018.</p><p>Evaluation protocol. On each dataset, we train on the long-tailed class-imbalanced training set and evaluate on its (balanced) validation/test set. On ImageNet-LT, we tune hyperparameters and select models on its val-set and report performance on the test-set. On CIFAR100-LT and iNaturalist, which only have train-val sets, we follow the literature <ref type="bibr" target="#b48">[49]</ref> that uses the val-sets to tune and benchmark. Following <ref type="bibr" target="#b48">[49]</ref>, we further report accuracy on three splits of classes that have varied numbers of training data: Many (&gt;100), Medium (20?100), and Few (&lt; 20).</p><p>Implementation. We train our models using PyTorch toolbox <ref type="bibr" target="#b58">[59]</ref> on GeForce GTX 2080Ti GPUs. The total time spent on this work is ?2 GPU years with respect to this GPU type. We train each model for 200 epochs, with batch size as 64 (for CIFAR and ImageNet-LT) / 512 (for iNaturalist), SGD optimizer with momentum 0.9, and cosine learning rate scheduler <ref type="bibr" target="#b50">[51]</ref> that gradually decays learning rates from 0.01 to 0. We also use random left-right flipping and cropping as our training augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We study (1) the impact of weight decay in LTR, (2) how to regularize classifier learning, (3) classifier weight norms and marginal likelihood distribution, and (4) the evolution of weight norms during training. We use CIFAR100-LT (IF=100) for this study (unless stated otherwise).</p><p>Weight decay. We set a single constant ? for all network parameters and focus on the first-stage training only, i.e., we use the standard cross-entropy loss to train a single network for classification. <ref type="figure">Fig. 4</ref> draws the top-1 accuracy as a function of ? on the validation sets of three benchmarks. Clearly, tuning ? boosts accuracy, even outperforming many state-of-the-art methods (cf. Tables 2 and 3)! Moreover, the optimal ? varies for different datasetslarger datasets need a smaller weight decay, intuitively because learning over more data helps generalization and so needs less regularization.</p><p>How to regularize classifier learning. To study how to apply the balancing techniques in the second-stage learning for classifiers, we also include ? -normalization <ref type="bibr" target="#b37">[38]</ref> because it is an effective non-learned technique that post-hoc scales the classifier learned in the first stage. We present salient conclusions based on the results in <ref type="figure">Fig. 6</ref> (more in the supplement). First, with an improved backbone (owing to a properly tuned weight decay in the first stage), ? -norm boosts from 42.00% to 51.31%! This demonstrates the importance of learning a backbone that has balanced weights ( <ref type="figure">Fig. 3)</ref>. Second, it is crucial to use a class-balanced (CB) loss <ref type="bibr" target="#b18">[19]</ref> to learn the classifier. However, solely using the CB loss without regularizers only slightly improves (from 46.08% to 47.09%); once regularized with weight decay, it boosts to 52.42%. Third, applying both MaxNorm and weight decay improves further (53.35%), and learning more layers (as a non-linear MLP classifier) improves to 53.55%.</p><p>Classifier's weight norms and marginal likelihood. Inspired by <ref type="bibr" target="#b60">[61]</ref>, we examine the marginal likelihood based on predictions on the (balanced) test-set, on which the ideal marginal likelihood follows a uniform distribution <ref type="bibr" target="#b60">[61]</ref>. We plot the marginal likelihood in <ref type="figure">Fig. 6</ref>, alongside the norm distribution of different models. Interestingly, L2normalization that "perfectly" balances classifier weights does not produce balanced marginal likelihood. In contrast, MaxNorm significantly helps learn balanced weights and balanced marginal likelihood. Combining MaxNorm, weight decay, and the CB loss, the model makes nearly "perfect" marginal likelihood with a small bias toward rareclasses in weight norms, presumably because it learns to emphasize rare-class accuracy.</p><p>Weight norm evolution during training. <ref type="figure" target="#fig_2">Fig. 2</ref> depicts how classifier's weight norms evolve during training for different models. Briefly, without regularization, weights in the naive model grow fast in norm. In contrast, weight decay prevents weights from growing too large, and MaxNorm quickly caps weights on a norm-ball surface and allows small weights to grow within the ball.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Benchmark Results</head><p>Compared Methods. Considering the rapid evolution of the LTR field <ref type="bibr" target="#b83">[84]</ref>, we compare against most relevant methods. We choose methods that are recently published and representative of different types, such as Focal <ref type="bibr" target="#b46">[47]</ref> for loss reweighting, PaCo <ref type="bibr" target="#b16">[17]</ref> for self-supervised pretraining and aggressive data augmentation, RIDE <ref type="bibr" target="#b72">[73]</ref> for ensembling expert models, SSD <ref type="bibr" target="#b45">[46]</ref> and DiVE <ref type="bibr" target="#b31">[32]</ref> for transfer learning, etc. For comparison, we report our methods including the naive model, the one trained with properly tuned weight decay, and models that have the second-stage learning for classifier with regularizers. <ref type="table" target="#tab_4">Tables 2 and 3</ref> list benchmarking results on the CIFAR100-LT datasets, and ImageNet-LT and iNaturalist, respectively.</p><p>Results. Without bells and whistles, simply tuning weight decay (WD) in the first-stage training significantly boosts LTR performance over naive training and outperforms many prior methods. For example, on CIFAR100-LT (IF100) in <ref type="table" target="#tab_4">Table 2</ref>, our WD model achieves 46.08%, outperforming the naive model (38.38%) and most of the compared methods including SSD (46.00%) <ref type="bibr" target="#b45">[46]</ref> and DiVE (45.35%) <ref type="bibr" target="#b31">[32]</ref>. With the second stage (classifier learning), simply post-hoc modifying (without learning) the classifier (learned in the first stage) significantly improves performance from 46.09% to 49.60% (by L2-normalization) <ref type="table">Table 1</ref>. Ablation study on CIFAR100-LT (IF=100) w.r.t top-1 accuracy (%). "CE": cross-entropy loss; "CB": class-balanced loss <ref type="bibr" target="#b18">[19]</ref>; "WD": weight decay; "Max": MaxNorm constraint; "?norm": ? -normalization <ref type="bibr" target="#b37">[38]</ref>; "+": fine-tuning the last layer(s) as the second-stage training. Here are salient conclusions. (1) Learning with a properly tuned WD boosts performance from 38.38% to 46.08%, which is +8% increase. (2) Re-training the last layer with CB and WD gives another boost (+6%) to 52.42%. (3) Based on the above, applying additional MaxNorm yields a slight improvement +1% (53.35%); finetuning the last two layers achieves 53.55%. (4) Finetuning more layers performs worse (cf. the supplement), presumably because CB induces modified gradients that affect feature learning and so hurt the final LTR performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Many and to 51.31% (by ? -normalization). By learning the classifier regularized with MaxNorm and/or weight decay, we achieve the state of the art (53.35%). Such a conclusion holds on all benchmarks. However, on the two large-scale datasets ImageNet-LT and iNaturalists in <ref type="table">Table 3</ref>, our methods rival prior art but underperform two types of methods that have "bells and whistles", including ensemble methods (RIDE <ref type="bibr" target="#b72">[73]</ref> and ACE <ref type="bibr" target="#b45">[46]</ref>) that learn and fuse multiple models, and self-supervised learning based methods (PaCo <ref type="bibr" target="#b16">[17]</ref> and SSD <ref type="bibr" target="#b45">[46]</ref>) that adopt aggressive data augmentation techniques <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Long-tailed recognition (LTR) is a crucial challenge for real-world data that tends to be imbalanced. Our work is motivated by the empirical observation that a model naively trained over long-tailed data has artificially large weights for common classes (because they have more data to train than rare classes). We propose to learn balanced weights via parameter regularization, including weight decay and MaxNorm regularizers. Our extensive study shows that properly applying these regularizers greatly boosts LTR performance. We introduce a simple approach that outperforms prior art on five long-tailed benchmarks. Because these reg- ularizers are underexplored in the long-tailed literature, we hope our study draws attention from the practitioners that parameter regularization should be the first method to consider, when addressing real-world problems related to the long-tailed distribution.</p><p>Limitations. While we focus on the orthogonal direction of parameter regularization to address LTR, we have not studied how our approaches complement existing techniques. For example, how to balance weights in training each of the expert models, or how to balance the weights alongside sophisticated data augmentation and selfsupervised pretraining. We also point out that other regularization techniques might be better at balancing weights, for example using Lp-norm weight decay where p =2 <ref type="bibr" target="#b2">[3]</ref>. We leave them to future work.</p><p>Societal Impact. Because real-world data tends to follow long-tailed distributions, our work has multiple positive societal impacts. For example, addressing the long-tail <ref type="table">Table 3</ref>. Benchmarking on ImageNet-LT and iNaturalists in top-1 accuracy (%). Please refer to <ref type="table" target="#tab_4">Table 2</ref> for methods' names and salient conclusions. We list the numbers of compared methods reported in their respective papers. Overall, our simple approach achieves competitive results with the prior methods particularly when they train a "single (expert)" model, although underperforms a few recent state-of-the-art methods which train and ensemble expert models (RIDE <ref type="bibr" target="#b72">[73]</ref> and ACE <ref type="bibr" target="#b9">[10]</ref>), or adopt self-supervised pretraining (e.g., PaCo <ref type="bibr" target="#b16">[17]</ref> and SSD <ref type="bibr" target="#b45">[46]</ref>) with aggressive data augmentation techniques <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30]</ref>. proves an important direction for studying bias and fairness in recognition <ref type="bibr" target="#b14">[15]</ref>. However, any system that makes it easier to train a fair classifier on long-tailed classes also makes it possible for a malicious agent to train a system that automatically discriminates against a certain subgroup for which only little training data is available. This is potentially a negative societal impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet-LT iNaturalist</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detailed Ablation Study</head><p>In <ref type="table">Table 4</ref>, we list more results in addition to the ablation study presented in the main paper. Please refer to the caption for salient conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Open-Source Code</head><p>Description. We release our code with two executable Jupyter Notebook files for demonstrating our approaches (w.r.t training and evaluation). The files will reproduce the results in the ablation study on the CIFAR100-LT dataset (with an imabalance factor 100). The Jupyter Notebook files are sufficiently self-explanatory with detailed comments, and displayed output. The first file compares the first-stage training between naive training and training with weight decay. The second file studies different regularizers in the second stage training. We advise the reader to run the files in order (if running them) because the second stage training (i.e., the second demo file) requires the saved model by the first file. Running the first file takes ?2 hours with a GPU (NVIDIA GeForce RTX 3090), and the second file takes a few minutes.</p><p>? demo1_first-stage-training.ipynb Running this file compares the first-stage training between a naive network (without weight decay) and a model with a tuned weight decay. It should achieve an overall accuracy ?39% and ?46% respectively on the CIFAR100-LT (imbalance factor 100).</p><p>? demo2_second-stage-training.ipynb</p><p>Running this file will compare various regularizers used in the second-stage training. It should achieve an overall accuracy &gt;52%.</p><p>Why Jupyter Notebook? We prefer to release the code using Jupyter Notebook (https://jupyter.org) because it allows for interactive demonstration for education purposes. In case the reader would like to run python script, using the following command can convert a Jupyter Notebook file XXX.ipynb into a Python script:</p><p>jupyter nbconvert --to script XXX.ipynb <ref type="table">Table 4</ref>. Ablation study on CIFAR100-LT (IF=100) w.r.t top-1 accuracy (%). "CE": cross-entropy loss; "CB": class-balanced loss <ref type="bibr" target="#b18">[19]</ref>; "WD": weight decay; "Max": MaxNorm constraint; "default-WD": using the weight decay tuned for the first-stage training; "? -norm": ? -normalization <ref type="bibr" target="#b37">[38]</ref>; "+": fine-tuning the last layer(s) as the second-stage training. Here are salient conclusions. (1) Learning with a properly tuned WD boosts performance from 38.38% to 46.08%, that is +8% increase. <ref type="formula">(2)</ref>  We suggest assigning &gt;1GB space to run all the files. The code will save checkpoints after every training epoch.</p><p>License. We release open-source code under the MIT License to foster future research in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Video Demo</head><p>The goal of this section is to demonstrate how weights' norms evolve during training. For demonstration, we train models on the CIFAR100-LT dataset with an imbalance factor 100. To do so, we modify the ResNet34 network architecture by inserting an additional 2-dim prelogit layer. This layer has weights W = [w ij ] ? R 2?K that project 2-dim pre-logit features to K-dim logits. At the logit layer, each filter weight w i (i.e., a row of W) is class-specific. Therefore, we can plot the K 2-dim class-specific weights as K points on a 2D plane. The MaxNorm constraint upper bounds the norm of each class-specific weight, i.e., w i 2 &lt; ?. <ref type="figure">Fig. 7</ref> plots per-class weights after three different training iterations. For better visualization, we suggest the reader to watch our video demo demo2D weight evolution.mp4 in our github repository https : / / github . com / ShadeAlsha / LTR -weight -balancing / blob / master/demo2D_weight_evolution.mp4. MaxNorm: iteration 16000 class 1-common class 100-rare class 1-common class 100-rare class 1-common class 100-rare class 1-common class 100-rare <ref type="figure">Figure 7</ref>. We plot per-class filter weights of the classifier as 2D points from (a) a naively trained network without weight decay, (b) a classifier with L2-normalization, (c) a classifier with weight decay, and (d) a classifier constrained by MaxNorm. All the networks are trained on the CIFAR100-LT dataset with imbalance factor as 100. The four columns denote training iterations: iteration-0 as random initialization, iteration-4000, iteration-8000, and iteration-16000. The naively trained network learns "imbalanced" weights, i.e., large weights for the common classes and small weights for the rare classes. The model trained with L2-normalization has constant weight norms. When trained with weight decay, the network has smaller yet more balanced weights for all the classes. The MaxNorm constrained network caps weight norms, encouraging small weights (from both common and rare classes) to grow, approaching to the surface of the norm ball. We refer the reader to the video demo demo2D weight evolution.mp4 for better visualization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>norms of per-class weights from the learned classifier vs. class cardinality</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Long-tailed recognition (LTR) requires training on long-tailed class distributed data (black curve in (a)). (a) Networks naively trained on such data are biased toward common classes in terms of higher accuracy (orange bars). (b)-left plots the L2 norms of perclass weights in the naive classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>How are per-class weight norms evolving in training (x-axis)? Classes are sorted w.r.t. cardinality (y-axis). (a) In a naive model, all weights grow in norm during training, while those of common classes grow much faster. (b) Because L2-normalization constrains weights to be unit-norm, weight norms stay constant during training. (c) Weight decay (WD) regularizes all weights to be small while still allowing them to grow. (d) MaxNorm caps large weights (of common classes) while letting small weights grow. (e) Combining weight decay and MaxNorm results in small and balanced weights in norm. All plots share the same color map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>For class-k, D k is the set of all its training examples and |D k | is its cardinality. The imbalance factor, IF= max k |D k | min k |D k | , measures how imbalanced the long-tailed training set is. For LTR, IF 1. LTR emphasizes classification accuracy averaged over classes, i.e., accuracy= 1 K k acc k , where acc k is the accuracy com-puted over testing examples of class-k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>s sorted by class cardinality class ID's sorted by class cardinality #images #images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Benchmarking on CIFAR-100-LT with different imbalance factors [100, 50, 10] w.r.t top-1 accuracy (%). Please refer to the caption of Table 1 for abbreviations; "+" methods use CB loss. WD makes a substantial impact on the training of LTR networks. Finetuning the classifier with proper regularization improves much further. This clearly shows the significance of parameter regularization in balancing weights for LTR. Somewhat surprisingly, properly tuning weight decay in the two-stage training paradigm outperforms all existing methods on these three datasets.</figDesc><table><row><cell>imbalance factor</cell><cell>100</cell><cell>50</cell><cell>10</cell></row><row><cell>CE [19]</cell><cell>38.32</cell><cell>43.85</cell><cell>55.71</cell></row><row><cell>CE+CB [19]</cell><cell>39.60</cell><cell>45.32</cell><cell>57.99</cell></row><row><cell>KD [33]</cell><cell>40.36</cell><cell>45.49</cell><cell>59.22</cell></row><row><cell>LDAM-DRW [12]</cell><cell>42.04</cell><cell>46.62</cell><cell>58.71</cell></row><row><cell>BBN [88]</cell><cell>42.56</cell><cell>47.02</cell><cell>59.12</cell></row><row><cell>LogitAjust [54]</cell><cell>42.01</cell><cell>47.03</cell><cell>57.74</cell></row><row><cell>LDAM+SSP [78]</cell><cell>43.43</cell><cell>47.11</cell><cell>58.91</cell></row><row><cell>Focal [47]</cell><cell>38.41</cell><cell>44.32</cell><cell>55.78</cell></row><row><cell>Focal+CB [19]</cell><cell>39.60</cell><cell>45.17</cell><cell>57.99</cell></row><row><cell>De-confound [71]</cell><cell>44.10</cell><cell>50.30</cell><cell>59.60</cell></row><row><cell>? -norm [38]</cell><cell>47.73</cell><cell>52.53</cell><cell>63.80</cell></row><row><cell>SSD [46]</cell><cell>46.00</cell><cell>50.50</cell><cell>62.30</cell></row><row><cell>DiVE [32]</cell><cell>45.35</cell><cell>51.13</cell><cell>62.00</cell></row><row><cell>DRO-LT [65]</cell><cell>47.31</cell><cell>57.57</cell><cell>63.41</cell></row><row><cell>PaCo [17]</cell><cell>52.00</cell><cell>56.00</cell><cell>64.20</cell></row><row><cell>ACE (4-expert) [10]</cell><cell>49.60</cell><cell>51.90</cell><cell>-</cell></row><row><cell>RIDE (4-expert) [73]</cell><cell>49.10</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Our methods (weight balancing)</cell><cell></cell></row><row><cell>naive</cell><cell>38.38</cell><cell>43.99</cell><cell>57.31</cell></row><row><cell>WD</cell><cell>46.08</cell><cell>52.71</cell><cell>66.03</cell></row><row><cell>+ L2norm</cell><cell>49.60</cell><cell>56.33</cell><cell>67.16</cell></row><row><cell>+ ? -norm</cell><cell>51.31</cell><cell>57.65</cell><cell>67.79</cell></row><row><cell>+ WD</cell><cell>52.42</cell><cell>57.47</cell><cell>67.96</cell></row><row><cell>+ Max</cell><cell>50.24</cell><cell>56.06</cell><cell>67.10</cell></row><row><cell>+ WD &amp; Max</cell><cell>53.35</cell><cell>57.71</cell><cell>68.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Re-training the last layer with CB and WD gives another boost (+6%) to 52.42%.(3) Based on the above, applying additional MaxNorm yields a slight improvement +1% (53.35%); finetuning the last two layers achieves 53.55%. (4) Finetuning more layers performs worse, presumably because CB induces modified gradients that affect feature learning, and so hurt the final LTR performance.Requirement. Running our code requires some common packages. We installed Python and most packages through Anaconda. A few other packages might not be installed automatically, such as Pandas, torchvision, and Py-Torch, which are required to run our code. Below are the versions of Python and PyTorch used in our work.</figDesc><table><row><cell>model</cell><cell>many median few</cell><cell>avg</cell></row><row><cell>on the last layer (classifier)</cell><cell></cell><cell></cell></row><row><cell>WD=0 (w/ CE)</cell><cell cols="2">64.05 35.80 11.43 38.38</cell></row><row><cell>+ ? -norm (? =1.0)</cell><cell cols="2">59.54 38.23 25.93 42.00</cell></row><row><cell>WD tuned (w/ CE)</cell><cell cols="2">76.94 44.28 12.17 46.08</cell></row><row><cell>+ ? -norm (? =1.9)</cell><cell cols="2">73.11 47.69 30.10 51.31</cell></row><row><cell>+ L2norm</cell><cell cols="2">76.09 47.74 20.87 49.60</cell></row><row><cell>+ CE &amp; L2norm</cell><cell cols="2">76.37 48.11 21.00 49.87</cell></row><row><cell>+ CE &amp; WD</cell><cell cols="2">76.97 45.94 14.00 47.22</cell></row><row><cell>+ CE &amp; Max</cell><cell cols="2">76.80 47.26 15.10 47.95</cell></row><row><cell cols="3">+ CE &amp; Max &amp; default-WD 76.89 47.06 13.90 47.55</cell></row><row><cell>+ CE &amp; Max &amp; WD</cell><cell cols="2">76.80 47.51 14.40 47.83</cell></row><row><cell>+ CB</cell><cell cols="2">77.00 45.89 13.60 47.09</cell></row><row><cell>+ CB &amp; L2norm</cell><cell cols="2">76.43 48.20 21.60 50.10</cell></row><row><cell>+ CB &amp; WD</cell><cell cols="2">72.77 49.74 31.80 52.42</cell></row><row><cell>+ CB &amp; Max</cell><cell cols="2">76.49 49.23 20.67 50.20</cell></row><row><cell cols="3">+ CB &amp; Max &amp; default-WD 76.20 48.91 21.50 50.24</cell></row><row><cell>+ CB &amp; WD &amp; Max</cell><cell cols="2">72.60 51.86 32.63 53.35</cell></row><row><cell>on the last two layers</cell><cell></cell><cell></cell></row><row><cell>+ CE &amp; WD &amp; Max</cell><cell cols="2">76.34 48.46 21.17 50.03</cell></row><row><cell>+ CB &amp; WD &amp; Max</cell><cell cols="2">71.37 51.17 35.53 53.55</cell></row><row><cell>on the last five layers</cell><cell></cell><cell></cell></row><row><cell>+ CE &amp; WD &amp; Max</cell><cell cols="2">76.03 48.14 20.87 49.72</cell></row><row><cell>+ CB &amp; WD &amp; Max</cell><cell cols="2">74.37 49.80 26.63 51.45</cell></row><row><cell cols="2">? Python version: 3.7.4 [GCC 7.3.0]</cell><cell></cell></row><row><cell>? PyTorch verion: 1.7.1</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work was supported by the CMU Argo AI Center for Autonomous Vehicle Research. SA was supported in part by the KAUST Gifted Student's Program (KGSP) and the CMU Robotics Institute Summer Scholars program. YXW was supported in part by NSF Grant 2106825 and the Jump ARCHES endowment.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In the appendix, we first supplement the ablation study with more results to justify the use of regularizers for better learning for long-tailed recognition (LTR). We then present our open-source code in Jupyter Notebook as a selfexplanatory tutorial. Lastly, we attach a video demo that shows how weights change during training with different regularizers.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The space lp, with mixed norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnes</forename><surname>Benedek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Panzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Duke Mathematical Journal</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="301" to="324" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>2012. 5</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiplier methods: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dimitri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="145" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Statistics for high-dimensional data: methods, theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>B?hlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ace: Ally complementary experts for solving long-tailed recognition in one-shot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<ptr target="https://github.com/kaidic/LDAM-DRW/blob/3193f05c1e6e8c4798c5419e97c5a479d991e3e9/cifar_train.py.commit6feb304" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with labeldistribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep quantization network for efficient image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingfu</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority oversampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Why is my classifier discriminatory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fredrik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parametric contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiequan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<ptr target="https://github.com/richardaecn/class-balanced-loss/blob/1d7857208a2abc03d84e35a9d5383af8225d4b4d/src/cifar_main.py.commit0ab6eb7" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">C4. 5, class imbalance, and cost sensitivity: why under-sampling beats over-sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on learning from imbalanced datasets II</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A multiple resampling method for learning from imbalanced data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Estabrooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeho</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Japkowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="36" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring classification equilibrium in long-tailed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjian</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Regularization for deep learning. Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="216" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Long-tailed multi-label visual recognition by collaborative training on uniform and rebalanced samplings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Borderline-smote: a new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Huan</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="878" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Comparing biases for minimal network construction with back-propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorien</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distilling virtual examples for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Yin-Yin He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021. 5</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep imbalanced learning for face recognition and attribute prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2781" to="2794" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking classbalanced methods for long-tailed visual recognition from a domain adaptation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Striking the right balance with uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning of deep feature representations from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferdous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3573" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Opengan: Open-set recognition via open data generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 1992</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Regularization for deep learning: A taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kukavcka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10686</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self supervision to distillation for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021. 5</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Open category detection with pac guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risheek</forename><surname>Garrepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hendrycks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">SGDR: stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Long-tail learning via logit adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<idno>ICLR, 2021. 8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Note on generalization, regularization and architecture selection in nonlinear learning systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moody</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks for Signal Processing Proceedings of IEEE Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Machine learning: a probabilistic perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Feature selection, l 1 vs. l 2 regularization, and rotational invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Bayesian Optimization: Open source constrained global optimization tool for Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Nogueira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">The pareto, zipf and other power laws. Economics letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="15" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Balanced meta-softmax for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunjun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunan</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020. 3</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Improving the taxonomy of fossil pollen using convolutional neural networks and superresolution microscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ingrid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaramillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisca</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos D&amp;apos;</forename><surname>Oboh-Ikuenobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Apolito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Surangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Punyasena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="28496" to="28505" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Distributional robustness loss for long-tail learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dvir</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Pegasos: Primal estimated sub-gradient solver for svm. Mathematical programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cotter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Meta-weight-net: Learning an explicit mapping for sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch/blob/90c8b2c0b66d17f78b67263861bc9d858fe20128/classification/config/CIFAR100_LT/feat_unifrom.yaml.commit54c07cf" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Longtailed classification by keeping the good and removing the bad momentum causal effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Long-tailed recognition by routing diverse distribution-aware experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Coordinate descent algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="34" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning from multiple experts: Self-paced knowledge distillation for long-tailed classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Rethinking the value of labels for improving class-imbalanced learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Feature transfer learning for face recognition with under-represented data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Stagewise training accelerates convergence of testing error over sgd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoning</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03934</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Distribution alignment: A unified framework for long-tail visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Deep long-tailed learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04596</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Camera pose matters: Improving depth prediction by mitigating pose distribution bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Domain decluttering: Simplifying images to mitigate synthetic-real domain shift and improve depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Unequaltraining for deep face recognition with long-tailed noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianteng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohai</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
