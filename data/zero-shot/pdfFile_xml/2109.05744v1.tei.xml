<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-grained Entity Typing via Label Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
							<email>hongyu@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
							<email>xiaoxinyan@baidu.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
							<email>xianpei@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">State Key Laboratory of Computer Science Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">State Key Laboratory of Computer Science Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<email>wu_hua@baidu.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-grained Entity Typing via Label Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional entity typing approaches are based on independent classification paradigms, which make them difficult to recognize interdependent, long-tailed and fine-grained entity types. In this paper, we argue that the implicitly entailed extrinsic and intrinsic dependencies between labels can provide critical knowledge to tackle the above challenges. To this end, we propose Label Reasoning Network(LRN), which sequentially reasons finegrained entity labels by discovering and exploiting label dependencies knowledge entailed in the data. Specifically, LRN utilizes an auto-regressive network to conduct deductive reasoning and a bipartite attribute graph to conduct inductive reasoning between labels, which can effectively model, learn and reason complex label dependencies in a sequence-toset, end-to-end manner. Experiments show that LRN achieves the state-of-the-art performance on standard ultra fine-grained entity typing benchmarks, and can also resolve the long tail label problem effectively. arXiv:2109.05744v1 [cs.CL] 13 Sep 2021</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fine-grained entity typing (FET) aims to classify entity mentions to a fine-grained semantic label set, e.g., classify "FBI agents" in "They were arrested by FBI agents." as {organization, administration, force, agent, police}. By providing finegrained semantic labels, FET is critical for entity recognition <ref type="bibr" target="#b12">(Lin et al., 2019a</ref><ref type="bibr">(Lin et al., ,b, 2020</ref><ref type="bibr">Zhang et al., 2021b,a)</ref> and can benefit many NLP tasks, such as relation extraction <ref type="bibr" target="#b35">(Yaghoobzadeh et al., 2017;</ref>, entity linking <ref type="bibr" target="#b21">(Onoe and Durrett, 2020)</ref> and question answering <ref type="bibr" target="#b37">(Yavuz et al., 2016)</ref>.</p><p>The fundamental challenge of FET comes from its large-scale and fine-grained entity label set, which leads to significant difference between FET and conventional entity typing. First, due to the * Corresponding authors. massive label set, it is impossible to independently recognize each entity label without considering their dependencies. For this, existing approaches use the predefined label hierarchies <ref type="bibr" target="#b25">(Ren et al., 2016a;</ref><ref type="bibr" target="#b28">Shimaoka et al., 2017;</ref><ref type="bibr" target="#b0">Abhishek et al., 2017;</ref><ref type="bibr" target="#b9">Karn et al., 2017;</ref><ref type="bibr" target="#b34">Xu and Barbosa, 2018;</ref><ref type="bibr" target="#b24">Ren, 2020)</ref> or label co-occurrence statistics from training data <ref type="bibr" target="#b23">(Rabinovich and Klein, 2017;</ref><ref type="bibr" target="#b33">Xiong et al., 2019;</ref><ref type="bibr" target="#b15">Lin and Ji, 2019)</ref> as external constraints. Unfortunately, these label structures or statistics are difficult to obtain when transferring to new scenarios. Second, because of the fine-grained and large-scale label set, many long tail labels are only provided with several or even no training instances. For example, in Ultra-Fine dataset <ref type="bibr" target="#b3">(Choi et al., 2018)</ref>, &gt;80% of entity labels are with &lt;5 instances, and more seriously 25% of labels never appear in the training data. However, training data can provide very limited direct information for these labels, and therefore previous methods commonly fail to recognize these long-tailed labels.</p><p>Fortunately, the implicitly entailed label dependencies in the data provide critical knowledge to tackle the above challenges. Specifically, the dependencies between labels exist extrinsically or intrinsically. On the one hand, the extrinsic dependencies reflect the direct connections between labels, which partially appear in the form of label hierarchy and co-occurrence. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>(a) the labels person, musician, composer are with extrinsic dependencies because they form a three-level taxonomy. Furthermore, singer and composer are also with extrinsic dependency because they often co-occur with each other. On the other hand, the intrinsic dependencies entail the indirect connections between labels through their underlying attributes. For the example in <ref type="bibr">Figure 1(b)</ref>, label theorist and scientist share the same underlying attribute of scholar. Such intrinsic dependencies provide an effective way to tackle the long tail labels, because many long tail labels are actually composed by non-long tail attributes which can be summarized from non-long tail labels.</p><p>To this end, this paper proposes Label Reasoning Network (LRN), which uniformly models, learns and reasons both extrinsic and intrinsic label dependencies without given any predefined label structures. Specifically, LRN utilizes an auto-regressive network to conduct deductive reasoning and a bipartite attribute graph to conduct inductive reasoning between labels. Both of these two kinds of mechanisms are jointly applied to sequentially generate fine-grained labels in an end-to-end, sequence-toset manner. <ref type="figure" target="#fig_0">Figure 1(c)</ref> shows several examples. To capture extrinsic dependencies, LRN introduces deductive reasoning (i.e., draw a conclusion based on premises) between labels, and formulates it using an auto-regressive network to predict labels based on both the context and previous labels. For example, given previously-generated label person of the mention they, as well as the context they theorize, LRN will deduce its new label theorist based on the extrinsic dependency between person and theorist derived from data. For intrinsic dependencies, LRN introduces inductive reasoning (i.e., gather generalized information to a conclusion), and utilizes a bipartite attribute graph to reason labels based on current activated attributes of previous labels. For example, if the attributes {expert, scholar} have been activated, LRN will induce a new label scientist based on the attribute-label relations. Consequently, by decomposing labels into attributes and associating long tail labels with frequent labels, LRN can also effectively resolve the long tail label problem by leveraging their nonlong tail attributes. Through jointly leveraging the extrinsic and intrinsic dependencies via deductive and inductive reasoning, LRN can effectively handle the massive label set of FET.</p><p>Generally, our main contributions are:</p><p>? We propose Label Reasoning Network, which uniformly models, automatically learns and effectively reasons the complex dependencies between labels in an end-to-end manner.</p><p>? To capture extrinsic dependencies, LRN utilizes deductive reasoning to sequentially reason labels via an auto-regressive network. In this way, extrinsic dependencies are discovered and exploited without predefined label structures.</p><p>? To capture intrinsic dependencies, LRN utilizes inductive reasoning to reason labels via a bipartite attribute graph. By decomposing labels into attributes and associating long-tailed labels with frequent attributes, LRN can effectively reason long-tailed and even zero-shot labels.</p><p>We conduct experiments on standard Ultra-Fine <ref type="bibr" target="#b3">(Choi et al., 2018)</ref> and OntoNotes <ref type="bibr" target="#b7">(Gillick et al., 2014)</ref> dataset. Experiments show that our method achieves new state-of-the-art performance: a 13% overall F1 improvement and a 44% F1 improvement in the ultra-fine granularity. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>One main challenge for FET is how to exploit complex label dependencies in the large-scale label set. Previous studies typically use predefined label hierarchy and co-occurrence structures estimated from data to enhance the models. To this end, <ref type="bibr" target="#b25">Ren et al. (2016a)</ref> L?pez and Strube (2020) embed labels into a highdimension or a new space. And the studies exploit co-occurrence structures including limiting the label range during label set prediction <ref type="bibr" target="#b23">(Rabinovich and Klein, 2017)</ref>, enriching the label representation by introducing associated labels <ref type="bibr">(Xiong et al.,</ref>  Ac-vate Func-on ? ? <ref type="figure">Figure 2</ref>: Overview of the process for LRN which contains an encoder, a deductive reasoning-based decoder and an inductive reasoning-based decoder. The figure shows: at step 1, the label person is predicted by deductive reasoning, and the attribute human is activated; at step 3, the label scientist is generated by inductive reasoning. 2019), or requiring latent label representation to reconstruct the co-occurrence structure <ref type="bibr" target="#b15">(Lin and Ji, 2019)</ref>. However, these methods require predefined label structures or statistics from training data, and therefore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods <ref type="bibr" target="#b17">(Ma et al., 2016;</ref><ref type="bibr" target="#b44">Zhou et al., 2018;</ref><ref type="bibr" target="#b38">Yuan and Downey, 2018;</ref><ref type="bibr" target="#b19">Obeidat et al., 2019;</ref><ref type="bibr" target="#b40">Zhang et al., 2020b;</ref>, or using data augmentation with denosing strategies <ref type="bibr" target="#b26">(Ren et al., 2016b;</ref><ref type="bibr" target="#b20">Onoe and Durrett, 2019;</ref><ref type="bibr" target="#b39">Zhang et al., 2020a;</ref><ref type="bibr" target="#b1">Ali et al., 2020)</ref> or utilizing external knowledge <ref type="bibr" target="#b4">(Corro et al., 2015;</ref><ref type="bibr" target="#b5">Dai et al., 2019)</ref> to introduce more external knowledge.</p><p>In this paper, we propose Label Reasoning Network, which is significantly different from previous methods because 1) by introducing deductive reasoning, LRN can capture extrinsic dependencies between labels in an end-to-end manner without predefined structures; 2) by introducing inductive reasoning, LRN can leverage intrinsic dependencies to predict long tail labels; 3) Through the sequenceto-set framework, LRN can consider two kinds of label dependencies simultaneously to jointly reason frequent and long tail labels.</p><p>3 Label Reasoning Network for FET <ref type="figure">Figure 2</ref> illustrates the framework of Label Reasoning Network. First, we encode entity mentions through a context-sensitive encoder, then sequentially generate entity labels via two label reasoning mechanisms: deductive reasoning for exploiting extrinsic dependencies and inductive reasoning for exploiting intrinsic dependencies. In our Seq2Set framework, the label dependency knowledge can be effectively modeled in the parameters of LRN, automatically learned from training data, and naturally exploited during the sequential label decoding process. In the following we describe these components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoding</head><p>For encoding, we form the input instance X as</p><formula xml:id="formula_0">"[CLS], x 1 , ..., [E 1 ], m 1 , ..., m k , [E 2 ], ..., x n " where [E 1 ], [E 2 ]</formula><p>are entity markers, m is mention word and x is context word. We then feed X to BERT and obtain the source hidden state H = {h 1 , ..., h n }. Finally, the hidden vector of <ref type="bibr">[CLS]</ref> token is used as sentence embedding g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deductive Reasoning for Extrinsic Dependencies</head><p>This section describes how to capture extrinsic dependencies for label prediction via a deductive reasoning mechanism. To this end, the deductive reasoning-based decoder sequentially generates labels based on both context and previous labels, e.g., "for his books" + person ? writer and "record an album" + person ? musician. In this way, a label is decoded by considering both context-based prediction and previous labels-based prediction.</p><p>Concretely, we utilize a LSTM-based autoregressive network as decoder and obtain the hidden state of decoder S = {s 0 , ..., s k }, where k is the number of predicted labels. We first initialize s 0 using sentence embedding g, then at each time step, two attention mechanisms -contextual attention and premise attention, are designed to capture context and label information for next prediction.</p><p>Contextual Attention is used to capture the context evidence for label prediction. For example, the context "they theorize" provides rich information for theorist label. Specifically, at each time step t, contextual attention identifies relevant context by assigning a weight ? ti to each h i in the source hidden state H:</p><formula xml:id="formula_1">e ti = v T c tanh(W c s t + U c h i ) (1) ? ti = exp(e ti ) n i=1 exp(e ti )<label>(2)</label></formula><p>where W c , U c , v c are weight parameters and s t is the hidden state of decoder at time step t. Then the context representation c t is obtained by:</p><formula xml:id="formula_2">c t = n i=1 ? ti h i (3)</formula><p>Premise Attention exploits the dependencies between labels for next label prediction. For example, if person has been generated, its hyponym label theorist will be highly likely to be generated in context "they theorize". Concretely, at each time step t, premise attention captures the dependencies to previous labels by assigning a weight ? tj to each s j of previous hidden states of decoder S &lt;t :</p><formula xml:id="formula_3">e tj = v T p tanh(W p s t + U p s j ) (4) ? tj = exp(e tj ) t?1 j=0 exp(e tj )<label>(5)</label></formula><p>where W p , U p , v p are weight parameters. Then the previous label information u t is obtained by:</p><formula xml:id="formula_4">u t = t?1 j=0 ? tj s j<label>(6)</label></formula><p>Label Prediction. Given the context representation c t and the previous label information u t , we use m t = [c t + g; u t + s t ] as input, and calculate the probability distribution over label set L:</p><formula xml:id="formula_5">s t = LSTM(s t?1 , W b y t?1 ) (7) o t = W o m t (8) y t = sof tmax(o t + I t )<label>(9)</label></formula><p>where W o and W b are weight parameters and we use the mask vector I t ? R L+1 <ref type="bibr" target="#b36">(Yang et al., 2018)</ref> to prevent duplicate predictions.</p><formula xml:id="formula_6">(I t ) i = ? inf , l i ? Y * t?1 1 , otherwise (10)</formula><p>where Y * t?1 is the predicted labels before step t and l i is the i th label in label set L. The label with maximum value in y t is generated and used as the input for the next time step until [EOS] is generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inductive Reasoning for Intrinsic Dependencies</head><p>Deductive reasoning can effectively capture extrinsic dependencies. However, labels can also have intrinsic dependencies if they share attributes, e.g., theorist and scientist shares scholar attribute.</p><p>To leverage intrinsic dependencies, LRN conducts inductive reasoning by associating labels to attributes via a bipartite attribute graph. A label will be generated if most of its attributes are activated. Instead of heuristically setting the number of attributes to be activated, we select labels based on their overall activation score from all attributes. By capturing such label-attribute relations, many long tail labels can be effectively predicted because they are usually related to non-long tail attributes.</p><p>To this end, we first design a bipartite attribute graph to represent attribute-label relations. Based on the bipartite attribute graph, at each time step, attributes will be activated based on the hidden state of decoder, and new labels will be inducted by reasoning over the activated attributes. For example, in <ref type="figure">Figure 2</ref> the predicted labels person, theorist and commander will correspondingly activate the attributes human, scholar and expert, and then the scientist label will be activated via inductive reasoning based on these attributes.</p><p>Bipartite Attribute Graph (BAG). BAG G = {V, E} is designed to capture the relations between attributes and labels. Specifically, nodes V contain attribute nodes V a and label nodes V l , and edges E only exist between attributes nodes and labels nodes, with the edge weight indicating the attribute-label relatedness. Attributes are represented using natural language words in BAG. <ref type="figure">Figure 2 shows</ref>   BAG Construction. Because there are many labels and many attributes, we dynamically build a local BAG during the decoding for each instance. In this way the BAG is very compact and the computation is very efficient <ref type="bibr" target="#b45">(Zupan et al., 1999)</ref>. In local BAG, we collect attributes in two ways: <ref type="formula">(1)</ref> We mask the entity mention in the sentence, and predict the [MASK] token using masked language model (this paper uses BERT-base-uncased), and the non-stop words whose prediction scores greater than a confidence threshold ? c will be used as attributes -we denote them as context attributes; Since PLM usually predicts high-frequency words, the attributes are usually not long-tailed, which facilitates modeling dependencies between head and tail labels. This mask-prediction strategy is also used in <ref type="bibr" target="#b32">Xin et al. (2018)</ref>, for collecting additional semantic evidence of entity labels. <ref type="formula" target="#formula_1">(2)</ref> We directly segment the entity mention into words using Stanza 2 , and all non-stop words are used as attributes -we denote them as entity attributes. <ref type="figure" target="#fig_3">Figure 3</ref> shows several attribute examples. Given attributes, we compute the attribute-label relatedness (i.e. E in G) using the cosine similarity between their GloVe embeddings <ref type="bibr" target="#b22">(Pennington et al., 2014)</ref>.</p><p>Reasoning over BAG. At each time step, we activate attributes in BAG by calculating their similarities to the current hidden state of decoder s t . For the i th attribute node V a (i) , its activation score is:</p><formula xml:id="formula_7">score (i) Va = ReLU (sim(W s s t , W a V a (i) ) (11)</formula><p>where W s is the weight parameter, W a is the attribute embedding (i.e., word embedding of attribute words). We use cosine distance to measure similarity and employ ReLU to activate attributes. Then we induce new labels by reasoning over the activated attributes as:</p><formula xml:id="formula_8">score (j) V l = na i=1 score (i) Va E ij<label>(12)</label></formula><p>2 https://pypi.org/project/stanza/ where n a is the number of attributes, V l (j) is the j th label nodes and E ij is the weight between them. Finally a label will be generated if its activation score is greater than a similarity threshold ? s .</p><p>Note that our inductive reasoning and deductive reasoning are jointly modeled in the same decoder, i.e., they share the same decoder hidden state but with different label prediction process. Once deductive reasoning-based decoder generates [EOS], the label prediction stops. Finally, we combine the predicted labels of both deductive reasoning and inductive reasoning as the final FET results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning</head><p>In FET, each instance is represented as {X , Y} where X is "[CLS], x 1 , ..., [E 1 ], m 1 , ..., m k , [E 2 ],..., x n " and Y = {y 1 , ..., y m } is the golden labels. To learn our model, we design two losses: set prediction loss for deductive reasoning-based decoding and BAG loss for inductive reasoning-based decoding.</p><p>Set Prediction Loss. In FET, cross entropy loss is not appropriate because the prediction results is a label set, i.e., {y * 1 , y * 2 , y * 3 } and {y * 3 , y * 2 , y * 1 } should have the same loss. Therefore we measure the similarity of two label set using the bipartite matching loss <ref type="bibr" target="#b29">(Sui et al., 2020)</ref>. Given the golden label set Y = {y 1 , ..., y m } and generated label set Y * = {y * 1 , ..., y * m }, the matching loss L(ij) S of y i and y * j is calculated by 13, then we use the Hungarian Algorithm <ref type="bibr" target="#b11">(Kuhn, 1955)</ref> to get the specific order of golden label set as Y = { y 1 , ..., y m } to obtain minimum matching loss L S :</p><formula xml:id="formula_9">L(ij) S = CE(y i , y * j )<label>(13)</label></formula><formula xml:id="formula_10">L S = CE( Y, Y * )<label>(14)</label></formula><p>where CE is cross-entropy.</p><p>BAG Loss. To make the model activate labels correctly, we add a supervisory loss to the bipartite attribute graph to active correct labels:</p><formula xml:id="formula_11">L A = ? |L| j=1 score (j) V l * y j<label>(15)</label></formula><formula xml:id="formula_12">y j = 1 , v j ? Y ?1 , v j / ? Y<label>(16)</label></formula><p>Final Loss. The final loss is a combination of set loss and BAG loss:</p><formula xml:id="formula_13">L = L S + ?L A<label>(17)</label></formula><p>where ? is the relative weight of these two losses 3 . Baselines For Ultra-Fine dataset, we compare with following baselines: Onoe and Durrett <ref type="formula" target="#formula_1">(2019)</ref> which offers two multi-classifiers using BERT and ELMo as encoder respectively, <ref type="bibr" target="#b3">Choi et al. (2018)</ref> which is a multi-classifier using GloVe+LSTM as encoder, <ref type="bibr" target="#b33">Xiong et al. (2019)</ref> which is a multiclassifier using GloVe+LSTM as encoder and exploits label co-occurrence via introducing associated labels to enrich the label representation, <ref type="bibr" target="#b16">L?pez and Strube (2020)</ref> which is a hyperbolic multiclassifier using GloVe. For OntoNotes dataset, in addition to the baselines for Ultra-Fine, we also compare with  which offers a multi-classifier using BERT as encoder, <ref type="bibr" target="#b15">Lin and Ji (2019)</ref> which offers a multi-classifier using ELMo as encoder and exploits label co-occurrence via requiring the latent representation to reconstruct the co-occurrence association and  which offers a multi-classifier using ELMo as encoder and exploits label hierarchy via designing a hierarchy-aware loss function.</p><p>Implementation We use BERT-Base(uncased) <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> as encoder, Adam optimizer <ref type="bibr" target="#b10">(Kingma and Ba, 2015)</ref> with learning rate of BERT as 5e-5 and of other parameters as 1e-3. The batch size is 32, encoder hidden size is 768, the decoder hidden size is 868 and label embedding size is 100, the dropout rate of decoder is 0.6. The confidence 3 In our auxiliary experiments, we find that its impact is minor, so this paper empirically sets it to 1. 4 Released in https://github.com/uwnlp/open_type Model P R F1 without label dependency <ref type="bibr" target="#b3">*Choi et al. (2018)</ref> 47.1 24.2 32.0 *ELMo <ref type="bibr" target="#b20">(Onoe and Durrett, 2019)</ref> 51.5 33.0 40.2 BERT <ref type="bibr" target="#b20">(Onoe and Durrett, 2019)</ref> 51.6 33.0 40.2 BERT <ref type="bibr">[in-house]</ref> 55.9 33.0 41.5 with label dependency *LABELGCN <ref type="bibr" target="#b33">(Xiong et al., 2019)</ref>  * means using augmented data. "without label dependency" methods formulated FET as multi-label classification without considering associations between labels. "with label dependency" methods leveraged associations between labels explicitly or implicitly.</p><p>threshold ? c and the similarity threshold ? s both are optimized on dev set and set as 0.1 and 0.2 respectively. We use the GloVe embedding <ref type="bibr" target="#b22">(Pennington et al., 2014)</ref> to represent the nodes of BAG and fix it while training. <ref type="table">Table 1</ref> shows the main results of all baselines and our method in two settings: LRN is the full model and LRN w/o IR is the model without inductive reasoning. For fair comparisons, we implement a baseline with same settings of LRN but replace the decoder with a multi-classifier same as <ref type="bibr">Choi et al. (2018) -BERT[in-house]</ref>. We can see that: 1) By performing label reasoning, LRN can effectively resolve the fine-grained entity typing problem. Compared with previous methods, our method achieves state-of-the-art performance with a F1 improvement from 40.2 to 45.4 on test set. This verified the necessity for exploiting label dependencies for FET and the effectiveness of our two label reasoning mechanisms. We believe this is because label reasoning can help FET by making the learning more data-efficient (i.e., labels can share knowledge) and the prediction of labels global coherent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Overall Results</head><p>2) Both deductive reasoning and inductive reasoning are useful for fine-grained label prediction. Compared with BERT[in-house], LRN w/o IR can achieve 4.3% F1 improvement by exploiting extrinsic dependencies via deductive reasoning. LRN can further improve F1 from 43.3 to 45.4 by exploiting intrinsic dependencies via inductive reasoning. We believe this is because deductive reasoning and inductive reasoning are two fundamental but different mechanisms, therefore, modeling them simultaneously will better leverage label dependencies to    <ref type="table">Table 4</ref>: Performance of the zero-shot, shot=1 and shot=2 label prediction. "Category" means how many kinds of types are predicted. "Prediction" means how many labels are generated. predict labels.</p><p>3) Seq2Set is an effective framework to model, learn and exploit label dependencies in an end-toend manner. Compared with LABELGCN <ref type="bibr" target="#b33">(Xiong et al., 2019)</ref> which heuristically exploits label cooccurrence structure, LRN can achieve a significant performance improvement. We believe this is because neural networks have strong ability for representing and learning label dependencies. And the end-to-end manner makes LRN can easily generalize to new scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect on Long Tail Labels</head><p>As described above, another advantage of our method is it can resolve the long tail problem by decomposing long tail labels to common attributes and modeling label dependencies between head and tail labels. Because the finer the label granularity, the more likely it to be a long tail label, we report the performance of each label granularity on dev set and test set same as previous works in <ref type="table" target="#tab_4">Table 2  and Table 3</ref>. Moreover, we report the performance of the labels with shot?2 in <ref type="table">Table 4</ref>. Based on these results, we find that: 1) LRN can effectively resolve the long tail label problem. Compared to BERT[in-house], LRN can significantly improve the F-score of ultra-fine granularity labels by 44% (22.6 ? 32.5) and recall more fine-grained labels (14.6 ? 26.0).</p><p>2) Both deductive reasoning and inductive reasoning are helpful for long tail label prediction, but with different underlying mechanisms: deductive reasoning exploits the extrinsic dependencies between labels, but inductive reasoning exploits the intrinsic dependencies between labels. LRN w/o IR cannot predict zero-shot labels because it resolves long tail labels by relating head labels with long tail labels, therefore it cannot predict unseen labels. By contrast, LRN can predict zero-shot labels via inductive reasoning because it can decompose labels into attributes. Furthermore, we found LRN w/o IR has higher precision for few-shot (shot=2) labels than BERT and LRN, we believe this is because inductive reasoning focuses on recalling more labels, which inevitably introduce some incorrect labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Detailed Analysis</head><p>Effect of Components To evaluate the effect of different components, we report the ablation results in <ref type="table" target="#tab_7">Table 5</ref>. We can see that: (1) Set prediction loss is effective: replacing it with cross-entropy loss will lead to a significant decrease. <ref type="formula" target="#formula_1">(2)</ref>    Effect of Attributes Set To explore the impact of entity attributes and context attributes in BAG, <ref type="figure">Figure 4</ref>(a) shows the results of different attributes configurations. We can see that: both attributes are useful, the context attribute has high coverage and may be noisy, while the entity attribute is opposite. However when introducing both of them, the information in entity attributes might help the context attributes to disambiguate them. This is similar to the effectiveness of contextual information in word sense disambiguation. As a result, these two kinds of attributes can complement each other. And <ref type="figure">Figure 4(b)</ref> shows the performance on different thresholds, and we optimize confidence threshold ? c = 0.1 and similarity threshold ? s = 0.2 on dev set. Notice that ? s is the threshold of activating labels and when ? s = 1, it is equivalent to LRN w/o IR .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of OntoNotes</head><p>To verify the generality of our method, we further conduct experiments on OntoNotes and report results of with and without augmentation data in   <ref type="bibr" target="#b3">(Choi et al., 2018)</ref> which contains 800K instances and therefore there're little few-shot labels in this setting. And * indicates using additional features to enhance the label representation.</p><p>in OntoNotes, we use the embedding of the last word of a label, e.g., /person/artist/director is represented using embedding of director.</p><p>We can see that: 1) LRN still achieves the best performance on both settings, which verified the robustness of our method. 2) Compared with Ultra-Fine, our method achieves a smaller improvement on OntoNotes. We found this is mainly because: First, OntoNotes has weaker label dependencies for its label set is smaller (89 vs 2519 for Ultra-Fine) and most of its labels are coarse-grained. Secondly, most labels in OntoNotes are frequent labels with many training instances, therefore the long tail label problem is not serious. This also explains why LRN w/o IR can achieve better performance than LRN in the setting of with augmentation data: the more the training instance, the less need for long tail prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Case Study</head><p>To intuitively present the learned label dependencies, <ref type="figure">Figure 5</ref> shows the label co-occurrence matrices of different models' predictions and ground truth, we can see that both LRN and LRN w/o IR can accurately learn label dependencies. <ref type="figure" target="#fig_5">Figure 6</ref> shows some prediction cases and demonstrates that deductive and inductive reasoning have quite different underlying mechanisms and predict quite different labels <ref type="table">.   person  president  politician  leader  singer  artist  musician  music  song  ministry  organization  group  association  team  show  television  play   person  president  politician  leader  singer  artist  musician  music  song  ministry  organization  group  association  team  show  television  play   BERT   person  president  politician  leader  singer  artist  musician  music  song  ministry  organization  group  association  team  show  television  play   person  president  politician  leader  singer  artist  musician  music  song  ministry  organization  group  association  team  show  television</ref>   <ref type="figure">Figure 5</ref>: Heat map of co-occurrence matrices of different models' prediction and ground truth. LRN w/o IR and LRN learn very similar co-occurrence matrices to Ground Truth.</p><p>[Estadio Jose Duarte de Paiva], is a multi-use stadium located in Brazil .</p><p>place ? structure ? stadium </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper proposes Label Reasoning Network, which uniformly models, learns and reasons complex label dependencies in a sequence-to-set, endto-end manner. LRN designs two label reasoning mechanisms for effective decoding -deductive reasoning to exploit extrinsic dependencies and inductive reasoning to exploit intrinsic dependencies. Experiments show that LRN can effectively cope with the massive label set on FET. And because our method uses no predefined structures, it can be easily generalized to new datasets and applied to other multi-classification tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Only ?,[they]  theorize, could collapse some of the war ship ? Examples of deductive reasoning based on the extrinsic dependency and inductive reasoning based on the intrinsic dependency, where the labels person, theorist and commander are deducted respectively and the label scientist is inducted from the attributes {expert, scholar}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>; Xu and Barbosa (2018); Wu et al. (2019); Chen et al. (2020) design new loss function to exploit label hierarchies. Abhishek et al. (2017) enhance the label representation by sharing parameters. Shimaoka et al. (2017); Murty et al. (2018);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Only such a potent force, [they] theorize, could collapse some of the war ship ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Examples of attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Ablation experiments of context attributes and entity attributes on Ultra-Fine dataset. (b) Performances of different confidence threshold ? c and similarity threshold ? s on dev set. and premise attention mechanisms are important for Seq2Set generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Cases of prediction results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>a BAG where V a contains words {scholar, expert, historian, ...}, V l are all entity labels in label set L, containing {student, musician, scientist, ...} ? the RTC would be forced until [cash] could be raised ? cash ? owner of the technology, receives [royalty payments]. royalty, payment fund, capital, interest, revenue fund, award, assistance, support object, money, currency, income, resource, financing object, money, award, payment, gift</figDesc><table><row><cell>Label</cell></row><row><cell>Entity Attribute</cell></row><row><cell>Context Attribute</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>?LABELGCN (Xiong et al., 2019) 49.3 28.1 35.8 66.2 68.8 67.5 43.9 40.7 42.2 42.4 14.2 21.3 HY Large (L?pez and Strube, 2020) 43.4 34.2 38.2 61.4 73.9 67.1 35.7 46.6 40.4 36.5 19.9 25.7 *ELMo (Onoe and Durrett, 2019) 50.7 33.1 40.1 66.9 80.7 73.2 41.7 46.2 43.8 45.6 17.4 25.2 BERT (Onoe and Durrett, 2019) 51.6 32.8 40.1 67.4 80.6 73.4 41.6 54.7 47.3 46.3 15.</figDesc><table><row><cell>Model</cell><cell>P</cell><cell>Total R</cell><cell>F</cell><cell>P</cell><cell>General R</cell><cell>F</cell><cell>P</cell><cell>Fine R</cell><cell>F</cell><cell>P</cell><cell>Ultra-Fine R</cell><cell>F</cell></row><row><cell>*Choi et al. (2018)</cell><cell cols="12">48.1 23.2 31.3 60.3 61.6 61.0 40.4 38.4 39.4 42.8 8.8 14.6</cell></row><row><cell cols="13">6 23.4</cell></row><row><cell>BERT[in-house]</cell><cell cols="12">54.1 32.1 40.3 68.8 79.2 73.6 43.8 57.4 49.7 50.7 14.6 22.6</cell></row><row><cell>LRN w/o IR</cell><cell cols="12">60.7 32.5 42.3 79.3 75.5 77.4 59.6 44.8 51.2 45.7 18.7 26.5</cell></row><row><cell>LRN</cell><cell cols="12">53.7 38.6 44.9 77.8 76.4 77.1 55.8 50.6 53.0 43.4 26.0 32.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Macro P/R/F1 of each label granularity on Ultra-Fine dev set, and long tail labels are mostly in the ultra-fine layer. * means using augmented data. ? We adapt the results from<ref type="bibr" target="#b16">L?pez and Strube (2020)</ref>.</figDesc><table><row><cell>Model</cell><cell>P</cell><cell>Total R</cell><cell>F</cell><cell>P</cell><cell>General R</cell><cell>F</cell><cell>P</cell><cell>Fine R</cell><cell>F</cell><cell>P</cell><cell cols="2">Ultra-Fine R</cell><cell>F</cell></row><row><cell>HY XLarge (L?pez and Strube, 2020)</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>69.1</cell><cell>/</cell><cell>/</cell><cell>39.7</cell><cell>/</cell><cell>/</cell><cell cols="2">26.1</cell></row><row><cell>BERT[in-house]</cell><cell cols="13">55.9 33.0 41.5 69.7 81.6 75.2 43.7 56.0 49.1 53.5 15.5 24.0</cell></row><row><cell>LRN w/o IR</cell><cell cols="13">61.2 33.5 43.3 78.3 76.7 77.5 61.6 44.1 51.4 47.8 19.9 28.1</cell></row><row><cell>LRN</cell><cell cols="13">54.5 38.9 45.4 77.4 76.7 77.1 58.4 50.4 54.1 43.5 26.4 32.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Macro P/R/F1 of different label granularity on Ultra-Fine test set.</figDesc><table><row><cell>Number of</cell><cell cols="2">Category Prediction</cell><cell cols="5">Shot=0 Correct Predicted Prec. Correct Predicted Shot=1</cell><cell>Prec.</cell><cell cols="3">Shot=2 Correct Predicted Prec.</cell></row><row><cell>BERT[in-house]</cell><cell>293</cell><cell>5683</cell><cell>0</cell><cell>0</cell><cell>/</cell><cell>1</cell><cell>1</cell><cell>100.0%</cell><cell>9</cell><cell>66</cell><cell>13.6%</cell></row><row><cell>LRN w/o IR</cell><cell>330</cell><cell>5740</cell><cell>0</cell><cell>0</cell><cell>/</cell><cell>1</cell><cell>3</cell><cell>33.3%</cell><cell>15</cell><cell>28</cell><cell>53.6%</cell></row><row><cell>LRN</cell><cell>997</cell><cell>7808</cell><cell>110</cell><cell>218</cell><cell>50.5%</cell><cell>67</cell><cell>252</cell><cell>26.6%</cell><cell>94</cell><cell>276</cell><cell>34.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Ablation results on Ultra-Fine dataset: PreAtt</cell></row><row><cell>denotes premise attention, ConAtt denotes contextual</cell></row><row><cell>attention, and -SetLoss denotes replacing set prediction</cell></row><row><cell>loss with cross-entropy loss.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>To embed labels</figDesc><table><row><cell>Encoder</cell><cell>Model</cell><cell>Acc MaF MiF</cell></row><row><cell></cell><cell>with augmentation</cell><cell></cell></row><row><cell cols="2">HYPER L?pez and Strube (2020)</cell><cell>47.4 75.8 69.4</cell></row><row><cell>LSTM</cell><cell>Choi et al. (2018) Xiong et al. (2019)</cell><cell>59.5 76.8 71.8 59.6 77.8 72.2</cell></row><row><cell>ELMo</cell><cell cols="2">*Onoe and Durrett (2019) 64.9 84.5 79.2 (Lin and Ji, 2019) 63.8 82.9 77.3</cell></row><row><cell></cell><cell>Wang et al. (2020)</cell><cell>61.1 81.8 76.3</cell></row><row><cell>BERT</cell><cell>BERT [in-house] LRN w/o IR</cell><cell>62.2 83.4 78.8 66.1 84.8 80.1</cell></row><row><cell></cell><cell>LRN</cell><cell>64.5 84.5 79.3</cell></row><row><cell></cell><cell cols="2">without augmentation</cell></row><row><cell>ELMo</cell><cell cols="2">*Onoe and Durrett (2019) 42.7 72.7 66.7 Chen et al. (2020) 58.7 73.0 68.1</cell></row><row><cell></cell><cell>Onoe and Durrett (2019)</cell><cell>51.8 76.6 69.1</cell></row><row><cell>BERT</cell><cell>BERT[in-house] LRN w/o IR</cell><cell>51.5 76.6 69.7 55.3 77.3 70.4</cell></row><row><cell></cell><cell>LRN</cell><cell>56.6 77.6 71.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Results on OntoNotes test set. Augmentation is the augmented data created by</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our source codes are openly available at https://github.com/loriqing/Label-Reasoning-Network</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fine-grained entity type classification by jointly learning representations and label embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Abhishek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Awekar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/e17-1075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-04-03" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="797" to="807" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fine-grained named entity typing over distantly supervised data based on refined representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifang</forename><surname>Muhammad Asif Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7391" to="7398" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical entity typing via multi-level learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunmo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07-05" />
			<biblScope unit="page" from="8465" to="8475" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ultra-fine entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FINET: context-aware fine-grained named entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Del Corro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdalghani</forename><surname>Abujabal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d15-1103</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="868" to="878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving fine-grained entity typing with entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1643</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="6209" to="6214" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA; Long and Short Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Contextdependent fine-grained entity type tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Huynh</surname></persName>
		</author>
		<idno>abs/1412.1820</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Building a fine-grained entity typing system overnight for a new X (X = language, domain, genre)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno>abs/1603.03112</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end trainable attentive decoder for hierarchical entity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Karn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulli</forename><surname>Waltinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/e17-2119</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017-04-03" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="752" to="758" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequence-to-nuggets: Nested entity mention detection via anchor-region networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1511</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy; Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5182" to="5192" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gazetteerenhanced attentive neural networks for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1646</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="6231" to="6236" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A rigorous study on named entity recognition: Can fine-tuning pretrained model lead to the promised land?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas Jing</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.592</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-11-16" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7291" to="7300" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An attentive fine-grained entity typing model with latent type representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1641</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="6196" to="6201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A fully hyperbolic neural model for hierarchical multi-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.42</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-20" />
			<biblScope unit="page" from="460" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Label embedding for zero-shot fine-grained named entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sa</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2016-12-11" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical losses and new resources for fine-grained entity typing and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irena</forename><surname>Radovanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Description-based zero-shot fine-grained entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasha</forename><surname>Obeidat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Shahbazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1087</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to denoise distantly-labeled data for entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2407" to="2417" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8576" to="8583" />
		</imprint>
	</monogr>
	<note>The Thirty-Fourth AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/d14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fine-grained entity typing with high-multiplicity assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2052</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="330" to="334" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fine-grained entity typing with hierarchical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quan Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2552" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">AFET: automatic finegrained entity typing by hierarchical partial-label embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d16-1144</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="1369" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Label noise reduction in entity typing by heterogeneous partial-label embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939822</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-08-13" />
			<biblScope unit="page" from="1825" to="1834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural zero-shot fine-grained entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366424.3382725</idno>
	</analytic>
	<monogr>
		<title level="m">Companion of The 2020 Web Conference 2020</title>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>ACM / IW3C2</publisher>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="846" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural architectures for fine-grained entity type classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonse</forename><surname>Shimaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/e17-1119</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04-03" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1271" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Joint entity and relation extraction with set prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianbo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/2011.01675</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An empirical study of pre-trained embedding on ultrafine entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2656" to="2661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling noisy hierarchical types in fine-grained entity typing: A content-based weighting approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Huai</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/731</idno>
		<ptr target="ij-cai.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-10" />
			<biblScope unit="page" from="5264" to="5270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Put it back: Entity typing with language model enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1121</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31" />
			<biblScope unit="page" from="993" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imposing label-relational inductive bias for extremely fine-grained entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deren</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1084</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="773" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural finegrained entity type classification with hierarchyaware loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="16" to="25" />
		</imprint>
	</monogr>
	<note>NAACL-. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Noise mitigation for neural entity typing and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadollah</forename><surname>Yaghoobzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/e17-1111</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04-03" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1183" to="1194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SGM: sequence generation model for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08-20" />
			<biblScope unit="page" from="3915" to="3926" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving semantic parsing via answer type inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izzeddin</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mudhakar</forename><surname>Srivatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d16-1015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="149" to="159" />
		</imprint>
	</monogr>
	<note>The Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Otyper: A neural architecture for open named entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02" />
			<biblScope unit="page" from="6037" to="6044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning with noise: Improving distantlysupervised fine-grained entity typing via automatic relabeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingkun</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/527</idno>
		<ptr target="ijcai.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3808" to="3815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">MZET: memory augmented zero-shot fine-grained named entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Ta</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.7</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-08" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">De-biasing distantly supervised named entity recognition via causal intervention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.371</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4803" to="4813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Denoising distantly supervised named entity recognition via a hypergeometric probabilistic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huidan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas Jing</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021-02-02" />
			<biblScope unit="page" from="14481" to="14488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ERNIE: enhanced language representation with informative entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1139</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Zero-shot open entity typing as typecompatible grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1231</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31" />
			<biblScope unit="page" from="2065" to="2076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning by discovering concept hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaz</forename><surname>Zupan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Bohanec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janez</forename><surname>Demsar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Bratko</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0004-3702(99)00008-9</idno>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="211" to="242" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
