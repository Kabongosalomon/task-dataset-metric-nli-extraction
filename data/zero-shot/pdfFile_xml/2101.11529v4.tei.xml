<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NTU-X: An Enhanced Large-scale Dataset for Improving Pose-based Recognition of Subtle Human Actions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Trivedi</surname></persName>
							<email>neel.trivedi@research.iiit.ac.in</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Thatipelli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><forename type="middle">Kiran</forename><surname>Sarvadevabhatla</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Trivedi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Thatipelli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kiran</surname></persName>
							<email>ravi.kiran@iiit.ac.in</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarvadevabhatla</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVIT</orgName>
								<orgName type="institution" key="instit2">IIIT-Hyderabad Hyderabad</orgName>
								<address>
									<postCode>500032</postCode>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">CVIT, IIIT-Hyderabad Hyderabad</orgName>
								<address>
									<postCode>500032</postCode>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">CVIT, IIIT-Hyderabad Hyderabad</orgName>
								<address>
									<postCode>500032</postCode>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NTU-X: An Enhanced Large-scale Dataset for Improving Pose-based Recognition of Subtle Human Actions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>ACM Reference Format: 2021. NTU-X: An Enhanced Large-scale Dataset for Improving Pose-based Recog-nition of Subtle Human Actions. In Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP &apos;21), December 19-22, 2021, Jodhpur, India, Chetan Arora, Parag Chaudhuri, and Subhransu Maji (Eds.). ACM, New York, NY, USA, Article 13, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Activity recognition and un- derstanding</term>
					<term>Artificial intelligence</term>
					<term>Computer vision</term>
					<term>Com- puter vision tasks</term>
					<term>KEYWORDS human action recognition, skeleton, dataset, human activity recog- nition, pose estimation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The lack of fine-grained joints (facial joints, hand fingers) is a fundamental performance bottleneck for state of the art skeleton action recognition models. Despite this bottleneck, community's efforts seem to be invested only in coming up with novel architectures. To specifically address this bottleneck, we introduce two new pose based human action datasets -NTU60-X and NTU120-X. Our datasets extend the largest existing action recognition dataset, NTU-RGBD. In addition to the 25 body joints for each skeleton as in NTU-RGBD, NTU60-X and NTU120-X dataset includes finger and facial joints, enabling a richer skeleton representation. We appropriately modify the state of the art approaches to enable training using the introduced datasets. Our results demonstrate the effectiveness of these NTU-X datasets in overcoming the aforementioned bottleneck and improve state of the art performance, overall and on previously worst performing action categories. Code and pretrained models can be found at https://github.com/skelemoa/ntu-x.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Understanding human actions from visual data is crucial for applications related to surveillance, interactive user interfaces and * Equal contribution Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). ICVGIP'21, December 2021, Jodhpur, India  multimedia systems. This task is usually accomplished with RGB videos as input <ref type="bibr" target="#b11">[12]</ref>. However, advances in technologies have enabled use of other modalities (e.g. depth <ref type="bibr" target="#b16">[17]</ref>) and systems such as Microsoft Kinect which can provide skeleton-like human pose representations <ref type="bibr" target="#b12">[13]</ref>. In particular, the introduction of large-scale skeleton-based human action datasets NTU RGB+D <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref> has shifted the focus towards skeleton-based human action recognition approaches. In contrast to full-frame RGB-based representations, 3D skeleton joints encode human body dynamics in a computationally efficient manner, preserve privacy and can offer greater robustness to view and illumination.</p><p>The recent adoption of graph neural networks which process the skeleton action sequence as a spatio-temporal graph has enabled a steady rise in average accuracy for skeleton action recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20]</ref>. However, an analysis of sorted per-class accuracies reveals that the actions with lowest accuracy involve the usage of fingers (see <ref type="table" target="#tab_2">Table 3</ref> <ref type="bibr">,4)</ref>. The underlying reason is that hand joints in Kinect-based skeletons provided in the original dataset are represented by just two finger joints <ref type="figure" target="#fig_1">(Figure 1-d)</ref>. As a result, actions involving subtle finger movements (e.g. 'eating', 'writing', arXiv:2101.11529v4 [cs.CV] 24 Nov 2021 'make ok sign', 'make victory sign') often fail to be recognized correctly. Sometimes, even the non-hand, main body joints are localized poorly by the Kinect-based capture system, as <ref type="figure" target="#fig_2">Figure 2</ref> shows. These shortcomings at raw data level cannot be addressed at the architecture level, i.e. by proposing novel architectures.</p><p>To address the mentioned data-level issues, we introduce NTU60-X and NTU120-X, curated and extended versions of the existing NTU dataset. Obtained from RGB videos present along with NTU skeleton data, the pose representations in the new dataset include 42 finger joints (21 for each hand), 51 facial keypoint joints and 25 body joints similar to those present in Kinect-based NTU-60 and NTU-120, for a total of 118 joints per skeleton (see <ref type="figure" target="#fig_1">Figure 1</ref>). We also modify state of the art approaches to enable experimental evaluation and benchmark the modified variants on NTU60-X and NTU120-X. As a result, we set the new state of the art benchmark on NTU-60 and NTU-120. Our results also demonstrate the benefit of the newly introduced datasets for overcoming the performance bottleneck mentioned earlier and enabling recognition of subtle human actions involving hand-based joints. Resources (source code, pre-trained models, analysis and videos) related to NTU-X are available at https://github.com/skelemoa/ntu-x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Prior to creation of NTU RGB-D dataset, a number of datasets enabled progress for skeleton-based human action recognition. MSR-Action3d <ref type="bibr" target="#b6">[7]</ref> was one of the first action recognition datasets which provided depth and skeleton joint modalities, albeit from a single viewpoint. However, it only covered a limited set of gaming actions (e.g. forward punching, side boxing). The Northwestern-UCLA dataset <ref type="bibr" target="#b17">[18]</ref> scaled up the diversity to include videos from multiple views and with actions performed by 10 different actors. The NTU RGB-D 60 dataset <ref type="bibr" target="#b13">[14]</ref> comprises of 60 action categories, performed by 40 subjects. Its extension NTURGB-D 120 <ref type="bibr" target="#b7">[8]</ref> is one of the largest and most diverse skeleton action dataset comprising of 120 actions performed by 106 subjects from 155 viewpoints.</p><p>Varying-view RGB-D Action Dataset (VAD) <ref type="bibr" target="#b5">[6]</ref> comprises viewvarying Kinect captured sequences covering the entire 360?view angles, containing 40 actions that are performed by 118 distinct performers. Unfortunately, the full dataset is not publicly available (as of current). Notably, the datasets mentioned above do not provide fine-grained joints for hands and faces which limits their utility for certain actions as mentioned previously.</p><p>An alternative approach for skeleton estimation infers the joints from RGB video frames without requiring specialized capture equipment. In the Kinetics-skeleton dataset <ref type="bibr" target="#b19">[20]</ref>, the 2D skeleton joint coordinates predicted from RGB frames are combined with the joint estimation confidence to obtain a pseudo 3D skeleton representation on videos from Kinetics-400 action dataset <ref type="bibr" target="#b1">[2]</ref>. However, the resulting skeleton dataset contains many invalid sequences <ref type="bibr" target="#b4">[5]</ref>.</p><p>We summarize the salient aspects of these datasets and our proposed NTU60-X and NTU120-X in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NTU-X</head><p>The NTU RGB+D dataset <ref type="bibr" target="#b13">[14]</ref> provides RGB videos along with 3D Kinect skeleton data. We first extract the RGB frames from the videos at the frame rate of 30 FPS. We estimate 3D poses from RGB frames using SMPL-X <ref type="bibr" target="#b10">[11]</ref>. SMPL-X uses strong 2D pose priors estimated using Openpose <ref type="bibr" target="#b0">[1]</ref> on each RGB frame. However, SMPL-X based pose estimation is rather slow and is reliant on optimization heuristics. It also fails on blurred images and in the presence of light occlusion. To compensate for these issues, we use ExPose <ref type="bibr" target="#b3">[4]</ref>. ExPose uses a part-wise attention-based model that feeds high resolution patches of the corresponding body parts to their dedicated refinement module. Unlike SMPL-X, ExPose estimates the full 3D pose (body, finger and face joints) from the RGB image without relying on 2D pose prior and is much faster compared to SMPL-X.</p><p>Since it is difficult to automatically select between SMPL-X and ExPose pose representations, we employ a semi-automatic approach to curate the final dataset. We use Openpose <ref type="bibr" target="#b0">[1]</ref> toolbox to estimate the 2D pose and associated confidence for the full-body joints. Openpose provides total 70 joints for face out of which we use 51 major joints as shown in <ref type="figure" target="#fig_1">Figure 1</ref>(b) to make the final skeleton of 118 joints (25 body + 21 ? 2 fingers + 51 face) for each frame of the clips. Keeping the intra-view and intra-subject variance of the NTU dataset in mind, we sample random videos covering each view per class of NTU and estimate the SMPL-X and ExPose outputs. We examine the quality of the skeleton backprojected to RGB frame and use the accuracy of alignment to select between ExPose and SMPL-X. Empirically, we observe that ExPose and SMPL-X perform equally well for single-person actions but SMPL-X, though slow, provides better pose estimates for multi-person action class sequences. To check which pose extraction method (ExPose or SMPLx) has been used for each of the classes in the NTU-X dataset, kindly refer to our Github project at https://github.com/skelemoa/ntu-x.</p><p>To ensure good dataset quality, we remove corrupted videos from the original dataset, using a procedure similar to one adopted for the original dataset <ref type="bibr" target="#b13">[14]</ref>. We also omit videos in which people are completely absent. Additionally, for some samples OpenPose provides poor estimates and hence we discard instances of such videos as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>To evaluate the impact of NTU60-X and NTU120-X on overall performance, we benchmarked models with state-of-the-art performance on NTU60 and NTU120. We selected DSTA-Net <ref type="bibr" target="#b14">[15]</ref>, 4s-ShiftGCN <ref type="bibr" target="#b2">[3]</ref>, MS-G3D <ref type="bibr" target="#b8">[9]</ref> and PA-ResGCN <ref type="bibr" target="#b15">[16]</ref> as the models to benchmark the newly introduced datasets. For the models DSTA-Net <ref type="bibr" target="#b14">[15]</ref>, MS-G3D <ref type="bibr" target="#b8">[9]</ref> and 4s-ShiftGCN <ref type="bibr" target="#b2">[3]</ref>, we updated the graph structure of the skeletons to incorporate the newly introduced joints. <ref type="figure" target="#fig_1">Figure 1(d)</ref> shows the skeleton topology for the original kinect data. We changed the input graph topology for these models according to our new skeleton structure as shown in <ref type="figure" target="#fig_1">Figure 1(a)</ref>.</p><p>PA-ResGCN <ref type="bibr" target="#b15">[16]</ref>, being a semantic part-based model, required more significant modification. Along with changes in input skeleton graph structure as done for the other two models, we defined new parts to incorporate the newly introduced joints and thus enable richer feature extraction. Since this model learns attentive weights for each of the input skeleton joints by dividing the skeleton into different parts, the definition of parts were also changed based on the NTU-X skeleton. In case of NTU-RGBD skeleton, PA-ResGCN defines total 5 parts: torso, left arm, right arm, left leg and right leg. In the new NTU-X skeleton, 3 additional parts were defined for 67 . Note that blurred RGB frame is included only for reference and is not part of skeleton data. The three classes mentioned -'eat meal', 'writing' and 'reading' are few of the most confused classes for NTU dataset (see <ref type="table" target="#tab_2">Table 3</ref>). As the zoomed insets illustrate, the quality of joints captured by NTU-X dataset is better compared to the original NTU dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Body Face Fingers Sequences Classes Joints MSR-Action3D <ref type="bibr" target="#b6">[7]</ref> ? 567 20 20 Northwestern-UCLA <ref type="bibr" target="#b17">[18]</ref> ? 1,475 10 21 VAD <ref type="bibr" target="#b5">[6]</ref> ? 25,600 40 25 NTU RGB+D <ref type="bibr" target="#b13">[14]</ref> ? 56,880 60 25 NTU RGB+D 120 <ref type="bibr" target="#b7">[8]</ref> ? 114,035 120 25 NTU60-X (Ours) ? ? ? 56,148 60 118 NTU120-X (Ours) ? ? ? 113,821 120 118 <ref type="table">Table 1</ref>: Comparison between NTU-X and some of the other publicly available skeleton-action recognition datasets. We are one of the first datasets to include body, face and hands joints in 3D for multi-person and occlusion case as well.</p><p>joints (body + fingers) skeleton: left fingers, right fingers and head, resulting in a total of 8 parts. For 118 joint (body + fingers + face) skeleton along with these 3 additional parts, one more part of face was added resulting in a total of 9 parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>The results of training the four selected models on the new NTU60-X and NTU120-X datasets, with finger joints included for Cross Subject protocol, are shown in <ref type="table">Table 2</ref>. Clearly, our modified DSTA-Net, MS-G3D, 4s-Shift-GCN and PA-ResGCN outperform their counterparts' performance on the original NTU60 dataset by a significant margin. For NTU120-X, all three models except PA-ResGCN outperform their counterparts' performance on NTU120 dataset. PA-ResGCN fails to surpass the orginal accuracy for 120 class dataset by a small margin. We hypothesize that this could be due to PA-ResGCN's architecture being too specific for the original Kinect skeleton setup and unable to handle the addition of extra added finger joints in the large-category (120 class) setting. We can also see that DSTA-Net not only beats its numbers on the original dataset, but also achieves state of the art performance among all the models with a margin of more than 2% for NTU60 dataset.(See highlighted cells in <ref type="table">Table 2</ref>). These results also support the fact that existing approaches, if provided better and richer joint data, have the capacity to perform   <ref type="table">Table 2</ref>: Results for top performing models of NTU60 and NTU120 dataset on NTU60-X dataset and NTU120-X (with finger joints) -see Section 4.1. The gray shaded columns show results on our newly introduced dataset. The blue highlighted cell corresponds to best overall performance for 60 and 120 class setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Class name NTU-60 NTU60-X  The NTU60-X column shows accuracies of the same classes but with models trained on our NTU60-X dataset (finger joints: Section 3). Thanks to availability of additional finger joint information in NTU-60X, we see visible performance improvement across all the models.</p><p>better. A detailed analysis of each model's performance and category level improvements is discussed next. <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table">Table 4</ref> list the five worst performing classes for all the four models on the original NTU60 and NTU120 datasets respectively along with their per class accuracy. The shaded columns in these tables provide the accuracy of these classes when the models are trained using the newly introduced NTU60-X and NTU120-X</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Class name NTU-120 NTU120-X  <ref type="table">Table 4</ref>: The NTU120 column shows accuracies of bottom 5 action classes for models trained on original NTU120 dataset. The NTU120-X column shows accuracies of the same classes but with models trained on our NTU120-X dataset (finger joints: Section 3). Thanks to availability of additional finger joint information in NTU-120X, we see visible performance improvement across all the models.</p><p>dataset. From these results, it is evident that most of the bottom performing classes for the original NTU datasets involve actions with fine finger movements (e.g. "writing", "type on keyboard", "eat meal", "make ok sign", "make victory sign"). When the models are provided with input data that includes finger joints, the per class accuracy for such categories is improved significantly. <ref type="figure" target="#fig_2">Figure 2</ref> and <ref type="figure" target="#fig_3">Figure 3</ref> also show that without inclusion of finger level joints, recognition of such action categories is ambiguous and difficult.  <ref type="table">Table 5</ref>: This table shows the bottom performing classes for all the models evaluated in this paper on the newly introduced NTU60-X and NTU120-X datasets. This table clearly indicates that the overall accuracy of bottom performing classes for the newly introduced NTU60-X and NTU120-X are higher than the overall accuracy of bottom performing classes for the orginal datasets, as shown in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table">Table 4   Table 5</ref> shows the worst performing classes for the NTU-X dataset for all the models. Comparing with accuracy of worst performing classes of the original NTU dataset given in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table">Table 4</ref>, we see that even the accuracy of worst performing classes of NTU-X dataset is, on average, higher compared to the original NTU dataset.</p><p>To further illustrate the performance boost we gain by including the finger level joints into the input skelton, we show the change in per class accuracy when going from orginal NTU dataset to newly introduced NTU-X dataset in <ref type="figure" target="#fig_4">Figure 4</ref> and <ref type="figure" target="#fig_5">Figure 5</ref> for the top performing model DSTA-Net <ref type="bibr" target="#b14">[15]</ref>(based on <ref type="table">Table 2</ref>).</p><p>The table provided in the inset of <ref type="figure" target="#fig_4">Figure 4</ref> shows classes which are benefited the most when going from NTU60 to NTU60-X dataset for the state-of-the-art performer (DSTA-Net). It is easy to see that these classes predominantly involve finger level actions such as "reading", "typing on keyboard", "playing with phone" and "writing". One can also observe that the gain for these classes is as high as 10-23 %. A similar trend can be seen in <ref type="figure" target="#fig_5">Figure 5</ref> which shows classes which benefit the most when going from NTU120 to NTU120-X dataset. Once again, the classes with highest gain involve finger level actions (e.g. "make of sign","hush") and the gain for these classes is in the range 17-25%. Both of these tables clearly indicate training recognition approaches on our proposed NTU-X dataset significantly boosts the performance of classes involving finger movements.</p><p>From <ref type="figure" target="#fig_4">Figure 4</ref> and <ref type="figure" target="#fig_5">Figure 5</ref>, we also note that training on NTU-X dataset (finger joints) is not beneficial for all the classes, with a performance drop seen in some cases. Most of these classes involve another person (e.g. "hugging other person", "take photo of other person", "playing rock-paper scissors" etc.). As per our understanding, capturing accurate skeletons for multiple people in a RGB frame is difficult which leads to poor pose extraction and ambiguity in classifying the sequences involving multiple people.</p><p>However, it is clear that the overall magnitude of gain is higher than the magnitude of drop in per class accuracy. Hence, the average accuracy is higher for NTU-X dataset than the original NTU dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>To examine the importance of body joints, finger joints and face joints individually, we also perform experiments with only body joints (25 joints), body + finger joints (67 joints) and body + fingers + face joints (118 joints) as well. The results of ablation study are    <ref type="table">Table 6</ref>: Results on different variants of NTU60-X and NTU120-X dataset to understand the contribution of the additional joints. (*: Ablations on DSTA-Net are done using only the Joint stream of the network which contributes most to its performance.)</p><p>shown in <ref type="table">Table 6</ref>. The performance degrades when face joints are included with the body and finger joints. One reason for this could be that the actions in NTU dataset do not involve significant facial motion. Hence, the additional joints of the face make the skeleton graph larger than necessary and difficult for model optimization.</p><p>Another possible reason could be that the existing models do not have a suitable architecture to handle the dense subgraph arising from the presence of facial keypoints. The poor results of models trained with only body joints (25 joints) are also in line with our hypothesis that the inclusion of finger joints in the input skeleton is crucial for better performance. In other words, the performance gain is not merely due to the shift from Kinect-based to RGB-based skeleton generation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we have shown that the lack of hand-level joints is a fundamental performance bottleneck in the skeleton data of the largest action recognition dataset, NTU-RGBD. To address this bottleneck, we contribute a carefully curated skeleton dataset which provides finger-level hand joints and facial keypoint joints. We appropriately modify the state of the art approaches to enable training using the introduced dataset. Our results demonstrate the effectiveness of the proposed dataset in enabling the modified approaches to overcome the aforementioned bottleneck and improve their performance, overall and on previously worst performing action categories. We also perform experiments to evaluate the relative importance of the introduced joints. We believe our contribution of new, expanded joint dataset will meet the twin objectives of improving performance and encouraging novel approaches in future. Going forward, we expect the research community to devise novel and efficient approaches for tackling dense skeleton representations present in our dataset.</p><p>Our 118 joints dataset, consisting of full body, fingers and even face joints can improve the recognition of actions based on expressions. This can help in capturing subtle changes in expression that would help in recognizing fine-grained actions (e.g. 'moving head up' and 'shaking head'). The significance of our work also arises from the emerging trend of fusing skeletal representations with other modalities (depth, RGB) for better performance in out of context, in-the-wild action recognition scenarios <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19]</ref>. The pretrained deep networks we introduce serve as a good starting point for such fusion based approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>? 2021 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-7596-2. https://doi.org/10.1145/3490035.3490270</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(a) The 118 joint skeleton introduced in the new NTU-X datasets. The 25 body joints are indicated by red dots.(b) 51 facial joints (c) 21 finger joints (d) 25 body joints present in original NTU datasets .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Sample skeletons from original NTU Kinect dataset (blue background) and proposed NTU-X dataset (pink background)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Samples showing 3d plot of the original NTU kinect skeletons and newly proposed NTU-X skeletons with corresponding RGB frames. The zoomed insets show the finger joints estimated in both NTU-Kinect and NTU-X and it clearly shows that NTU-X represents the action much more comprehensively than original NTU-Kinect data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The % gain in per class accuracy for best performing model (DSTA-Net) after training on newly introduced NTU60-X dataset. The x-axis shows category id. The inset tables show actions with largest and least gain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>The % gain in per class accuracy for best performing model (DSTA-Net) after training on newly introduced NTU120-X dataset. The x-axis shows category id. The inset tables show actions with largest and least gain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>The NTU60 column shows accuracies of bottom 5 action classes for models trained on original NTU60 dataset.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENTS</head><p>(Portions of) the research in this paper used the NTU RGB+D (or NTU RGB+D 120) Action Recognition Dataset made available by the ROSE Lab at the Nanyang Technological University, Singapore. This material is based upon work supported by the Google Cloud Research Credits program with the award GCP19980904.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Skeleton-Based Action Recognition with Shift Graph Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Monocular Expressive Body Regression through Body-Driven Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<ptr target="https://expose.is.tue.mpg.de" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo Vadis, Skeleton Action Recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Thatipelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubh</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourav</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi Kiran</forename><surname>Sarvadevabhatla</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-021-01470-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-021-01470-y" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2021-05-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A large-scale varying-view RGB-D action dataset for arbitrary-view human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanli</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feixiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Heng Tao Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10681</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Action recognition based on a bag of 3d points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops. IEEE</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2916873</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2019.2916873" />
		<title level="m">NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inte-gralAction: Pose-driven Feature Integration for Robust Human Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeseung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Expressive Body Capture: 3D Hands, Face, and Body from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Survey on Vision-based Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Poppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.imavis.2009.11.014</idno>
		<ptr target="https://doi.org/10.1016/j.imavis.2009.11.014" />
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="976" to="990" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
	<note>Image Vision Comput.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A Survey on 3D Skeleton-Based Action Recognition Using Learning Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Bin Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05907</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Decoupled Spatial-Temporal Attention Network for Skeleton-Based Action-Gesture Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stronger, Faster and More Explainable: A Graph Convolutional Baseline for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifeng</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394171.3413802</idno>
		<ptr target="https://doi.org/10.1145/3394171.3413802" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia (ACMMM)</title>
		<meeting>the 28th ACM International Conference on Multimedia (ACMMM)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1625" to="1633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mining Actionlet Ensemble for Action Recognition with Depth Cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Jiang Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.2941</idno>
		<title level="m">Crossview Action Modeling, Learning and Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
		<title level="m">Mimetics: Towards Understanding Human Actions Out of Context. arXiv</title>
		<imprint/>
	</monogr>
	<note>n.d.. n. d.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
