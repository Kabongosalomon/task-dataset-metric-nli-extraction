<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Review Representations with User Attention and Product Attention for Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center of Novel Software Technology and Industrialization</orgName>
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023, 210023</postCode>
									<settlement>Nanjing, Nanjing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center of Novel Software Technology and Industrialization</orgName>
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023, 210023</postCode>
									<settlement>Nanjing, Nanjing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunyan</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center of Novel Software Technology and Industrialization</orgName>
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023, 210023</postCode>
									<settlement>Nanjing, Nanjing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center of Novel Software Technology and Industrialization</orgName>
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023, 210023</postCode>
									<settlement>Nanjing, Nanjing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
							<email>chenjj@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Collaborative Innovation Center of Novel Software Technology and Industrialization</orgName>
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023, 210023</postCode>
									<settlement>Nanjing, Nanjing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Review Representations with User Attention and Product Attention for Sentiment Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural network methods have achieved great success in reviews sentiment classification. Recently, some works achieved improvement by incorporating user and product information to generate a review representation. However, in reviews, we observe that some words or sentences show strong user's preference, and some others tend to indicate product's characteristic. The two kinds of information play different roles in determining the sentiment label of a review. Therefore, it is not reasonable to encode user and product information together into one representation. In this paper, we propose a novel framework to encode user and product information. Firstly, we apply two individual hierarchical neural networks to generate two representations, with user attention or with product attention. Then, we design a combined strategy to make full use of the two representations for training and final prediction. The experimental results show that our model obviously outperforms other state-of-the-art methods on IMDB and Yelp datasets. Through the visualization of attention over words related to user or product, we validate our observation mentioned above.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Sentiment analysis aims to determine people's attitudes towards some topic or the overall polarity to a document, interaction, or event. In recent years, sentiment analysis draws increasing attention of researchers and industries because of the rapid growth of online review sites such as Amazon, Yelp and IMDB. In this work, we focus on the task of documentlevel review sentiment classification, which is a fundamental task in the field of sentiment analysis and opinion mining <ref type="bibr" target="#b18">(Pang and Lee 2008)</ref>. The task aims to infer the overall sentiment intensity (e.g. 1-5 stars on the review site Yelp) of review documents written by users for products.</p><p>Dominating studies follow <ref type="bibr" target="#b18">(Pang and Lee 2005;</ref><ref type="bibr" target="#b19">Pang, Lee, and Vaithyanathan 2002)</ref> and take sentiment classification as a special case of text classification problem. They usually regard user-marked sentiment polarities or ratings as labels and use machine learning algorithms to build sentiment classifiers with text features. Following the idea, many works devote to designing effective features from text <ref type="bibr" target="#b19">(Pang, Lee, and Vaithyanathan 2002;</ref><ref type="bibr" target="#b19">Qu, Ifrim, and Weikum 2010)</ref> or additional sentiment lexicons <ref type="bibr" target="#b6">(Ding, Liu, and Yu 2008;</ref><ref type="bibr" target="#b24">Taboada et al. 2011;</ref><ref type="bibr" target="#b13">Kiritchenko, Zhu, and Mohammad 2014)</ref>. Motivated by the great success of deep learning in computer vision <ref type="bibr" target="#b14">(Krizhevsky, Sutskever, and Hinton 2012)</ref>, speech recognition <ref type="bibr" target="#b4">(Dahl et al. 2012</ref>) and natural language processing <ref type="bibr" target="#b1">(Bengio et al. 2003</ref>), more recent methods use neural networks to learn low-dimensional and continuous text representations without any feature engineering <ref type="bibr" target="#b9">(Glorot, Bordes, and Bengio 2011;</ref><ref type="bibr" target="#b19">Socher et al. 2011;</ref><ref type="bibr" target="#b20">Socher et al. 2012;</ref><ref type="bibr" target="#b21">Socher et al. 2013;</ref><ref type="bibr" target="#b11">Kim 2014)</ref>. These models achieve very competitive performances in sentiment classification.</p><p>Despite neural network based approaches have been quite effective for sentiment classification <ref type="bibr" target="#b11">(Johnson and Zhang 2015;</ref><ref type="bibr" target="#b26">Tang, Qin, and Liu 2015a)</ref>, they typically only focus on the text content while ignoring the crucial influences of users and products. It is a common sense that the user's preference and product's characteristic make a significant effect on the ratings. For different users, same word might express different emotional intensity. For example, a lenient user may use "good" to evaluate an ordinary product while a critical user might use "good" to express an excellent attitude. Similarly, product's characteristic also have an effect on review ratings. Reviews of high-quality products tend to receive higher ratings compared to those of low-quality products.</p><p>In order to incorporate user and product information into sentiment classification, <ref type="bibr" target="#b26">Tang, Qin, and Liu (2015b)</ref> introduce a word-level preference matrix and a representation vector for each user and product into CNN sentiment classifier. The model achieves some improvements, but suffers from high model complexity and only considers wordlevel user and product information rather than semanticlevel. <ref type="bibr" target="#b2">Chen et al. (2016a)</ref> consider user and product information together and incorporate them into one review representation via attention mechanism <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2015)</ref>. However, in reviews, we observe that some words or sentences show strong user's preference, and some others tend to indicate product's characteristic. For example, for the review "the bar area is definitely good 'people watching' and i love the modern contemporary d?cor.", the word "good", "modern" and "contemporary" describe the characteristic of product, and the "love" shows strong user's sentiment. Opinions are more related to products and emotions are more centered on the user. They are called rational evaluation and emotional evaluation respectively by <ref type="bibr" target="#b16">Liu (2012)</ref>. Obviously, the two kinds of information have different effects on inferring sentiment label of the review. Intuitively, a review has different latent semantic representations with different views from users or products. Therefore, it is unreasonable to encode user and product information together into one representation.</p><p>In this paper, we address the above issues by encoding user and product information, respectively, into two hierarchical networks to generate two individual text representations with user attention or product attention. Then, we design a combined strategy to make most use of the two representations for training and final prediction, which proves effective. The experimental results show that our model obviously outperforms other state-of-the-art methods on IMDB and Yelp datasets. We open the source code in GitHub. <ref type="bibr">1</ref> The main contributions of our work are as follows: ? We propose a novel framework to encode review from two views for sentiment classification. With user attention and product attention, respectively, two representations are generated, which are concatenated for further classification. ? For better learning the neural network, we introduce a combined strategy to improve review representations. With a weighted loss function, better representations from two views can be achieved and further help sentiment classification. ? The experimental results demonstrate that our model achieves obvious and consistent improvements compared to all state-of-the-art methods. Some visualization cases also validate the effectiveness and interpretability of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background Long Short-Term Memory</head><p>Long Short-Term Memory(LSTM) <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber 1997)</ref> is widely used for text modeling because of its excellent performance on sequence modeling, especially for long documents. In order to address the problem of long-term dependencies, the LSTM architecture introduces a memory cell that is able to preserve cell state over long periods of time.</p><p>There are three gates to protect and control the state flow in LSTM unit. At each time step t, given an input vector x t , the current cell state c t and hidden state h t can be updated with previous cell state c t?1 and hidden state h t?1 as follows:</p><formula xml:id="formula_0">i t f t o t = ? ? ? (W [h t?1 ; x t ] + b) ,<label>(1)</label></formula><formula xml:id="formula_1">c t = tanh (W c [h t?1 ; x t ] + b c ) , (2) c t = f t ? t?1 + i t ? t ,<label>(3)</label></formula><formula xml:id="formula_2">h t = o t tanh(c t ),<label>(4)</label></formula><p>where i t , f t and o t are gate activations and in [0, 1], ? is the logistic sigmoid function and stands for element-wise multiplication. Intuitively, the forget gate f t controls the extent to which the previous memory cell is forgotten, the input gate i t controls how much each unit is updated, and the output gate o t controls the exposure of the internal memory state. The hidden state h t denotes output information of LSTM unit's internal memory cell. In order to increase the amount of input information available to the network, a more common approach is to adopt bidirectional LSTM to model text semantics both from forward and backward. For sequence vectors [x 1 , x 2 , ? ? ? , x T ], the forward LSTM reads sequence from x 1 to x T and the backward LSTM reads sequence from x T to x 1 . Then we concatenate the forward hidden state ? ? h t and backward hid-</p><formula xml:id="formula_3">den state ? ? h t , i.e., h t = ? ? h t ; ? ? h t ,</formula><p>where the [?; ?] denotes concatenation operation. The h t summarizes the information of the whole sequence centered around x t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Mechanism</head><p>Inspired by human visual attention, the attention mechanism is proposed by <ref type="bibr" target="#b0">Bahdanau, Cho, and Bengio (2015)</ref> in machine translation, which is introduced into the Encoder-Decoder framework to select the reference words in source language for words in target language. It is also used in image caption generation <ref type="bibr" target="#b26">(Xu et al. 2015)</ref>, parsing <ref type="bibr" target="#b26">(Vinyals et al. 2015)</ref>, natural language question answering <ref type="bibr" target="#b23">(Sukhbaatar et al. 2015)</ref>. <ref type="bibr" target="#b27">Yang et al. (2016)</ref> and <ref type="bibr" target="#b2">Chen et al. (2016a)</ref> explore hierarchical attention mechanism to select informative words or sentences for the semantics of document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document-level Sentiment Classification</head><p>Document-level sentiment classification usually aims to predict the corresponding sentiment label of review text. In general, a review is written by a user u ? U for a product p ? P . We denote the review as a document d with n sentences {s 1 , s 2 , ? ? ? , s n } and l i is the length of i-th sentence. The i-th sentence s i consists of l i words {w i1 , w i2 , ? ? ? , w ili }. For modelling document-level text semantics, we can employ bidirectional LSTM with hierarchical structure to obtain document representation. In word level, each word w ij is mapped to its embedding w ij ? R d . BiLSTM network receives [w i1 , w i2 , ? ? ? , w ili ] and generates hidden states</p><formula xml:id="formula_4">[h i1 , h i2 , ? ? ? , h ili ].</formula><p>Then we can fetch the last hidden state h ili or feed [h i1 , h i2 , ? ? ? , h ili ] to an average pooling layer or use attention mechanism to obtain the sentence representation s i . In sentence level, we feed the generated sentence representations [s 1 , s 2 , ? ? ? , s n ] into BiLSTM and then obtain the document representation d in a similar way. Finally, d is taken as feature of softmax classifier to predict sentiment label of the review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>It is obvious that not all words/sentences contribute equally to the review text semantics for different users and different products. In addition, in reviews, we observe that some words or sentences show strong user's preference, and some others tend to indicate product's characteristic. The two kinds of information play different roles in inferring the sentiment label of reviews, which implies a review has different latent semantic representations in user's view and product's view. In order to address the issue, we propose a novel framework to incorporate user and product information into sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Framework</head><p>We refer to our model as HUAPA for convenience. An illustration of HUAPA is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. It consists of two components mainly: a hierarchical user attention network and a hierarchical product attention network. With attention mechanism, the former incorporates user information into review document modeling while the latter incorporates product information. Then, we concatenate the two review representations as the final representation to predict user's overall sentiment on a review about a product. In addition, we design a combined strategy to enhance review representations for sentiment classification. Specifically, we add a softmax classifier respectively to three review representation d u , d p and d, and introduce a weighted loss function as optimization objective, which proves effective. We will present the details of HUAPA in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical User Attention</head><p>From the user's point of view, not all words reflect equally user's preference or sentiment. In order to address the issue, we use user attention mechanism to extract user-specific words that are significant to the meaning of sentence. Finally, the sentence representation is aggregated by the representations of those informative words. Formally, the enhanced sentence representation s u i is a weighted sum of word-level hidden states in user's view as:</p><formula xml:id="formula_5">s u i = li j=1 ? u ij h u ij ,<label>(5)</label></formula><p>where h u ij is the hidden state of the j-th word in the i-th sentence, ? u ij is the attention weight of h u ij and measures the importance of the j-th word for current user. We map each user u into a continuous and real valued vector u ? R du , where d u denotes the dimension of user embeddings. Specifically, the attention weight ? u ij for each hidden state can be defined as:</p><formula xml:id="formula_6">e(h u ij , u) = (v u w ) tanh W u wh h u ij + W u wu u + b u w , (6) ? u ij = exp e h u ij , u li k=1 exp (e (h u ik , u))<label>(7)</label></formula><p>where v u w is weight vector and (v u w ) represents its transpose, W u wh and W u wu are weight matrices, e(?) is a score function which scores the importance of words for composing sentence representation about current user.</p><p>Similarly, different sentences contribute unequally to document semantics for users. Therefore, in sentence level, we also use a attention mechanism with user vector u in word level to generate the document representation. The docu- </p><formula xml:id="formula_7">e (h u i , u) = (v u s ) tanh (W u sh h u i + W u su u + b u s ) , (8) ? u i = exp (e (h u i , u)) n k=1 exp (e (h u k , u)) ,<label>(9)</label></formula><formula xml:id="formula_8">d u = n i=1 ? u i h u i ,<label>(10)</label></formula><p>where h u i is the hidden state of the i-th sentence in a review document, ? u i is the weight of hidden state h u i in sentence level and can be calculated similar to the word level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical Product Attention</head><p>It is also true for different products that every word or sentence contributes different information to the text semantics. Based on the common sense, hierarchical product attention incorporates product information into review representation similar to hierarchical user attention. In product's view, the sentence representation s p i and document representation d p of a review are obtained formally as:</p><formula xml:id="formula_9">s p i = li j=1 ? p ij h p ij ,<label>(11)</label></formula><formula xml:id="formula_10">d p = n i=1 ? p i h p i ,<label>(12)</label></formula><p>where ? p ij and ? p i are the weight of hidden state h p ij in word level and h p i in sentence level respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combined Strategy</head><p>In order to make full use of document representation d u and d p , we design a combined strategy for training and the final prediction.</p><p>Since document representation d u and d p are high level representations of review in user's view and product's view respectively. Hence, we concatenate them as the final review representation for sentiment classification without feature engineering:</p><formula xml:id="formula_11">d = [d u ; d p ] .</formula><p>(13) Specifically, we use a linear layer and a softmax layer to project review representation d into review sentiment distribution of C classes:</p><formula xml:id="formula_12">p = softmax (Wd + b) .<label>(14)</label></formula><p>In our model, the cross-entropy error between ground truth distribution of review sentiment and p is defined as loss 1 :</p><formula xml:id="formula_13">loss 1 = ? d?T C c=1 p g c (d) ? log (p c (d)) ,<label>(15)</label></formula><p>where p g c is the probability of sentiment label c with ground truth being 1 and others being 0, T represents the training set.</p><p>To make the review representations d u and d p both have certain predictive capability and futher improve performance for sentiment classification, we add a softmax classifier respectively to d u and d p . The corresponding losses are defined as follows:</p><formula xml:id="formula_14">p u = softmax (W u d u + b u ) ,<label>(16)</label></formula><formula xml:id="formula_15">loss 2 = ? d?T C c=1 p g c (d) ? log (p u c (d)) ,<label>(17)</label></formula><formula xml:id="formula_16">p p = softmax (W p d p + b p ) ,<label>(18)</label></formula><formula xml:id="formula_17">loss 3 = ? d?T C c=1 p g c (d) ? log (p p c (d)) ,<label>(19)</label></formula><p>where p u is the predicted sentiment distribution of with user's view and p p is the predicted sentiment distribution of with product's view. The final loss of our model is a weighted sum of loss 1 , loss 2 and loss 3 as:</p><formula xml:id="formula_18">L = ? 1 loss 1 + ? 2 loss 2 + ? 3 loss 3 .<label>(20)</label></formula><p>The loss 2 and loss 3 are introduced as supervised information to improve review representations and further help sentiment classification. Note that, we predict review sentiment label according to the distribution p because it contains both user information and product information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We conduct experiments on several real-world datasets to validate the effectiveness of our model and report empirical results in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Settings</head><p>We conduct experiments on three sentiment classification datasets 2 with user and product information, which are from IMDB and Yelp Dataset Challenge in 2013 and 2014 <ref type="bibr" target="#b26">(Tang, Qin, and Liu 2015b</ref>  ground truth label. They are defined as follows:</p><formula xml:id="formula_19">Accuracy = T N ,<label>(21)</label></formula><formula xml:id="formula_20">RM SE = N k=1 (gd k ? pr k ) 2 N ,<label>(22)</label></formula><p>where T is the numbers of predicted sentiment labels that are same with ground truth sentiment labels of reviews, N is the numbers of review documents, gd k represents the ground sentiment label, and pr k denotes predicted sentiment label. We pre-train the 200-dimensional word embeddings on each dataset with SkipGram <ref type="bibr" target="#b17">(Mikolov et al. 2013)</ref>. The word embeddings are not fine-tuned when training, so hierarchical user attention and hierarchical product attention use same word embeddings. We set the user embeddings dimension and product embeddings dimension to be 200, and randomly initialize them from a uniform distribution U (?0.01, 0.01). The dimensions of hidden states in LSTM cell are set to 100. In this setting, a bidirectional LSTM gives us 200 dimensional output for word/sentence representation. To speed up training, we limit that a review document has 40 sentences at most and every sentence has no more than 50 words. We use Adam <ref type="bibr" target="#b12">(Kingma and Ba 2015)</ref> to update parameters when training and empirically set initial learning rate to be 0.005. Finally, We select the best parameters based on performance on the validation set, and evaluate the parameters on the test set. Note that, we do not use any regularization or dropout <ref type="bibr" target="#b22">(Srivastava et al. 2014)</ref> to improve performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>We compare our model HUAPA with several baseline methods for document-level review sentiment classification:</p><p>Majority assigns the majority sentiment label in training set to each review document in test set.</p><p>Trigram uses unigrams, bigrams and trigrams as features to train a SVM classifier with LibLinear <ref type="bibr" target="#b7">(Fan et al. 2008)</ref>.</p><p>TextFeature extracts sophisticated text features to train SVM classifier, such word/character n-grams, sentiment lexicon features, cluster features, etc. <ref type="bibr" target="#b13">(Kiritchenko, Zhu, and Mohammad 2014)</ref>.</p><p>UPF extracts user leniency and corresponding product popularity features <ref type="bibr" target="#b8">(Gao et al. 2013</ref>) from training data, and further concatenates them with the features in Trigram and TextFeature.</p><p>AvgWordvec averages word embeddings of a document to generate document representation, then feeds it into a SVM classifier as features.</p><p>SSWE learns sentiment-specific word embeddings (SSWE) <ref type="bibr" target="#b25">(Tang et al. 2014)</ref>, and uses max/min/average pooling to obtain document representation which is used as features for a SVM classifier.</p><p>RNTN + RNN uses the Recursive Neural Tensor Network (RNTN) <ref type="bibr" target="#b21">(Socher et al. 2013</ref>) to obtain sentence representations, then feeds them into the Recurrent Neural Network (RNN). Afterwards, the hidden vectors of RNN are averaged to generate document representation for sentiment classification.</p><p>Paragraph Vector implements the Distributed Memory Model of Paragraph Vectors <ref type="bibr" target="#b15">(Le and Mikolov 2014)</ref>   <ref type="table">Table 3</ref>: Effect of user attention and product attention. HUA only uses user information and local text, and HPA only uses product information and local text.  ument sentiment classification. The window size is tuned on validation set. JMARS is a recommendation algorithm <ref type="bibr" target="#b5">(Diao et al. 2014</ref>), which uses the information of users and aspects with collaborative filtering and topic modeling to predict document sentiment rating.</p><p>UPNN introduces a word-level preference matrix and a representation vector for each user and each product into CNN sentiment classifier (Kim 2014). The meaning of words can modified in the input layer with the preference matrix. Finally, it concatenates the user/product representation vectors with generated review representation as features fed into softmax layer <ref type="bibr" target="#b26">(Tang, Qin, and Liu 2015b)</ref>.</p><p>LUPDR uses recurrent neural network to embed temporal relations of reviews into the categories of distributed user and product representations learning for the sentiment classification of reviews <ref type="bibr" target="#b3">(Chen et al. 2016b)</ref>.</p><p>NSC uses hierarchical LSTM model to encode review text for sentiment classification.</p><p>NSC+LA implements the idea of local semantic attention <ref type="bibr" target="#b27">(Yang et al. 2016</ref>) based on NSC.</p><p>NSC+UPA puts user and product information account together and uses hierarchical LSTM model with attention mechanism to generate a review representation for sentiment classification. <ref type="bibr" target="#b2">Chen et al. (2016a)</ref> do not implement the model NSC+UPA and NSC+LA with bidirectional LSTM. To make the experimental results more convincing, we implement and train them in our experimental settings. In addition to LUPDR and the models related to NSC, we report the results in <ref type="bibr" target="#b26">(Tang, Qin, and Liu 2015b</ref>) since we use the same datasets for other baseline methods above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Comparisons</head><p>The experimental results are given in <ref type="table" target="#tab_2">Table 2</ref>, which are divided into two parts: the models only using the local text information, and the models incorporating both local text information and the global user and product information.</p><p>From the first part, we can see that the majority performs very poor because it does not use any text, user, and product information. Compared to the methods taking SVM as classifier, hierarchical neural networks achieve better performances generally. In addition, the results show NSC+LA obtains a considerable improvements based on NSC, which proves that importance of selecting more meaningful words and sentences in sentiment classification. It is also a main reason that attention mechanism is introduced into sentiment classification.</p><p>From the second part, we observe that the methods considering user and product information achieve more or less improvements compared to the corresponding methods in the first part. For example, TextFeature+UPF achieves 0.5% improvement and NSC+UPA obtains 2.3% improvement on Yelp2013 in accuracy. The comparisons indicate that the user and product information is helpful for sentiment classification.</p><p>The experimental results show that our proposed model with user attention and product attention achieves best performance on all datasets. We can see improvements regardless of data scale. For smaller dataset such as Yelp2013 and IMDB, our model outperforms the previous best stateof-the-art method by 2.8% and 1.7% respectively in accuracy. This finding is consistent on larger dataset. Our model achieves improvement by 1.7% on dataset Yelp2014 in accuracy. The observations demonstrate that our model incorporates user and product information in a more effective way, which finally improves review representations for sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Analysis: Effect of User Attention and Product Attention</head><p>To investigate the effects of single user attention or product attention, we also implement independent hierarchical user attention network (HUA) and hierarchical product attention network (HPA). <ref type="table">Table 3</ref> shows the performance of single attention mechanism with user or product information. From the table, we can observe that:</p><p>? Compared to the model NSC+LA(BiLSTM) only using local semantic attention, HUA and HPA both achieve some improvements, which validates the rationality of incorporating user and product into sentiment classification via attention mechanism. The results also indicate that user attention or product attention can capture more information related to sentiment. ? The user information is more effective than the product information to enhance review representations. Although some words or sentences in reviews show product's characteristic, the ratings are finally decided by users. Hence, it is reasonable that the discrimination of user's preference is more obvious than product's characteristic. ? Compared to single user attention or product attention, our model achieves better performance, which indicates that user and product information both contribute to our model. The results demonstrate our user attention and product attention mechanism can catch the specific user's preference and product's characteristic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Analysis: Effect of the Different Weighted Loss</head><p>The ? 1 , ? 2 , and ? 3 respectively represent the weight of loss 1 , loss 2 and loss 3 . We investigate the effect of different weighted loss by empirically adjusting their proportion. When ? 2 is set to 0, we do not use loss 2 to enhance the review representations. Similarly, we set ? 3 to 0 to avoid the effect of loss 3 . The experimental results are in <ref type="table" target="#tab_4">Table 4</ref>. From the <ref type="table" target="#tab_4">Table 4</ref> and <ref type="table" target="#tab_2">Table 2</ref>, we can observe that:</p><p>? Compared to other state-of-the-art methods, our model without loss 2 and loss 3 also achieve consistent improvements on the three datasets. It indicates that our attention mechanism is more effective in incorporating user information and product information. ? When considering loss 2 or loss 3 , HUAPA both obtains some improvements. It is obvious that full HUAPA model achieves best performance. The results demonstrate that with the designed combined strategy, better review representations from two views can be achieved and further help sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study for Visualization of Attention</head><p>To validate our observation and demonstrate the effectiveness of user attention and product attention, we take some review instances in Yelp2013 dataset as example. We visualize the attention weights of these reviews at word level in HUAPA. The results are shown in <ref type="figure">Figure 2</ref>. Note that, the darker color means higher weight. For the review 1, we can see that the word "love" has highest attention weight in user attention and the word "cool" has highest attention weight in product attention. In fact, it is intuitive that "love" usually expresses user's affection or preference, and "cool" is used to describe the characteristic of product. There are also some reviews that user's preference is inconsistent with product's characteristic, such as review 3 in <ref type="figure">Figure 2</ref>. The content of the review 3 is "much of it was quite good but I was disappointed with the spider roll.". It is user attention product attention user attention product attention user attention product attention review 1: we love the ambiance and how cool this place is. review 2: the bar area is definitely good `people watching' and i love the modern contemporary d?cor.</p><p>review 3: much of it was quite good but I was disappointed with the spider roll. <ref type="figure">Figure 2</ref>: Visualization of user attention and product attention over words obvious that the word "good" indicates the product's characteristic and the word "disappointed" shows user's negative sentiment. Our model not only catches such information but also gives right prediction. The visualizations of attention demonstrate that our model can capture global user's preference and product's characteristic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a novel framework incorporating user and product information for sentiment classification. Specifically, we firstly apply two individual hierarchical neural networks to generate two representations, with user attention or with product attention. Then, we design a combined strategy to make full use of the two representations for training and final prediction. We evaluate our model on several sentiment classification datasets. The experimental results show that our model achieves obvious and consistent improvements compared to other state-of-the-art methods. Finally, the visualizations of attention also show our model can capture user information and product information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The architecture of Hierarchical User Attention and Product Attention neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of IMDB, Yelp2013 and Yelp2014 datasets ment representation d u in user's view is obtained via:</figDesc><table><row><cell>Datasets</cell><cell>#classes</cell><cell>#docs</cell><cell cols="6">#users #products #docs/user #docs/product #sens/doc #words/sen</cell></row><row><cell>IMDB</cell><cell>10</cell><cell>84,919</cell><cell>1,310</cell><cell>1,635</cell><cell>64.82</cell><cell>51.94</cell><cell>16.08</cell><cell>24.54</cell></row><row><cell>Yelp 2013</cell><cell>5</cell><cell>78,966</cell><cell>1,631</cell><cell>1,633</cell><cell>48.42</cell><cell>48.36</cell><cell>10.89</cell><cell>17.38</cell></row><row><cell>Yelp 2014</cell><cell>5</cell><cell cols="2">231,163 4,818</cell><cell>4,194</cell><cell>47.97</cell><cell>55.11</cell><cell>11.41</cell><cell>17.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). The statistics of the datasets are summarized inTable 1. The datasets are split into three parts, 80% for training, 10% for validation, and the remaining 10% for test. We use standard Accuracy to measure the overall sentiment classification performance, and RM SE to measure the divergences between predicted sentiment label and</figDesc><table><row><cell>Models</cell><cell cols="2">IMDB Acc. RMSE</cell><cell cols="2">Yelp 2013 Acc. RMSE</cell><cell cols="2">Yelp 2014 Acc. RMSE</cell></row><row><cell cols="5">Models without user and product information</cell><cell></cell><cell></cell></row><row><cell>Majority</cell><cell cols="2">0.196 2.495</cell><cell cols="2">0.411 1.060</cell><cell cols="2">0.392 1.097</cell></row><row><cell>Trigram</cell><cell cols="2">0.399 1.783</cell><cell cols="2">0.569 0.814</cell><cell cols="2">0.577 0.804</cell></row><row><cell>TextFeature</cell><cell cols="2">0.402 1.793</cell><cell cols="2">0.556 0.845</cell><cell cols="2">0.572 0.800</cell></row><row><cell>AvgWordvec+SVM</cell><cell cols="2">0.304 1.985</cell><cell cols="2">0.526 0.898</cell><cell cols="2">0.530 0.893</cell></row><row><cell>SSWE+SVM</cell><cell cols="2">0.312 1.973</cell><cell cols="2">0.549 0.849</cell><cell cols="2">0.557 0.851</cell></row><row><cell>Paragraph Vector</cell><cell cols="2">0.341 1.814</cell><cell cols="2">0.554 0.832</cell><cell cols="2">0.564 0.802</cell></row><row><cell>RNTN+Recurrent</cell><cell cols="2">0.400 1.764</cell><cell cols="2">0.574 0.804</cell><cell cols="2">0.582 0.821</cell></row><row><cell cols="3">UPNN(CNN and no UP) 0.405 1.629</cell><cell cols="2">0.577 0.812</cell><cell cols="2">0.585 0.808</cell></row><row><cell>NSC</cell><cell cols="2">0.443 1.465</cell><cell cols="2">0.627 0.701</cell><cell cols="2">0.637 0.686</cell></row><row><cell>NSC+LA</cell><cell cols="2">0.487 1.381</cell><cell cols="2">0.631 0.706</cell><cell cols="2">0.630 0.715</cell></row><row><cell>NSC+LA(BiLSTM)</cell><cell cols="2">0.490 1.325</cell><cell cols="2">0.638 0.691</cell><cell cols="2">0.646 0.678</cell></row><row><cell cols="5">Models with user and product information</cell><cell></cell><cell></cell></row><row><cell>Trigram+UPF</cell><cell cols="2">0.404 1.764</cell><cell cols="2">0.570 0.803</cell><cell cols="2">0.576 0.789</cell></row><row><cell>TextFeature+UPF</cell><cell cols="2">0.402 1.774</cell><cell cols="2">0.561 1.822</cell><cell cols="2">0.579 0.791</cell></row><row><cell>JMARS</cell><cell>N/A</cell><cell>1.773</cell><cell>N/A</cell><cell>0.985</cell><cell>N/A</cell><cell>0.999</cell></row><row><cell>UPNN(CNN)</cell><cell cols="2">0.435 1.602</cell><cell cols="2">0.596 0.784</cell><cell cols="2">0.608 0.764</cell></row><row><cell>UPNN(NSC)</cell><cell cols="2">0.471 1.443</cell><cell cols="2">0.631 0.702</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>LUPDR</cell><cell cols="2">0.488 1.451</cell><cell cols="2">0.639 0.694</cell><cell cols="2">0.639 0.688</cell></row><row><cell>NSC+UPA</cell><cell cols="2">0.533 1.281</cell><cell cols="2">0.650 0.692</cell><cell cols="2">0.667 0.654</cell></row><row><cell>NSC+UPA(BiLSTM)</cell><cell cols="2">0.529 1.247</cell><cell cols="2">0.655 0.672</cell><cell cols="2">0.669 0.654</cell></row><row><cell>HUAPA</cell><cell cols="2">0.550 1.185</cell><cell cols="2">0.683 0.628</cell><cell cols="2">0.686 0.626</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Reviews sentiment classification results. Acc.(Accuracy, higher is better) and RMSE(lower is better) are the evaluation metrics. The best performances are in bold. HUAPA outperforms previous best state-of-the-art method significantly(p &lt; 0.01).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Effect of the different weighted loss.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/wuzhen247/HUAPA</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://ir.hit.edu.cn/ dytang/paper/acl2015/dataset.7z</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their insightful comments. This work was supported by the 863 program(No. 2015AA015406) and the NSFC(No. 61472183, 61672277).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural sentiment classification with user and product attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1650" to="1659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning user and product distributed representations using a sequence model for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="34" to="44" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TASLP</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Diao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A holistic lexicon-based approach to opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">;</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling user leniency and product popularity for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1107" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bordes</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bengio ; Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<meeting><address><addrLine>Kim</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sentiment analysis of short informal texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">;</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="723" to="762" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutskever</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hinton ; Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikolov ;</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">; B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
	<note>Opinion mining and sentiment analysis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The bag-of-opinions method for review rating prediction from sparse text patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Vaithyanathan ; Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vaithyanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ifrim</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ifrim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Socher et al. 2012</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sukhbaatar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lexicon-based methods for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Taboada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="307" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning sentiment-specific word embedding for twitter sentiment classification</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning semantic representations of users and products for document level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Liu ; Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno>Xu et al. 2015</idno>
	</analytic>
	<monogr>
		<title level="m">Neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
