<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Structured Domain Adaptation with Online Relation Regularization for Unsupervised Person Re-ID</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Structured Domain Adaptation with Online Relation Regularization for Unsupervised Person Re-ID</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-unsupervised domain adaptation</term>
					<term>person re- identification</term>
					<term>domain translation</term>
					<term>relation consistency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation (UDA) aims at adapting the model trained on a labeled source-domain dataset to an unlabeled target-domain dataset. The task of UDA on open-set person re-identification (re-ID) is even more challenging as the identities (classes) do not have overlap between the two domains. One major research direction was based on domain translation [1], [2], [3], <ref type="bibr" target="#b3">[4]</ref>, which, however, has fallen out of favor in recent years due to inferior performance compared to pseudo-label-based methods [7], [8], [9], <ref type="bibr" target="#b9">[10]</ref>. We argue that the domain translation has great potential on exploiting the valuable source-domain data but existing methods did not provide proper regularization on the translation process. Specifically, previous methods only focus on maintaining the identities of the translated images while ignoring the inter-sample relations during translation. To tackle the challenges, we propose an end-to-end structured domain adaptation framework with an online relation-consistency regularization term. During training, the person feature encoder is optimized to model inter-sample relations on-the-fly for supervising relation-consistency domain translation, which in turn, improves the encoder with informative translated images. The encoder can be further improved with pseudo labels, where the source-to-target translated images with ground-truth identities and target-domain images with pseudo identities are jointly used for training. In the experiments, our proposed framework is shown to achieve state-of-the-art performance on multiple UDA tasks of person re-ID. With the synthetic?real translated images from our structured domaintranslation network, we achieved second place in the Visual Domain Adaptation Challenge (VisDA) in 2020.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>P ERSON re-identification (re-ID) <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> aims at identifying images of the same person across multiple cameras. Despite great advances of deep learningbased re-ID methods in recent years, large domain gaps still pose great challenges on generalizing the trained models from a labeled source domain to an unlabeled target domain. There are two main research directions towards solving the problem of domain adaptive person re-ID, i.e., domain translation-based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b16">[17]</ref> and pseudo-label-based methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, where the latter ones dominate the current literature with state-of-the-art performance.</p><p>Although domain translation-based methods have fallen out of favor in recent years due to their uncompetitive performance, we argue that they have great potential to make full use of valuable source-domain data with accurate identity labels if they are imposed with proper regularization strategy. Translating source-domain images to the target domain to create new training samples with identity labels is at the core of domain translation-based methods. Previous works used identity-based regularization (e.g., classification loss <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b17">[18]</ref>, contrastive loss <ref type="bibr" target="#b0">[1]</ref> or triplet loss <ref type="bibr" target="#b4">[5]</ref>) to preserve the ID-related appearance during image translation, i.e., forcing different identities' images to be well separated from each other after translation. However, we observe that their domaintranslated images cannot well maintain inter-sample relations even with such ID-based constraints. The inter-sample relations in our paper are represented by semantic affinities/similarities between samples' encoded features in the latent space, rather than the simple positive/negative relations given by the groundtruth identity labels. As demonstrated in <ref type="figure">Figure 1(a)</ref>, in existing image-to-image translation frameworks, although images of the same class can still be well classified into the same class after translation, the inter-sample affinities in the latent space might not be well maintained to effectively regularize the translation process. For example, in the first case of <ref type="figure">Figure  1</ref>(c), persons from two distinct identities are in yellow and orange, respectively. After domain translation by CycleGAN <ref type="bibr" target="#b5">[6]</ref> and SPGAN <ref type="bibr" target="#b0">[1]</ref>, the images of the different persons show similar color and appearances. The inter-sample relations are not well maintained. In the second case, a male and a female are in the same color -blue. After translation by CycleGAN <ref type="bibr" target="#b5">[6]</ref>, the two images of the male change to green and blue, respectively. After translation by SPGAN <ref type="bibr" target="#b0">[1]</ref>, the two images for the male are both in green while the girl is still in blue. The inter-sample relations are not well maintained by the existing translation-based methods. We argue that maintaining intersample relations during image-to-image translation is critical for generating informative training samples, which cannot be effectively regularized by identity-based regularization in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>To tackle this challenge, we propose an end-to-end structured domain adaptation (SDA) framework with a novel online relation-consistency regularization term. It consists of a structured domain-translation network, a source-domain person arXiv:2003.06650v3 [cs.CV] 5 May 2022 (a) Existing domain translation-based UDA methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> with ID-based regularizations.</p><p>(b) Our proposed structured domain translation with a relationconsistency regularization.</p><p>(c) Domain-translated triplets by CycleGAN <ref type="bibr" target="#b5">[6]</ref>, SPGAN <ref type="bibr" target="#b1">[2]</ref> and our structured domain-translation method.  <ref type="figure">Fig. 1. (a)</ref> Although images of the same class are well gathered together after translation by existing domain translation-based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, the original intra-/inter-class distributions are disturbed. The inter-sample relations of source-to-target translated images are not maintained. (b) Our proposed online relation regularization better preserves the inter-sample relations for generating informative training samples, improving cross-domain person re-ID. (c) Example triplets translated by CycleGAN <ref type="bibr" target="#b5">[6]</ref>, SPGAN <ref type="bibr" target="#b0">[1]</ref> and our proposed method. Best viewed in color.</p><p>image encoder and a target-domain encoder. The structured domain-translation network generates the source-to-target translated images, which can benefit the training of the targetdomain encoder. At the same time, two domains' encoders are coupledly trained to model inter-sample relations on-thefly, which regularize the training of the domain-translation network. We alternatively optimize the target-domain encoder and structured domain-translation network in each iteration, in order to exploit their mutual benefits. More specifically, the structured domain-translation network adopts the CycleGAN <ref type="bibr" target="#b5">[6]</ref> architecture for translating sourceand target-domain images. A novel relation-consistency loss is proposed to regularize the training of source-to-target domain translation for maintaining the original inter-sample relations, which are generated by the source-domain encoder on-the-fly ( <ref type="figure">Figure 1</ref>(b) and (c)). Note that our relation-consistency loss is different from conventional ID-based constraints in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, which only apply "one-hot" regularization that requires translated images of different classes to be well separated after translation but ignores the inter-sample relations.</p><p>Meanwhile, the source-domain encoder is trained with source-domain images and ground-truth identities. An improved target-domain encoder is trained with both the source-totarget translated images and target-domain images via a joint cross-domain label system, which is constructed with their associated ground-truth and pseudo labels. In this way, both the target-domain encoder and its generated pseudo labels can be improved with the optimization of the structured domaintranslation network. As shown in the first case of <ref type="figure">Figure  1</ref>(c), the triplet translated by our method show distinguishable appearance for images of different persons. In the second case, the clothes of the male and the female are both translated to the same color (green). Our proposed method is able to effectively maintain their original inter-sample relations during translation via the proposed structured domain-translation network and relation-consistency loss.</p><p>In summary, the contributions of this paper are three-fold:</p><p>? To properly exploit the valuable source-domain data in domain translation-based UDA methods, we propose a novel online relation-consistency regularization term to better supervise the domain translation process. ? The domain-translated images can serve as informative training samples to improve the target-domain encoder and help generate more accurate pseudo labels. The domaintranslation network and target-domain encoder alternately promote each other to achieve optimal re-ID performance. ? Our framework achieves state-of-the-art performance on multiple domain adaptive person re-ID benchmarks. Moreover, the proposed structured domain-translation method contributes to our solution <ref type="bibr" target="#b18">[19]</ref> in the Visual Domain Adaptation Challenge (VisDA) in 2020, which ranks second out of 153 teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Methods for Unsupervised Domain Adaptation (UDA) on Person Re-ID</head><p>There are two main categories for existing UDA methods on person re-ID, domain translation-based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b16">[17]</ref> and pseudo-label-based methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b20">[21]</ref>. This paper focuses on improving the former one, since we argue that the domain translation is effective to properly exploit the valuable source-domain images and their ground-truth identities, but existing works did not provide enough regularizations on the translation process. Note that two kinds of methods are complementary to each other and can promote each other to achieve optimal performance. 1) Domain translation-based UDA methods for person re-ID: Domain translation-based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b16">[17]</ref> aimed at fine-tuning the target-domain re-ID model with source-to-target translated images and their ground-truth identities. In order to preserve the original identities of the translated images, ID-based regularizations were adopted on either pixel level <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b16">[17]</ref> or feature level <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> via the contrastive loss <ref type="bibr" target="#b0">[1]</ref>, classification loss <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b17">[18]</ref> or triplet loss <ref type="bibr" target="#b4">[5]</ref>. Specifically, PTGAN <ref type="bibr" target="#b1">[2]</ref> and SBSGAN <ref type="bibr" target="#b16">[17]</ref> imposed pixel-level constraints on maintaining the color consistency during the domain translation. SPGAN <ref type="bibr" target="#b0">[1]</ref> and CGAN-TM <ref type="bibr" target="#b4">[5]</ref> further maximized the feature-level similarities between translated images and the original ones.</p><p>Although the existing ID-based regularizations can somewhat preserve the original person appearance and the inter-identity images can be separated after translation, they are too weak to properly maintain the original inter-sample relations and distributions of source-domain data during translation. We argue that maintaining the inter-sample relations and the original distributions are critical for preserving the knowledge from the source domain and generating informative training samples, but unfortunately, ignored by existing works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>2) Pseudo-label-based UDA methods for person re-ID: Pseudo-label-based methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> achieved state-of-the-art performances by modeling relations between unlabeled target-domain data with generated pseudo labels, where a clustering-based pipeline was found effective. PUL <ref type="bibr" target="#b6">[7]</ref> and UDAP <ref type="bibr" target="#b21">[22]</ref> first proposed a self-training scheme with clustering labels. SSG <ref type="bibr" target="#b8">[9]</ref> and PAST <ref type="bibr" target="#b7">[8]</ref> further extended this type of methods by introducing human part features and progressive training strategy. Recently, MMT <ref type="bibr" target="#b9">[10]</ref> was proposed to adopt coupledly trained networks as well as their meanteacher networks for mutual training, achieving state-of-the-art performances.</p><p>These methods generally focused on using only the targetdomain data, and we found that they could be further improved by properly exploiting the valuable source-domain data with ground-truth identities, where domain translation has great potential. Specifically, existing pseudo-label-based methods can be strengthened by jointly training with target-domain images and source-to-target translated images.</p><p>Note that the domain translation is crucial in jointly training, since the significant domain gaps may even harm the feature learning when directly training with two domains' raw images (see <ref type="table" target="#tab_4">Table VII</ref> "Baseline" v.s. "Baseline + raw source-domain data"). Our proposed structured domain-translation method can be well compatible with pseudo-label-based methods, including a clustering-based baseline <ref type="bibr" target="#b21">[22]</ref> or state-of-the-art MMT <ref type="bibr" target="#b9">[10]</ref> (see Section IV-C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generic Methods for Unsupervised Domain Adaptation</head><p>Feature-level and pixel-level adaptations were commonly adopted by UDA methods for tackling more general tasks. The feature-level adaptation methods <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> aimed at aligning the feature distributions between the source and target domains by learning domain-invariant features with a domain adversarial discriminator <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b26">[27]</ref> or reducing the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b31">[32]</ref> distance between domains. However, such methods are unable to handle the open-set re-ID problem with disjoint label systems in two domains <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>.</p><p>The other category of pixel-level adaptation methods <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> minimized the domain shifts by translating images to the same domain, which has been widely studied in semantic segmentation. These pixel-level adaptation methods focused on the cross-domain class/prediction consistency, which are related to our method. However, they still ignored the consistency of inter-sample relations and data distributions during translation, facing the same challenge as translated-based UDA methods for person re-ID <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Relation Preserving Embedding</head><p>Relation preserving embedding <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> aims at encoding features that preserve certain prior structure or relational information among samples. There exist a large number of embedding techniques, which mainly include conventional matrix factorization-based methods <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> and deep learning-based methods <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Matrix factorization-based methods try to represent the inter-sample relations/affinities in the form of a matrix, such as adjacency matrix, Laplacian matrix, node transition probability matrix, Katz similarity matrix, etc. However, those algorithms cannot be used as regularizations in our framework, as their objective functions are optimized via matrix factorization and do not support gradient-based optimization. Deep learning-based methods were also studied to capture non-linear structures among data points. However, these algorithms are different from our method in two aspects. First, those methods have different purposes from our method, i.e. they target at directly encoding low-dimensional features to maintain certain intersample relations while we maintain inter-sample relations to regularize the translation network training. More importantly, they adopt different types of manually-specified inter-sample relations as learning targets while our inter-sample relational supervisions are online generated and are also alternatively optimized with the domain-translation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. STRUCTURED DOMAIN ADAPTATION</head><p>We propose a structured domain adaptation framework with a novel online relation-consistency regularization term to tackle unsupervised domain adaptation (UDA) for person re-ID. The overall framework, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, consists of a structured domain-translation network and two domain-specific person image encoders. The translation network and targetdomain encoder are jointly optimized and promote each other to learn more discriminative person features.</p><p>The key innovation of our framework lies in the generation of informative training samples by translating source-domain images into the target domain under the relation-consistency regularization generated by the image encoders on-the-fly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Source-domain Encoder Pre-training</head><p>We pre-train the source-domain person image encoder F s for (1) providing "ground-truth" inter-sample relations between source-domain images to regularize the proposed structured domain translation, and (2) providing weight initialization for the target-domain person image encoder F t . Given source-domain samples X s , the encoder F s is trained to transform each sample x s ? X s into a feature vector f s = F s (x s ). If the feature vector f s is properly embedded, it could be used to correctly predict its ground-truth identity y s with a learnable classifier C s : f s ? {1, ? ? ? , p s }, where p s is the number of identities in the source domain. A cross-entropy classification loss ce and a triplet loss <ref type="bibr" target="#b41">[42]</ref> are adopted jointly for training,</p><formula xml:id="formula_0">L s enc (F s ,C s ) = E x s ?X s [ ce (C s (f s ), y s )] + E x s ?X s ( f s ? f s p + m ? f s ? f s n ) + ,<label>(1)</label></formula><p>where (?) + = max(0, ?) with a margin m, and the subscripts p , n denote the mini-batch's hardest positive and negative feature indexes for the anchor f s . Once trained, F s is frozen to provide stable regularizations for inter-sample relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Structured Domain Translation</head><p>We propose a structured domain-translation (SDT) network to generate informative training samples by translating sourcedomain images X s to the target domain, which focuses not only on image-style transfer but more on how to maintain their original inter-sample relations and distributions. We adopt the widely-used CycleGAN <ref type="bibr" target="#b5">[6]</ref> architecture for our translation network, which is trained to translate images along two directions with corresponding generators G s?t and G t?s .</p><p>1) Conventional cycle generation losses: The general training objective of a CycleGAN <ref type="bibr" target="#b5">[6]</ref> for image-to-image translation consists of the adversarial losses L s adv , L t adv , the cyclic reconstruction loss L cyc and the appearance consistency loss L apr . We adopt the loss function of LSGAN <ref type="bibr" target="#b42">[43]</ref> with two domain discriminators D s and D t as</p><formula xml:id="formula_1">L s adv (G t?s , D s ) = E x s ?X s D s (x s ) 2 + E x t ?X t D s (G t?s (x t )) ? 1 2 , L t adv (G s?t , D t ) = E x t ?X t D t (x t ) 2 + E x s ?X s D t (G s?t (x s )) ? 1 2 . (2)</formula><p>The cyclic reconstruction loss supervises the pixel-level generation by translating the images along two directions twice,</p><formula xml:id="formula_2">L cyc (G s?t ,G t?s ) = E x s ?X s G t?s (G s?t (x s )) ? x s 1 + E x t ?X t G s?t (G t?s (x t )) ? x t 1 .<label>(3)</label></formula><p>The appearance consistency loss <ref type="bibr" target="#b43">[44]</ref> help maintain the general color composition after domain translation,</p><formula xml:id="formula_3">L apr (G s?t , G t?s ) = E x s ?X s G t?s (x s ) ? x s 1 + E x t ?X t G s?t (x t ) ? x t 1 . (4)</formula><p>Despite the fact that the above loss terms guide the sourcedomain images to have target-domain image style and the appearance consistency loss L apr can preserve the person appearance to some extent, the generated images generally fail to maintain their original identities, where inter-class images cannot be well separated after translation. To tackle the problem, existing domain translation-based methods adopts ID-based constraints to regularize the translation process, e.g., contrastive loss <ref type="bibr" target="#b0">[1]</ref>, classification loss <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b17">[18]</ref> or triplet loss <ref type="bibr" target="#b4">[5]</ref>.</p><p>However, we observe that "one-hot" ID-based regularizations adopted by existing works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b4">[5]</ref> only require the translated images to be well separated according to their identities, but are not able to preserve the accurate inter-sample structures as well as the original source-domain data distributions <ref type="figure">(Figure 1(c)</ref>). As a result, the translated images are not accurate enough to optimize the target-domain image encoder. We argue that properly maintaining the original inter-sample relations is crucial to the domain translation.</p><p>2) Online relation-consistency regularization: Since there are no "ground-truth" inter-sample relations, we propose to use the pre-trained F s to provide relation supervision on-the-fly for effectively regularizing the translation. Intuitively, inter-sample relations can be modeled by the ratio of feature similarities between images. In person re-ID, we found that triplets with intra-/inter-identity samples are generally representative and can be used for modeling inter-sample relations.</p><p>Given a source-domain image x s , its positive sample x s p with the same identity, and its negative sample x s n with a different identity, we can measure their similarity-based intersample relations on-the-fly among the triplet with a softmax-like function,</p><formula xml:id="formula_4">R(x s ; F s ) = exp f s , f s p exp f s , f s p + exp f s , f s n ? [0, 1),<label>(5)</label></formula><p>where f s , f s p , f s n are the features encoded by the pre-trained source-domain encoder F s on the image samples x s , x s p , x s n , respectively, and ?, ? is the inner product between two feature vectors to measure their similarity. Similar to <ref type="bibr" target="#b41">[42]</ref>, we utilize only the most difficult triplet of each sample x s within a batch, i.e., the hardest positive (f s p ) and negative (f s n ) samples for each f s . Note that R(x s ; F s ) is a continuous value in [0, 1) to measure the ratio of pairwise similarities.</p><p>After translating source-domain images to the target domain by G s?t , we obtain the features of the source-to-target translated triplet (f s?t , f s?t p , f s?t n ), which are encoded by the target-domain encoder F t (to be discussed in Section III-C). Similarly, the continuous similarity ratio in [0, 1) between the translated images can also be measured by a softmax-like function as</p><formula xml:id="formula_5">R(x s ; G s?t , F t ) = exp f s?t , f s?t p exp f s?t , f s?t p + exp f s?t , f s?t n ? [0, 1). (6)</formula><p>We claim that, if the domain-translation network well preserves the source-domain images' inter-sample relations, their softmax-triplet responses before and after the translation should be as close as possible. Based on this assumption, a novel relation-consistency loss is introduced to regularize the inter-sample relations after the source-to-target translation. The loss is modeled as a "soft" binary cross-entropy loss,</p><formula xml:id="formula_6">L rc (G s?t ) = E x s ?X s bce R(x s ; G s?t , F t ), R(x s ; F s ) , (7) where bce (p, q) = ?q log p ? (1 ? q) log (1 ? p) with the soft label q ? [1, 0).</formula><p>In other words, we use the sourcedomain inter-sample relations R(x s ; F s ) as soft learning targets for supervising the translated inter-sample relations</p><formula xml:id="formula_7">R(x s ; G s?t , F t ).</formula><p>We have also investigated alternative designs of our relationconsistency loss for measuring the inter-sample relations and regularizing the translation process, which will be further discussed in Section IV-E2.</p><p>3) Differences from existing ID-based regularizations: There are two key differences between the proposed online relation-consistency loss (Eq. <ref type="formula">(7)</ref>) and ID-based regularizations in existing works, e.g., classification loss <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b17">[18]</ref>, contrastive loss <ref type="bibr" target="#b0">[1]</ref> or triplet loss.</p><p>? Existing ID-based regularizations only require the samples from different classes to be well separated after translation, which is too weak to maintain inter-sample relations and their original data distributions. As long as the translated samples are classified correctly, even if they did not maintain inter-sample relations well, they receive little penalty. In contrast, our proposed online regularization aims at maintaining continuous and more sensitive relation measurements (Eq. <ref type="formula" target="#formula_4">(5)</ref>) during domain translation. ? Existing regularizations utilize static learning targets (identity labels), while our regularization term generates relation measurements with the current image encoders on-the-fly to provide adaptive supervisions. In other words, previous ones only acquire knowledge from ground-truth labels, while ours exploits valuable knowledge from both ground-truth labels and the pre-trained source-domain encoder.</p><p>Besides the domain translation-based UDA methods as mentioned above, there exist some works <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> which leveraged generative models on fully-supervised person re-ID tasks. They also focused on preserving person identities with ID-based regularizations, which are too weak to maintain intersample relations as discussed above. </p><formula xml:id="formula_8">L sdt (G s?t , G t?s , D s , D t ) = ? rc L rc (G s?t ) + ? cyc L cyc (G s?t , G t?s ) + ? apr L apr (G s?t , G t?s ) + ? adv L s adv (G t?s , D s ) + L t adv (G s?t , D t ) .<label>(8)</label></formula><p>Here ? rc , ? cyc , ? adv and ? apr are the weighting factors for different loss terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Target-domain Encoder with Translated Images</head><p>For training the target-domain encoder F t in our framework, arbitrary pseudo-label-based methods (e.g., UDAP <ref type="bibr" target="#b21">[22]</ref>, MMT <ref type="bibr" target="#b9">[10]</ref>) can be adopted as a baseline, and we can improve them by jointly training two domains' images, i.e., the source-to-target images translated by our SDT and raw target-domain images. Specifically, we can create a unified training image set X = X s?t ? X t with a unified label set to supervise a cross-domain identity classifier C t : f ? {1, ? ? ? , p s +p t }, where both the labeled source-to-target translated images X s?t and the pseudo-labeled target-domain images X t serve as informative training samples with non-overlapping real or pseudo identity labels.</p><p>Note that the pseudo label creation is a general pipeline in UDA tasks and is not the focus of our method. Our framework can also work without pseudo labels (to be discussed in Section IV-D1), i.e., only the labeled source-to-target translated images are adopted for training the target-domain encoder.</p><p>Here, we take the modified version of a clustering-based baseline <ref type="bibr" target="#b21">[22]</ref> as an example. Note that the original version of <ref type="bibr" target="#b21">[22]</ref> only adopted the triplet loss for training, while our modified version adopts both the cross-entropy classification loss and triplet loss to achieve better performance. Targetdomain data X t 's encoded features {f t } are clustered intop t classes and images within the same cluster are assigned the same pseudo label. The target-domain encoder F t can then be trained in a fully-supervised manner. Specifically, each sample x ? X is assigned a corresponding label y ? {1, ? ? ? , p s +p t }, and F t is optimized with the objective function similar to source-domain encoder learning in Eq. (1) but with a jointdomain label set,</p><formula xml:id="formula_9">L t enc (F t , C t ) = E x?X ce (C t (f ), y) + E x?X ( f ? f p + m ? f ? f n ) + . (9)</formula><p>The target-domain encoder F t can therefore take full advantages of (1) the source-to-target images translated by our G s?t , which better maintains their inter-sample relations, and (2) the unified cross-domain label set that consists of both the valuable ground-truth source-domain identity labels and the target-domain pseudo labels. F t trained by this strategy is shown to encode more discriminative features for distinguishing target-domain identities.</p><p>In our overall framework, the source-domain encoder F s is fixed after pre-training, and the structured domain-translation (SDT) network and the target-domain encoder F t alternately promote each other via joint training to achieve optimal re-ID performance. When fixing F t , it measures translated intersample relations (Eq. (6)) for regularizing SDT via L rc (Eq. <ref type="formula">(7)</ref>). When fixing SDT, it generates training samples X s?t to optimize F t (Eq. <ref type="formula">(9)</ref>). Once F t is further trained to achieve better re-ID performance on the target domain, it could in turn generate more accurate pseudo labels and measure more accurate inter-sample relations for further improving SDT.</p><p>After training, only F t is used to encode target-domain samples into features for pairwise similarity estimation without extra parameters and computational costs. The overall algorithm is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Evaluation Metric</head><p>We evaluate our framework on four real?real adaptation tasks for person re-ID 1 , including DukeMTMC-reID?Market-1501, Market-1501?DukeMTMC-reID, Market-1501?MSMT17 and DukeMTMC-reID?MSMT17 following the experimental setup in state-of-the-arts <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>? DukeMTMC-reID <ref type="bibr" target="#b47">[48]</ref>  identities are used for training. Besides the widely-used real?real benchmarks above, we also adopt the proposed structured domain-translation method in the synthetic?real adaptation task of the Visual Domain Adaptation Challenge (VisDA-2020) 2 .</p><p>? The synthetic source-domain dataset PersonX <ref type="bibr" target="#b49">[50]</ref> is generated based on Unity <ref type="bibr" target="#b50">[51]</ref> engine, containing 20,280 images out of 700 identities captured by 6 cameras. The target-domain dataset consists of real-world images captured from 5 cameras, i.e., <ref type="bibr" target="#b12">13,</ref><ref type="bibr">198</ref> images for training, 377 images for the query of target_val, 3,600 images for the gallery of target_val, 1,578 images for the query of target_test and 24,006 images for the gallery of target_test.</p><p>Only the train set for the real-world images are used for training. The target_val is used for evaluation offline and the target_test is used for evaluation online. Note that both the train and test sets for source and target domains are required in our experiments, where only train sets are used for model optimization and the test sets are used for model evaluation. Specifically, for source-domain pretraining, the source-domain train set is used for training and the source-domain test set is used for evaluation. For our structured domain-translation training, the train sets of both source and targets domains are used for training, and the target-domain test set is used for evaluation. Mean Average Precision (mAP) and Cumulative Matching Characteristics (CMC) accuracies are utilized to test the methods performance.</p><p>B. Implementation Details 1) Network architecture: We adopt ResNet-50 <ref type="bibr" target="#b51">[52]</ref> as the backbone for the source-domain and target-domain person image encoders, which are initialized with ImageNet-pretrained <ref type="bibr" target="#b52">[53]</ref> weights. We adopt the CycleGAN <ref type="bibr" target="#b5">[6]</ref> architecture for our generators G s?t , G t?s and the PatchGAN <ref type="bibr" target="#b53">[54]</ref> architecture for our discriminators D s , D t . Specifically, each generator consists of three convolution-IN-ReLU blocks, nine residual blocks <ref type="bibr" target="#b51">[52]</ref>, two deconvolution-IN-ReLU blocks and the last one convolution mapping feature maps to RGB images. The targetdomain encoder F t and structured domain-translation network are alternately updated in each iteration to avoid unstable training. Furthermore, we adopt a momentum encoder <ref type="bibr" target="#b54">[55]</ref> (denoted as F t * ) to replace F t in Eq. (6) for measuring more stable triplet relations after domain translation. In particular, we denote the parameters of F t and F t * as ? (T ) and ? 2) Training data organization: We adopt a P K sampler for training, i.e. mini-batch = P identities ? K samples. For source-domain pre-training, each mini-batch contains 56 sourcedomain images of P = 8 ground-truth identities (K = 7 for each identity). When jointly training the structured domaintranslation network and target-domain encoder, each mini-batch contains 56 source-domain images of 8 ground-truth identities and 56 target-domain images of 8 pseudo identities. The pseudo identities are assigned by clustering algorithm and updated before each epoch. With such a P K sampler, the training samples for two domains can be well balanced even the two domains' datasets differ a lot regarding their scales. All images are resized to 256?128. Randomly erasing <ref type="bibr" target="#b55">[56]</ref>, cropping and flipping are applied to each image.</p><p>3) Network optimization: ADAM optimizer is adopted to optimize the networks with weighting factors ? rc = 1, ? adv = 1, ? cyc = 10, ? apr = 0.5 and triplet margin m = 0.3. The initial learning rates (lr) are set to 0.00035 for person image encoders and 0.0002 for the SDT network. The source-domain pretraining iterates for 30 epochs and the learning rate decreases to 1/10 of its previous value every 10 epochs. The proposed joint training scheme (Algorithm 1) iterates for 50 epochs, where the learning rate is constant for the first 25 epochs and then gradually decreases to 0 for another 25 epochs following the formula lr = lr ? (1.0 ? max(0, epoch ? 25)/25).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with State-of-the-arts</head><p>We compare our proposed SDA framework with state-of-theart methods on four domain adaptive re-ID tasks in <ref type="table" target="#tab_4">Table I and  Table II</ref>. Our method is plug-and-play with existing pseudolabel-based target domain encoders. Note that the focus of our SDA is to generate informative training samples rather than pseudo label refinery as the previous methods. The translated images by our SDA are used as additional training samples to further improve the pseudo-label-based encoder.</p><p>1) Modified UDAP <ref type="bibr" target="#b21">[22]</ref> as a baseline for the targetdomain encoder: Following the label generation pipeline introduced by a clustering-based baseline method UDAP <ref type="bibr" target="#b21">[22]</ref>, we adopted DBSCAN <ref type="bibr" target="#b74">[75]</ref> to generate target-domain pseudo labels for our target-domain encoder. Rather than using the sole triplet loss in the original UDAP <ref type="bibr" target="#b21">[22]</ref>, we modified it by using both classification loss and triplet loss as the training objective to achieve better baseline performance, dubbed as "UDAP <ref type="bibr" target="#b21">[22]</ref> w/ DBSCAN (modified)" in <ref type="table">Table I</ref>. The detailed introduction of our modified training objective can be found at Eq. <ref type="bibr" target="#b8">(9)</ref>. Also note that, we denote our SDA model as "Our SDA w/ DBSCAN" instead of "Our SDA + UDAP w/ DBSCAN", since the SDA model is trained based on our modified version of UDAP instead of the original one. Our SDA outperforms the baseline UDAP <ref type="bibr" target="#b21">[22]</ref> by large margins, which indicates the effectiveness of our generated source-to-target training samples.</p><p>We also tested k-means to generate target-domain pseudo labels for our target-domain encoder on SDA with the optimal k value following the state-of-the-art <ref type="bibr" target="#b9">[10]</ref>, i.e., 500 for Duke?Market, 700 for Market?Duke, 1500 for Duke?MSMT and Market?MSMT. The results are denoted as "Our SDA w/ k-means". The training pipeline is the same as "Our SDA w/ DBSCAN", except for the clustering algorithm. Our SDA is consistently effective without the need of setting k to be close to the actual identity numbers. As shown in <ref type="table" target="#tab_4">Table III</ref>, even with different k's, our SDA stably improves the already strong baselines, which are trained with only the targetdomain samples and clustering pseudo labels <ref type="bibr" target="#b21">[22]</ref>. Note that the value of k can be considered as a hyper-parameter, and can be determined by searching for the optimal performance on the validation set. Selecting proper hyper-parameters for clusteringbased methods would not limit their usage in practical use.</p><p>2) MMT <ref type="bibr" target="#b9">[10]</ref> as a baseline for the target-domain encoder: Although MMT <ref type="bibr" target="#b9">[10]</ref> shows superior performances over "Ours SDA w/ k-means" by adopting dual networks with two times more parameters and computations for mutual training, our SDA is well compatible with it and can be combined to achieve further improvements. Specifically, the target-domain encoder is trained with source-to-target translated images and target-domain raw images under the training pipeline of MMT, where the source-to-target images are translated by SDA. Both soft losses and hard losses in MMT are adopted for training. The combination "Our SDA w/ k-means+MMT <ref type="bibr" target="#b9">[10]</ref>" shows further 4.3% and 5.3% mAP gains on Duke?Market and Market?Duke. Note that adopting existing domain translationbased methods (e.g., SPGAN <ref type="bibr" target="#b0">[1]</ref>) on top of MMT <ref type="bibr" target="#b9">[10]</ref> even degrades the performance, since MMT <ref type="bibr" target="#b9">[10]</ref> itself is already very strong and can only be boosted by informative enough translated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with Domain Translation-based Methods</head><p>As previous translation-based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> did not utilize pseudo labels and therefore cannot be directly compared with. We carefully design comparative experiments to verify the importance of the proposed online relationconsistency regularization term.</p><p>1) Our framework without pseudo labels: We evaluate our framework using a target-domain encoder without pseudo labels, which is a common strategy in previous translationbased methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b3">[4]</ref>, i.e., the target-domain encoder is trained with only source-to-target translated images and their source-domain identities. As shown in <ref type="table">Table IV</ref>, our method stably outperforms existing domain translation-based methods in terms of mAP on both Duke?Market and Market?Duke adaptation tasks without generating pseudo labels in the target domain. Specifically, we outperform state-of-the-art method CGAN-TM [5] by 3.7% and 1.4% mAP on Duke?Market and Market?Duke, respectively. Note that CGAN-TM <ref type="bibr" target="#b4">[5]</ref> achieved slightly better top-1/5/10 performance than our method on Market?Duke since they adopted a deeper DenseNet-121 as the backbone, compared to our plain ResNet-50 backbone.</p><p>In order to further verify the effectiveness of our introduced relation regularization term, we conduct an experiment by removing the L rc from "Our SDA w/o pseudo labels", dubbed "Our SDA w/o pseudo labels (w/o L rc )" in <ref type="table">Table IV</ref>. We observe significant performance drops when discarding such a relation regularization term from training.</p><p>2) Existing ID-based regularizations in our framework: Existing translation-based UDA methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> adopted identity-based losses with static targets to regularize the domain translation, including contrastive loss in SPGAN <ref type="bibr" target="#b0">[1]</ref>, classification loss in eSPGAN <ref type="bibr" target="#b2">[3]</ref> and CR-GAN <ref type="bibr" target="#b3">[4]</ref>, and triplet loss in CGAN-TM <ref type="bibr" target="#b4">[5]</ref>. Generally, the previous losses only require the source-to-target translated images to be correctly classified after translation, but ignore the intersample relations and similarities in their original domain. Since our pseudo-label-based target-domain encoder shows much better baseline performance than theirs, for fair comparison, we replace the online relation regularization L rc (Eq. <ref type="formula">(7)</ref>) in our framework with the previous methods' ID-based regularizations to demonstrate the effectiveness of our regularization.</p><p>The results are reported in <ref type="table">Table V</ref>. We observe that replacing our proposed L rc with previous regularization (denoted as "L rc ? ID-based contrastive loss <ref type="bibr" target="#b0">[1]</ref>", "L rc ? ID-based classification loss <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b17">[18]</ref>", "L rc ? ID-based triplet loss TABLE I. Unsupervised domain adaptation performances by state-of-the-art methods and our proposed SDA on person re-ID datasets, e.g., DukeMTMC-reID <ref type="bibr" target="#b47">[48]</ref>, Market-1501 <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>DukeMTMC [5]", "L rc ? ID-based classification loss &amp; triplet loss") all lead to worse performances than our method, demonstrating the superiority of our stronger relation regularization term over the weaker regularizations in previous domain translation-based UDA methods.</p><p>3) Domain Translation Examples: Besides the example triplets as shown in <ref type="figure">Figure 1 (c)</ref>, we also illustrate several translated examples of CycleGAN <ref type="bibr" target="#b5">[6]</ref>, SPGAN <ref type="bibr" target="#b0">[1]</ref> and our method in <ref type="figure">Figure 3</ref>. SPGAN adopts ID-based regularizations (i.e., contrastive loss), showing inferior generation results than our method. ID-based regularizations are too weak to preserve inter-sample relations during translation. For instance, the man in the first row appears to be in different colors (e.g., orange, yellow and green) within a tuple after translation by CycleGAN and SPGAN.</p><p>Besides the illustration of the generated images, we also evaluate the quality of translated samples by calculating the FID score <ref type="bibr" target="#b76">[77]</ref>. Specifically, we use FID metric to compute the similarity between the source-to-target translated dataset and the raw target-domain dataset. As demonstrated in <ref type="table">Table  VI</ref>, our method considerably outperforms SPGAN [1] on the task of Market?Duke. Note that the FID score can only evaluate the quality of generated images, but cannot evaluate the preserved inter-sample relations and affinities. One possible way to evaluate the preserved inter-sample relations of translated images is to use them as additional training samples for re-ID training and then evaluate the trained model by the re-ID metric, just as what we did in the above sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Studies</head><p>We conduct ablation studies on Duke?Market and Market?Duke tasks to analyze the effectiveness of each component in our framework. Detailed ablation experiments can be found in <ref type="table" target="#tab_4">Table VII.</ref> 1) Effectiveness of source-to-target translated images: We treat the target-domain encoder F t trained with only target-domain images and clustering-based pseudo labels as our baseline model, which can be treated as a reproduction of UDAP <ref type="bibr" target="#b21">[22]</ref>. Our framework significantly outperforms the baseline model by properly exploiting valuable source-domain data (see "Ours (full)" vs. "Baseline" in <ref type="table" target="#tab_4">Table VII)</ref>.</p><p>A na?ve way to use source-domain images is to directly train on both domains' raw images, denoted as "Baseline+raw source-domain data". The performance is even worse than the baseline on Duke?Market due to the large domain gaps, which indicates the necessity of properly leveraging different domains' images.  <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> did not provide trained models or translated images, thus not be illustrated here. However, we have carefully discussed and compared them with their ID-based regularizations in Section IV-D1 &amp; IV-D2. Best viewed in color. Our SDA can also be integrated and benefit state-of-theart pseudo-label-based method <ref type="bibr" target="#b9">[10]</ref> (see "Our SDA+MMT" vs. "MMT" in <ref type="table">Table I</ref>). The improvements demonstrate the effectiveness of source-to-target images translated by our structured domain translation method.</p><p>2) Alternative designs of online relation-consistency regularization: Our SDT applies regularizations on the softmaxtriplet relations (Eq. <ref type="formula">(7)</ref>). We further explore two alternative forms, prediction-consistency regularization L pc and batchall relation-consistency regularization L brc <ref type="table" target="#tab_4">(Table VIII)</ref> to verify the effectiveness of our proposed inter-sample relation constraint.</p><p>Specifically, the prediction-consistency regularization ensures that each individual image in the source domain should maintain the same "soft" class prediction after source-to-target translation. The loss function is formulated as</p><formula xml:id="formula_10">L pc (G s?t ) = E x s ?X s [?C s (f s ) ? log(C t (f s?t ))].</formula><p>As shown in <ref type="table" target="#tab_4">Table VIII</ref>, 3.1% and 2.4% mAP drops are observed on the two tasks. Here the prediction-consistency regularization L pc is different from conventional ID-based classification regularization <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b17">[18]</ref>, which can be formulated as E x s ?X s [?y s ? log(C t (f s?t ))] with "one-hot" ground-truth label y s . Briefly, L pc is supervised by "soft" C s (f s ) while IDbased classification regularization is supervised by "hard" y s . C s (f s ) is predicted by the source-domain encoder, which has captured the original inter-sample relations in the latent space. Note in this work, we denote the detailed data distributions and structured relations among data points, rather than the simple positive and negative identity relations, as the inter-sample relations. L pc can better preserve the inter-sample relations, while the ID-based classification regularization can only ensure that images belonging to different identities are well separated. Our L rc (Eq. <ref type="formula">(7)</ref>) aims at preserving relations within hardest triplets, while the alternative batch-all relation-consistency loss L brc tries to preserve all available relations within batches, no matter whether they are easy or hard. The batch-all regularization term follows the similarity-preserving loss in <ref type="bibr" target="#b39">[40]</ref>, which preserves inter-sample affinities in knowledge distillation. We model the batch-all inter-sample relations by measuring the similarities</p><formula xml:id="formula_11">R(x s ; F s ) = softmax[ f s , f s 1 , ? ? ? , f s , f s k ].</formula><p>The term consists of pairwise dot products between each sample x s and all other ones in the same batch. The similarity vector is normalized by a softmax function and a soft cross-entropy loss is adopted to regularize all the relations after translation,</p><formula xml:id="formula_12">L brc (G s?t ) = E x s ?X s [?R(x s ; F s ) ? log R(x s ; G s?t , F t )].</formula><p>1.8% mAP and 2.1% mAP drops can be observed on the two tasks. The reason might be that batch-all relations contain many easy cases that cannot provide effective supervisions for training.</p><p>To show the necessity of adopting relation regularization during translation, we also tested totally removing L rc from our framework, dubbed "Ours w/o relation-consistency loss L rc " in <ref type="table" target="#tab_4">Table VII</ref>. Significant mAP decreases of 3.4% and 3.8% are observed on Duke?Market and Market?Duke tasks. We observe that, replacing our relation-consistency loss L rc with either conventional ID-based constraints <ref type="table">(Table V)</ref> or alternative designs of relation-consistency regularization <ref type="table" target="#tab_4">(Table VIII)</ref> would achieve only slight improvements over "Ours w/o relation-consistency loss L rc ". This is because the pseudo-label-based baseline is already strong enough to achieve satisfactory performance on the target domain. Further improvements are quite challenging and require informative enough translated images as additional training samples. The comparison results well validate the effectiveness of our proposed relation-consistency regularization.</p><p>3) Effectiveness of training with the unified label set: We observe that the target-domain encoder also benefits from the unified label set by training the classifier on all the p s + p t classes across the two domains. To show it, we design an experiment with separate classifiers for source-to-target translated images and target-domain images, i.e., C t : f ? {1, ? ? ? , p s +p t } is split into C s?t : f s?t ? {1, ? ? ? , p s } and C t : f t ? {1, ? ? ? ,p t }. We report the performance in <ref type="table" target="#tab_4">Table VII</ref> as "Ours w/o unified label system". 1.6% and 1.9% mAP drops are observed on the two tasks, which indicate the effectiveness of modeling the relations between two domains. 4) Further benefits from the momentum encoder F t * : As described in Section IV-B, we utilize a momentum encoder <ref type="bibr" target="#b54">[55]</ref> for more stable training and better performance. To verify that the main contribution is not from the momentum encoder, we perform an experiment by removing F t * while keeping all other components unchanged. The experimental results are denoted as "Ours w/o momentum encoder <ref type="bibr" target="#b54">[55]</ref>" in <ref type="table" target="#tab_4">Table VII</ref>. We observe slight drops of 1.1% and 1.4% mAP on two tasks. 5) Performance w/o joint training of domain-translation network and target-domain encoder: The domain-translation network and target-domain encoder in our framework promote each other via joint training. However, a simpler training scheme would be to first train a source-to-target translation network with the proposed regularization and to translate all source-domain images to the target domain. A pseudo-labelbased target-domain encoder is then trained with such fixed source-to-target translated images and target-domain images. Note that in this scheme, a target-domain encoder pre-trained with only unlabeled data and clustering labels is used for regularizing the SDT training.  <ref type="figure">Fig. 4</ref>. Hyper-parameter analysis on the loss weight ?rc for our proposed framework ("Our SDA w/ k-means") on the task of Duke?Market.</p><p>We evaluate both our framework and existing translationbased methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b0">[1]</ref> when adopting such a separate training strategy and k-means clustering for pseudo label generation. The results in <ref type="table">Table IX</ref> show that the informative training samples generated by our proposed structured domain-translation (SDT) network could effectively improve the already strong baseline even without our joint training scheme, while the source-to-target images generated by CycleGAN and SPGAN might even worsen the performance because their generated images might not well capture the distributions of target-domain data and maintain their original inter-sample relations.</p><p>6) Hyper-parameter analysis: There are five hyperparameters in our training scheme (Section IV-B), including the loss weights ? rc , ? adv , ? cyc , ? apr and the triplet margin m. Note that the values of ? adv , ? cyc and ? apr are directly inherited from CycleGAN <ref type="bibr" target="#b5">[6]</ref>, and the setting of m = 0.3 is widely acknowledged in re-ID-related tasks <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b9">[10]</ref>. We therefore conduct the experiments of hyper-parameter analysis on ? rc , which is also the most important factor in our introduced structured domain-translation network. As shown in <ref type="figure">Figure 4</ref>, our proposed model is robust when the value of ? rc varies from 0.1 to 2.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Application of SDA in VisDA-2020 Challenge</head><p>VisDA-2020 Challenge introduces a synthetic?real adaptation task, where the labeled synthetic images and the unlabeled real-world images are provided. The final performances on the test set of the real-world domain are used for ranking the teams <ref type="figure">Fig. 5</ref>. Source-to-target translated images in VisDA-2020. Note persons on each row are of the same identity. Due to the large illumination variations, (a) the woman in red appears in both red and black, and (b) another woman in blue appears in both blue and white after being translated by SPGAN <ref type="bibr" target="#b0">[1]</ref>. In contrast, thanks to the online relation-consistency regularization, our SDT could better maintain a consistent appearance for person images of the same identity in both two cases.</p><p>in the challenge. Note that the model's performance on the test set is evaluated on the online server and the ground-truth labels cannot be accessed. We therefore tune our models on the validation set during the challenge, and also report the performance on the validation set for comparison in this paper.</p><p>We propose to adopt the structured domain-translation (SDT) network to translate the synthetic images to have the real-world style. Specifically, we adopt the "w/o joint training" version of SDA as mentioned in Section IV-E5 with three main training steps: 1) Pre-training the source-domain encoder with only labeled data; 2) Pre-training the target-domain encoder with only unlabeled data and clustering labels; 3) Training the SDT network with the online relation-consistency regularization provided by the two encoders on-the-fly via Eq. <ref type="bibr" target="#b7">(8)</ref>. The main reasons why we did not use the "joint training" version of SDA for the competition is that increasing model sizes and numbers of parameters can result in more gains on the dataset, which is important in the competition. "w/o joint training" can save GPU memory and allow training much larger deep models.</p><p>After being trained, the SDT network can be adopted to generate informative training samples with ground-truth identities by translating all the source-domain images to the target domain. In order to verify the effectiveness of our SDT, we adopt a target-domain encoder with a ResNet50-IBN backbone. The encoder is trained with only synthetic-to-real translated images and their ground-truth identities, and then tested on the validation set of the target domain. As shown in <ref type="table">Table X</ref>, training with "synthetic-to-real images by SDT" could achieve much more performance gain than training with "synthetic-to-real images by SPGAN". Besides the quantitative TABLE IX. Comparison with domain translation-based UDA methods with pseudo labels (via k-means clustering) but without jointly training the domaintranslation network and target-domain encoder. Source-to-target images translated by SPGAN <ref type="bibr" target="#b0">[1]</ref> were provided by the authors and can be directly used for training the pseudo-label-based encoder.  comparison, we also visualize the translation results in <ref type="figure">Figure 5</ref>, where our SDT could better preserve the inter-sample relations than SPGAN <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods w/o Joint</head><p>In our solution to VisDA-2020 Challenge, multi-model ensemble and an improved MMT <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b9">[10]</ref> are further introduced to fine-tune the network with both synthetic?real images translated by SDT and raw unlabeled real-world images, achieving superior performance, as illustrated in <ref type="figure" target="#fig_2">Figure 6</ref>. The gain of our proposed SDT over the new baseline model and new adaption task demonstrates its generalization capability. Our final solution ranks second out of 153 teams. TABLE X. Ablation study on the effectiveness of synthetic-to-real images translated by SDT in VisDA-2020. The experiments adopt the backbone of ResNet50-IBN <ref type="bibr" target="#b78">[79]</ref> and post-processing techniques (e.g., camera bias subtraction <ref type="bibr" target="#b79">[80]</ref>, re-ranking <ref type="bibr" target="#b80">[81]</ref>) are employed. The results are evaluated on the target_val set and only the top-100 matches are considered for mAP. Note that this result is not the final result in the challenge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION AND CONCLUSION</head><p>In this work, we propose an end-to-end structured domain adaptation framework with a novel online relationconsistency regularization term to tackle the unsupervised domain adaptation (UDA) problem for person re-ID. The structuredly translated images in our method are shown to be informative samples for improving the training of pseudolabel-based encoder. The joint optimization scheme of domaintranslation network and re-ID encoder is effective, however, it still has difficulty on handling industrial-scale datasets. Further improvements are called for. Beyond the person re-ID, our proposed inter-sample relation-consistency regularization may benefit other related UDA tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of our structured domain adaptation (SDA) framework and the novel online relation-consistency regularization term. The domaintranslation network and the target-domain encoder alternately promote each other via joint training to achieve optimal re-ID performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>? ?)? (T ) , where ? (0) * = ? (0) and ? = 0.999 is the momentum coefficient. Intuitively, the momentum encoder could provide more reliable inter-sample relations for regularizing the Structured Domain Translation (SDT) since it eases the training bias caused by unstable translation results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>The flow chart of our solution in VisDA-2020 Challenge, which consists of three steps: structured domain adaptation (SDA), pre-training with source-to-target translated images and fine-tuning on the target domain with the improved MMT framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1</head><label>1</label><figDesc>Structured domain adaptation for person re-ID Require: Labeled source-domain data X s , unlabeled target-domain data X t ; Require: Weighting factors ?rc, ?cyc, ? adv , ?apr for Eq. (8); 1: Pre-train source-domain encoder F s by minimizing Eq. (1) on X s ; 2: Initialize target-domain encoder F t by loading the weights of F s ; 3: for n in [1, num_epochs] do 4: Create pseudo labels by clustering F t (X t ); 5: for each mini-batch B s ? X s , B t ? X t do 6: Translate B s into the target domain as B s?t by G s?t ; 7: Update G s?t , G t?s by minimizing the objective function Eq. (8) with D s , D t fixed, where the inter-sample relations are measured by F s and F t on-the-fly; 8: Update F t by minimizing the objective function Eq. (9) with B s?t ? B t ; 9: Update D s , D t by maximizing the objective function Eq. (8) with G s?t , G t?s fixed. Joint training objective: During training, we fix F s and alternately update F t and SDT in each iteration to avoid bias amplification, where the SDT network is optimized with</figDesc><table><row><cell>10:</cell><cell>end for</cell></row><row><cell cols="2">11: end for</cell></row><row><cell>4)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>contains 36,411 images of 702 identities for training and another 702 identities for testing, with all the images captured from 8 cameras. Market-1501 [49] consists of 12,936 images of 751 identities for training and 19,281 images of 750 identities for testing, which are shot by 6 cameras.</figDesc><table /><note>?? MSMT17 [2] is the most challenging dataset with 126,441 images of 4,101 identities from 15 cameras, where 1,041</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II .</head><label>II</label><figDesc>Unsupervised domain adaptation performances by state-of-the-art methods and our proposed SDA on MSMT17<ref type="bibr" target="#b1">[2]</ref>.</figDesc><table><row><cell>Methods</cell><cell>mAP</cell><cell cols="2">Market-1501?MSMT17 top-1 top-5</cell><cell>top-10</cell><cell>mAP</cell><cell cols="2">DukeMTMC-reID?MSMT17 top-1 top-5</cell><cell>top-10</cell></row><row><cell>Domain translation-based UDA methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PTGAN [2] (CVPR'18)</cell><cell>2.9</cell><cell>10.2</cell><cell>-</cell><cell>24.4</cell><cell>3.3</cell><cell>11.8</cell><cell>-</cell><cell>27.4</cell></row><row><cell>Pseudo-label-based UDA methods &amp; Others</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ECN [24] (CVPR'19)</cell><cell>8.5</cell><cell>25.3</cell><cell>36.3</cell><cell>42.1</cell><cell>10.2</cell><cell>30.2</cell><cell>41.5</cell><cell>46.8</cell></row><row><cell>SSG [9] (ICCV'19)</cell><cell>13.2</cell><cell>31.6</cell><cell>-</cell><cell>49.6</cell><cell>13.3</cell><cell>32.2</cell><cell>-</cell><cell>51.2</cell></row><row><cell>ECN++ [64] (TPAMI'20)</cell><cell>15.2</cell><cell>40.4</cell><cell>53.1</cell><cell>58.7</cell><cell>16.0</cell><cell>42.5</cell><cell>55.9</cell><cell>61.5</cell></row><row><cell>MMCL [65] (CVPR'20)</cell><cell>15.1</cell><cell>40.8</cell><cell>51.8</cell><cell>56.7</cell><cell>16.2</cell><cell>43.6</cell><cell>54.3</cell><cell>58.9</cell></row><row><cell>DG-Net++ [18] (ECCV'20)</cell><cell>22.1</cell><cell>48.4</cell><cell>60.9</cell><cell>66.1</cell><cell>22.1</cell><cell>48.8</cell><cell>60.9</cell><cell>65.9</cell></row><row><cell>D-MMD [69] (ECCV'20)</cell><cell>13.5</cell><cell>29.1</cell><cell>46.3</cell><cell>54.1</cell><cell>15.3</cell><cell>34.4</cell><cell>51.1</cell><cell>58.5</cell></row><row><cell>JVTC [70] (ECCV'20)</cell><cell>20.3</cell><cell>45.4</cell><cell>58.4</cell><cell>64.3</cell><cell>19.0</cell><cell>42.1</cell><cell>53.4</cell><cell>58.9</cell></row><row><cell>GPR [71] (ECCV'20)</cell><cell>20.4</cell><cell>43.7</cell><cell>56.1</cell><cell>61.9</cell><cell>24.3</cell><cell>51.7</cell><cell>64.0</cell><cell>68.9</cell></row><row><cell>NRMT [72] (ECCV'20)</cell><cell>19.8</cell><cell>43.7</cell><cell>56.5</cell><cell>62.2</cell><cell>20.6</cell><cell>45.2</cell><cell>57.8</cell><cell>63.3</cell></row><row><cell>UDAP [22] w/ DBSCAN (modified)</cell><cell>20.3</cell><cell>45.7</cell><cell>58.6</cell><cell>64.5</cell><cell>22.8</cell><cell>49.3</cell><cell>61.7</cell><cell>67.2</cell></row><row><cell>Our SDA w/ DBSCAN</cell><cell>23.2</cell><cell>49.5</cell><cell>62.2</cell><cell>67.7</cell><cell>25.6</cell><cell>54.4</cell><cell>66.4</cell><cell>71.3</cell></row><row><cell>Our SDA w/ k-means</cell><cell>20.6</cell><cell>46.8</cell><cell>59.9</cell><cell>65.0</cell><cell>23.0</cell><cell>51.7</cell><cell>64.2</cell><cell>69.6</cell></row><row><cell>MMT [10] w/ k-means (ICLR'20)</cell><cell>22.9</cell><cell>49.2</cell><cell>63.1</cell><cell>68.8</cell><cell>23.3</cell><cell>50.1</cell><cell>63.9</cell><cell>69.8</cell></row><row><cell>Our SDA + MMT [10] w/ k-means</cell><cell>29.0</cell><cell>57.0</cell><cell>69.5</cell><cell>74.1</cell><cell>30.3</cell><cell>59.6</cell><cell>71.7</cell><cell>76.2</cell></row><row><cell>Source images</cell><cell cols="2">CycleGAN [6]</cell><cell cols="2">SPGAN [1]</cell><cell></cell><cell>Ours</cell><cell></cell><cell></cell></row></table><note>Fig. 3. Domain-translated examples of CycleGAN [6], SPGAN [1] and our method. Note persons on each row are of the same identity. Other translation-based methods</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III .</head><label>III</label><figDesc>Comparison with different values of k in our SDA when adopting k-means on Market?Duke.</figDesc><table><row><cell>k value</cell><cell cols="2">Baseline mAP top-1</cell><cell>mAP</cell><cell>Ours</cell><cell>top-1</cell></row><row><cell>500</cell><cell>46.7</cell><cell>65.9</cell><cell>53.8 (+7.1)</cell><cell></cell><cell>70.6 (+4.7)</cell></row><row><cell>700</cell><cell>50.1</cell><cell>68.2</cell><cell>56.7 (+6.6)</cell><cell></cell><cell>74.0 (+5.8)</cell></row><row><cell>900</cell><cell>48.9</cell><cell>66.6</cell><cell>56.0 (+7.1)</cell><cell></cell><cell>72.9 (+6.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV .TABLE V .</head><label>IVV</label><figDesc>Comparison with domain translation-based UDA methods using target-domain encoder without pseudo labels. "Our SDA w/o pseudo labels" indicates training target-domain encoder with only source-to-target translated images and their ground-truth identities. Comparison between our online relation-consistency regularization Lrc and ID-based regularizations in previous domain translation-based methods. k-means algorithm is adopted here to generate pseudo labels.</figDesc><table><row><cell>Methods w/o Pseudo Labels</cell><cell></cell><cell cols="4">DukeMTMC-reID?Market-1501 mAP top-1 top-5 top-10</cell><cell cols="4">Market-1501?DukeMTMC-reID mAP top-1 top-5 top-10</cell></row><row><cell>PTGAN [2] (CVPR'18)</cell><cell></cell><cell>-</cell><cell>38.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>27.4</cell><cell>-</cell><cell>-</cell></row><row><cell>SPGAN [1] (CVPR'18)</cell><cell></cell><cell>22.8</cell><cell>51.5</cell><cell>70.1</cell><cell>76.8</cell><cell>22.3</cell><cell>41.1</cell><cell>56.6</cell><cell>63.0</cell></row><row><cell>CR-GAN [4] (ICCV'19)</cell><cell></cell><cell>29.6</cell><cell>59.6</cell><cell>-</cell><cell>-</cell><cell>30.0</cell><cell>52.2</cell><cell>-</cell><cell>-</cell></row><row><cell>SBSGAN [17] (ICCV'19)</cell><cell></cell><cell>27.3</cell><cell>58.5</cell><cell>-</cell><cell>-</cell><cell>30.8</cell><cell>53.5</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">CGAN-TM [5] (TIP'20) (DenseNet-121)</cell><cell>31.3</cell><cell>61.4</cell><cell>78.4</cell><cell>84.9</cell><cell>32.9</cell><cell>54.9</cell><cell>68.8</cell><cell>74.3</cell></row><row><cell>Our SDA w/o pseudo labels</cell><cell></cell><cell>35.0</cell><cell>64.5</cell><cell>79.5</cell><cell>84.6</cell><cell>34.3</cell><cell>53.1</cell><cell>67.1</cell><cell>72.4</cell></row><row><cell>Our SDA w/o pseudo labels (w/o Lrc)</cell><cell></cell><cell>31.0</cell><cell>59.9</cell><cell>75.2</cell><cell>82.0</cell><cell>30.2</cell><cell>51.2</cell><cell>66.0</cell><cell>70.9</cell></row><row><cell>Methods</cell><cell></cell><cell cols="4">DukeMTMC-reID?Market-1501 mAP top-1 top-5 top-10</cell><cell cols="4">Market-1501?DukeMTMC-reID mAP top-1 top-5 top-10</cell></row><row><cell>Lrc ? ID-based contrastive loss [1]</cell><cell></cell><cell>56.5</cell><cell>78.7</cell><cell>91.5</cell><cell>94.0</cell><cell>51.7</cell><cell>69.9</cell><cell>80.5</cell><cell>83.3</cell></row><row><cell cols="2">Lrc ? ID-based classification loss [3], [4], [18]</cell><cell>63.4</cell><cell>84.9</cell><cell>92.7</cell><cell>95.1</cell><cell>53.8</cell><cell>70.9</cell><cell>81.9</cell><cell>85.6</cell></row><row><cell>Lrc ? ID-based triplet loss [5]</cell><cell></cell><cell>64.1</cell><cell>85.2</cell><cell>92.9</cell><cell>95.4</cell><cell>54.5</cell><cell>72.1</cell><cell>82.3</cell><cell>85.9</cell></row><row><cell cols="2">Lrc ? ID-based classification loss &amp; triplet loss</cell><cell>64.5</cell><cell>85.0</cell><cell>93.1</cell><cell>95.6</cell><cell>54.7</cell><cell>72.6</cell><cell>82.4</cell><cell>85.9</cell></row><row><cell>Our SDA w/ Lrc</cell><cell></cell><cell>66.4</cell><cell>86.4</cell><cell>93.1</cell><cell>95.6</cell><cell>56.7</cell><cell>74.0</cell><cell>84.1</cell><cell>87.7</cell></row><row><cell cols="4">TABLE VI. Evaluation of generated images by domain translation-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">methods for UDA person re-ID in terms of the FID score [77].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SPGAN [1]</cell><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Market-1501?DukeMTMC-reID</cell><cell>47.6</cell><cell>77.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DukeMTMC-reID?Market-1501</cell><cell>46.1</cell><cell>46.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII .TABLE VIII .</head><label>VIIVIII</label><figDesc>Ablation studies for our proposed framework ("Our SDA w/ k-means") on individual components. Comparison with the optional relation-consistency regularizations in our SDA with k-means.</figDesc><table><row><cell>Methods</cell><cell></cell><cell></cell><cell cols="4">DukeMTMC-reID?Market-1501 mAP top-1 top-5 top-10</cell><cell cols="3">Market-1501?DukeMTMC-reID mAP top-1 top-5 top-10</cell></row><row><cell>Source-domain pre-trained</cell><cell></cell><cell></cell><cell>21.7</cell><cell>47.8</cell><cell>64.2</cell><cell>70.1</cell><cell>14.4</cell><cell>26.7</cell><cell>39.8</cell><cell>45.6</cell></row><row><cell cols="3">Baseline (only target-domain data + pseudo labels)</cell><cell>59.0</cell><cell>80.7</cell><cell>90.5</cell><cell>93.4</cell><cell>50.1</cell><cell>68.2</cell><cell>79.2</cell><cell>82.7</cell></row><row><cell cols="2">Baseline + raw source-domain data</cell><cell></cell><cell>53.2</cell><cell>77.5</cell><cell>88.9</cell><cell>92.1</cell><cell>50.8</cell><cell>69.1</cell><cell>80.1</cell><cell>83.1</cell></row><row><cell cols="2">Ours w/o relation-consistency loss Lrc</cell><cell></cell><cell>63.0</cell><cell>84.3</cell><cell>92.7</cell><cell>95.2</cell><cell>52.9</cell><cell>69.8</cell><cell>81.3</cell><cell>84.5</cell></row><row><cell>Ours w/o unified label system</cell><cell></cell><cell></cell><cell>64.8</cell><cell>86.0</cell><cell>93.1</cell><cell>95.2</cell><cell>54.8</cell><cell>73.1</cell><cell>83.1</cell><cell>85.5</cell></row><row><cell cols="2">Ours w/o momentum encoder [55]</cell><cell></cell><cell>65.3</cell><cell>86.1</cell><cell>93.1</cell><cell>95.3</cell><cell>55.3</cell><cell>72.5</cell><cell>82.8</cell><cell>85.5</cell></row><row><cell>Ours (full)</cell><cell></cell><cell></cell><cell>66.4</cell><cell>86.4</cell><cell>93.1</cell><cell>95.6</cell><cell>56.7</cell><cell>74.0</cell><cell>84.1</cell><cell>87.7</cell></row><row><cell>Regularization</cell><cell cols="2">Duke?Market mAP top-1</cell><cell cols="2">Market?Duke mAP top-1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Prediction-consistency Lpc</cell><cell>63.3</cell><cell>84.5</cell><cell>54.3</cell><cell>71.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Batch-all relations L brc [40]</cell><cell>64.6</cell><cell>86.0</cell><cell>54.6</cell><cell>71.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our Lrc</cell><cell>66.4</cell><cell>86.4</cell><cell>56.7</cell><cell>74.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code for general datasets is available at https://github.com/SDA-ReID/SDA.<ref type="bibr" target="#b1">2</ref> Code for VisDA 2020 challenge is available at https://github.com/yxgeee/ VisDA-ECCV20.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="994" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Similarity-preserving image-image domain adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10551</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Instance-guided context rendering for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="232" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cgan-tm: A novel domain-to-domain transferring method for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5641" to="5651" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-training with progressive augmentation for unsupervised cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8222" to="8231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-similarity grouping: A simple unsupervised cross domain adaptation approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6112" to="6121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vehiclenet: Learning robust visual representation for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A strong baseline and batch normalization neck for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2597" to="2609" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glad: Global-localalignment descriptor for scalable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="986" to="999" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploit the unknown gradually: One-shot video-based person re-identification by stepwise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5177" to="5186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification via cross-camera similarity exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5481" to="5490" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Progressive learning for person re-identification with one example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2872" to="2881" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sbsgan: Suppression of interdomain background shift for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9527" to="9536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Joint disentangling and adaptation for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improved mutual mean-teaching for unsupervised domain adaptive re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.10313</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Leveraging virtual and real person for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2444" to="2453" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature affinity-based pseudo labeling for semi-supervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2891" to="2902" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptive re-identification: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">107173</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by soft multilabel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="598" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation using regularized hyper-graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3758" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Progressive unsupervised person re-identification by tracklet association with spatio-temporal regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Part-aware progressive unsupervised domain adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sample-problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Open set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panareda</forename><surname>Busto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Open set domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="153" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Crdoco: Pixel-level domain transfer with cross-domain consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1791" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Structure preserving embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="937" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Paul</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pose transferrable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4099" to="4108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fd-gan: Pose-guided feature distilling gan for robust person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1222" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2138" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dissecting person re-identification from the viewpoint of viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="608" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">John riccitiello sets out to identify the engine of growth for unity technologies (interview)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Riccitiello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VentureBeat. Interview with Dean Takahashi</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Transferable joint attribute-identity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2275" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Disjoint label space transfer learning with common factorised space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3288" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A bottom-up clustering approach to unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8738" to="8745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Adaptation and re-identification network: An unsupervised deep transfer learning approach to person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C. Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A novel unsupervised camera-aware domain adaptation framework for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8080" to="8089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Cross-dataset person re-identification via unsupervised pose disentanglement and adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7919" to="7929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning to adapt invariance in memory for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification via multilabel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">990</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Style normalization and restitution for generalizable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3143" to="3152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Ad-cluster: Augmented discriminative clustering for domain adaptive person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9021" to="9030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">An attention-driven two-stage clustering method for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation in the dissimilarity space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mekhazni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhuiyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ekladious</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Joint visual and temporal consistency for unsupervised domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Generalizing person re-identification by camera-aware invariance learning and cross-domain mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation with noise resistible mutual-training for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Deep credible metric learning for unsupervised domain adaptation person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Global distance-distributions separation for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, ser. KDD&apos;96</title>
		<meeting>the Second International Conference on Knowledge Discovery and Data Mining, ser. KDD&apos;96</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">M2m-gan: Many-to-many generative adversarial transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03768</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="464" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Voc-reid: Vehicle re-identification based on vehicle-orientation-camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="602" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Re-ranking person reidentification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
