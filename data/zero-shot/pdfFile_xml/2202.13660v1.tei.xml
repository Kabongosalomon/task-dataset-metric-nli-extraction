<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FUSIONCOUNT: EFFICIENT CROWD COUNTING VIA MULTISCALE FEATURE FUSION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Warwick Mathematics Institute University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanchez</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanaya</forename><surname>Guha</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FUSIONCOUNT: EFFICIENT CROWD COUNTING VIA MULTISCALE FEATURE FUSION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Crowd density estimation</term>
					<term>multiscale feature fu- sion</term>
					<term>efficient crowd counting</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art crowd counting models follow an encoder-decoder approach. Images are first processed by the encoder to extract features. Then, to account for perspective distortion, the highest-level feature map is fed to extra components to extract multiscale features, which are the input to the decoder to generate crowd densities. However, in these methods, features extracted at earlier stages during encoding are underutilised, and the multiscale modules can only capture a limited range of receptive fields, albeit with considerable computational cost. This paper proposes a novel crowd counting architecture (FusionCount), which exploits the adaptive fusion of a large majority of encoded features instead of relying on additional extraction components to obtain multiscale features. Thus, it can cover a more extensive scope of receptive field sizes and lower the computational cost. We also introduce a new channel reduction block, which can extract saliency information during decoding and further enhance the model's performance. Experiments on two benchmark databases demonstrate that our model achieves state-of-the-art results with reduced computational complexity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Crowd counting aims to automatically estimate the number of individuals present in a scene from an image or video. It can be applied in numerous areas, such as traffic control <ref type="bibr" target="#b0">[1]</ref>, biological studies <ref type="bibr" target="#b1">[2]</ref>, and recently, social distancing monitoring <ref type="bibr" target="#b2">[3]</ref>.</p><p>Over the years, models for crowd counting have evolved from using classical regression models, such as random forests <ref type="bibr" target="#b3">[4]</ref> and Gaussian processes <ref type="bibr" target="#b4">[5]</ref>, to high-performing convolutional neural networks (CNNs) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. These deep nets usually adopt an encoderdecoder approach: First, an image is fed to the encoder to learn data representations (feature maps). The decoder then exploits the highest-level representation (the output from the encoder's last layer) to generate the density map, which is the distribution of the crowd. Since the convolutional and pooling blocks of VGG networks <ref type="bibr" target="#b9">[10]</ref>, where each layer exploits kernels of a fixed size, constitute most encoders, the size of receptive fields remains constant across the last encoded feature map. Thus, this representation can only handle images where crowds are of similar scales. However, people are usually depicted in various sizes because of the camera perspective, and as a result, the encoded feature should also have different receptive field sizes to model the scale variation. Multi-column structures <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref> have been proposed to solve this problem. However, it has been recently shown that features from each column are almost identical, and training deep models of this type can be very unproductive <ref type="bibr" target="#b6">[7]</ref>. Therefore, to solve this scale issue, state-of-the-art methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13</ref>] employ a multiscale module that further processes the encoded representation and generates a feature map with different receptive field sizes. However, such a strategy ignores that those feature maps extracted by shallower encoding layers already provide information about different scales, and leveraging extra components makes the overall model more computationally expensive.</p><p>Hence, our contribution in this paper is a novel multiscale mechanism that addresses the scale issue by leveraging the majority of features generated from the encoder to avoid extra feature-extraction modules and keep the computational cost low. This design incorporates a comprehensive range of receptive field sizes (6 to 192), covering almost all possible scales a person can depict in a crowd image. Experiments on two benchmark databases (ShanghaiTech A &amp; B <ref type="bibr" target="#b5">[6]</ref>) demonstrate that our model can achieve state-of-the-art or comparable results with significantly fewer floating-point operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Early crowd counting approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> are based on object detectors, while later works tend to avoid them because of their sensitivity to occlusion and the enormous efforts required to annotate bounding boxes. Some of these non-detection-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> treat crowd counting as a regression problem: they learn low-level feature representations first, from which the total count is then directly regressed. The training losses of these approaches depend only on the ground-truth count (a scalar) and do not consider crowd density distribution, hence suffering poor generalisation. Thus, these models have soon been superseded by algorithms <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12]</ref> that instead predict crowd densities, and these density-based methods primarily rely on CNNs.</p><p>Since 3D spatial locations have to be projected onto a 2D space while an RGB crowd image is captured from the real world, people can be depicted in different sizes due to perspective distortion, and the variation in scales can severely affect density estimation. To solve this problem, in <ref type="bibr" target="#b20">[21]</ref>, extra geometric information is exploited to adapt their model to different scenes, but this information is not always provided. Therefore, later methods tend to learn the scales of crowds implicitly. For example, Hydra-CNN <ref type="bibr" target="#b19">[20]</ref> divides an image into a pyramid of patches, each representing a different scale and fed to a separate encoder head. Then, all encoded features are concatenated without further processing and utilised to generate the density map. This approach neglects the fact that scales vary continuously across the whole image. CAN <ref type="bibr" target="#b7">[8]</ref> is then proposed to address this issue. The whole model involves only one encoder, so a spatial pyramid pooling module <ref type="bibr" target="#b22">[22]</ref> is leveraged to make it scale-aware. Then, features are averaged according to learnable weights to ensure that receptive field sizes of the fused feature changes smoothly. However, this architecture is less efficient since it does not exploit low-level features extracted during encoding. These representations, along with the final high-level feature map, can provide information about different scales because they have disparate receptive field sizes. Also, the multiscale module in <ref type="bibr" target="#b7">[8]</ref> uses only four filter sizes, thereby covering a limite range of scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">OUR MODEL</head><p>This section exhaustively describes our proposed crowd counting model, FusionCount. It extensively leverages representations learned during encoding to compute first-phase multiscale features, and its decoder further fuses these scale-aware features to generate the density map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Encoding</head><p>Following common practices <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> in the field, VGG-16 <ref type="bibr" target="#b9">[10]</ref> is employed as our model's encoder. The last max-pooling and fully connected layers are removed since they are accountable for class prediction. Therefore, the encoder comprises 13 convolutional layers and four max-pooling layers. Since the two feature maps extracted before the first max-pooling operation are not sufficiently informative, only features learned afterwards are preserved and fused. Thus, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the encoder outputs 15 feature maps in total, denoted by xj, j = 1, 2, ? ? ? , 15. These feature maps are divided into four groups according to their heights and widths: x1 -x3, x4 -x7, x8 -x11, and x12 -x15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Fusion</head><p>Suppose xi, xi+1, ? ? ? , x i+k is a group of feature maps of interest, where xi is generated by a max-pooling layer and has c channels, while the rest are are produced by convolutional layers and have c * channels. To assemble a scale-aware representation from them, we adopt a similar strategy as the one proposed in <ref type="bibr" target="#b7">[8]</ref>, and our method is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. Given that scales change continuously accross an image, the receptive field size of the output multiscale feature, which models the scales, should also be spatially continuous. To accomplish this aim, we fuse these features via weighted averaging, where weight maps are learned from each feature's spatial importance.</p><p>Firstly, since xi and xi+j (j ? {1, ? ? ? , k}) may not have the same number of channels, we use point-wise convolution to expand xi, and denote the output by x * i . Notice that since xi is the output of a max-pooling layer and the derivation of x * i does not involve any integration of neighbouring spatial information, x * i is less influenced by background noise. With such an advantage, all other representations are compared against it. Namely, following <ref type="bibr" target="#b7">[8]</ref>, we propose a similar concept to contrast features:</p><formula xml:id="formula_0">cj = xi+j ? x * i ,<label>(1)</label></formula><p>with j = 1, ? ? ? , k. Since x * i and xi+j (j = 1, ? ? ? , k) have different receptive field sizes, the contrast features cj incorporate disparities between any spatial location and its neighbouring pixels. Thus, they can facilitate learning the boundary of each person, thereby determining each receptive field size's relative importance. We then compute the weights ?j as follows:</p><formula xml:id="formula_1">?j = ?(cj),<label>(2)</label></formula><p>where ? denotes the sigmoid function. <ref type="bibr" target="#b0">1</ref> Using these weights, ?i+1, ? ? ? ? i+k , we then combine {xi, xi+1, ? ? ? , x i+k } adaptively and concatenate the averaged feature map with xi. Finally, we add a bottleneck layer to reduce computation. This process can be expressed as:</p><formula xml:id="formula_2">f = Conv xi k j=1 ?j xi+j ,<label>(3)</label></formula><p>where f , Conv, "|" and " " denote the fused feature, the bottleneck layer, channel-wise concatenation, and element-wise product, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Decoding</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we denote the first-phase multiscale features generated from the four groups via (3) as f 1 , f 2 , f 3 and f 4 . These features are further fused in the reverse order (see <ref type="figure" target="#fig_2">Fig. 3</ref>). To combine f 4 and f 3 via addition, we first transform the shape of f 4 . Specifically, the number of channels of f 4 is lowered, while its spatial size is expanded. Traditionally, a single point-wise convolutional kernel is used to reduce the nubmer of channels. However, inspired by dilated convolution, which has been shown to have the ability to extract deeper salient information while maintaining the spatial resolution <ref type="bibr" target="#b6">[7]</ref>, we propose a new channel reduction module. Our two-stream module comprises a dilated convolution block and one bottleneck layer, and the two columns are connected via addition. <ref type="bibr" target="#b1">2</ref> To match the spatial resolution of f 3 , we use bilinear interpolation to double the size of f 4 after channel redcution. Then these two features can be fused by summation. The new fused feature is then combined with f 2 following a similar pattern -we first modify its dimensions via the proposed channel reduction module and bilinear interpolation, and then add it to f 2 . This process is followed iteratively until all features are fused. Finally, we feed the final fused feature to the output layer to generate the estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We use the ShanghaiTech A &amp; B datasets <ref type="bibr" target="#b5">[6]</ref> for model evaluation and comparison. In ShanghaiTech A, there are 482 crowd im- 400 for training and 316 for testing), with an average count of approximately 123. Images from this dataset are taken from a surveillance view in a shopping street and are therefore less crowded. Also, considering that these images have a fixed resolution (768 ? 1024), this dataset is more suitable for real-world applications e.g., public safety monitoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment Settings</head><p>During recent years, loss functions based on probability theory, such as Bayes' theorem and Wasserstein distance, have been shown to help models achieve stronger generalisation capabilities and have thus gained increased popularity. We use the DM-Count loss <ref type="bibr" target="#b23">[23]</ref> to supervise the training of FusionCount. An Adam optimiser <ref type="bibr" target="#b24">[24]</ref> with a learning rate 1e-5 and batch size two is leveraged for optimisation. In order to make fair comparisons with other approaches, we use the default data splits. Given that some images have intractably large sizes, from each input image, two patches with a size of 384 ? 512 are cropped and used for training. Our model and its training are implemented in the PyTorch [25] 1.10 framework, and the platform for training is a server with an NVIDIA RTX 3090 GPU and Ubuntu 20.04 LTS OS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>Following prior works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">23]</ref>, the mean absolute error (MAE) and the root mean squared error (RMSE) of total counts are employed as evaluation metrics. FusionCount is compared with state-of-the-art models that have a similar computational complexity (quantified by the number of multiplications and additions involved in the inference on a 1080 ? 1920 RGB image). In particular, CSRNet <ref type="bibr" target="#b6">[7]</ref>, CAN <ref type="bibr" target="#b7">[8]</ref>, BL <ref type="bibr" target="#b8">[9]</ref> and DM-Count <ref type="bibr" target="#b23">[23]</ref>, all of which employ VGGs <ref type="bibr" target="#b9">[10]</ref> as encoders, are encompassed for comparison. CSRNet uses dilated convolution in decoding to extract saliency. In addition to this characteristic, CAN also includes a spatial pyramid pooling block to generate multiscale features. BL and DM-Count have no innovations in terms of model architectures, and their contributions are particularly novel loss functions based on probability theory.  Ground-truth annotations are marked by blue dots, and the orange shades represent the estimated densities. We also report 'GT' i.e., ground-truth counts of people in the images, 'Pred' i.e., the predicted counts. 'RE' denotes the correpsonding relateive error.</p><p>Four instances from the test sets of ShanghaiTech A &amp; B are depicted in <ref type="figure" target="#fig_3">Fig. 4</ref>. The left column shows the original input images, and the right column illustrates the ground-truth and the predicted density maps, which are indicated by blue dots and orange shades, respectively. The example in the first row proves that our model can still work well in highly crowded cases. Although in this 600 ? 900 image, there are over 1,600 people, our model still achieves an accurate prediction with a relatively small error (1.98%). Other rows demonstrate that our model is capable of effectively dealing with scale changes. In these images, crowds in the lower parts of the scene have larger scales, while those in the upper part of the scene have the smallest scales. Our model can make correct predictions for both cases with negligible errors (0.95%, 0.70% and 0.05%, respectively). Thus, all these four instances confirm our model's strong counting ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Effectiveness of Contrast Features</head><p>To confirm that the contrast features can boost the model's performance, we create a variant for our model, whose only difference from FusionCount is the feature fusion strategy. In this variant, weights are directly generated from encoded features xi+j instead of contrast features ci+j. We train this variant on ShanghaiTech B <ref type="bibr" target="#b5">[6]</ref> with the same setting. The MAE and RMSE of this variant are 7.6 and 12.9, respectively, which are larger than those of FusionCount (6.9 and 11.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>This section proves the effectiveness of the channel reduction module proposed in Section 3.3. Experiments are conducted on Shang-haiTech B <ref type="bibr" target="#b5">[6]</ref>. The results, which are shown in <ref type="table">Table 2</ref>, demonstrate that both the point-wise convolutional layer and the two chained dilated convolutional layers are indispensable. Theoretically, they work in a complementary way in reducing the number of channels and extract saliency. <ref type="table">Table 2</ref>. Effects of point-wise convolution and the number of dilated convolutional layers in the channel reduction module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point-wise Conv</head><p>Dilated Conv No. MSE RMSE 2 1 0 6.9 11.8 7. <ref type="bibr" target="#b5">6</ref> 13.0 8. <ref type="bibr" target="#b7">8</ref> 15.0 9.5 15.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we proposed a new crowd counting architecture, Fu-sionCount, which smartly utilises a large majority of features generated during the encoding process to handle perspective distortion. Unlike existing approaches, FusionCount avoids further extraction of multiscale features, thereby significantly reducing overall computation. To this end, We have also improved an existing multiscale fusion mechanism and devised a novel channel reduction block. Experiments on the ShanghaiTech databases demonstrated that Fusion-Count can outperform relevant state-of-the-art approaches of similar computational complexity. As part of our future work, we are working on accounting for any contextual information in the features fused at the decoding process. Such information can help to more effectively deal with scale changes, as the way the first-stage fusion of encoded features does.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The encoder of our proposed model FusionCount: Only the first 17 layers of the original VGG-16 are leveraged, and feature maps are collected starting from the third layer. Numbers in purple are features' receptive field sizes and those tuples (h ? w ? c) in gray indicate their sizes, assuming the input image has the size of 224 ? 224 ? 3. Features with the same spatial resolution are grouped together for the first-phase fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The feature fusion modules of FusionCount: In each group, weights are computed from contrast features. Then features from convolutional layers are averaged by using these weights and subsequently concatenated with the feature map from the pooling layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The decoding process of FusionCount: starting from f 4 , the proposed channel reduction module first decreases its number of channels. The result is then upsampled and fused with another firs-phase multiscale feature f 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative results of FusionCount: Four ground-truth density maps are visualised along with their estimations by our model FusionCount. The top two images are from ShanghaiTech A and the bottom two are from ShanghaiTech B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparision of our model FusionCount with state-of-theart models of similar sizes. The best and the second best results are indicated in bold and underlined typefaces, respectively.</figDesc><table><row><cell>Model</cell><cell>Mult-Adds</cell><cell cols="2">SH A MAE RMSE</cell><cell>MAE</cell><cell>SH B RMSE</cell></row><row><cell>CSRNet [7]</cell><cell>856.99 G</cell><cell>68.2</cell><cell>115.0</cell><cell>10.6</cell><cell>16.0</cell></row><row><cell>CAN [8]</cell><cell>908.05 G</cell><cell>62.3</cell><cell>100.0</cell><cell>7.8</cell><cell>12.2</cell></row><row><cell>BL [9]</cell><cell>853.70 G</cell><cell>62.8</cell><cell>101.8</cell><cell>7.7</cell><cell>12.7</cell></row><row><cell>DM-Count [23]</cell><cell>853.70 G</cell><cell>59.7</cell><cell>95.7</cell><cell>7.4</cell><cell>11.8</cell></row><row><cell>FusionCount (ours)</cell><cell>815.00 G</cell><cell>62.2</cell><cell>101.2</cell><cell>6.9</cell><cell>11.8</cell></row><row><cell cols="6">ages collected from the Internet. Three hundred of them constitute</cell></row><row><cell cols="6">the training set, and the rest comprise the test set. Scenes in this</cell></row><row><cell cols="6">dataset are highly congested, with an average count of about 501.</cell></row><row><cell cols="6">Also, since images have different resolutions (height and width val-</cell></row><row><cell cols="6">ues range from 182 to 1024), training models on this dataset can be</cell></row><row><cell cols="6">tricky. The size of ShanghaiTech B is larger (716 instances in total;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>shows</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Based on experiements (see Section 4.4), the performance of the overall model drops if contrast features are not utilised to compute weights.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The effectiveness of the combination of the dilated convolution block and the point-wise convolution is verified in Section 4.5.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic car counting method for unmanned aerial vehicle images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Moranduzzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farid</forename><surname>Melgani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1635" to="1647" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tasselnet: counting maize tassels in the wild via local counts regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plant methods</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vision-based crowd counting and social distancing monitoring using tiny-yolov4 and deepsort</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Immanuel</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valencia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elmer</forename><forename type="middle">P</forename><surname>Dadios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><forename type="middle">M</forename><surname>Fillone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Puno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renann</forename><forename type="middle">G</forename><surname>Baldovino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Kerwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Billones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Smart Cities Conference (ISC2)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian poisson regression for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="545" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="589" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1091" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Contextaware crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5094" to="5103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian loss for crowd count estimation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6141" to="6150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scale aggregation network for accurate and efficient crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="757" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Encoder-decoder based convolutional neural networks with multi-scale-aware modules for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pongpisit</forename><surname>Thanasutives</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Numao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boonserm</forename><surname>Kijsirikul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2382" to="2389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic multi-scale aggregation network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglun</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2008" to="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimation of number of people in crowded scenes using perspective transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaw-Yeh</forename><surname>Sheng-Fuu Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Systems, Man, and Cybernetics -Part A: Systems and Humans</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="645" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 19th International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Marked point processes for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weina</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2913" to="2920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature mining for localised crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference, BMVC 2012</title>
		<editor>Richard Bowden, John P. Collomosse, and Krystian Mikolajczyk</editor>
		<meeting><address><addrLine>Surrey, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cumulative attribute space for age and crowd density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2467" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generating highquality crowd density maps using contextual pyramid cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1879" to="1888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>O?oro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><forename type="middle">J</forename><surname>L?pez-Sastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Bastian Leibe, Jiri Matas; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="615" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Incorporating side information by adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debarun</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, I. Guyon, U. V</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distribution matching for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huidong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">Hoai</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1595" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Track Proceedings, Yoshua Bengio and Yann LeCun</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>3rd International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
