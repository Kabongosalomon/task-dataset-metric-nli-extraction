<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust fine-tuning of zero-shot models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Gontijo-Lopes</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
						</author>
						<title level="a" type="main">Robust fine-tuning of zero-shot models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large pre-trained models such as CLIP or ALIGN offer consistent accuracy across a range of data distributions when performing zero-shot inference (i.e., without fine-tuning on a specific dataset). Although existing fine-tuning methods substantially improve accuracy on a given target distribution, they often reduce robustness to distribution shifts. We address this tension by introducing a simple and effective method for improving robustness while fine-tuning: ensembling the weights of the zero-shot and fine-tuned models (WiSE-FT). Compared to standard fine-tuning, WiSE-FT provides large accuracy improvements under distribution shift, while preserving high accuracy on the target distribution. On ImageNet and five derived distribution shifts, WiSE-FT improves accuracy under distribution shift by 4 to 6 percentage points (pp) over prior work while increasing ImageNet accuracy by 1.6 pp. WiSE-FT achieves similarly large robustness gains (2 to 23 pp) on a diverse set of six further distribution shifts, and accuracy gains of 0.8 to 3.3 pp compared to standard fine-tuning on seven commonly used transfer learning datasets. These improvements come at no additional computational cost during fine-tuning or inference. * These authors contributed equally.</p><p>Accuracy on the reference distribution (e.g., ImageNet) Accuracy on the distribution shifts M od el s tr ai ne d on re fe re nc e di st rib ut io n tr ai n se t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Z e ro -s h o t C L IP m o d e ls</head><p>Effective robustness Fine-tuned CLIP Schematic: fine-tuning CLIP on the reference distribution leads to higher accuracy on the reference distribution but less robustness Accuracy on the reference distribution (e.g., ImageNet) Accuracy on the distribution shifts M od el s tr ai ne d on re fe re nc e di st rib ut io n tr ai n se t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Z e ro -s h o t C L IP m o d e ls</head><p>Weight-space ensemble for ? ? [0, 1]:</p><p>Schematic: our method, WiSE-FT leads to better accuracy on the distribution shifts without decreasing accuracy on the reference distribution Vary ingamixingac oe ff i c ie nta? 55 60 65 70 75 80 85 ImageNet (top-1, %) 30 35 40 45 50 55 60 65 70 75 Avg. accuracy on 5 distribution shifts Real data: our method 75 76 77 78 79 80 81 82 83 84 85 86 87 ImageNet (top-1, %) 67 68 69 70 71 72 73 74 75 76 77 Avg. accuracy on 5 distribution shifts +1.6 pp ImageNet +4.5 pp Distribution shifts Real data: our method (zoomed-in) CLIP zero-shot models Linear fit (CLIP zero-shot models) CLIP fine-tuned end-to-end CLIP fine-tuned with a linear classifier (prior work) Weight-space ensemble (end-to-end) Weight-space ensemble (linear classifier) Weight-space ensemble with ? = 0.5 Standard ImageNet models Linear fit (standard ImageNet models) y = x</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A foundational goal of machine learning is to develop models that work reliably across a broad range of data distributions. Over the past few years, researchers have proposed a variety of distribution shifts on which current algorithmic approaches to enhance robustness yield little to no gains <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b69">70]</ref>. While these negative results highlight the difficulty of learning robust models, large pre-trained models such as CLIP <ref type="bibr" target="#b81">[82]</ref>, ALIGN <ref type="bibr" target="#b44">[45]</ref> and BASIC <ref type="bibr" target="#b76">[77]</ref> have recently demonstrated unprecedented robustness to these challenging distribution shifts. The success of these models points towards pre-training on large, heterogeneous datasets as a promising direction for increasing robustness. However, an important caveat is that these robustness improvements are largest in the zero-shot setting, i.e., when the model performs inference without fine-tuning on a specific target distribution.</p><p>In a concrete application, a zero-shot model can be fine-tuned on extra application-specific data, which often yields large performance gains on the target distribution. However, in the experiments of Radford et al. <ref type="bibr" target="#b81">[82]</ref> and Pham et al. <ref type="bibr" target="#b76">[77]</ref>, fine-tuning comes at the cost of robustness: across several natural distribution shifts, the accuracy of their fine-tuned models is lower than that of the original zero-shot model. This leads to a natural question:</p><p>Can zero-shot models be fine-tuned without reducing accuracy under distribution shift?</p><p>As pre-trained models are becoming a cornerstone of machine learning, techniques for fine-tuning them on downstream applications are increasingly important. Indeed, the question of robustly fine-tuning pre-trained <ref type="figure" target="#fig_43">Figure 1</ref>: (Top left) Zero-shot CLIP models exhibit moderate accuracy on the reference distribution (x-axis, the target for fine-tuning) and high effective robustness (accuracy on the distribution shifts beyond the baseline models). In contrast, standard fine-tuning-either end-to-end or with a linear classifier (final layer)-attains higher accuracy on the reference distribution but less effective robustness. (Top right) Our method linearly interpolates between the zero-shot and fine-tuned models with a mixing coefficient ? ? [0, 1]. (Bottom) On five distribution shifts derived from ImageNet (ImageNetV2, ImageNet-R, ImageNet Sketch, ObjectNet, and ImageNet-A), WiSE-FT improves average accuracy relative to both the zero-shot and fine-tuned models while maintaining or improving accuracy on ImageNet. models has recently also been raised as an open problem by several authors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b76">77]</ref>. Andreassen et al. <ref type="bibr" target="#b2">[3]</ref> explored several fine-tuning approaches but found that none yielded models with improved robustness at high accuracy. Furthermore, Taori et al. <ref type="bibr" target="#b96">[97]</ref> demonstrated that no current algorithmic robustness interventions provide consistent gains across the distribution shifts where zero-shot models excel.</p><p>In this paper, we conduct an empirical investigation to understand and improve fine-tuning of zero-shot models from a distributional robustness perspective. We begin by measuring how different fine-tuning approaches (last-layer vs. end-to-end fine-tuning, hyperparameter changes, etc.) affect the accuracy under distribution shift of the resulting fine-tuned models. Our empirical analysis uncovers two key issues in the standard fine-tuning process. First, the robustness of fine-tuned models varies substantially under even small changes in hyperparameters, but the best hyperparameters cannot be inferred from accuracy on the target distribution alone. Second, more aggressive fine-tuning (e.g., using a larger learning rate) yields larger accuracy improvements on the target distribution, but can also reduce accuracy under distribution shift by a large amount.</p><p>Motivated by the above concerns, we propose a robust way of fine-tuning zero-shot models that addresses the aforementioned trade-off and achieves the best of both worlds: increased performance under distribution shift while maintaining or even improving accuracy on the target distribution relative to standard fine-tuning. In addition, our method simplifies the choice of hyperparameters in the fine-tuning process.</p><p>Our method ( <ref type="figure" target="#fig_43">Figure 1</ref>) has two steps: first, we fine-tune the zero-shot model on the target distribution. Second, we combine the original zero-shot and fine-tuned models by linearly interpolating between their weights, which we refer to as weight-space ensembling. Interpolating model parameters is a classical idea in convex optimization dating back decades (e.g., see <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b78">79]</ref>). Here, we empirically study model interpolation for non-convex models from the perspective of distributional robustness. Interestingly, linear interpolation in weight-space still succeeds despite the non-linearity in the activation functions of the neural networks.</p><p>Weight-space ensembles for fine-tuning (WiSE-FT) substantially improve accuracy under distribution shift compared to prior work while maintaining high performance on the target distribution. Concretely, on ImageNet <ref type="bibr" target="#b16">[17]</ref> and five of the natural distribution shifts studied by Radford et al. <ref type="bibr" target="#b81">[82]</ref>, WiSE-FT applied to standard end-to-end fine-tuning improves accuracy under distribution shift by 4 to 6 percentage points (pp) over prior work while maintaining or improving the ImageNet accuracy of the fine-tuned CLIP model. Relative to the zero-shot model, WiSE-FT improves accuracy under distribution shift by 1 to 9 pp. Moreover, WiSE-FT improves over a range of alternative approaches such as regularization and evaluating at various points throughout fine-tuning. These robustness gains come at no additional computational cost during fine-tuning or inference.</p><p>While our investigation centers around CLIP, we observe similar trends for other zero-shot models including ALIGN <ref type="bibr" target="#b44">[45]</ref>, BASIC <ref type="bibr" target="#b76">[77]</ref>, and a ViT model pre-trained on JFT <ref type="bibr" target="#b20">[21]</ref>. For instance, WiSE-FT improves the ImageNet accuracy of a fine-tuned BASIC-L model by 0.4 pp, while improving average accuracy under distribution shift by 2 to 11 pp.</p><p>To understand the robustness gains of WiSE-FT, we first study WiSE-FT when fine-tuning a linear classifier (last layer) as it is more amenable to analysis. In this linear case, our procedure is equivalent to ensembling the outputs of two models, and experiments point towards the complementarity of model predictions as a key property. For end-to-end fine-tuning, we connect our observations to earlier work on the phenomenology of deep learning. Neyshabur et al. <ref type="bibr" target="#b72">[73]</ref> found that end-to-end fine-tuning the same model twice yielded two different solutions that were connected via a linear path in weight-space along which error remains low, known as linear mode connectivity <ref type="bibr" target="#b24">[25]</ref>. Our observations suggest a similar phenomenon along the path generated by WiSE-FT, but the exact shape of the loss landscape and connection between error on the target and shifted distributions are still open problems.</p><p>In addition to the aforementioned ImageNet distribution shifts, WiSE-FT consistently improves robustness on a diverse set of six additional distribution shifts including: (i) geographic shifts in satellite imagery and wildlife recognition (WILDS-FMoW, WILDS-iWildCam) <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b5">6]</ref>, (ii) reproductions of the popular image classification dataset CIFAR-10 with a distribution shift (CIFAR-10.1 and CIFAR-10.2) <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b61">62]</ref>, and (iii) datasets with distribution shift induced by temporal perturbations in videos (ImageNet-Vid-Robust and YTBB-Robust) <ref type="bibr" target="#b87">[88]</ref>. Beyond the robustness perspective, WiSE-FT also improves accuracy compared to standard fine-tuning, reducing the relative error rate by 4-49% on a range of seven datasets: ImageNet, CIFAR-10, CIFAR-100 <ref type="bibr" target="#b53">[54]</ref>, Describable Textures <ref type="bibr" target="#b13">[14]</ref>, Food-101 <ref type="bibr" target="#b9">[10]</ref>, SUN397 <ref type="bibr" target="#b102">[103]</ref>, and Stanford Cars <ref type="bibr" target="#b52">[53]</ref>. Even when fine-tuning data is scarce, reflecting many application scenarios, we find that WiSE-FT improves performance.</p><p>Overall, WiSE-FT is simple, universally applicable in the problems we studied, and can be implemented in a few lines of code. Hence we encourage its adoption for fine-tuning zero-shot models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and experimental setup</head><p>Our experiments compare the performance of zero-shot models, corresponding fine-tuned models, and models produced by WiSE-FT. To measure robustness, we contrast model accuracy on two related but different  <ref type="figure">Figure 2</ref>: Samples of the class lemon, from the reference distribution ImageNet <ref type="bibr" target="#b16">[17]</ref> and the derived distribution shifts considered in our main experiments: ImageNet-V2 <ref type="bibr" target="#b82">[83]</ref>, ImageNet-R <ref type="bibr" target="#b36">[37]</ref>, ImageNet Sketch <ref type="bibr" target="#b99">[100]</ref>, ObjectNet <ref type="bibr" target="#b3">[4]</ref>, and ImageNet-A <ref type="bibr" target="#b37">[38]</ref>.</p><p>distributions, a reference distribution D ref which is the target for fine-tuning, and shifted distribution D shift . <ref type="bibr" target="#b0">1</ref> We assume both distributions have test sets for evaluation, and D ref has an associated training set S tr ref which is typically used for training or fine-tuning. The goal for a model is to achieve both high accuracy and consistent performance on the two distributions D ref and D shift . This is a natural goal as humans often achieve similar accuracy across the distribution shifts in our study <ref type="bibr" target="#b88">[89]</ref>.</p><p>For  <ref type="bibr" target="#b37">[38]</ref>, a test set of natural images misclassified by a ResNet-50 <ref type="bibr" target="#b33">[34]</ref> for 200 ImageNet classes. <ref type="figure">Figure 2</ref> illustrates the five distribution shifts.</p><p>Effective robustness and scatter plots. To compare the robustness of models with different accuracies on the reference distribution, we follow the effective robustness framework introduced by Taori et al. <ref type="bibr" target="#b96">[97]</ref>. Effective robustness quantifies robustness as accuracy beyond a baseline trained only on the reference distribution.</p><p>A useful tool for studying (effective) robustness are scatter plots that illustrate model performance under distribution shift <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b96">97]</ref>. These scatter plots display accuracy on the reference distribution on the x-axis and accuracy under distribution shift on the y-axis, i.e., a model f is shown as a point (Acc ref (f ), Acc shift (f )). <ref type="figure" target="#fig_43">Figure 1</ref> exemplifies these scatter plots with both schematics and real data. For the distribution shifts we study, accuracy on the reference distribution is a reliable predictor of accuracy under distribution shift <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b69">70]</ref>. In other words, there exists a function ? : </p><formula xml:id="formula_0">(f ) = Acc shift (f ) ? ?(Acc ref (f )).</formula><p>In the corresponding scatter plots, effective robustness is vertical movement above expected accuracy under distribution shift <ref type="figure" target="#fig_43">(Figure 1, top)</ref>. Effective robustness thereby disentangles accuracy changes on the reference distribution from the effect of robustness interventions. When we say that a model is robust to distribution shift, we mean that effective robustness is positive. Taori et al. <ref type="bibr" target="#b96">[97]</ref> observed that no algorithmic robustness intervention consistently achieves substantial effective robustness across the distribution shifts in <ref type="figure">Figure 2</ref>-the first method to do so was zero-shot CLIP. Empirically, when applying logit (or probit) axis scaling, models trained on the reference distribution approximately lie on a linear trend <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b69">70]</ref>. As in Taori et al. <ref type="bibr" target="#b96">[97]</ref>, we apply logit axis scaling and show 95% Clopper-Pearson confidence intervals for the accuracies of select points.</p><p>Zero-shot models and CLIP. We primarily explore CLIP models <ref type="bibr" target="#b81">[82]</ref>, although we also investigate other zero-shot models including ALIGN <ref type="bibr" target="#b44">[45]</ref>, BASIC <ref type="bibr" target="#b76">[77]</ref> and a ViT model pre-trained on JFT <ref type="bibr" target="#b20">[21]</ref>. Zero-shot models exhibit effective robustness and lie on a qualitatively different linear trend ( <ref type="figure" target="#fig_43">Figure 1</ref>). CLIP-like models are pre-trained using image-caption pairs from the web. Given a set of image-caption pairs {(x 1 , s 1 )..., (x B , s B )}, CLIP-like models train an image-encoder g and text-encoder h such that the similarity g(x i ), h(s i ) is maximized relative to unaligned pairs. CLIP-like models perform zero-shot k-way classification given an image x and class names C = {c 1 , ..., c k } by matching x with potential captions. For instance, using caption s i = "a photo of a {c i }" for each class i, the zero-shot model predicts the class via arg max j g(x), h(s j ) . <ref type="bibr" target="#b1">2</ref> Equivalently, one can construct W zero-shot ? R d?k with columns h(s j ) and compute outputs f (x) = g(x) W zero-shot . Unless explicitly mentioned, our experiments use the CLIP model ViT-L/14@336px, although all CLIP models are displayed in our scatter plots (additional details provided in Appendix D.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Weight-space ensembles for fine-tuning</head><p>This section describes and motivates our proposed method, WiSE-FT, which consists of two simple steps. First, we fine-tune the zero-shot model on application-specific data. Second, we combine the original zero-shot and fine-tuned models by linearly interpolating between their weights, also referred to as weight-space ensembling. WiSE-FT can be implemented in a few lines of PyTorch, and we provide example code in Appendix A.</p><p>The zero-shot model excels under distribution shift while standard fine-tuning achieves high accuracy on the reference distribution. Our motivation is to combine these two models into one that achieves the best of both worlds. Weight-space ensembles are a natural choice as they ensemble without extra computational cost. Moreover, previous work has suggested that interpolation in weight space may improve performance when models share part of their optimization trajectory <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b72">73]</ref>.</p><p>Step 1: Standard fine-tuning. As in Section 2, we let S tr ref denote the dataset used for fine-tuning and g denote the image encoder used by CLIP. We are now explicit in writing g(x, V enc ) where x is an input image and V enc are the parameters of the encoder g. Standard fine-tuning considers the model f (x, ?) = g (x, V enc ) W classifier where W classifier ? R d?k is the classification head and ? = [V enc , W classifier ] 2 For improved accuracy, the embedding of a few candidate captions are averaged, e.g., s (1) i ="a photo of a {c i }" and s <ref type="bibr" target="#b1">(2)</ref> i ="a picture of a {c i }" (referred to as prompt ensembling <ref type="bibr" target="#b81">[82]</ref>).</p><p>are the parameters of f . We then solve arg min ?</p><formula xml:id="formula_1">(xi,yi)?S tr ref (f (x i , ?), y i ) + ?R(?)</formula><p>where is the crossentropy loss and R is a regularization term (e.g., weight decay). We consider the two most common variants of fine-tuning: end-to-end, where all values of ? are modified, and fine-tuning only a linear classifier, where V enc is fixed at the value learned during pre-training. Appendices D.2 and D.3 provide additional details.</p><p>Step 2: Weight-space ensembling. For a mixing coefficient ? ? [0, 1], we consider the weight-space ensemble between the zero-shot model with parameters ? 0 and the model obtained via standard fine-tuning with parameters ? 1 . The predictions of the weight-space ensemble wse are given by</p><formula xml:id="formula_2">wse(x, ?) = f (x, (1 ? ?) ? ? 0 + ? ? ? 1 ) ,<label>(1)</label></formula><p>i.e., we use the element-wise weighted average of the zero-shot and fined-tuned parameters. When fine-tuning only the linear classifier, weight-space ensembling is equivalent to the traditional output-space ensemble <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26]</ref> </p><formula xml:id="formula_3">(1 ? ?) ? f (x, ? 0 ) + ? ? f (x, ? 1 ) since Equation 1 decomposes as (1 ? ?) ? g(x, V enc ) W zero-shot + ? ? g(x, V enc ) W classifier .</formula><p>As neural networks are non-linear with respect to their parameters, ensembling all layers-as we do when end-to-end fine-tuning-typically fails, achieving no better accuracy than a randomly initialized neural network <ref type="bibr" target="#b24">[25]</ref>. However, as similarly observed by previous work where part of the optimization trajectory is shared <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b72">73]</ref>, we find that the zero-shot and fine-tuned models are connected by a linear path in weight-space along which accuracy remains high (explored further in Section 5.2).</p><p>Remarkably, as we show in Section 4, WiSE-FT improves accuracy under distribution shift while maintaining high performance on the reference distribution relative to fine-tuned models. These improvements come without any additional computational cost as a single set of weights is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>This section presents our key experimental findings. First, we show that WiSE-FT boosts the accuracy of a fine-tuned CLIP model on five ImageNet distribution shifts studied by Radford et al. <ref type="bibr" target="#b81">[82]</ref>, while maintaining or improving ImageNet accuracy. Next, we present additional experiments, including more distribution shifts, the effect of hyperparameters, accuracy improvements on the reference distribution, and experiments in the low-data regime. Finally, we demonstrate that our findings are more broadly applicable by exploring WiSE-FT for BASIC <ref type="bibr" target="#b76">[77]</ref>, ALIGN <ref type="bibr" target="#b44">[45]</ref>, and a ViT-H/14 <ref type="bibr" target="#b20">[21]</ref> model pre-trained on JFT-300M <ref type="bibr" target="#b92">[93]</ref>.</p><p>Main results: ImageNet and associated distribution shifts. As illustrated in <ref type="figure" target="#fig_43">Figure 1</ref>, when the mixing coefficient ? varies from 0 to 1, wse(?, ?) is able to simultaneously improve accuracy on both the reference and shifted distributions. A breakdown for each dataset is shown in Appendix C.1. <ref type="table">Table 1</ref> presents our main results on ImageNet and five derived distribution shifts. WiSE-FT (end-to-end, ?=0.5) outperforms numerous strong models in both average accuracy under distribution shift and the average accuracy on the reference and shifted distributions. While future work may lead to more sophisticated strategies for choosing the mixing coefficient ?, ?=0.5 yields close to optimal performance across a range of experiments. Hence, we recommend ?=0.5 when no domain knowledge is available. Appendix B further explores the effect of ?. Moreover, results for twelve additional backbones are shown in Appendix C.</p><p>Robustness on additional distribution shifts. Beyond the five distribution shifts derived from ImageNet, WiSE-FT consistently improves robustness on a diverse set of further distributions shifts including geographic shifts in satellite imagery and wildlife recognition (WILDS-FMoW <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b12">13]</ref>, WILDS-iWildCam <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b5">6]</ref>  <ref type="table">Table 1</ref>: Accuracy of various methods on ImageNet and derived distribution shifts for CLIP ViT-L/14@336px <ref type="bibr" target="#b81">[82]</ref>. E2E: end-to-end; LC: linear classifier. Avg shifts displays the mean performance among the five distribution shifts, while Avg reference, shifts shows the average of ImageNet (reference) and Avg shifts. For optimal ?, we choose the single mixing coefficient that maximizes the column. Results for additional models are provided in Appendix C.7.</p><p>30% accuracy on the WILDS distribution shifts, and WiSE-FT provides improvements regardless. Appendix C.2 ( <ref type="figure" target="#fig_15">Figure 9</ref> and <ref type="table" target="#tab_12">Table 6</ref>) includes more detailed results.</p><p>Hyperparameter variation and alternatives. As illustrated by <ref type="figure">Figure 3</ref>, moderate changes in standard hyperparameters such as the learning rate or the number of epochs can substantially affect performance under distribution shift. Moreover, these performance differences cannot be detected reliably from model performance on reference data alone. For instance, while training for 10 epochs with learning rate 3 ? 10 ?5 and 3 ? 10 ?6 lead to a small accuracy difference on ImageNet (0.3 pp), accuracy under distribution shift varies by as much as 8 pp.</p><p>Furthermore, tuning hyperparameters on ImageNet data can also reduce robustness. For instance, while moving from small to moderate learning rates (10 ?7 to 3 ? 10 ?5 ) improves performance on ImageNet by 5 pp, it also deteriorates accuracy under distribution shift by 8 pp.</p><p>WiSE-FT addresses this brittleness of hyperparameter tuning: even when using a learning rate 3 ? 10 ?5 where standard fine-tuning leads to low robustness, applying WiSE-FT removes the trade-off between accuracy on the reference and shifted distributions. The models which can be achieved by varying ? are as good or better than those achievable by other hyperparameter configurations. Then, instead of searching over a wide range of hyperparameters, only ? needs to be considered. Moreover, evaluating different values of ? does not require training new models.</p><p>There is no hyperparameter in <ref type="figure">Figure 3</ref> which can be varied to match or exceed the optimal curve produced by WiSE-FT. In our experiments, this frontier is reached only through methods that average model weights, either using WiSE-FT or with a more sophisticated averaging scheme: keeping an exponential moving average of all model iterates (EMA, <ref type="bibr" target="#b94">[95]</ref>). Comparisons with EMA are detailed in Appendix C.3.2.</p><p>Additional comparisons are also presented in Appendix C.3, including distillation, additional regularization, and CoOp <ref type="bibr" target="#b111">[112]</ref>. Finally, Appendix C.4 recreates <ref type="figure">Figure 3</ref> with stronger data augmentation and finds similar trends.</p><p>Accuracy gains on reference distributions. Beyond robustness to distribution shift, <ref type="table" target="#tab_5">Table 2</ref> demonstrates that WiSE-FT also improves accuracy after fine-tuning on seven datasets. When fine-tuning end-to-end * Although this table considers ImageNet class names, ObjectNet provides alternative class names which can improve the performance of zero-shot CLIP by 2.3 percentage points (Appendix D.4). Avg. accuracy on 5 distribution shifts 2 epochs <ref type="bibr">4 10</ref> Hyperparameter: Fix learning rate, vary number of epochs LR = 1 ? 10 ?7 LR = 1 ? 10 ?6 LR = 3 ? 10 ?6 LR = 1 ? 10 ?5 LR = 2 ? 10 ?5 LR = 3 ? 10 ?5 <ref type="bibr" target="#b64">65</ref> 70 <ref type="bibr">75 80</ref> ImageNet  Beyond CLIP. <ref type="figure" target="#fig_4">Figure 4</ref> illustrates that WiSE-FT is generally applicable to zero-shot models beyond CLIP, and beyond models pre-trained contrastively with image-text pairs. First, we interpolate between the weights of the zero-shot and fine-tuned BASIC-L model <ref type="bibr" target="#b76">[77]</ref>, finding that ?=0.5 improves average accuracy on five distribution shifts derived from ImageNet by over 7 pp while improving ImageNet accuracy by 0.4 pp relative to the fine-tuned BASIC-L model (a per-dataset breakdown is provided in <ref type="figure" target="#fig_4">Figure 24</ref> and <ref type="table" target="#tab_5">Table 12</ref> of the Appendix). As in Pham et al. <ref type="bibr" target="#b76">[77]</ref>, the model is fine-tuned using a contrastive loss and half of the ImageNet training data. WiSE-FT provides improvements on both reference and shifted distributions, despite these experimental differences.</p><p>Next, we consider the application of WiSE-FT to a ViT-H/14 model <ref type="bibr" target="#b20">[21]</ref> pre-trained on JFT-300M <ref type="bibr" target="#b92">[93]</ref>, where the zero-shot classifier is constructed by manually identifying a class correspondence (details provided in Section C.7.2). WiSE-FT improves performance under distribution shift over both the zero-shot and fine-tuned models. When ?=0.8, WiSE-FT outperforms the fine-tuned model by 2.2 pp on distribution shifts, while maintaining ImageNet performance within 0.2 pp of the fine-tuned model. This result demonstrates that WiSE-FT can be successfully applied even to models which do not use contrastive image-text pre-training.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avg. accuracy on 5 distribution shifts</head><p>ViT-H/14 (JFT) <ref type="bibr" target="#b74">75</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="80">85</head><p>ImageNet (top-1, %)  Finally, we apply WiSE-FT to the ALIGN model of Jia et al. <ref type="bibr" target="#b44">[45]</ref>, which is similar to CLIP but is pre-trained with a different dataset, finding similar trends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>This section further analyzes the empirical phenomena we have observed so far. We begin with the case where only the final linear layer is fine-tuned and predictions from the weight-space ensemble can be factored into the outputs of the zero-shot and fine-tuned model. Next, we connect our observations regarding end-to-end fine-tuning with earlier work on the phenomenology of deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Zero-shot and fine-tuned models are complementary</head><p>In this section, we find that the zero-shot and fine-tuned models have diverse predictions, both on reference and shifted distributions. Moreover, while the fine-tuned models are more confident on the reference distribution, the reverse is true under distribution shift.</p><p>Zero-shot and fine-tuned models are diverse. In certain cases, ensemble accuracy is correlated with diversity among the constituents <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b29">30]</ref>. If two models make coincident mistakes, so will their ensemble, and no benefit will be gained from combining them. Here, we explore two measures of diversity: prediction diversity, which measures the fraction of examples for which two classifiers disagree but one is correct; and Centered Kernel Alignment Complement, the complement of CKA <ref type="bibr" target="#b50">[51]</ref>. Additional diversity measures and details are provided in Appendix E. In <ref type="figure" target="#fig_5">Figure 5</ref> (left), we show that the zero-shot and fine-tuned models are diverse both on the reference and shifted distributions, despite sharing the same backbone. As a point of comparison, we include avg. diversity measures between two linear classifiers fine-tuned with random splits on half of ImageNet, 3 denoted in orange in <ref type="figure" target="#fig_5">Figure 5</ref>. Models are more confident where they excel. In order for the ensemble model to be effective, it should leverage each model's expertise based on which distribution the data is from. Here, we empirically show that this occurs on a number of datasets we consider. First, we examine the cases where the models being ensembled disagree. We say the zero-shot model overrides the fine-tuned model if their predictions disagree and the zero-shot prediction matches that of the weight-space ensemble. Similarly, if models disagree and the linear classifier prediction matches the ensemble, we say the zero-shot is overridden. <ref type="figure" target="#fig_5">Figure 5</ref> (middle) shows the fraction of samples where the zero-shot model overrides and is overridden by the fine-tuned linear classifier for ?=0.5. Other than ImageNetV2, which was collected to closely reproduce ImageNet, the zero-shot model overrides the linear classifier more than it is overridden on the distribution shifts.</p><p>Additionally, we are interested in measuring model confidence. Recall that we are ensembling quantities before a softmax is applied, so we avoid criteria that use probability vectors, e.g., Guo et al. <ref type="bibr" target="#b32">[33]</ref>. Instead, we consider the margin ? between the largest and second largest output of each classifier. <ref type="figure" target="#fig_5">Figure 5</ref> (right) shows that the zero-shot model is more confident in its predictions under distribution shift, while the reverse is true on the reference distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">An error landscape perspective</head><p>We now turn to empirical phenomena we observe when weight-space ensembling all layers in the network. Specifically, this section formalizes our observations and details related phenomena. Recall that the weightspace ensemble of ? 0 and ? 1 is given by</p><formula xml:id="formula_4">f (x, (1 ? ?) ? ? 0 + ? ? ? 1 ) (Equation 1).</formula><p>For a distribution D and model f , let Acc D,f (?) denote the expected accuracy of f evaluated with parameters ? on distribution D.</p><p>Observation 1: As illustrated in <ref type="figure">Figure 6</ref>, on ImageNet and the five associated distribution shifts we consider</p><formula xml:id="formula_5">Acc D,f ((1 ? ?) ? ? 0 + ? ? ? 1 ) ? (1 ? ?) ? Acc D,f (? 0 ) + ? ? Acc D,f (? 1 )<label>(2)</label></formula><p>for all ? ? [0, 1].</p><p>Note that equation 2 uses the baseline of linearly interpolating between the accuracies of the two endpoints, which is always achievable by using weights ? 1 with probability ? and using model ? 0 otherwise. In the case where the accuracy of both endpoints are similar, Equation 2 is equivalent to the definition of Linear Mode Connectivity of Frankle et al. <ref type="bibr" target="#b24">[25]</ref>.</p><p>To assist in contextualizing Observation 1, we review related phenomena. Neural networks are nonlinear, hence weight-space ensembles only achieve good performance in exceptional cases-interpolating the weights of two networks trained from a random initialization results in no better accuracy than a random classifier ImageNet-A <ref type="figure">Figure 6</ref>: On ImageNet and the main distribution shifts we consider, linearly interpolating between the weights of ? 0 and ? 1 exceeds the baseline of linearly interpolating the accuracies of the two models for all ? (Observation 1). Moreover, there exists an ? for which WiSE-FT outperforms both the zero-shot and fine-tuned models (Observation 2). <ref type="bibr" target="#b24">[25]</ref>. Linear mode connectivity has been observed by Frankle et al. <ref type="bibr" target="#b24">[25]</ref>; Izmailov et al. <ref type="bibr" target="#b42">[43]</ref> when part of the training trajectory is shared, and by Neyshabur et al. <ref type="bibr" target="#b72">[73]</ref> when two models are fine-tuned with a shared initialization. In particular, the observations of Neyshabur et al. <ref type="bibr" target="#b72">[73]</ref> may elucidate why weight-space ensembles attain high accuracy in the setting we consider, as they suggest that fine-tuning remains in a region where solutions are connected by a linear path along which error remains low. Instead of considering the weight-space ensemble of two fine-tuned models, we consider the weight-space ensemble of the pre-trained and fine-tuned models. This is only possible for a pre-trained model capable of zero-shot inference such as CLIP.</p><formula xml:id="formula_6">Acc ((1 ? ?) ? ? 0 + ? ? ? 1 ) [Linearly interpolating the weights] (1 ? ?) ? Acc (? 0 ) + ? ? Acc (? 1 ) [Linear endpoint accuracy interpolation baseline] max {Acc (? 0 ) , Acc (? 1 )}</formula><p>Observation 2: As illustrated by <ref type="figure">Figure 6</ref>, on ImageNet and the five associated distribution shifts we consider, weight-space ensembling (end-to-end) may outperform both the zero-shot and fine-tuned models, i.e., there exists an ? for which</p><formula xml:id="formula_7">Acc D,f ((1 ? ?) ? ? 0 + ? ? ? 1 ) ? max {Acc D,f (? 0 ) , Acc D,f (? 1 )}.</formula><p>We are not the first to observe that when interpolating between models, the accuracy of models along the path may exceed that of either endpoint <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b101">102]</ref>. Neyshabur et al. <ref type="bibr" target="#b72">[73]</ref> conjecture that interpolation could produce solutions closer to the true center of a basin. In contrast to Neyshabur et al. <ref type="bibr" target="#b72">[73]</ref>, we interpolate between models which observe different data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Robustness. Understanding how models perform under distribution shift remains an important goal, as real world models may encounter data from new environments <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b97">98]</ref>. Previous work has studied model behavior under synthetic <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b1">2]</ref> and natural distribution shift <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">38]</ref>. Interventions used for synthetic shifts do not typically provide robustness to many natural distribution shifts <ref type="bibr" target="#b96">[97]</ref>. In contrast, accuracy on the reference distribution is often a reliable predictor for accuracy under distribution shift <ref type="bibr" target="#b105">[106,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b96">97,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b69">70]</ref>. On the other hand, D'Amour et al. <ref type="bibr" target="#b15">[16]</ref> show that accuracy under certain distribution shifts cannot be reliably inferred from accuracy on the reference distribution. We observe a similar phenomenon when fine-tuning with different hyperparameters (Section 4, <ref type="figure">Figure 3</ref>).</p><p>Pre-training and transfer learning. Pre-training on large amounts of data is a powerful technique for building high-performing machine learning systems <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b11">12]</ref>. One increasingly popular class of vision models are those pre-trained with auxiliary language supervision, which can be used for zero-shot inference <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b108">109]</ref>. When pre-trained models are adapted to a specific distribution through standard fine-tuning, effective robustness deteriorates at convergence <ref type="bibr" target="#b2">[3]</ref>. In natural language processing, previous work proposed stable fine-tuning methods that incur computational overhead <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b112">113]</ref>, alleviating problems such as representational collapse <ref type="bibr" target="#b0">[1]</ref>. More generally, a variety of methods have attempted to mitigate catastrophic forgetting <ref type="bibr" target="#b66">[67]</ref>. Kirkpatrick et al. <ref type="bibr" target="#b47">[48]</ref>; Zenke et al. <ref type="bibr" target="#b107">[108]</ref> explored weighted quadratic regularization for sequential learning. Xuhong et al. <ref type="bibr" target="#b104">[105]</ref> showed that, for fine-tuning, the simple quadratic regularization explored in Section 4 performs best, while Lubana et al. <ref type="bibr" target="#b62">[63]</ref> explored the connection between quadratic regularization and interpolation. Andreassen et al. <ref type="bibr" target="#b2">[3]</ref> found that many approaches from continual learning do not provide robustness to multiple natural distribution shifts. Finally, Li et al. <ref type="bibr" target="#b58">[59]</ref> investigate the effect of fine-tuning hyperparameters on performance.</p><p>Traditional (output-space) ensembles. Traditional ensemble methods, which we refer to as output-space ensembles, combine the predictions (outputs) of many classifiers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b25">26]</ref>. Typically, output-space ensembles outperform individual classifiers and provide uncertainty estimates under distribution shift that are more callibrated than baselines <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b91">92]</ref>. In contrast to these works, we consider the ensemble of two models which have observed different data. Output-space ensembles require more computational resources as they require a separate pass through each model. Compared to an ensemble of 15 models trained on the same dataset, Mustafa et al. <ref type="bibr" target="#b71">[72]</ref> find an improvement of 0.8-1.6 pp under distribution shift (on ImageNetV2, ImageNet-R, ObjectNet, and ImageNet-A) by ensembling a similar number of models pre-trained on different datasets. In contrast, we see an improvement of 2-15 pp from ensembling two models. Moreover, as we ensemble in weight-space, no extra compute is required compared to a single model.</p><p>Weight-space ensembles. Weight-space ensembles linearly interpolate between the weights of different models <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b94">95]</ref>. For example, Izmailov et al. <ref type="bibr" target="#b42">[43]</ref> average checkpoints saved throughout training for improved performance. Indeed, averaging the weights along the training trajectory is a central method in optimization <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b73">74]</ref>. For instance, Zhang et al. <ref type="bibr" target="#b109">[110]</ref> propose optimizing with a set of fast and slow weights, where every k steps, these two sets of weights are averaged and a new trajectory begins. Here, we revisit these techniques from a distributional robustness perspective and consider the weight-space ensemble of models which have observed different data.</p><p>Concurrent and subsequent work. Topics including robust fine-tuning, ensembles for improved robustness, and interpolating the weights of fine-tuned models are studied in concurrent and subsequent work. Kumar et al. <ref type="bibr" target="#b54">[55]</ref> observe that fine-tuning end-to-end often results in higher accuracy on the reference distribution but lower accuracy under distribution shift, compared to linear classifier fine-tuning. To address this, Kumar et al. <ref type="bibr" target="#b54">[55]</ref> first fine-tune a linear classifier and use this as the initialization for end-to-end fine-tuning. We consider fine-tuning zero-shot models, and so we begin with a classifier (i.e., the zero-shot classifier) which we are using as the initialization for end-to-end fine-tuning. In a separate work, Kumar et al. <ref type="bibr" target="#b55">[56]</ref> find that calibrated output-space ensembles can be used to mitigate accuracy trade-offs. In Figures 10 and 25 of the Appendix, we observe that it is possible to mitigate accuracy trade-offs with output-space ensembles even without calibration.</p><p>Hewitt et al. <ref type="bibr" target="#b39">[40]</ref> explore the application of output-space ensembles and distillation to mitigate accuracy trade-offs which arise in fine-tuning models for natural language generation. Hewitt et al. <ref type="bibr" target="#b39">[40]</ref> observe that output-space ensembles mainly outperform distillation, which we observe for a separate domain in <ref type="figure" target="#fig_43">Figure 13</ref> of the Appendix. Gontijo-Lopes et al. <ref type="bibr" target="#b30">[31]</ref> explore output-space ensembles of models across hyper-parameters, architectures, frameworks, and datasets. They find that specializing in subdomains of data leads to high ensemble performance. Finally, Matena and Raffel <ref type="bibr" target="#b65">[66]</ref> introduce a method of combining models in weight-space that goes beyond linear interpolation with a single mixing-coefficient as employed in WiSE-FT. Specifically, Matena and Raffel <ref type="bibr" target="#b65">[66]</ref> employ Fisher information as a measure of per-parameter importance.</p><p>While their experiments do not examine accuracy under distribution shift, their goal of combining differing expertise into one shared model is well aligned with ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations, impact, and conclusion</head><p>Limitations. While we expect our findings to be more broadly applicable to other domains such as natural language processing, our investigation here is limited to image classification. Exploring fine-tuning for object detection and natural language processing are interesting directions for future work. Moreover, although the interpolation parameter setting ?=0.5 provides good overall performance, we leave the question of finding the optimal ? for specific target distributions to future work.</p><p>Impact. Radford et al. <ref type="bibr" target="#b81">[82]</ref> and Brown et al. <ref type="bibr" target="#b11">[12]</ref> extensively discuss the broader impact of large zero-shot models and identify potential causes of harm including model biases and potential malicious uses such as surveillance systems. WiSE-FT is a fine-tuning method that builds on such models, and thus may perpetuate their negative impact.</p><p>Conclusion. WiSE-FT can substantially improve performance under distribution shift with minimal or no loss in accuracy on the target distribution compared to standard fine-tuning. We view WiSE-FT as a first step towards more sophisticated fine-tuning schemes and anticipate that future work will continue to leverage the robustness of zero-shot models for building more reliable neural networks.  <ref type="table">Table 3</ref> compares the performance of WiSE-FT using a fixed mixing coefficient ?=0.5 with the fixed optimal mixing coefficient. On ImageNet and the five derived distribution shifts, the average performance of the optimal ? is 0 to 0.4 percentage points better than that of ?=0.5. Due to its simplicity and effectiveness, we recommend using ?=0.5 when no domain knowledge is available. Finding the optimal value of the mixing coefficient for any distribution is an interesting question for future work. Unlike other hyperparameters, no re-training is required to test different ?, so tuning is relatively cheap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Pseudocode for WiSE-FT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Mixing coefficient</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional experiments</head><p>This section supplements the results of Section 4. First, in Section C.1 we provide a breakdown of <ref type="figure" target="#fig_43">Figure 1</ref> for each distribution shift. Next, in Section C.2 we provide effective robustness scatter plots for six additional distribution shifts, finding WiSE-FT to provide consistent improvements under distribution shift without any loss in performance on the reference distribution. Section C.3 compares WiSE-FT with additional alternatives including distillation and CoOp <ref type="bibr" target="#b111">[112]</ref>. Beyond robustness, Section C.5 demonstrates that WiSE-FT can provide accuracy improvements on reference data, with a focus on the low-data regime. Section C.6 showcases that the accuracy improvements under distribution shift are not isolated to large models, finding similar trends across scales of pre-training computes. Section C.7 explores the application of WiSE-FT for additional models such as ALIGN <ref type="bibr" target="#b44">[45]</ref>, a ViT-H/14 model pre-trained on JFT <ref type="bibr" target="#b20">[21]</ref> and BASIC <ref type="bibr" target="#b76">[77]</ref>. Finally, Section C.8 ensembles zero-shot CLIP with an independently trained classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Breakdown of CLIP experiments on ImageNet</head><p>In contrast to <ref type="figure" target="#fig_4">Figures 1 and 4</ref>  <ref type="table">Table 3</ref>: Difference in performance (percentage points) between WiSE-FT using the optimal mixing coefficient and a fixed value of ?=0.5 for CLIP ViT-B/16 and ViT-L/14@336. For each cell in the table, the optimal mixing coefficient ? is chosen individually such that the corresponding metric is maximized. Results for all mixing coefficients are available in <ref type="table" target="#tab_9">Tables 4 and 5</ref>. Avg shifts displays the mean performance among the five distribution shifts, while Avg reference, shifts shows the average of ImageNet (reference) and Avg shifts.</p><p>To assist in contextualizing the results, the scatter plots we display also show a wide range of machine learning models from a comprehensive testbed of evaluations <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b69">70]</ref>, including: models trained on S tr D (standard training); models trained on additional data and fine-tuned using S tr D (trained with more data); and models trained using various existing robustness interventions, e.g. special data augmentation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36]</ref> or adversarially robust models <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b86">87]</ref>.</p><p>Additionally, <ref type="table" target="#tab_9">Tables 4 and 5</ref>             , CIFAR-10.1 <ref type="bibr" target="#b82">[83]</ref>, CIFAR-10.2 <ref type="bibr" target="#b61">[62]</ref>, WILDS-FMoW <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b12">13]</ref>, and WILDS-iWildCam <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b5">6]</ref>. <ref type="figure" target="#fig_15">Figure 9</ref> displays the effective robustness scatter plots for the six additional distribution shifts discussed in Section 4 (analogous results provided in <ref type="table" target="#tab_12">Table 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Robustness on additional distribution shifts</head><p>Concretely, we consider: (i) ImageNet-Vid-Robust and YTBB-Robust, datasets with distribution shift induced by temporal perturbations in videos <ref type="bibr" target="#b87">[88]</ref>; (ii) CIFAR-10.1 <ref type="bibr" target="#b82">[83]</ref> and CIFAR-10.2 <ref type="bibr" target="#b61">[62]</ref>, reproductions of the popular image classification dataset CIFAR-10 [54] with a distribution shift; (iii) WILDS-FMoW, a satellite image recognition task where the test set has a geographic and temporal distribution shift <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b12">13]</ref>; (iv) WILDS-iWildCam, a wildlife recognition task where the test set has a geographic distribution shift <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b5">6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Comparison with alternative methods</head><p>We now extend Section 4 and compare WiSE-FT to additional methods of fine-tuning. We begin with contrasting the weight-space and output-space ensemble. Next, we show the that varying the decay parameter of an exponential moving average also moves along the curve produced by WiSE-FT. Finally, we compare with additional methods when fine-tuning only a linear classifier including distillation and various forms of regularization.   <ref type="bibr" target="#b61">[62]</ref>, WILDS-FMoW <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b12">13]</ref>, and WILDS-iWildCam <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b5">6]</ref>. Reported numbers are percentages. This is the corresponding table for <ref type="figure" target="#fig_15">Figure 9</ref>. This table displays results for fine-tuning only a linear classifier for ImageNet-Vid-Robust and YTBBRobust and end-to-end fine-tuning for the remainder.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3.2 Comparison to exponential moving averages</head><p>Weight-averaging along the trajectory can improve the performance of models. For instance, Szegedy et al. <ref type="bibr" target="#b94">[95]</ref> use a running average of the model parameters for their Inception-v2 model. The exponential moving average (EMA) is a standard technique for keeping a running average of model parameters and is implemented in libraries such as Optax <ref type="bibr" target="#b38">[39]</ref> and Pytorch ImageNet Models <ref type="bibr" target="#b100">[101]</ref>.</p><p>This section explores two variants of EMA for model parameters ? ? R n . The first variant is a debiased EMA, where debiasing is done as in Kingma and Ba <ref type="bibr" target="#b46">[47]</ref> (Algorithm 1). For each iteration t ? {1, ..., T } let ? t ? R n be the model parameters at step t and let ? t ? R n be the EMA at step t. For t = 0, ? 0 ? 0, otherwise ? t ? ? ? ? t?1 + (1 ? ?) ? ? t where ? is a decay hyperparameter. The final debiased EMA is given by ? T /(1 ? ? T ). Results for various decay hyperparameters are illustrated by <ref type="figure" target="#fig_17">Figure 11</ref>.</p><p>Next, we explore a variant of EMA that is biased towards the initialization ? 0 . As before, ? t ? ? ? ? t?1 + (1 ? ?) ? ? t . However ? 0 is now initialized to be ? 0 , instead of zeros. Moreover, at the end of fine-tuning we use the biased estimate ? T . Results for this variant are illustrated by <ref type="figure" target="#fig_18">Figure 12</ref>.</p><p>Section 4 ( <ref type="figure">Figure 3</ref>) showed that decreasing learning rate, training epochs, or early stopping leads to solutions that lie below the curve produced by WiSE-FT. On the other hand, using an exponential moving average <ref type="bibr" target="#b73">74</ref>   <ref type="figure" target="#fig_43">Figure 13</ref>: Accuracy on the reference and shifted distributions of WiSE-FT and the alternatives described in Section C.3.3.</p><p>(EMA) and varying the EMA decay ? can move along or slightly outside or along the curve produced by WiSE-FT. For instance, solutions using the second EMA variant follow the WiSE-FT curve. Indeed, applying WiSE-FT with mixing coefficient 1 ? ? T to the debiased EMA variant exactly recovers the second EMA variant described above. Moreover, further applying WiSE-FT to EMA solutions (i.e., interpolating the weights of the zero-shot model with the EMA solution) can lead to additional robustness. We also evaluate EMA along the fine-tuning trajectory, finding improved performance under distribution shift for the variant biased towards the initialization. For the debiased EMA, each model along the trajectory is debiased by 1/(1 ? ? t ). As shown in <ref type="figure" target="#fig_17">Figures 11,12</ref>, evaluations along the trajectory underperform solutions generated by applying WiSE-FT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3.3 Additional comparisons when fine-tuning a linear classifier</head><p>We compare against several additional alternatives when fine-tuning only a linear classifier. As this setting is computationally cheaper compared to end-to-end, it allows for comprehensive experimentation. Many of the examined approaches exhibit a concave trend in effective robustness plots, although WiSE-FT matches methods requiring more compute or offers better performance <ref type="figure" target="#fig_43">(Figure 13</ref>).</p><p>Random interpolation. This method uses either the zero-shot or fine-tuned linear classifier depending on a (biased) coin flip. For hyperparameter ? ? [0, 1] outputs are computed as</p><formula xml:id="formula_8">(1 ? ?) ? f (x, ? 0 ) + ? ? f (x, ? 1 )</formula><p>where ? is a Bernoulli(?) random variable. For this method and all others with a hyperparameter ? ? [0, 1] we evaluate models for ? ? {0, 0.05, 0.1, ..., 1}.</p><p>Ensembling softmax outputs. Instead of ensembling in weight space, this method combines softmax probabilities assigned by the zero-shot and fine-tuned linear classifier. Concretely, for hyperparameter ? ? [0, 1] outputs are computed as (1 ? ?) ? softmax(f (x, ? 0 )) + ? ? softmax(f (x, ? 1 )). This method performs comparably to weight-space ensembling but requires slightly more compute.</p><p>Linear classifier with various regularizers. We explore fine-tuning linear classifiers with four regularization strategies: no regularization, weight decay, L1 regularization, and label smoothing <ref type="bibr" target="#b70">[71]</ref>. Linear classifiers are trained with mini-batch optimization, using the AdamW optimizer <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b75">76]</ref> with a cosine-annealing learning rate schedule <ref type="bibr" target="#b59">[60]</ref>. This method is significantly faster and less memory-intensive than the L-BFGS implementation used by Radford et al. <ref type="bibr" target="#b81">[82]</ref> at ImageNet scale with similar accuracy. Additional details on hyperparameters and more analyses are provided in Appendix D.3.</p><p>Two variants of this method are shown in <ref type="figure" target="#fig_43">Figure 13</ref>, one for which the the linear classifier is initialized randomly and another for which the linear classifier is initialized with the zero-shot weights (denoted warmstart). If the convex problem is solved then the initialization does not play a role. However we are using mini-batch optimization and, in certain cases, terminating training before an optimum is reached.</p><p>Distillation. Network distillation <ref type="bibr" target="#b40">[41]</ref> trains one network to match the outputs of another. We use this technique to fine-tune while matching the outputs of the zero-shot model with weights ? 0 . For a hyperparameter ? ? [0, 1] and cross-entropy loss we fine-tune ? according to the minimization objective</p><formula xml:id="formula_9">(xi,yi)?S tr D (1 ? ?) ? (f (x i , ?), y i ) + ? ? (f (x i , ?), f (x i , ? 0 )) .<label>(3)</label></formula><p>Regularization towards zero-shot. We train a linear classifier with an additional regularization term which penalizes movement from the zero-shot classifier's weights. For a hyperparameter ? ? {1 ? 10 ?8 , 5 ? 10 ?8 , 1 ? 10 7 , ..., 5 ? 10 2 } we add the regularization term ? W ? W zero-shot 2 F where W is the linear classifier being fine-tuned. In most cases this method performs slightly worse than distillation.</p><p>Finally, <ref type="figure" target="#fig_4">Figure 14</ref> and <ref type="table" target="#tab_15">Table 7</ref> demonstrate that WiSE-FT achieves better accuracy than the recently proposed CoOp method <ref type="bibr" target="#b111">[112]</ref> on ImageNet and four derived distribution shifts. Instead of fine-tuning network parameters, CoOp instead learns continuous embedding for the language prompts. We note that CoOp and WiSE-FT could be used in conjunction in future work. We compare with the ViT-B/16 section in <ref type="table" target="#tab_15">Table 7</ref> of Zhou et al. <ref type="bibr" target="#b111">[112]</ref>. For comparison we use the same CLIP model as CoOp and also train only on 16 images per class. When end-to-end fine-tuning we use 10 epochs and learning rate 10 ?5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Changes in data augmentation</head><p>In the majority of our experiments we follow Radford et al. <ref type="bibr" target="#b81">[82]</ref> in using minimal data augmentation. However, <ref type="figure" target="#fig_4">Figure 14</ref> recreates <ref type="figure">Figure 3</ref> with the default ImageNet train augmentation used in PyTorch ImageNet Models <ref type="bibr" target="#b100">[101]</ref>, which includes random cropping, horizontal flipping and color jitter. As shown in <ref type="figure" target="#fig_4">Figure 14</ref>, we find similar trends with this stronger data augmentation. Further investigating the effect of data augmentation remains an interesting direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet (IN) INV2 IN-R IN-A IN Sketch</head><p>CoOp <ref type="bibr" target="#b111">[112]</ref> 71   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Accuracy improvements on reference datasets</head><p>Beyond robustness, <ref type="figure" target="#fig_43">Figure 16</ref> demonstrates that WiSE-FT can provide accuracy improvements on ImageNet and a number of datasets considered by Kornblith et al. <ref type="bibr" target="#b51">[52]</ref>: CIFAR-10, CIFAR-100 <ref type="bibr" target="#b53">[54]</ref>, Describable Textures <ref type="bibr" target="#b13">[14]</ref>, Food-101 <ref type="bibr" target="#b9">[10]</ref>, SUN397 <ref type="bibr" target="#b102">[103]</ref>, and Stanford Cars <ref type="bibr" target="#b52">[53]</ref>. This is surprising as standard fine-tuning optimizes for low error on the reference distribution. <ref type="figure" target="#fig_43">Figure 16</ref> supplements <ref type="table" target="#tab_5">Table 2</ref> by providing accuracy information for all mixing coefficients ?.</p><p>In many application-specific scenarios, only a small amount of data is available for fine-tuning. Accordingly, we examine the performance of WiSE-FT when only k examples per class are used for fine-tuning on the seven aforementioned datasets (k = {1, 5, 10, 25, 50}). In contrast with <ref type="figure" target="#fig_43">Figure 16</ref>, we now fine-tune only the linear classifier allowing for comprehensive experiments. Average results are shown in <ref type="figure" target="#fig_12">Figure 17</ref>, while <ref type="figure" target="#fig_14">Figures 18 and 19</ref> provide a breakdown for all datasets. Hyperparameter: Fix learning rate, vary number of epochs LR = 1 ? 10 ?7 LR = 1 ? 10 ?6 LR = 3 ? 10 ?6 LR = 1 ? 10 ?5 LR = 2 ? 10 ?5 LR = 3 ? 10 ?5 <ref type="bibr" target="#b64">65</ref> 70 <ref type="bibr">75 80</ref> ImageNet (top-1, %) Top-1 accuracy, % + 1.6pp</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Food101</head><p>Weight-space ensemble CLIP fine-tuned end-to-end CLIP zero-shot <ref type="figure" target="#fig_43">Figure 16</ref>: The accuracy of WiSE-FT (end-to-end) with mixing coefficient ? on ImageNet and a number of datasets considered by Kornblith et al. <ref type="bibr" target="#b51">[52]</ref>: CIFAR-10, CIFAR-100 <ref type="bibr" target="#b53">[54]</ref>, Describable Textures <ref type="bibr" target="#b13">[14]</ref>, Food-101 <ref type="bibr" target="#b9">[10]</ref>, SUN397 <ref type="bibr" target="#b102">[103]</ref>, and Stanford Cars <ref type="bibr" target="#b52">[53]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Percentage points</head><p>Accuracy gain over the zero-shot model   <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b52">53]</ref>. For k = 1, the zero-shot model outperforms the fine-tuned linear classifier, and ensembles closer to the zero-shot model (small ?) yield high performance. When more data is available, the reverse is true, and higher values of ? improve performance. <ref type="figure" target="#fig_14">Figures 18 and 19</ref> display a breakdown for all datasets.  Accuracy gain over best Optimal ? ? = 0 (CLIP zero-shot) ? = 0.25 ? = 0.5 ? = 0.75 ? = 1.0 (fine-tuned linear classifier)  Accuracy gain over linear classifier  Avg. accuracy gain (pp) Avg. accuracy on 5 distribution shifts   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Robustness across scales of pre-training compute</head><p>The strong correlation between standard test accuracy and accuracy under distribution shift holds from low to high performing models. This offers the opportunity to explore robustness for smaller, easy to run models. Our exploration began with the lowest accuracy CLIP models and similar trends held at scale.   <ref type="table">Table 8</ref>: WiSE-FT accuracy on ImageNet and derived distribution shifts for various models fine-tuned end-to-end. Avg shifts displays the mean performance among the five distribution shifts, while Avg reference, shifts shows the average of ImageNet (reference) and Avg shifts. For optimal ?, we choose the single mixing coefficient that maximizes the column. <ref type="table">Table 8</ref> summarizes the results for the main models we study, CLIP, ALIGN, BASIC and a ViT model pre-trained on JFT. Details are provided in the subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7 WiSE-FT and additional models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7.1 ALIGN</head><p>In addition to CLIP, we show WiSE-FT to be effective for an additional zero-shot model, ALIGN <ref type="bibr" target="#b44">[45]</ref>. Results are shown in <ref type="figure">Figure 22</ref> and <ref type="table" target="#tab_20">Table 9</ref>. End-to-end fine-tuning is performed using AdamW, which we found to perform slightly better than SGD + momentum. The model is fine-tuned for 40,000 steps with a batch size of 512, a maximum learning rate of 5 ? 10 ?6 , and weight decay of 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet-A (top-1, %)</head><p>Weight-space ensemble (linear classifier)</p><p>Weight-space ensemble (end-to-end) Varying the L2 regularization coefficient ALIGN zero-shot ALIGN fine-tuned with a linear classifier ALIGN fine-tuned end-to-end Weight-space ensemble with ? = 0.5 <ref type="figure">Figure 22</ref>: WiSE-FT applied to ALIGN <ref type="bibr" target="#b44">[45]</ref>. We also show the effect of varying the L2 regularization strength for linear classifier fine-tuning.</p><p>preprocessing as at evaluation time. The weights of the zero-shot model are calibrated using temperature scaling on the ImageNet training set before performing WiSE-FT.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7.2 JFT pre-training</head><p>We also investigate whether WiSE-FT can provide gains for models trained using a standard image classification objective on the JFT-300M dataset <ref type="bibr" target="#b92">[93]</ref>. Results are shown in <ref type="figure" target="#fig_36">Figure 23</ref> and <ref type="table" target="#tab_22">Table 10</ref>. For 973/1000 ImageNet classes, we were able to manually identify a corresponding class from the 18K classes in JFT. We use this mapping between ImageNet and JFT classes to obtain zero-shot ImageNet weights from the final layer weights of the pre-trained ViT-H/14 model from Dosovitskiy et al. <ref type="bibr" target="#b20">[21]</ref>. We also train a linear classifier on the fixed penultimate layer of the same ViT-H/14 model using L-BFGS without label smoothing with softmax cross-entropy loss, and fine-tune end-to-end using AdamW with maximum learning rate 5 ? 10 ?6 and weight decay 0.1 for 20k iterations at batch size 512 with sigmoid cross-entropy loss. As for CLIP models, our learning rate schedule consists of 500 steps of linear warmup followed by cosine decay. All ViT-H/14 models are trained and evaluated on 224 ? 224 pixel images. For fair evaluation, we prevent fine-tuned solutions from predicting the 27 classes with no plausible corresponding JFT class at all points on the WiSE-FT curve but still include these points in the denominator when computing accuracy.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.8 Ensembling zero-shot CLIP with independently trained models</head><p>So far we have shown that a zero-shot model can be used to improve performance under distribution shift of the derived fine-tuned model. Here, we investigate whether this improvement is specific to fine-tuned models. On the contrary, we find that the performance under distribution shift of independently trained models improves when ensembling with robust models. Note that in the general case where the models being ensembled have different architectures, we are unable to perform weight-space ensembling; instead, we ensemble the outputs of each model. This increases the computational cost of inference, in contrast to the results shown in Section 4.</p><p>Concretely, we ensemble zero-shot CLIP with two Noisy Student EfficientNet models <ref type="bibr" target="#b103">[104,</ref><ref type="bibr" target="#b95">96]</ref>: (i) EfficientNet-B6 <ref type="figure" target="#fig_5">(Figure 25, left)</ref>, with performance on the reference distribution comparable to the end-to-end fine-tuned CLIP model; and (ii) EfficientNet-L2 <ref type="figure" target="#fig_5">(Figure 25, right)</ref>, the strongest model available on PyTorch ImageNet Models <ref type="bibr" target="#b100">[101]</ref>. In both cases, we observe substantial improvements from ensembling-13.6 pp and 6.9 pp in average accuracy under distribution shift without reducing performance on the reference dataset. Further results are shown in  <ref type="table" target="#tab_25">Table 13</ref>: Accuracy of various independently trained models ensembled with CLIP on ImageNet and derived distribution shifts. OSE denotes output-space ensembling. Avg shifts displays the mean performance among the five distribution shifts, while Avg reference, shifts shows the average of ImageNet (reference) and Avg shifts.</p><p>? We initialize the final classification layer with the zero-shot classifier used by CLIP. We scale the zeroshot classifier weights by the temperature parameter of the pre-trained CLIP model at initialization, and do not include a temperature parameter during fine-tuning.</p><p>? As the zero-shot classifier expects the outputs of the image-encoder g to be normalized, we continue to normalize the outputs of g during fine-tuning.</p><p>When fine-tuning end-to-end, unless otherwise mentioned, we use the AdamW optimizer <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b75">76]</ref> and choose the largest batch size such that the model fits into 8 GPUs (512 for ViT-B/16). Unless otherwise mentioned, we use the default PyTorch AdamW hyperparameters ? 1 = 0.9, ? 2 = 0.999, = 10 ?8 , weight decay of 0.1 and a cosine-annealing learning rate schedule <ref type="bibr" target="#b59">[60]</ref> with 500 warm-up steps. Unless otherwise mentioned we use a learning rate of 3 ? 10 ?5 , gradient clipping at global norm 1 and fine-tune for a total of 10 epochs. Additionally, unless otherwise mentioned we use the same data augmentations as <ref type="bibr" target="#b81">[82]</ref>, randomly cropping a square from resized images with the largest dimension being 336 pixels for ViT-L/14@336px and 224 for the remaining models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Fine-tuning a linear classifier</head><p>This section extends the description of linear classifier training from Appendix C.3 with details on hyperparameters and additional analyses. In each of the four regularization strategies-no regularization, weight decay, L1 regularization, and label smoothing-we run 64 hyperparameter configurations. For each trial, mini-batch size is drawn uniformly from {64, 128, 256} and learning rate is set to 10 ?? with ? chosen uniformly at random from the range [0, 6]. Hyperparameters for each regularization strategy are as follows: (i) The weight decay coefficient is set to 10 ?? where ? is chosen uniformly at random from [0, 4] for each trial; (ii) The L1 regularization coefficient is set to 10 ?? where ? is chosen uniformly at random from <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref> for each trial; (iii) The label smoothing <ref type="bibr" target="#b70">[71]</ref> coefficient ? is chosen uniformly at random from [0, 0.25] for each trial. The linear classifier used for ensembling attains the best performance in-distribution. The hyperparameters 75 80 <ref type="bibr">85 90</ref> ImageNet (class-subsampled) (top-1, %) ObjectNet (top-1, %) <ref type="bibr" target="#b74">75</ref> 80 <ref type="bibr">85 90</ref> ImageNet (class-subsampled) (top-1, %) from this trial are then used in the distillation and regularization experiments described in Appendix C.3. In the low-data regime (Section C.5), this process is repeated for each k and dataset.</p><p>When training linear classifiers with k images per class as in Section C.5 the maximum number of epochs T is scaled approximately inversely proportional to the amount of data removed (e.g., with half the data we train for twice as many epochs so the number of iterations is consistent). To choose the T we use default PyTorch AdamW hyperparameters (learning rate 0.001, weight decay 0.01) and double the number of epochs until performance saturates. For each random hyperparameter run we choose the epochs uniformly from {1, ..., T }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 ObjectNet</head><p>The zero-shot models in <ref type="table">Table 1</ref> use the ImageNet class names instead of the ObjectNet class names. However, this adaptation to class shift improves performance by 2.3% <ref type="bibr" target="#b81">[82]</ref>. Out of the five datasets used for the majority of the experiments in Section 3, ObjectNet is the only dataset for which this is possible. In <ref type="figure" target="#fig_39">Figure  26</ref> we compare weight-space ensembles with and without adaptation to class shift.   F When do weight-space ensembles approximate output-space ensembles?</p><p>In practice we observe a difference between weight-space and output-space ensembling. However, it is worth noting that these two methods of ensembling are not as different as they initially appear. In certain regimes a weight-space ensemble approximates the corresponding output-space ensemble-for instance, when training is well approximated by a linear expansion, referred to as the NTK regime <ref type="bibr" target="#b43">[44]</ref>. Fort et al. <ref type="bibr" target="#b23">[24]</ref> find that a linear expansion becomes more accurate in the later phase of neural network training, a phase which closely resembles fine-tuning.</p><p>Consider the set ? = {(1 ? ?)? 0 + ?? 1 : ? ? [0, 1]} consisting of all ? which lie on the linear path between ? 0 and ? 1 . Proof. We may begin with the weight-space ensemble and retrieve the output-space ensemble</p><formula xml:id="formula_10">f ((1 ? ?)? 0 + ?? 1 ) (12) = f (? 0 ) + ?f (? 0 ) ((1 ? ?)? 0 + ?? 1 ? ? 0 ) (13) = f (? 0 ) + ??f (? 0 ) (? 1 ? ? 0 ) (14) = f (? 0 ) + ??f (? 0 ) (? 1 ? ? 0 ) + ?f (? 0 ) ? ?f (? 0 ) (15) = (1 ? ?)f (? 0 ) + ? f (? 0 ) + ?f (? 0 ) (? 1 ? ? 0 )<label>(16)</label></formula><formula xml:id="formula_11">= (1 ? ?)f (? 0 ) + ?f (? 1 )<label>(17)</label></formula><p>where the first and final line follow by the linearity assumption.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>ImageNet (Deng et al.) ImageNetV2 (Recht et al.) ImageNet-R (Hendrycks et al.) ImageNet Sketch (Wang et al.) ObjectNet (Barbu et al.) ImageNet-A (Hendrycks et al.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 : 2 2</head><label>32</label><figDesc>The robustness of fine-tuned models varies substantially under even small changes in hyperparameters. Applying WiSE-FT addresses this brittleness and can remove the trade-off between accuracy on the reference and shifted distributions. Results shown for CLIP ViT-B/16 fine-tuned with cosine-annealing learning rate schedule and all models in the top left and top middle plots are fine-tuned with AdamW<ref type="bibr" target="#b60">[61]</ref>. Moreover, regularize to zero-shot appends the regularizer ? ? ? ? 0 to the fine-tuning objective, where ? 0 are the parameters of the zero-shot model. on ImageNet, CIFAR-10, CIFAR-100, Describable Textures, Food-101, SUN397, and Stanford Cars, WiSE-FT reduces relative error by 4 to 49%. Even though standard fine-tuning directly optimizes for high accuracy on the reference distribution, WiSE-FT achieves better performance. Appendix C.5 includes more details, including explorations in the low-data regime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ensemble (end-to-end) Weight-space ensemble with ? = 0.5 BASIC-L zero-shot BASIC-L fine-tuned end-to-end ViT-H/14 (JFT) zero-shot ViT-H/14 (JFT) fine-tuned end-to-end ALIGN zero-shot ALIGN fine-tuned end-to-end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>WiSE-FT applied to BASIC-L [77], a ViT-H/14 [21] model pre-trained on JFT-300M [93] and ALIGN [45].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>(Left) Zero-shot and fine-tuned models exhibit diversity in their predictions. (Middle) On most distribution shifts, the zero-shot model overrides the linear classifier more than it is overridden. The reverse is true for ImageNet (reference). (Right) Similarly, zero-shot models are more confident under distribution shift, while the reverse is true on the reference distribution. The margin ? f measures the average difference between the largest and second largest unormalized output for classifier f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>show the performance of WiSE-FT for various values of the mixing coefficient ? on ImageNet and five derived distribution shifts, for CLIP ViT-L/14@336 and the ViT-B/16 model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(top-1, %) CLIP zero-shot models Linear fit (CLIP zero-shot models) CLIP fine-tuned end-to-end CLIP fine-tuned with a linear classifier (prior work) Weight-space ensemble (end-to-end) Weight-space ensemble (linear classifier) Weight-space ensemble with ? = 0.5 Standard ImageNet models Linear fit (standard ImageNet models) Trained with more data Existing robustness interventions y = x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>A per-dataset breakdown of the key experimental results(Figure 1). WiSE-FT improves accuracy on ImageNet and five derived distribution shifts. Standard ImageNet models, models trained with more data, and existing robustness interventions are from the testbed of Taori et al.<ref type="bibr" target="#b96">[97]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>(top-1, %) CLIP zero-shot models Linear fit (CLIP zero-shot models) CLIP fine-tuned end-to-end CLIP fine-tuned with a linear classifier (prior work) Weight-space ensemble (end-to-end) Weight-space ensemble (linear classifier) Standard ImageNet models Linear fit (standard ImageNet models) Trained with more data Existing robustness interventions y = x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 :</head><label>8</label><figDesc>A zoomed-out version ofFigure 7. WiSE-FT improves accuracy on ImageNet and five derived distribution shifts. Standard ImageNet models, models trained with more data, and existing robustness interventions are from the testbed of Taori et al.<ref type="bibr" target="#b96">[97]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 9 :</head><label>9</label><figDesc>WiSE-FT improves accuracy under distribution shift relative to standard fine-tuning on ImageNet-Vid-Robust, YTBB-Robust<ref type="bibr" target="#b87">[88]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 10 :</head><label>10</label><figDesc>on 5 distribution shifts CLIP zero-shot Linear fit (CLIP zero-shot) Weight-space ensemble (end-to-end)Output-space ensemble (end-to-end) Comparing the weight-space ensemble f (x, (1 ? ?) ? ? 0 + ? ? ? 1 ) with the output-space ensemble (1??)f (x, ? 0 )+??f (x, ? 1 ) when fine-tuning end-to-end with learning rate 3?10 ?5 . Note that the output-space ensemble requires 2x compute.C.3.1 Output-space ensemblesFigure 10compares the weight-space ensemble f (x, (1 ? ?) ? ? 0 + ? ? ? 1 ) with the output-space ensemble (1 ? ?)f (x, ? 0 ) + ? ? f (x, ? 1 ). Both exhibit a favorable trend, though the output-space ensemble requires twice as much compute. Section F further explores the relation between the weight-space and output-space ensemble.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 11 :</head><label>11</label><figDesc>on 5 distribution shifts Linear fit (CLIP zero-shot models) WiSE-FT Curves EMA throughout fine-tuning CLIP zero-shot models Standard fine-tuning (No EMA) Fine-tuned with EMA (decay 0.99) Fine-tuned with EMA (decay 0.999) Fine-tuned with EMA (decay 0.9999) Fine-tuned with EMA (decay 0.99999) Fine-tuned with EMA (decay 0.999999) Results for the debiased variant of EMA described in Appendix C.3.2. EMA improves accuracy on both ImageNet and on the distribution shifts, and further applying WiSE-FT to EMA solutions can improve robustness. The solutions with no EMA, decay 0.99, and decay 0.999 are overlapping in the plot, as are the solutions with decay 0.99999 and 0.999999.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 12 :</head><label>12</label><figDesc>on 5 distribution shifts Linear fit (CLIP zero-shot models) WiSE-FT Curves EMA throughout fine-tuning CLIP zero-shot models Fine-tuned with EMA (decay 0.9999) Fine-tuned with EMA (decay 0.99992) Fine-tuned with EMA (decay 0.99994) Fine-tuned with EMA (decay 0.99997) Fine-tuned with EMA (decay 0.99999) Standard fine-tuning (No EMA) Results for the variant of EMA biased towards the initialization, described in Appendix C.3.2. Varying the EMA decay ? moves along the curve produced by WiSE-FT. Applying WiSE-FT to EMA solutions moves further along the curve produced by WiSE-FT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 14 :</head><label>14</label><figDesc>Comparing WiSE-FT with CoOp [112]. Both methods fine-tune the ViT-B/16 CLIP model on 16 examples per class of ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 14 .</head><label>14</label><figDesc>shot models Linear fit (CLIP zero-shot models)Weight-space ensemble (end-to-end) The robustness of fine-tuned models varies substantially under even small changes in hyperparameters. Applying WiSE-FT addresses this brittleness and can remove the trade-off between accuracy on the reference and shifted distributions. Results shown for CLIP ViT-B/16 fine-tuned with cosineannealing learning rate schedule and ImageNet data augmentation from Pytorch ImageNet Models<ref type="bibr" target="#b100">[101]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>4</head><label></label><figDesc>Accuracy gain over the model that performs best on average ? = 0.0 (CLIP zero-shot) ? = 0.25 ? = 0.5 ? = 0.75 ? = 1.0 (fine-tuned linear classifier) Optimal ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 17 :</head><label>17</label><figDesc>WiSE-FT can improve accuracy over the linear classifier and zero-shot model in the low data regime. On the x-axis we consider k = {1, 5, 10, 25, 50} examples per class for fine-tuning. On the y-axis we display accuracy improvements of WiSE-FT averaged over seven datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 18 :</head><label>18</label><figDesc>WiSE-FT improves accuracy over the linear classifier and zero-shot model in the low data regime. On the x-axis we consider k = {1, 5, 10, 25, 50} examples per class and the full training set. On the y-axis we consider the accuracy improvement of WiSE-FT over the (top) zero-shot model, (middle) fine-tuned linear classifier, and (bottom) best of the zero-shot and fine-tuned linear classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>5 ? 1 Figure 19 :</head><label>5119</label><figDesc>= 0.75 ? = 1.0 (fine-tuned linear classifier) WiSE-FT improves accuracy over the linear classifier and zero-shot model in the low data regime. On the x-axis we consider k = {1, 5, 10, 25, 50} examples per class and the full training set. On the y-axis we consider the accuracy improvement of WiSE-FT over the (top) zero-shot model, (middle) fine-tuned linear classifier, and (bottom) best of the zero-shot and fine-tuned linear classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 20 :</head><label>20</label><figDesc>WiSE-FT provides benefits for all CLIP models. Accuracy under distribution shift can be improved relative to the linear classifier with less than ? {0, 0.1, 1} percentage points (pp) loss in accuracy on the reference distribution, across orders of magnitude of training compute. The CLIP model RN50x64 requires the most GPU hours to train.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure 21 :</head><label>21</label><figDesc>CLIP zero-shot models Linear fit (CLIP zero-shot models) CLIP fine-tuned end-to-end CLIP fine-tuned with a linear classifier (prior work)Weight-space ensemble (end-to-end)Weight-space ensemble (linear classifier) Weight-space ensemble with ? = 0.5 Standard ImageNet models Linear fit (standard ImageNet models) Trained with more data Existing robustness interventions y = x WiSE-FT improves accuracy on the reference and shifted distributions for numerous distribution shifts with a smaller CLIP ViT-B/16 model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head></head><label></label><figDesc>Figure 20shows improved accuracy under distribution shift with minimal loss on reference performance across orders of magnitude of pre-training compute with WiSE-FT when fine-tuning a linear classifier. Moreover, inFigure 21we recreate the experimental results for ImageNet and five associated distribution shifts with a smaller CLIP ViT-B/16 model, finding similar trends. Recall that unless otherwise mentioned our experiments use the larger CLIP model (ViT-L/14@336px).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure 23 :</head><label>23</label><figDesc>WiSE-FT applied to ViT-H/14<ref type="bibr" target="#b20">[21]</ref> pre-trained on JFT. We also show the effect of varying the L2 regularization strength for linear classifier fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Figure 25 :</head><label>25</label><figDesc>Ensembling with a zero-shot model improves accuracy under distribution shift of an independently trained model. (Left) Output-space ensembling with an independently trained model (NoisyStudent EfficientNet-B6) with comparable performance to the end-to-end fine-tuned model on the reference distribution. (Right) Output-space ensembling with an independently trained model with strong performance on the reference distribution (NoisyStudent EfficientNet-L2). Results averaged over the five distribution shifts as in Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>Figure 26 :</head><label>26</label><figDesc>Linear fit (CLIP zero-shot) CLIP fine-tuned with a linear classifier Effective robustness scatter plots for ObjectNet, with and without adapting to class shift. Left: Using ImageNet class names to construct the zero-shot classifier. Right: Using ObjectNet class names to construct the zero-shot classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>Figure 27 :</head><label>27</label><figDesc>between zero-shot and linear classifier Diversity between two linear classifiers Prediction Diversity (PD) across different models and datasets Prediction Diversity (PD) for multiple datasets and CLIP models (Equation 4). between zero-shot and linear classifier Diversity between two linear classifiers Cohen's Kappa Complement (CC) across different models and datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>Figure 28 :Figure 29 :</head><label>2829</label><figDesc>Cohen's Kappa Complement (CC) for multiple datasets and CLIP models (Equation 7). between zero-shot and linear classifier Diversity between two linear classifiers Average Kullback-Leibler divergence (KL) across different models and datasets Average KL Divergence (KL) for multiple datasets and CLIP models (Equation 9). between zero-shot and linear classifier Diversity between two linear classifiers Centered Kernel Alignment Complement (CKAC) across different models and datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>Figure 30 :</head><label>30</label><figDesc>Central Kernel Alignment Complement (CKAC) for multiple datasets and CLIP models (Equation 11).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>Proposition 1 .</head><label>1</label><figDesc>When f (?) = f (? 0 ) + ?f (? 0 ) (? ? ? 0 ) for all ? ? ?, the weight-and output-space ensemble of ? 0 and ? 1 are equivalent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ImageNet-V2 (IN-V2)<ref type="bibr" target="#b82">[83]</ref>, a reproduction of the ImageNet test set with distribution shift? ImageNet-R (IN-R)<ref type="bibr" target="#b36">[37]</ref>, renditions (e.g., sculptures, paintings) for 200 ImageNet classes? ImageNet Sketch (IN-Sketch)<ref type="bibr" target="#b99">[100]</ref>, which contains sketches instead of natural images? ObjectNet<ref type="bibr" target="#b3">[4]</ref>, a test set of objects in various scenes with 113 classes overlapping with ImageNet</figDesc><table /><note>a model f , we let Acc ref (f ) and Acc shift (f ) refer to classification accuracy on the reference and shifted test sets, respectively. We consider k-way image classification, where x i is an image with corresponding label yi ? {1, ..., k}. The outputs of f are k-dimensional vectors of non-normalized class scores. Distribution shifts. Taori et al. [97] categorized distribution shifts into two broad categories: (i) synthetic, e.g., ? -adversarial examples or artificial changes in image contrast, brightness, etc. [35, 8, 7, 29, 2]; and (ii) natural, where samples are not perturbed after acquisition and changes in data distributions arise through naturally occurring variations in lighting, geographic location, crowdsourcing process, image styles, etc. [97, 83, 37, 38, 49]. Following Radford et al. [81], our focus here is on natural distribution shifts as they are more representative of the real world when no active adversary is present. Specifically, we present our key results for five natural distribution shifts derived from ImageNet (i.e., S tr ref is ImageNet): ?? ImageNet-A (IN-A)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>WiSE-FT (?=0.5) 86.8 (+0.6) 99.3 (+0.7) 93.3 (+1.1) 93.3 (+1.7) 84.6 (+2.8) 83.2 (+2.5) 96.1 (+1.6) WiSE-FT (opt. ?) 87.1 (+0.9) 99.5 (+0.8) 93.4 (+1.2) 93.6 (+2.0) 85.2 (+3.3) 83.3 (+2.6) 96.2 (+1.8)</figDesc><table><row><cell></cell><cell>ImageNet</cell><cell cols="2">CIFAR10 CIFAR100</cell><cell>Cars</cell><cell>DTD</cell><cell>SUN397</cell><cell>Food101</cell></row><row><cell>Standard fine-tuning</cell><cell>86.2</cell><cell>98.6</cell><cell>92.2</cell><cell>91.6</cell><cell>81.9</cell><cell>80.7</cell><cell>94.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Beyond robustness, WiSE-FT can improve accuracy after fine-tuning on several datasets.</figDesc><table><row><cell>Avg. accuracy on 5 distribution shifts</cell><cell>76 78 80 82 84</cell><cell>85</cell><cell>86</cell><cell>BASIC-L</cell><cell>87</cell><cell>88</cell><cell>64 66 68 70 72</cell><cell>70</cell><cell>75</cell><cell>80</cell><cell>85</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">ImageNet (top-1, %)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ImageNet (top-1, %)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Algorithm 1 Pytorch pseudocode for WiSE-FT</figDesc><table><row><cell>def wse(model, zeroshot_checkpoint, finetuned_checkpoint, alpha):</cell></row><row><cell># load state dicts from checkpoints</cell></row><row><cell>theta_0 = torch.load(zeroshot_checkpoint)["state_dict"]</cell></row><row><cell>theta_1 = torch.load(finetuned_checkpoint)["state_dict"]</cell></row><row><cell># make sure checkpoints are compatible</cell></row><row><cell>assert set(theta_0.keys()) == set(theta_1.keys())</cell></row><row><cell># interpolate between all weights in the checkpoints</cell></row><row><cell>theta = {</cell></row><row><cell>key: (1-alpha) * theta_0[key] + alpha * theta_1[key]</cell></row><row><cell>for key in theta_0.keys()</cell></row><row><cell>}</cell></row><row><cell># update the model (in-place) according to the new weights</cell></row><row><cell>model.load_state_dict(theta)</cell></row><row><cell>def wise_ft(model, dataset, zeroshot_checkpoint, alpha, hparams):</cell></row><row><cell># load the zero-shot weights</cell></row><row><cell>theta_0 = torch.load(zeroshot_checkpoint)["state_dict"]</cell></row><row><cell>model.load_state_dict(theta_0)</cell></row><row><cell># standard fine-tuning</cell></row><row><cell>finetuned_checkpoint = finetune(model, dataset, hparams)</cell></row></table><note># perform weight-space ensembling (in-place) wse(model, zeroshot_checkpoint, finetuned_checkpoint, alpha)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>, where our key experimental results for ImageNet and five derived distribution shifts are averaged, we now display the results separately for each distribution shift. Results are provided in Figures 7, 8. IN-V2 IN-R IN-Sketch ObjectNet IN-A shifts ref., shifts</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Distribution shifts</cell><cell></cell><cell>Avg</cell><cell>Avg</cell></row><row><cell cols="2">IN (ref.) ViT-B/16, end-to-end 0.9</cell><cell>0.4</cell><cell>1.4</cell><cell>0.2</cell><cell>0.4</cell><cell>2.4</cell><cell>0.5</cell><cell>0.0</cell></row><row><cell>ViT-B/16, linear classifier</cell><cell>1.8</cell><cell>0.6</cell><cell>1.2</cell><cell>0.1</cell><cell>0.2</cell><cell>0.6</cell><cell>0.1</cell><cell>0.2</cell></row><row><cell>ViT-L/14@336, end-to-end</cell><cell>0.3</cell><cell>0.0</cell><cell>0.9</cell><cell>0.3</cell><cell>1.0</cell><cell>1.1</cell><cell>0.5</cell><cell>0.1</cell></row><row><cell>ViT-L/14@336, linear classifier</cell><cell>1.6</cell><cell>0.6</cell><cell>0.2</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Distribution shifts Avg Avg IN (ref.) IN-V2 IN-R IN-Sketch ObjectNet IN-A shifts ref., shifts WiSE-FT, end-to-end</figDesc><table><row><cell>?=0.00</cell><cell>76.6</cell><cell>70.5</cell><cell>89.0</cell><cell>60.9</cell><cell>68.5</cell><cell>77.6</cell><cell>73.3</cell><cell>74.9</cell></row><row><cell>?=0.05</cell><cell>78.7</cell><cell>72.6</cell><cell>89.6</cell><cell>62.2</cell><cell>69.5</cell><cell>79.0</cell><cell>74.6</cell><cell>76.7</cell></row><row><cell>?=0.10</cell><cell>80.4</cell><cell>74.2</cell><cell>89.9</cell><cell>63.1</cell><cell>70.4</cell><cell>79.8</cell><cell>75.5</cell><cell>78.0</cell></row><row><cell>?=0.15</cell><cell>81.9</cell><cell>75.4</cell><cell>90.1</cell><cell>63.8</cell><cell>71.1</cell><cell>80.4</cell><cell>76.2</cell><cell>79.1</cell></row><row><cell>?=0.20</cell><cell>83.2</cell><cell>76.5</cell><cell>90.3</cell><cell>64.3</cell><cell>71.6</cell><cell>80.8</cell><cell>76.7</cell><cell>80.0</cell></row><row><cell>?=0.25</cell><cell>84.2</cell><cell>77.5</cell><cell>90.3</cell><cell>64.6</cell><cell>72.1</cell><cell>81.0</cell><cell>77.1</cell><cell>80.7</cell></row><row><cell>?=0.30</cell><cell>85.1</cell><cell>78.3</cell><cell>90.3</cell><cell>64.9</cell><cell>72.1</cell><cell>81.0</cell><cell>77.3</cell><cell>81.2</cell></row><row><cell>?=0.35</cell><cell>85.7</cell><cell>78.7</cell><cell>90.1</cell><cell>65.0</cell><cell>72.0</cell><cell>81.0</cell><cell>77.4</cell><cell>81.6</cell></row><row><cell>?=0.40</cell><cell>86.2</cell><cell>79.2</cell><cell>89.9</cell><cell>65.0</cell><cell>71.9</cell><cell>80.7</cell><cell>77.3</cell><cell>81.8</cell></row><row><cell>?=0.45</cell><cell>86.6</cell><cell>79.4</cell><cell>89.6</cell><cell>64.9</cell><cell>71.6</cell><cell>80.6</cell><cell>77.2</cell><cell>81.9</cell></row><row><cell>?=0.50</cell><cell>86.8</cell><cell>79.5</cell><cell>89.4</cell><cell>64.7</cell><cell>71.1</cell><cell>79.9</cell><cell>76.9</cell><cell>81.8</cell></row><row><cell>?=0.55</cell><cell>87.0</cell><cell>79.3</cell><cell>88.9</cell><cell>64.5</cell><cell>70.7</cell><cell>79.1</cell><cell>76.5</cell><cell>81.8</cell></row><row><cell>?=0.60</cell><cell>87.1</cell><cell>79.2</cell><cell>88.5</cell><cell>64.1</cell><cell>70.1</cell><cell>78.2</cell><cell>76.0</cell><cell>81.5</cell></row><row><cell>?=0.65</cell><cell>87.1</cell><cell>79.3</cell><cell>87.8</cell><cell>63.6</cell><cell>69.6</cell><cell>77.4</cell><cell>75.5</cell><cell>81.3</cell></row><row><cell>?=0.70</cell><cell>87.1</cell><cell>79.1</cell><cell>87.0</cell><cell>63.1</cell><cell>68.9</cell><cell>76.5</cell><cell>74.9</cell><cell>81.0</cell></row><row><cell>?=0.75</cell><cell>87.0</cell><cell>78.8</cell><cell>86.1</cell><cell>62.5</cell><cell>68.1</cell><cell>75.2</cell><cell>74.1</cell><cell>80.5</cell></row><row><cell>?=0.80</cell><cell>86.9</cell><cell>78.4</cell><cell>85.1</cell><cell>61.7</cell><cell>67.4</cell><cell>73.8</cell><cell>73.3</cell><cell>80.1</cell></row><row><cell>?=0.85</cell><cell>86.8</cell><cell>78.0</cell><cell>84.0</cell><cell>61.0</cell><cell>66.4</cell><cell>72.0</cell><cell>72.3</cell><cell>79.5</cell></row><row><cell>?=0.90</cell><cell>86.7</cell><cell>77.6</cell><cell>82.8</cell><cell>60.0</cell><cell>65.5</cell><cell>69.9</cell><cell>71.2</cell><cell>79.0</cell></row><row><cell>?=0.95</cell><cell>86.5</cell><cell>77.2</cell><cell>81.3</cell><cell>59.0</cell><cell>64.3</cell><cell>67.7</cell><cell>69.9</cell><cell>78.2</cell></row><row><cell>?=1.00</cell><cell>86.2</cell><cell>76.8</cell><cell>79.8</cell><cell>57.9</cell><cell>63.3</cell><cell>65.4</cell><cell>68.6</cell><cell>77.4</cell></row><row><cell>WiSE-FT, linear classifier</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>?=0.00</cell><cell>76.6</cell><cell>70.5</cell><cell>89.0</cell><cell>60.9</cell><cell>69.1</cell><cell>77.7</cell><cell>73.4</cell><cell>75.0</cell></row><row><cell>?=0.05</cell><cell>77.6</cell><cell>71.3</cell><cell>89.2</cell><cell>61.3</cell><cell>69.3</cell><cell>78.3</cell><cell>73.9</cell><cell>75.8</cell></row><row><cell>?=0.10</cell><cell>78.4</cell><cell>72.1</cell><cell>89.4</cell><cell>61.7</cell><cell>69.6</cell><cell>78.8</cell><cell>74.3</cell><cell>76.3</cell></row><row><cell>?=0.15</cell><cell>79.3</cell><cell>72.8</cell><cell>89.5</cell><cell>62.1</cell><cell>70.0</cell><cell>79.0</cell><cell>74.7</cell><cell>77.0</cell></row><row><cell>?=0.20</cell><cell>80.0</cell><cell>73.5</cell><cell>89.6</cell><cell>62.4</cell><cell>70.3</cell><cell>79.3</cell><cell>75.0</cell><cell>77.5</cell></row><row><cell>?=0.25</cell><cell>80.8</cell><cell>74.1</cell><cell>89.7</cell><cell>62.6</cell><cell>70.5</cell><cell>79.5</cell><cell>75.3</cell><cell>78.0</cell></row><row><cell>?=0.30</cell><cell>81.5</cell><cell>74.8</cell><cell>89.7</cell><cell>62.8</cell><cell>70.7</cell><cell>79.5</cell><cell>75.5</cell><cell>78.5</cell></row><row><cell>?=0.35</cell><cell>82.1</cell><cell>75.4</cell><cell>89.8</cell><cell>62.9</cell><cell>70.7</cell><cell>79.6</cell><cell>75.7</cell><cell>78.9</cell></row><row><cell>?=0.40</cell><cell>82.7</cell><cell>75.8</cell><cell>89.7</cell><cell>63.0</cell><cell>70.7</cell><cell>79.6</cell><cell>75.8</cell><cell>79.2</cell></row><row><cell>?=0.45</cell><cell>83.2</cell><cell>76.1</cell><cell>89.7</cell><cell>63.0</cell><cell>70.7</cell><cell>79.6</cell><cell>75.8</cell><cell>79.5</cell></row><row><cell>?=0.50</cell><cell>83.7</cell><cell>76.3</cell><cell>89.6</cell><cell>63.0</cell><cell>70.7</cell><cell>79.7</cell><cell>75.9</cell><cell>79.8</cell></row><row><cell>?=0.55</cell><cell>84.1</cell><cell>76.5</cell><cell>89.5</cell><cell>62.9</cell><cell>70.5</cell><cell>79.6</cell><cell>75.8</cell><cell>79.9</cell></row><row><cell>?=0.60</cell><cell>84.4</cell><cell>76.7</cell><cell>89.3</cell><cell>62.7</cell><cell>70.3</cell><cell>79.5</cell><cell>75.7</cell><cell>80.1</cell></row><row><cell>?=0.65</cell><cell>84.7</cell><cell>76.8</cell><cell>89.1</cell><cell>62.6</cell><cell>70.1</cell><cell>79.4</cell><cell>75.6</cell><cell>80.2</cell></row><row><cell>?=0.70</cell><cell>85.0</cell><cell>76.9</cell><cell>88.9</cell><cell>62.3</cell><cell>69.9</cell><cell>79.1</cell><cell>75.4</cell><cell>80.2</cell></row><row><cell>?=0.75</cell><cell>85.1</cell><cell>76.8</cell><cell>88.4</cell><cell>61.9</cell><cell>69.7</cell><cell>78.9</cell><cell>75.1</cell><cell>80.1</cell></row><row><cell>?=0.80</cell><cell>85.3</cell><cell>76.9</cell><cell>87.9</cell><cell>61.4</cell><cell>69.3</cell><cell>78.5</cell><cell>74.8</cell><cell>80.0</cell></row><row><cell>?=0.85</cell><cell>85.3</cell><cell>76.7</cell><cell>87.4</cell><cell>60.9</cell><cell>68.8</cell><cell>78.1</cell><cell>74.4</cell><cell>79.8</cell></row><row><cell>?=0.90</cell><cell>85.3</cell><cell>76.4</cell><cell>86.8</cell><cell>60.3</cell><cell>68.4</cell><cell>77.3</cell><cell>73.8</cell><cell>79.5</cell></row><row><cell>?=0.95</cell><cell>85.3</cell><cell>76.2</cell><cell>86.1</cell><cell>59.5</cell><cell>67.7</cell><cell>76.8</cell><cell>73.3</cell><cell>79.3</cell></row><row><cell>?=1.00</cell><cell>85.2</cell><cell>75.8</cell><cell>85.3</cell><cell>58.7</cell><cell>67.2</cell><cell>76.1</cell><cell>72.6</cell><cell>78.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>WiSE-FT accuracy on the reference and shifted distributions for various values of the mixing coefficient ?. Results shown for CLIP ViT-L/14@336. Note that ?=0.0 corresponds to the zero-shot model, while ? = 1.0 corresponds to standard fine-tuning. Avg shifts displays the mean performance among the five distribution shifts, while Avg reference, shifts shows the average of ImageNet (reference) and Avg shifts.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Distribution shifts</cell><cell></cell><cell>Avg</cell><cell>Avg</cell></row><row><cell></cell><cell cols="8">IN (ref.) IN-V2 IN-R IN-Sketch ObjectNet IN-A shifts ref., shifts</cell></row><row><cell>WiSE-FT, end-to-end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>?=0.00</cell><cell>68.3</cell><cell>61.9</cell><cell>77.6</cell><cell>48.2</cell><cell>53.0</cell><cell>49.8</cell><cell>58.1</cell><cell>63.2</cell></row><row><cell>?=0.05</cell><cell>70.7</cell><cell>64.0</cell><cell>78.6</cell><cell>49.6</cell><cell>54.5</cell><cell>51.5</cell><cell>59.6</cell><cell>65.2</cell></row><row><cell>?=0.10</cell><cell>72.9</cell><cell>65.7</cell><cell>79.4</cell><cell>50.8</cell><cell>55.7</cell><cell>52.5</cell><cell>60.8</cell><cell>66.8</cell></row><row><cell>?=0.15</cell><cell>74.8</cell><cell>67.2</cell><cell>79.9</cell><cell>51.7</cell><cell>56.6</cell><cell>53.5</cell><cell>61.8</cell><cell>68.3</cell></row><row><cell>?=0.20</cell><cell>76.4</cell><cell>68.7</cell><cell>80.1</cell><cell>52.5</cell><cell>57.1</cell><cell>54.2</cell><cell>62.5</cell><cell>69.5</cell></row><row><cell>?=0.25</cell><cell>77.8</cell><cell>69.9</cell><cell>80.1</cell><cell>53.1</cell><cell>57.4</cell><cell>54.6</cell><cell>63.0</cell><cell>70.4</cell></row><row><cell>?=0.30</cell><cell>78.9</cell><cell>70.6</cell><cell>80.1</cell><cell>53.6</cell><cell>57.5</cell><cell>54.6</cell><cell>63.3</cell><cell>71.1</cell></row><row><cell>?=0.35</cell><cell>79.7</cell><cell>71.5</cell><cell>79.9</cell><cell>53.9</cell><cell>57.6</cell><cell>54.3</cell><cell>63.4</cell><cell>71.5</cell></row><row><cell>?=0.40</cell><cell>80.5</cell><cell>72.1</cell><cell>79.6</cell><cell>54.1</cell><cell>57.7</cell><cell>53.8</cell><cell>63.5</cell><cell>72.0</cell></row><row><cell>?=0.45</cell><cell>81.2</cell><cell>72.4</cell><cell>79.3</cell><cell>54.0</cell><cell>57.5</cell><cell>53.2</cell><cell>63.3</cell><cell>72.2</cell></row><row><cell>?=0.50</cell><cell>81.7</cell><cell>72.8</cell><cell>78.7</cell><cell>53.9</cell><cell>57.3</cell><cell>52.2</cell><cell>63.0</cell><cell>72.3</cell></row><row><cell>?=0.55</cell><cell>82.1</cell><cell>73.0</cell><cell>78.0</cell><cell>53.8</cell><cell>56.6</cell><cell>51.4</cell><cell>62.6</cell><cell>72.3</cell></row><row><cell>?=0.60</cell><cell>82.4</cell><cell>72.9</cell><cell>77.2</cell><cell>53.4</cell><cell>56.2</cell><cell>50.0</cell><cell>61.9</cell><cell>72.2</cell></row><row><cell>?=0.65</cell><cell>82.6</cell><cell>73.1</cell><cell>76.3</cell><cell>53.0</cell><cell>55.5</cell><cell>48.9</cell><cell>61.4</cell><cell>72.0</cell></row><row><cell>?=0.70</cell><cell>82.6</cell><cell>73.2</cell><cell>75.2</cell><cell>52.4</cell><cell>55.0</cell><cell>47.4</cell><cell>60.6</cell><cell>71.6</cell></row><row><cell>?=0.75</cell><cell>82.6</cell><cell>73.1</cell><cell>73.9</cell><cell>51.8</cell><cell>54.3</cell><cell>46.0</cell><cell>59.8</cell><cell>71.2</cell></row><row><cell>?=0.80</cell><cell>82.5</cell><cell>72.8</cell><cell>72.7</cell><cell>51.0</cell><cell>53.5</cell><cell>44.6</cell><cell>58.9</cell><cell>70.7</cell></row><row><cell>?=0.85</cell><cell>82.3</cell><cell>72.4</cell><cell>71.1</cell><cell>50.0</cell><cell>52.7</cell><cell>42.9</cell><cell>57.8</cell><cell>70.0</cell></row><row><cell>?=0.90</cell><cell>82.1</cell><cell>72.0</cell><cell>69.5</cell><cell>48.9</cell><cell>51.7</cell><cell>40.9</cell><cell>56.6</cell><cell>69.3</cell></row><row><cell>?=0.95</cell><cell>81.7</cell><cell>71.5</cell><cell>67.7</cell><cell>47.6</cell><cell>50.7</cell><cell>38.8</cell><cell>55.3</cell><cell>68.5</cell></row><row><cell>?=1.00</cell><cell>81.3</cell><cell>70.9</cell><cell>65.6</cell><cell>46.3</cell><cell>49.6</cell><cell>36.7</cell><cell>53.8</cell><cell>67.5</cell></row><row><cell>WiSE-FT, linear classifier</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>?=0.00</cell><cell>68.4</cell><cell>62.6</cell><cell>77.6</cell><cell>48.2</cell><cell>53.8</cell><cell>50.0</cell><cell>58.4</cell><cell>63.4</cell></row><row><cell>?=0.05</cell><cell>69.9</cell><cell>63.7</cell><cell>77.9</cell><cell>48.9</cell><cell>54.2</cell><cell>50.6</cell><cell>59.1</cell><cell>64.5</cell></row><row><cell>?=0.10</cell><cell>71.3</cell><cell>64.8</cell><cell>78.2</cell><cell>49.5</cell><cell>54.7</cell><cell>51.0</cell><cell>59.6</cell><cell>65.5</cell></row><row><cell>?=0.15</cell><cell>72.5</cell><cell>65.8</cell><cell>78.4</cell><cell>50.0</cell><cell>55.1</cell><cell>51.1</cell><cell>60.1</cell><cell>66.3</cell></row><row><cell>?=0.20</cell><cell>73.6</cell><cell>66.6</cell><cell>78.4</cell><cell>50.5</cell><cell>55.3</cell><cell>51.5</cell><cell>60.5</cell><cell>67.0</cell></row><row><cell>?=0.25</cell><cell>74.7</cell><cell>67.4</cell><cell>78.4</cell><cell>50.8</cell><cell>55.3</cell><cell>51.8</cell><cell>60.7</cell><cell>67.7</cell></row><row><cell>?=0.30</cell><cell>75.6</cell><cell>68.0</cell><cell>78.3</cell><cell>51.1</cell><cell>55.4</cell><cell>51.7</cell><cell>60.9</cell><cell>68.2</cell></row><row><cell>?=0.35</cell><cell>76.4</cell><cell>68.8</cell><cell>78.2</cell><cell>51.3</cell><cell>55.5</cell><cell>51.6</cell><cell>61.1</cell><cell>68.8</cell></row><row><cell>?=0.40</cell><cell>77.1</cell><cell>69.0</cell><cell>77.8</cell><cell>51.3</cell><cell>55.5</cell><cell>51.4</cell><cell>61.0</cell><cell>69.0</cell></row><row><cell>?=0.45</cell><cell>77.7</cell><cell>69.4</cell><cell>77.6</cell><cell>51.3</cell><cell>55.4</cell><cell>51.3</cell><cell>61.0</cell><cell>69.3</cell></row><row><cell>?=0.50</cell><cell>78.2</cell><cell>69.9</cell><cell>77.2</cell><cell>51.2</cell><cell>55.3</cell><cell>51.2</cell><cell>61.0</cell><cell>69.6</cell></row><row><cell>?=0.55</cell><cell>78.6</cell><cell>70.1</cell><cell>76.7</cell><cell>51.0</cell><cell>55.0</cell><cell>50.9</cell><cell>60.7</cell><cell>69.7</cell></row><row><cell>?=0.60</cell><cell>79.0</cell><cell>70.2</cell><cell>76.1</cell><cell>50.8</cell><cell>54.7</cell><cell>50.5</cell><cell>60.5</cell><cell>69.8</cell></row><row><cell>?=0.65</cell><cell>79.3</cell><cell>70.4</cell><cell>75.7</cell><cell>50.4</cell><cell>54.5</cell><cell>50.1</cell><cell>60.2</cell><cell>69.8</cell></row><row><cell>?=0.70</cell><cell>79.6</cell><cell>70.4</cell><cell>75.2</cell><cell>50.1</cell><cell>54.2</cell><cell>49.9</cell><cell>60.0</cell><cell>69.8</cell></row><row><cell>?=0.75</cell><cell>79.7</cell><cell>70.4</cell><cell>74.6</cell><cell>49.7</cell><cell>53.9</cell><cell>49.5</cell><cell>59.6</cell><cell>69.7</cell></row><row><cell>?=0.80</cell><cell>79.8</cell><cell>70.5</cell><cell>73.9</cell><cell>49.3</cell><cell>53.6</cell><cell>49.0</cell><cell>59.3</cell><cell>69.5</cell></row><row><cell>?=0.85</cell><cell>79.9</cell><cell>70.4</cell><cell>73.2</cell><cell>48.7</cell><cell>53.3</cell><cell>48.6</cell><cell>58.8</cell><cell>69.3</cell></row><row><cell>?=0.90</cell><cell>80.0</cell><cell>70.3</cell><cell>72.4</cell><cell>48.1</cell><cell>52.8</cell><cell>47.8</cell><cell>58.3</cell><cell>69.2</cell></row><row><cell>?=0.95</cell><cell>79.9</cell><cell>70.1</cell><cell>71.7</cell><cell>47.5</cell><cell>52.6</cell><cell>46.9</cell><cell>57.8</cell><cell>68.8</cell></row><row><cell>?=1.00</cell><cell>79.9</cell><cell>69.8</cell><cell>70.8</cell><cell>46.9</cell><cell>52.1</cell><cell>46.4</cell><cell>57.2</cell><cell>68.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>WiSE-FT accuracy on the reference and shifted distributions for various values of the mixing coefficient ?. Results shown for CLIP ViT-B/16. Note that ?=0.0 corresponds to the zero-shot model, while ? = 1.0 corresponds to standard fine-tuning. Avg shifts displays the mean performance among the five distribution shifts, while Avg reference, shifts shows the average of ImageNet (reference) and Avg shifts.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Imagenet-Vid-Robust</cell><cell></cell><cell></cell><cell></cell><cell cols="3">YTBB-Robust</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-10.1</cell></row><row><cell>Imagenet-Vid-Robust (pm-0, %)</cell><cell>55 60 65 70 75 80 85 90 95</cell><cell>90 92 94</cell><cell>96</cell><cell>98</cell><cell>+9.0pp OOD -0.1pp ID</cell><cell>YTBB-Robust (pm-0, %)</cell><cell>40 45 50 55 60 65 70 75 80 85 90 95</cell><cell>90 92 94</cell><cell>96</cell><cell>98</cell><cell>+23.2pp OOD -0.1pp ID</cell><cell>CIFAR-10.1 (top-1, %)</cell><cell>75 80 85 90 95</cell><cell cols="3">85 87 89 91 93 95</cell><cell>97</cell><cell>99 Relative to LC: +2.0pp OOD +1.2pp ID Relative to E2E: +1.7pp OOD +0.7pp ID</cell></row><row><cell></cell><cell></cell><cell cols="4">ImageNet (class-subsampled) (top-1, %)</cell><cell></cell><cell></cell><cell cols="4">ImageNet (class-subsampled) (top-1, %)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR10 (top-1, %)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-10.2</cell><cell></cell><cell></cell><cell></cell><cell cols="3">WILDS-FMoW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">WILDS-iWildCam</cell></row><row><cell>CIFAR-10.2 (top-1, %)</cell><cell>70 75 80 85 90 95</cell><cell></cell><cell></cell><cell></cell><cell>Relative to LC: +2.1pp OOD +1.2pp ID Relative to E2E: +2.1pp OOD +0.7pp ID</cell><cell>OOD worst region accuracy</cell><cell>20 25 30 35 40 45 50</cell><cell cols="2">Relative to LC: +14.2pp OOD +16.9pp ID Relative to E2E: +3.5pp OOD -0.3pp ID</cell><cell></cell><cell></cell><cell>OOD test macro F1</cell><cell>45 15 20 25 30 35 40</cell><cell></cell><cell cols="2">Relative to LC: +4.5pp OOD +1.0pp ID Relative to E2E: +6.2pp OOD +3.6pp ID</cell></row><row><cell></cell><cell>65</cell><cell>75 80 85</cell><cell>90</cell><cell>95</cell><cell></cell><cell></cell><cell>15</cell><cell cols="4">20 25 30 35 40 45 50 55 60 65 70 75</cell><cell></cell><cell>10</cell><cell>10</cell><cell>15</cell><cell cols="2">20 25 30 35 40 45 50 55</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">CIFAR10 (top-1, %)</cell><cell></cell><cell></cell><cell cols="4">In-distribution test accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ID test macro F1</cell></row><row><cell></cell><cell></cell><cell cols="4">CLIP zero-shot models</cell><cell></cell><cell></cell><cell cols="4">Weight-space ensemble (end-to-end)</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Linear fit (standard ImageNet models)</cell></row><row><cell></cell><cell></cell><cell cols="4">Linear fit (CLIP zero-shot models)</cell><cell></cell><cell></cell><cell cols="4">Weight-space ensemble (linear classifier)</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Trained with more data</cell></row><row><cell></cell><cell></cell><cell cols="4">CLIP fine-tuned end-to-end</cell><cell></cell><cell></cell><cell cols="4">Weight-space ensemble with ? = 0.5</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Existing robustness interventions</cell></row><row><cell></cell><cell></cell><cell cols="4">CLIP fine-tuned with a linear classifier</cell><cell></cell><cell></cell><cell cols="4">Standard ImageNet models</cell><cell></cell><cell></cell><cell></cell><cell>y = x</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Zero-shot Fine-tuned WiSE-FT, ?=0.5 WiSE-FT, optimal ?</figDesc><table><row><cell>ImageNet-Vid-Robust (pm-0)</cell><cell>95.9</cell><cell>86.5</cell><cell>95.5</cell><cell>96.5</cell></row><row><cell>YTBBRobust (pm-0)</cell><cell>95.8</cell><cell>66.5</cell><cell>89.7</cell><cell>96.0</cell></row><row><cell>CIFAR-10.1 (top-1)</cell><cell>92.5</cell><cell>95.9</cell><cell>97.6</cell><cell>98.0</cell></row><row><cell>CIFAR-10.2 (top-1)</cell><cell>88.8</cell><cell>91.3</cell><cell>93.4</cell><cell>94.4</cell></row><row><cell>WILDS-FMoW: ID test (accuracy)</cell><cell>28.0</cell><cell>73.3</cell><cell>73.0</cell><cell>74.8</cell></row><row><cell>WILDS-FMoW: OOD worst region accuracy</cell><cell>23.8</cell><cell>46.0</cell><cell>49.5</cell><cell>49.7</cell></row><row><cell>WILDS-iWildCam: ID test macro F1</cell><cell>15.1</cell><cell>52.1</cell><cell>55.8</cell><cell>55.8</cell></row><row><cell>WILDS-iWildCam: OOD test macro F1</cell><cell>15.5</cell><cell>39.9</cell><cell>46.1</cell><cell>46.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>WiSE-FT improves results on ImageNet-Vid-Robust, YTBB-Robust [88], CIFAR-10.1 [83], CIFAR-</cell></row><row><cell>10.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>Comparing WiSE-FT with CoOp [112]. Both methods fine-tune the ViT-B/16 CLIP model on 16 examples per class of ImageNet. Also see Figure 14.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>1. The learning rate schedule consisted of 500 steps of linear warmup followed by cosine decay. The linear classifier is trained using L-BFGS and no label smoothing. All models are evaluated on 360 ? 360 pixel crops obtained by taking the central 87.5% square region of the test set images. For end-to-end fine-tuning, we take 299 ? 299 pixel Inception-style random crops from the original ImageNet images during training; for linear classifier training, we use the same</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>72</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ImageNetV2 (top-1, %)</cell><cell>66 68 70 72 74 76 78 80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ImageNet-R (top-1, %)</cell><cell>82 84 86 88 90 92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ImageNet Sketch (top-1, %)</cell><cell>60 62 64 66 68 70</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>64</cell><cell>72 74 76 78 80</cell><cell>82</cell><cell>84</cell><cell>86</cell><cell>88</cell><cell>80</cell><cell>72 74 76 78 80</cell><cell>82</cell><cell>84</cell><cell>86</cell><cell>88</cell><cell>58</cell><cell>72 74 76 78 80</cell><cell>82</cell><cell>84</cell><cell>86</cell><cell>88</cell></row><row><cell></cell><cell></cell><cell cols="3">ImageNet (top-1, %)</cell><cell></cell><cell></cell><cell></cell><cell cols="3">ImageNet (top-1, %)</cell><cell></cell><cell></cell><cell></cell><cell cols="3">ImageNet (top-1, %)</cell><cell></cell></row><row><cell></cell><cell>68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ObjectNet (top-1, %)</cell><cell>60 62 64 66</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70 72 74 76 78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>58</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>56</cell><cell>72 74 76 78 80</cell><cell>82</cell><cell>84</cell><cell>86</cell><cell>88</cell><cell>66</cell><cell>72 74 76 78 80</cell><cell>82</cell><cell>84</cell><cell>86</cell><cell>88</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">ImageNet (top-1, %)</cell><cell></cell><cell></cell><cell></cell><cell cols="3">ImageNet (top-1, %)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 9 :</head><label>9</label><figDesc>WiSE-FT accuracy on the reference and shifted distributions for various values of the mixing coefficient ?. Results shown for ALIGN, fine-tuned end-to-end (top) and with a linear classifier (bottom). Note that ?=0.0 corresponds to the zero-shot model, while ? = 1.0 corresponds to standard fine-tuning. Avg shifts displays the mean performance among the five distribution shifts, while Avg reference, shifts shows the average of ImageNet (reference) and Avg shifts.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>Distribution shifts Avg Avg IN (ref.) IN-V2 IN-R IN-Sketch ObjectNet IN-A shifts ref., shifts WiSE-FT, edn-to-end</figDesc><table><row><cell>?=0.00</cell><cell>72.9</cell><cell>66.1</cell><cell>85.9</cell><cell>57.0</cell><cell>59.2</cell><cell>58.4</cell><cell>65.3</cell><cell>69.1</cell></row><row><cell>?=0.05</cell><cell>74.1</cell><cell>67.3</cell><cell>86.4</cell><cell>57.9</cell><cell>60.4</cell><cell>59.9</cell><cell>66.4</cell><cell>70.2</cell></row><row><cell>?=0.10</cell><cell>75.3</cell><cell>68.3</cell><cell>86.9</cell><cell>58.8</cell><cell>61.2</cell><cell>60.8</cell><cell>67.2</cell><cell>71.2</cell></row><row><cell>?=0.15</cell><cell>76.5</cell><cell>69.5</cell><cell>87.4</cell><cell>59.7</cell><cell>62.2</cell><cell>61.7</cell><cell>68.1</cell><cell>72.3</cell></row><row><cell>?=0.20</cell><cell>77.5</cell><cell>70.8</cell><cell>87.9</cell><cell>60.5</cell><cell>63.0</cell><cell>62.8</cell><cell>69.0</cell><cell>73.2</cell></row><row><cell>?=0.25</cell><cell>78.5</cell><cell>71.6</cell><cell>88.3</cell><cell>61.2</cell><cell>63.8</cell><cell>63.5</cell><cell>69.7</cell><cell>74.1</cell></row><row><cell>?=0.30</cell><cell>79.6</cell><cell>72.5</cell><cell>88.6</cell><cell>61.8</cell><cell>64.4</cell><cell>64.3</cell><cell>70.3</cell><cell>74.9</cell></row><row><cell>?=0.35</cell><cell>80.6</cell><cell>73.5</cell><cell>88.9</cell><cell>62.3</cell><cell>64.9</cell><cell>64.8</cell><cell>70.9</cell><cell>75.8</cell></row><row><cell>?=0.40</cell><cell>81.5</cell><cell>74.1</cell><cell>89.1</cell><cell>62.8</cell><cell>65.3</cell><cell>65.4</cell><cell>71.3</cell><cell>76.4</cell></row><row><cell>?=0.45</cell><cell>82.2</cell><cell>74.8</cell><cell>89.2</cell><cell>63.3</cell><cell>65.6</cell><cell>65.8</cell><cell>71.7</cell><cell>77.0</cell></row><row><cell>?=0.50</cell><cell>82.9</cell><cell>75.4</cell><cell>89.3</cell><cell>63.8</cell><cell>65.8</cell><cell>66.2</cell><cell>72.1</cell><cell>77.5</cell></row><row><cell>?=0.55</cell><cell>83.4</cell><cell>75.9</cell><cell>89.3</cell><cell>64.0</cell><cell>66.0</cell><cell>66.3</cell><cell>72.3</cell><cell>77.8</cell></row><row><cell>?=0.60</cell><cell>83.9</cell><cell>76.4</cell><cell>89.3</cell><cell>64.3</cell><cell>66.0</cell><cell>66.6</cell><cell>72.5</cell><cell>78.2</cell></row><row><cell>?=0.65</cell><cell>84.3</cell><cell>76.8</cell><cell>89.1</cell><cell>64.5</cell><cell>65.9</cell><cell>66.4</cell><cell>72.5</cell><cell>78.4</cell></row><row><cell>?=0.70</cell><cell>84.7</cell><cell>77.1</cell><cell>88.9</cell><cell>64.5</cell><cell>65.8</cell><cell>66.0</cell><cell>72.5</cell><cell>78.6</cell></row><row><cell>?=0.75</cell><cell>84.9</cell><cell>77.4</cell><cell>88.5</cell><cell>64.5</cell><cell>65.6</cell><cell>65.3</cell><cell>72.3</cell><cell>78.6</cell></row><row><cell>?=0.80</cell><cell>85.2</cell><cell>77.6</cell><cell>88.1</cell><cell>64.4</cell><cell>65.2</cell><cell>64.8</cell><cell>72.0</cell><cell>78.6</cell></row><row><cell>?=0.85</cell><cell>85.3</cell><cell>77.8</cell><cell>87.5</cell><cell>64.1</cell><cell>64.7</cell><cell>63.8</cell><cell>71.6</cell><cell>78.4</cell></row><row><cell>?=0.90</cell><cell>85.4</cell><cell>77.8</cell><cell>86.8</cell><cell>63.7</cell><cell>64.4</cell><cell>63.2</cell><cell>71.2</cell><cell>78.3</cell></row><row><cell>?=0.95</cell><cell>85.4</cell><cell>77.8</cell><cell>85.9</cell><cell>63.3</cell><cell>63.9</cell><cell>62.2</cell><cell>70.6</cell><cell>78.0</cell></row><row><cell>?=1.00</cell><cell>85.4</cell><cell>77.6</cell><cell>84.9</cell><cell>62.8</cell><cell>63.1</cell><cell>60.8</cell><cell>69.8</cell><cell>77.6</cell></row><row><cell>WiSE-FT, linear classifier</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>?=0.00</cell><cell>72.9</cell><cell>66.1</cell><cell>85.9</cell><cell>57.0</cell><cell>59.2</cell><cell>58.4</cell><cell>65.3</cell><cell>69.1</cell></row><row><cell>?=0.05</cell><cell>74.0</cell><cell>67.3</cell><cell>86.3</cell><cell>57.5</cell><cell>60.3</cell><cell>59.2</cell><cell>66.1</cell><cell>70.0</cell></row><row><cell>?=0.10</cell><cell>75.1</cell><cell>68.3</cell><cell>86.7</cell><cell>58.1</cell><cell>61.2</cell><cell>60.1</cell><cell>66.9</cell><cell>71.0</cell></row><row><cell>?=0.15</cell><cell>76.1</cell><cell>69.1</cell><cell>87.0</cell><cell>58.5</cell><cell>61.8</cell><cell>60.8</cell><cell>67.4</cell><cell>71.8</cell></row><row><cell>?=0.20</cell><cell>77.1</cell><cell>70.0</cell><cell>87.3</cell><cell>59.0</cell><cell>62.4</cell><cell>61.1</cell><cell>68.0</cell><cell>72.5</cell></row><row><cell>?=0.25</cell><cell>78.0</cell><cell>71.0</cell><cell>87.5</cell><cell>59.5</cell><cell>63.0</cell><cell>61.6</cell><cell>68.5</cell><cell>73.2</cell></row><row><cell>?=0.30</cell><cell>78.8</cell><cell>71.7</cell><cell>87.7</cell><cell>59.8</cell><cell>63.3</cell><cell>61.9</cell><cell>68.9</cell><cell>73.8</cell></row><row><cell>?=0.35</cell><cell>79.6</cell><cell>72.2</cell><cell>87.8</cell><cell>60.1</cell><cell>63.6</cell><cell>62.2</cell><cell>69.2</cell><cell>74.4</cell></row><row><cell>?=0.40</cell><cell>80.3</cell><cell>72.9</cell><cell>87.9</cell><cell>60.4</cell><cell>63.6</cell><cell>62.3</cell><cell>69.4</cell><cell>74.8</cell></row><row><cell>?=0.45</cell><cell>80.9</cell><cell>73.4</cell><cell>88.0</cell><cell>60.5</cell><cell>63.8</cell><cell>62.5</cell><cell>69.6</cell><cell>75.2</cell></row><row><cell>?=0.50</cell><cell>81.5</cell><cell>73.8</cell><cell>88.0</cell><cell>60.7</cell><cell>63.9</cell><cell>62.5</cell><cell>69.8</cell><cell>75.7</cell></row><row><cell>?=0.55</cell><cell>81.9</cell><cell>74.1</cell><cell>88.0</cell><cell>60.8</cell><cell>63.7</cell><cell>62.5</cell><cell>69.8</cell><cell>75.8</cell></row><row><cell>?=0.60</cell><cell>82.4</cell><cell>74.4</cell><cell>87.9</cell><cell>60.8</cell><cell>63.5</cell><cell>62.4</cell><cell>69.8</cell><cell>76.1</cell></row><row><cell>?=0.65</cell><cell>82.8</cell><cell>74.7</cell><cell>87.8</cell><cell>60.7</cell><cell>63.2</cell><cell>62.3</cell><cell>69.7</cell><cell>76.2</cell></row><row><cell>?=0.70</cell><cell>83.1</cell><cell>75.0</cell><cell>87.6</cell><cell>60.7</cell><cell>63.0</cell><cell>62.0</cell><cell>69.7</cell><cell>76.4</cell></row><row><cell>?=0.75</cell><cell>83.4</cell><cell>75.2</cell><cell>87.4</cell><cell>60.5</cell><cell>62.7</cell><cell>61.8</cell><cell>69.5</cell><cell>76.5</cell></row><row><cell>?=0.80</cell><cell>83.6</cell><cell>75.4</cell><cell>87.1</cell><cell>60.2</cell><cell>62.4</cell><cell>61.4</cell><cell>69.3</cell><cell>76.4</cell></row><row><cell>?=0.85</cell><cell>83.7</cell><cell>75.4</cell><cell>86.7</cell><cell>59.8</cell><cell>61.9</cell><cell>60.7</cell><cell>68.9</cell><cell>76.3</cell></row><row><cell>?=0.90</cell><cell>83.9</cell><cell>75.4</cell><cell>86.3</cell><cell>59.4</cell><cell>61.4</cell><cell>60.3</cell><cell>68.6</cell><cell>76.2</cell></row><row><cell>?=0.95</cell><cell>84.0</cell><cell>75.3</cell><cell>85.7</cell><cell>58.9</cell><cell>61.0</cell><cell>59.4</cell><cell>68.1</cell><cell>76.0</cell></row><row><cell>?=1.00</cell><cell>84.0</cell><cell>75.1</cell><cell>85.1</cell><cell>58.3</cell><cell>60.4</cell><cell>58.8</cell><cell>67.5</cell><cell>75.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 10 :</head><label>10</label><figDesc>WiSE-FT accuracy on the reference and shifted distributions for various values of the mixing coefficient ?. Results shown for ViT-H/14 pre-trained on JFT-300M, fine-tuned end-to-end (top) and with a linear classifier (bottom). Note that ?=0.0 corresponds to the zero-shot model, while ? = 1.0 corresponds to standard fine-tuning. Avg shifts displays the mean performance among the five distribution shifts, while Avg reference, shifts shows the average of ImageNet (reference) and Avg shifts.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 11 :</head><label>11</label><figDesc>WiSE-FT accuracy on the reference and shifted distributions for various values of the mixing coefficient ?. Results shown for BASIC-M using ImageNet class names. Note that ?=0.0 corresponds to the zero-shot model, while ? = 1.0 corresponds to standard fine-tuning. Avg shifts displays the mean performance among the five distribution shifts, while Avg reference, shifts shows the average of ImageNet (reference) and Avg shifts.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 12 :</head><label>12</label><figDesc>WiSE-FT accuracy on the reference and shifted distributions for various values of the mixing coefficient ?. Results shown for BASIC-L using ImageNet class names. Note that ?=0.0 corresponds to the zero-shot model, while ? = 1.0 corresponds to standard fine-tuning. Avg shifts displays the mean performance among the five distribution shifts, while Avg reference, shifts shows the average of ImageNet (reference) and Avg shifts.</figDesc><table><row><cell>Avg. accuracy on 5 distribution shifts</cell><cell>60 65 70 75 80</cell><cell>80</cell><cell>85 Avg. in-distribution accuracy 90 Comparable IN-trained model</cell><cell>Avg. accuracy on 5 distribution shifts</cell><cell>60 65 70 75 80</cell><cell>80</cell><cell>85 Avg. in-distribution accuracy 90 Strong IN-trained model</cell><cell>CLIP zero-shot Linear fit (CLIP zero-shot) CLIP fine-tuned end-to-end Weight-space ensemble (end-to-end) NS EfficientNet-B6 Ensemble with NS EfficientNet-B6 NS EfficientNet-L2 Ensemble with NS EfficientNet-L2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 13 .</head><label>13</label><figDesc>This section extends Section 2 with more details on inference with the CLIP zero-shot model. First, in all settings we use the CLIP model ViT-L/14@336px, except when explicitly mentioned otherwise. Second, CLIP learns a temperature parameter which is factored into the learned weight matrix W zero-shot described in Section 2. Finally, to construct W zero-shot we ensemble the 80 prompts provided by CLIP at https: //github.com/openai/CLIP. However, we manually engineer prompts for five datasets: WILDS-FMoW, WILDS-iWildCam, Stanford Cars, Describable Textures and Food-101, which are found in the code.</figDesc><table><row><cell>D Experimental details</cell></row><row><cell>D.1 CLIP zero-shot</cell></row><row><cell>D.2 End-to-end fine-tuning</cell></row></table><note>Two important experimental details for end-to-end fine-tuning are as follows:</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">D ref and D shift are sometimes referred to as in-distribution (ID) and out-of-distribution (OOD). In this work, we include evaluations of zero-shot models, which are not trained on data from the reference distribution, so referring to D ref would be imprecise. For clarity, we avoid the ID/OOD terminology.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Two linear classifiers fine-tuned on the same data converge to similar solutions, resulting in negligible diversity. As a stronger baseline, we fine-tune classifiers on different subsets of ImageNet, with half of the data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Anders Andreassen, Tim Dettmers, Jesse Dodge, Katie Everett, Samir Gadre, Ari Holtzman, Sewon Min, Mohammad Norouzi, Nam Pho, Ben Poole, Sarah Pratt, Alec Radford, Jon Shlens, and Rohan Taori for helpful discussions and draft feedback, Hyak at UW for computing support, Rosanne Liu for fostering the collaboration, and Basil Mustafa for providing an earlier version of the mapping between JFT and ImageNet classes. This work is in part supported by NSF IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543 and gifts from Allen Institute for Artificial Intelligence.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7.3 BASIC</head><p>We apply WiSE-FT to BASIC <ref type="bibr" target="#b76">[77]</ref>, fine-tuning both the image and text encoder with a contrastive loss on half of the ImageNet training data, as in Pham et al. <ref type="bibr" target="#b76">[77]</ref>. Results are shown in <ref type="figure">Figure 24</ref> and <ref type="table">Tables 11 and  12</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Diversity measures</head><p>Let S = {(x (i) , y (i) ), 1 ? i ? N } be a classification set with input data x (i) and labels y (i) ? {1, ..., C}, where C is the number of classes. A classifier f is a function that maps inputs x to logits f (x) ? R C , yielding predictions? = arg max 1?c?C f (x) c . We consider measures of diversity M(f, g, S) between two classifiers f and g and the dataset S. For simplicity,?</p><p>(i) f is used to denote the predictions from classifier f given inputs x (i) (and similarly for g).</p><p>Prediction Diversity (PD). One of the most intuitive ways to measure diversity between pairs of classifiers is to compute the fraction of samples where they disagree while one is correct <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b90">91]</ref>. Formally, the prediction diversity PD is defined as:</p><p>where</p><p>Cohen's Kappa Complement (CC). Cohen's kappa coefficient is a measure of agreement between two annotators <ref type="bibr" target="#b67">[68]</ref>. Here, we use it's complement as a diversity measure between two classifiers:</p><p>where p e is the expected agreement between the classifiers and p o is the empirical probability of agreement.</p><p>Formally, if n f,k is the number of samples where classifier f predicted label k (i.e. n f,k = 1?i?N 1[? i f = k]), then:</p><p>KL Divergence (KL). The Kullback-Leibler divergence measures how different a probability distribution is from another. Let p</p><p>for a classifier f , and let p</p><p>f,c be the probability assigned to class c. We consider the average KL-divergence over all samples as a diversity measure:</p><p>Centered Kernel Alignment Complement (CKAC). CKA is a similarity measure that compares two different sets of high-dimensional representations <ref type="bibr" target="#b50">[51]</ref>. It is commonly used for comparing representations of two neural networks, or determining correspondences between two hidden layers of the same network. CKA measures the agreement between two matrices containing the pair-wise similarities of all samples in a dataset, where each matrix is constructed according to the representations of a model. More formally, let S ? R N ?d denote the d-dimensional features for all samples in a dataset S, pre-processed to center the columns. For two models f and g yielding similarity matrices S f and S g , CKA is defined as:</p><p>where ||S|| F denotes the Frobenius norm of the matrix S. Larger CKA values indicate larger similarities between the representations of the two models, and thus, smaller diversity. We define the diversity measure CKAC as:</p><p>Note that CKAC is computationally expensive to compute for large datasets. For this reason, in our experiments with distributions larger than 10,000 samples, we randomly sample 10,000 to compute this measure.</p><p>Diversity across different architectures We extend <ref type="figure">Figure 5</ref> to show results for all combinations of diversity measures, datasets, and CLIP models. Similarly to before, the baselines compares models with the same encoder, with two linear classifiers trained on different subsets of ImageNet with half of the data. Results are shown in <ref type="figure">Figures 27-30</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Better fine-tuning by reducing representational collapse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshat</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=OQ08SN70M1V" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Michael A Alcorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengfei</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shinn</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1811.11553" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The evolution of out-ofdistribution robustness throughout fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2106.15831" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An empirical comparison of voting classification algorithms: Bagging, boosting, and variants. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kohavi</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/article/10.1023/A:1007515423169</idno>
		<ptr target="https://link.springer.com/article/10.1023/A:1007515423169" />
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The iwildcam 2021 competition dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arushi</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elijah</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vighnesh</forename><surname>Birodkar</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2105.03494" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR) FGVC8 Workshop</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wild patterns: Ten years after the rise of adversarial machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1712.03141" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igino</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Nedim?rndi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roli</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1708.06131" />
	</analytic>
	<monogr>
		<title level="m">Joint European conference on machine learning and knowledge discovery in databases</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simran</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brunskill</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2108.07258" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<ptr target="https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bagging predictors. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/article/10.1007/BF00058655</idno>
		<ptr target="https://link.springer.com/article/10.1007/BF00058655" />
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.14165" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Functional map of the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Christie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Fendley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mukherjee</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1711.07846" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sammy</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1311.3618" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Certified adversarial robustness via randomized smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elan</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1902.02918" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Underspecification presents challenges for credibility in modern machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Alexander D&amp;apos;amour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Alipanahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Deaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2011.03395" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/5206848" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Virtex: Learning visual representations from textual annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.06666" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1708.04552" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/3-540-45014-9_1</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/3-540-45014-9_1" />
	</analytic>
	<monogr>
		<title level="m">International workshop on multiple classifier systems</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.11929" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring the landscape of spatial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1712.02779" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust physical-world attacks on deep learning visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Earlence</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atul</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadayoshi</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1707.08945" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mansheej</forename><surname>Gintare Karolina Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepideh</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kharaghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.15110" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Linear mode connectivity and the lottery ticket hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolina</forename><surname>Gintare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carbin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1912.05671" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S002200009791504X" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The elements of statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer series in statistics</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brendel</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1811.12231" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalisation in humans and deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Carlos R Medina Temme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Heiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wichmann</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1808.08750" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">No one representation to rule them all: Overlapping features of training methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Gontijo-Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin D</forename><surname>Cubuk</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2007.01434" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">No one representation to rule them all: overlapping features of training methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Gontijo-Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2110.12899" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Qualitatively characterizing neural network optimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxe</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1412.6544" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1706.04599" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1512.03385" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1903.12261" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">AugMix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1912.02781" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.16241" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1907.07174" />
		<title level="m">Natural adversarial examples. Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Optax: composable gradient transformation and optimisation, in jax!</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eren</forename><surname>Sezener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Hennigan</surname></persName>
		</author>
		<ptr target="http://github.com/deepmind/optax" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ensembles and cocktails: Robust finetuning for natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Michael</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=qXucB21w1C3" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021 Workshop on Distribution Shifts</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1503.02531" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS) Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The random subspace method for constructing decision forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam</forename><surname>Tin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/709601" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1803.05407" />
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Hongler</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1806.07572" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duerig</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2102.05918" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1911.03437" />
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1612.00796" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<publisher>PNAS</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">WILDS: A benchmark of in-the-wild distribution shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Michael</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">Lanas</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irena</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Stavness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imran</forename><forename type="middle">S</forename><surname>Earnshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Kundaje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2012.07421" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1912.11370" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Similarity of neural network representations revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1905.00414" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1805.08974" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/6755945" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Fine-tuning distorts pretrained features and underperforms out-of-distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robbie</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=UYneFzXSJWh" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Calibrated ensembles: A simple way to mitigate ID-OOD accuracy tradeoffs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=dmDE-9e9F_x" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021 Workshop on Distribution Shifts</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ludmila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher J</forename><surname>Kuncheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whitaker</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1022859003006</idno>
		<ptr target="https://doi.org/10.1023/A:1022859003006" />
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1612.01474" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Rethinking the hyperparameters for fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2002.11770" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1608.03983" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bkg6RiCqY7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Harder or different? a closer look at distribution shift in dataset reproduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangyun</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Nott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Todeschini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Vahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<ptr target="http://www.gatsby.ucl.ac.uk/~balaji/udl2020/accepted-papers/UDL2020-paper-101.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML) Workshop on Uncertainty and Robustness in Deep Learning, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">How do quadratic regularizers prevent catastrophic forgetting: The role of interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekdeep</forename><surname>Singh Lubana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puja</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Dick</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2102.02805" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Analyzing monotonic linear interpolation in neural network loss landscapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grosse</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2104.11044" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1706.06083" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Merging models with fisher-weighted averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2111.09832" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Catastrophic interference in connectionist networks: The sequential learning problem. Psychology of Learning and Motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0079742108605368" />
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mchugh</surname></persName>
		</author>
		<title level="m">Interrater reliability: the kappa statistic. Biochemia medica</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The effect of natural distribution shift on question answering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Krauth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.14444" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Accuracy on the line: on the strong correlation between out-ofdistribution and in-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiori</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2107.04649" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1906.02629" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Deep ensembles for low-data transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr? Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.06866" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">What is being transferred in transfer learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanie</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2008.11687" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1803.02999" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Can you trust your model&apos;s uncertainty? evaluating predictive uncertainty under dataset shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Ovadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1906.02530" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1912.01703" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Combined scaling for zero-shot transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2111.10050" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli B Juditsky</forename><surname>Polyak</surname></persName>
		</author>
		<idno type="DOI">https:/epubs.siam.org/doi/abs/10.1137/0330046?journalCode=sjcodc</idno>
		<ptr target="https://epubs.siam.org/doi/abs/10.1137/0330046?journalCode=sjcodc" />
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">New method of stochastic approximation type. Automation and remote control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Teodorovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polyak</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Dataset shift in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Qui?onero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwaighofer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Mit Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/better-language-models/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2103.00020" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Do ImageNet classifiers generalize to ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1902.10811" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Efficient estimations from a slowly convergent robbins-monro process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ruppert</surname></persName>
		</author>
		<ptr target="https://ecommons.cornell.edu/handle/1813/8664" />
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Provably robust deep learning via adversarially trained smoothed classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Hadi Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Razenshteyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bubeck</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1906.04584" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Learning visual representations with caption annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Bulent Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2008.01392" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Adversarial training for free!</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldstein</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1904.12843" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Do image classifiers generalize across time?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1906.02168" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Evaluating machine accuracy on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horia</forename><surname>Mania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/shankar20c/shankar20c.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1403.6382" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">The sources of increased accuracy for two proposed boosting algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Skalak</surname></persName>
		</author>
		<ptr target="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.40.2269&amp;rep=rep1&amp;type=pdf" />
	</analytic>
	<monogr>
		<title level="m">American Association for Artificial Intelligence (AAAI), Integrating Multiple Learned Models Workshop</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Diverse ensembles improve calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asa</forename><forename type="middle">Cooper</forename><surname>Stickland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2007.04206" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML) Workshop on Uncertainty and Robustness in Deep Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1707.02968" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Scalability in perception for autonomous driving: Waymo open dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1912.04838" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1512.00567" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v97/tan19a/tan19a.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Measuring robustness to natural distribution shifts in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2007.00644" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<ptr target="https://people.csail.mit.edu/torralba/publications/datasets_cvpr11.pdf" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tram?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1705.07204" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1905.13549" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Learning neural network subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Horton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rastegari</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2102.10472" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Sun database: Exploring a large collection of scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliva</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/article/10.1007/s11263-014-0748-y</idno>
		<ptr target="https://link.springer.com/article/10.1007/s11263-014-0748-y" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1911.04252" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Explicit inductive bias for transfer learning with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Li Xuhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davoine</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.01483" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Cold case: The lost mnist digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chhavi</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1905.10498" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Billion-scale semi-supervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>I Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahajan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1905.00546" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedemann</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.04200" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Lit: Zero-shot transfer with locked-image text tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2111.07991" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Lookahead optimizer: k steps forward, 1 step back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Michael R Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1907.08610" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Contrastive learning of medical visual representations from paired images and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langlotz</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.00747" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Learning to prompt for vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2109.01134" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Freelb: Enhanced adversarial training for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.11764" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Weight-space ensemble (end-to-end) CoOp CLIP zero-shot models Weight-space ensemble with ? = 0.5 CLIP fine-tuned with a linear classifier CLIP fine-tuned end-to-end</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">JFT) fine-tuned with a linear classifier ViT-H/14 (JFT) fine-tuned end-to-end Weight-space ensemble with ? = 0</title>
		<idno>ViT-H/14</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
				<title level="m">Distribution shifts Avg Avg IN (ref.) IN-V2 IN-R IN-Sketch ObjectNet IN-A shifts ref., shifts</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
				<title level="m">Avg Avg IN (ref.) IN-V2 IN-R IN-Sketch ObjectNet IN-A shifts ref., shifts</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
