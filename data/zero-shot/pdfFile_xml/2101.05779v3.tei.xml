<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 STRUCTURED PREDICTION AS TRANSLATION BETWEEN AUGMENTED NATURAL LANGUAGES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Paolini</surname></persName>
							<email>paoling@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Krone</surname></persName>
							<email>kronej@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ma</surname></persName>
							<email>jieman@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
							<email>aachille@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
							<email>ranubhai@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Xiang</surname></persName>
							<email>bxiang@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soatto</surname></persName>
							<email>soattos@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 STRUCTURED PREDICTION AS TRANSLATION BETWEEN AUGMENTED NATURAL LANGUAGES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.</p><p>2. We apply our framework to (1) joint entity and relation extraction; (2) named entity recognition;</p><p>(3) relation classification; (4) semantic role labeling; (5) coreference resolution; (6) event extraction; (7) dialogue state tracking (Sections 4 and 5). In all cases we achieve at least comparable results to the current state-of-the-art, and we achieve new state-of-the-art performance on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012).</p><p>3. We also train a single model simultaneously on all tasks (multi-task learning), obtaining comparable or better results as compared with single-task models (Section 5.1).</p><p>4. We show that, thanks to the improved transfer of knowledge about label semantics, we can significantly improve the performance in the few-shot regime over previous approaches (Section 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">STRUCTURED PREDICTION TASKS</head><p>Joint entity and relation extraction. Format and details for this task are provided in Section 3.</p><p>Named entity recognition (NER). This is an entity-only particular case of the previous task.</p><p>Relation classification. For this task, we are given an input sentence with head and tail entities and seek to classify the type of relation between them, choosing from a predefined set of relations. Since the head entity does not necessarily precede the tail entity in the input sentence, we add a phrase "The relationship between [ head ] and [ tail ] is" after the original input sentence. The output repeats this phrase, followed by the relation type. In the following example, the head and tail entities are "Carmen Melis" and "soprano" which have a voice type relation. Input: Born in Bologna, Orlandi was a student of the famous Italian [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ] and voice teacher [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis ] in Milan. The relationship between [ Carmen Melis</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Structured prediction refers to inference tasks where the output space consists of structured objects, for instance graphs representing entities and relations between them. In the context of natural language processing (NLP), structured prediction covers a wide range of problems such as entity and relation extraction, semantic role labeling, and coreference resolution. For example, given the input sentence "Tolkien's epic novel The Lord of the Rings was published in 1954-1955, years after the book was completed" we might seek to extract the following graphs (respectively in a joint entity and relation extraction, and a coreference resolution task): Most approaches handle structured prediction by employing task-specific discriminators for the various types of relations or attributes, on top of pre-trained transformer encoders such as BERT <ref type="bibr" target="#b10">(Devlin et al., 2019</ref>). Yet, this presents two limitations. First, a discriminative classifier cannot easily leverage latent knowledge that the pre-trained model may already have about the meaning (semantics) of task labels such as person and author. For instance, knowing that a person can write a book would greatly simplify learning the author relation in the example above. However, discriminative models are usually trained without knowledge of the label semantics (their targets are class numbers), thus preventing such positive transfer. Second, since the architecture of a discriminative model is adapted to the specific task, it is difficult to train a single model to solve many tasks, or to fine-tune a model from a task to another (transfer learning) without changing the task-specific components of the discriminator. Hence, our main question is: can we design a framework to solve different  <ref type="figure">Figure 1</ref>: Our TANL model translates between input and output text in augmented natural language, and the output is then decoded into structured objects. structured prediction tasks with the same architecture, while leveraging any latent knowledge that the pre-trained model may have about the label semantics?</p><p>In this paper, we propose to solve this problem with a text-to-text model, by framing it as a task of Translation between Augmented Natural Languages (TANL). <ref type="figure">Figure 1</ref> shows how the previous example is handled within our framework, in the case of three different structured prediction tasks. The augmented languages are designed in a way that makes it easy to encode structured information (such as relevant entities) in the input, and to decode the output text into structured information.</p><p>We show that out-of-the-box transformer models can easily learn this augmented language translation task. In fact, we successfully apply our framework to a wide range of structured prediction problems, obtaining new state-of-the-art results on many datasets, and highly competitive results on all other datasets. We achieve this by using the same architecture and hyperparameters on all tasks, the only difference among tasks being the augmented natural language formats. This is in contrast with previous approaches that use task-specific discriminative models. The choice of the input and output format is crucial: by using annotations in a format that is as close as possible to natural language, we allow transfer of latent knowledge that the pre-trained model has about the task, improving performance especially in a low-data regime. Nested entities and an arbitrary number of relations are neatly handled by our models, while being typical sources of complications for previous approaches. We implement an alignment algorithm to robustly match the structural information extracted from the output sentence with the corresponding tokens in the input sentence.</p><p>We also leverage our framework to train a single model to solve all tasks at the same time, and show that it achieves comparable or better results with respect to training separately on each task. To the best of our knowledge, this is the first model to handle such a variety of structured prediction tasks without any additional task-specific modules.</p><p>To summarize, our key contributions are the following.</p><p>1. We introduce TANL, a framework to solve several structure prediction tasks in a unified way, with a common architecture and without the need for task-specific modules. We cast structured prediction tasks as translation tasks, by designing augmented natural languages that allow us to encode structured information as part of the input or output. Robust alignment ensures that extracted structure is matched with the correct parts of the original sentence (Section 3).</p><p>5. We show that, while our model is purely generative (it outputs a sentence, not class labels), it can be evaluated discriminatively by using the output token likelihood as a proxy for the class score, resulting in more accurate predictions (Section 3 and Appendix A.3).</p><p>The code is available at https://github.com/amazon-research/tanl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Many classical methods for structured prediction (SP) in NLP are generalizations of traditional classification algorithms and include, among others, Conditional Random Fields <ref type="bibr" target="#b29">(Lafferty et al., 2001)</ref>, Structured Perceptron <ref type="bibr" target="#b8">(Collins, 2002)</ref>, and Structured Support Vector Machines <ref type="bibr" target="#b50">(Tsochantaridis et al., 2004)</ref>. More recently, multiple efforts to integrate SP into deep learning methods have been proposed. Common approaches include placing an SP layer as the final layer of a neural net <ref type="bibr" target="#b9">(Collobert et al., 2011)</ref> and incorporating SP directly into DL models <ref type="bibr" target="#b12">(Dyer et al., 2015)</ref>.</p><p>Current state-of-the-art approaches for SP in NLP train a task-specific classifier on top of the features learned by a pre-trained language model, such as BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>. In this line of work, BERT MRC <ref type="bibr" target="#b35">(Li et al., 2019a)</ref> performs NER using two classification modules to predict respectively the first and the last tokens corresponding to an entity for a given input sentence. For joint entity and relation extraction, SpERT <ref type="bibr" target="#b14">(Eberts &amp; Ulges, 2019</ref>) uses a similar approach to detect token spans corresponding to entities, followed by a relation classification module. In the case of coreference resolution, many approaches employ a higher-order coreference model <ref type="bibr" target="#b31">(Lee et al., 2018)</ref> which learns a probability distribution over all possible antecedent entity token spans.</p><p>Also related to this work are papers on sequence-to-sequence (seq2seq) models for multi-task learning and SP. <ref type="bibr">Raffel et al. (2019)</ref> describe a framework to cast problems such as translation and summarization as text-to-text tasks in natural language, leveraging the transfer learning power of a transformer-based language model. Other sequence-to-sequence approaches solve specific structured prediction tasks by generating the desired output directly: see for example WDec (Nayak &amp; Ng, 2020) for entity and relation extraction, and SimpleTOD <ref type="bibr" target="#b22">(Hosseini-Asl et al., 2020)</ref> and <ref type="bibr">SOLOIST (Peng et al., 2020)</ref> for dialogue state tracking. Closer to us, <ref type="bibr">GSL (Athiwaratkun et al., 2020)</ref>, which introduced the term augmented natural language, showed early applications of the generative approach in sequence labeling tasks such as slot labeling, intent classification, and named entity recognition without nested entities. Our approach is also related to previous works that use seq2seq approaches to perform parsing <ref type="bibr" target="#b51">(Vinyals et al., 2015;</ref><ref type="bibr" target="#b13">Dyer et al., 2016;</ref><ref type="bibr" target="#b7">Choe &amp; Charniak, 2016;</ref><ref type="bibr">Rongali et al., 2020)</ref>, with the main difference that we propose a general framework that uses augmented natural languages as a way to unify multiple tasks and exploit label semantics. In some cases (e.g., relation classification), our output format resembles that of a question answering task <ref type="bibr">(McCann et al., 2018)</ref>. This paradigm has recently proved to be effective for some structured prediction tasks, such as entity and relation extraction and coreference resolution <ref type="bibr" target="#b37">(Li et al., 2019c;</ref><ref type="bibr">Zhao et al., 2020;</ref>. Additional task-specific prior work is discussed in Appendix A.</p><p>Finally, TANL enables easy multi-task structured prediction (Section 5.1). Recent work has highlighted benefits of multi-task learning <ref type="bibr" target="#b5">(Changpinyo et al., 2018)</ref> and transfer learning <ref type="bibr" target="#b53">(Vu et al., 2020)</ref> in NLP, especially in low-resource scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>We frame structured prediction tasks as text-to-text translation problems. Input and output follow specific augmented natural languages that are appropriate for a given task, as shown in <ref type="figure">Figure 1</ref>. In this section, we describe the format design concept and the decoding procedure we use for inference.</p><p>Augmented natural languages. We use the joint entity and relation extraction task as our guiding example for augmented natural language formats. Given a sentence, this task aims to extract a set of entities (one or more consecutive tokens) and a set of relations between pairs of entities. Each predicted entity and relation has to be assigned to an entity or a relation type. In all the datasets considered, the relations are asymmetric; i.e., it is important which entity comes first in the relation (the head entity) and which comes second (the tail entity). Below is the augmented natural language designed for this task (also shown in <ref type="figure">Figure 1</ref>): Specifically, the desired output replicates the input sentence and augments it with patterns that can be decoded into structured objects. For this task, each group consisting of an entity and possibly some relations is enclosed by the special tokens [ ]. A sequence of |-separated tags describes the entity type and a list of relations in the format "X = Y", where X is the relation type, and Y is another entity (the tail of the relation). Note that the objects of interest are all within the enclosed patterns "[ . . . | . . . ]". However, we replicate all words in the input sentence, as it helps reduce ambiguity when the sentence contains more than one occurrence of the same entity. It also improves learning, as shown by our ablation studies (Section 5.3 and Appendix B). In the target output sentence, entity and relation types are described in natural words (e.g. person, location) -not abbreviations such as PER, LOC -to take full advantage of the latent knowledge that a pre-trained model has about those words.</p><p>For certain tasks, additional information can be provided as part of the input, such as the span of relevant entities in semantic role labeling or coreference resolution (see <ref type="figure">Figure 1</ref>). We detail the input/output formats for all structured prediction tasks in Section 4.</p><p>Nested entities and multiple relations. Nested patterns allow us to represent hierarchies of entities. In the following example from the ADE dataset, the entity "lithium toxicity" is of type disease, and has a sub-entity "lithium" of type drug. The entity "lithium toxicity" is involved in multiple relations: one of type effect with the entity "acyclovir", and another of type effect with the entity "lithium". In general, the relations in the output can occur in any order. Decoding structured objects. Once the model generates an output sentence in an augmented natural language format, we decode the sentence to obtain the predicted structured objects, as follows.</p><p>1. We remove all special tokens and extract entity types and relations, to produce a cleaned output.</p><p>If part of the generated sentence has an invalid format, that part is discarded. 2. We match the input sentence and the cleaned output sentence at the token levels using the dynamic programming (DP) based Needleman-Wunsch alignment algorithm <ref type="bibr">(Needleman &amp; Wunsch, 1970)</ref>. We then use this alignment to identify the tokens corresponding to entities in the original input sentence. This process improves the robustness against potentially imperfect generation by the model, as shown by our ablation studies (Section 5.3 and Appendix B). 3. For each relation proposed in the output, we search for the closest entity that exactly matches the predicted tail entity. If such an entity does not exist, the relation is discarded. 4. We discard entities or relations whose predicted type does not belong to the dataset-dependent list of types.</p><p>To better explain the DP alignment in step 2, consider the example below where the output contains a misspelled entity word, "Aciclovir" (instead of "acyclovir"). The cleaned output containing the word "Aciclovir", tokenized as "A-cicl-o-vir", is matched to "a-cycl-o-vir" in the input, from which we deduce that it refers to "acyclovir".</p><p>Generated output: Six days after starting [ Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir | drug ] she exhibited signs of [ [ lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium | drug ] toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity | disease | effect = Aciclovir | effect = lithium ]. Cleaned output: Six days after starting Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir Aciclovir she exhibited signs of lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity .</p><p>Multi-task learning. Our method naturally allows us to train a single model on multiple datasets that can cover many structured prediction tasks. In this setting, we add the dataset name followed by the task separator : (for example, "ade :") as a prefix to each input sentence.</p><p>Categorical prediction tasks. For tasks such as relation prediction, where there is a limited number of valid outputs, an alternative way to perform classification is to compute class scores of all possible outputs and predict the class with the highest score. We demonstrate that we can use the output sequence likelihood as a proxy for such score. This method offers a more robust way to perform the evaluation in low resource scenarios where generation can be imperfect (see Appendix A.3). This approach is similar to the method proposed by dos  for ranking with language models.</p><p>Semantic role labeling (SRL). Here we are given an input sentence along with a predicate, and seek to predict a list of arguments and their types. Every argument corresponds to a span of tokens that correlates with the predicate in a specific manner (e.g. subject, location, or time). The predicate is marked in the input, whereas arguments are marked in the output and are assigned an argument type. In the following example, "sold" is the predicate of interest. Event extraction. This task requires extracting (1) event triggers, each indicating the occurrence of a real-world event and (2) trigger arguments indicating the attributes associated with each trigger. In the following example, there are two event triggers, "attacked" of type attack and "injured" of type injury. We perform trigger detection using the same format as in NER, as shown below. To perform argument extraction, we consider a single trigger as input at a time. We mark the trigger (with its type) in the input, and we use an output format similar to joint entity and relation extraction. Below, we show an argument extraction example for the trigger "attacked", where two arguments need to be extracted, namely, "Two soldiers" of type target and "yesterday" of type attack time. Coreference resolution. This is the task of grouping individual text spans (mentions) referring to the same real-world entity. For each mention that is not the first occurrence of a group, we reference with the first mention. In the following example, "his" refers to "Barack Obama" and is marked as Dialogue state tracking (DST). Here we are given as input a history of dialogue turns, typically between a user (trying to accomplish a goal) and an agent (trying to help the user). The desired output is the dialogue state, consisting of a value for each key (or slot name) from a predefined list. In the input dialogue history, we add the prefixes "[ user ] :" and "[ agent ] :" to delineate user and agent turns, respectively. Our output format consists of a list of all slot names with their predicted values. We add "[ belief ]" delimiters to help the model know when to stop generating the output sequence. We tag slots that are not mentioned in the dialogue history with the value "not given" (we do not show them in the example below, for brevity). </p><formula xml:id="formula_0">[</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we show that our TANL framework, with the augmented natural languages outlined in Section 4, can effectively solve the structured prediction tasks considered and exceeds the previous state of the art on multiple datasets.</p><p>All our experiments start from a pre-trained T5-base model <ref type="bibr">(Raffel et al., 2019)</ref>. To keep our framework as simple as possible, hyperparameters are the same across all experiments, except for some dataset-specific ones, such as the maximum sequence length. Details about the experimental setup, datasets, and baselines are described in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SINGLE-TASK AND MULTI-TASK EXPERIMENTS</head><p>We use three data settings in our experiments: (1) single dataset, (2) multiple datasets for the same task (multi-dataset), and (3) all datasets across all tasks (multi-task). <ref type="table" target="#tab_8">Table 1</ref> shows the results. 1 With the single-task setup, we achieve state-of-the-art performance on the following datasets: ADE, NYT, and ACE2005 (joint entity and relation extraction), FewRel and TACRED (relation classification), CoNLL-2005 and CoNLL-2012 (semantic role labeling). For example, we obtain a +6.2 absolute improvement in F1 score on the NYT dataset over the previous state of the art. Interestingly, this result is higher than the performance of models that use ground-truth entities to perform relation extraction, such as REDN <ref type="bibr" target="#b33">(Li &amp; Tian, 2020)</ref>, which achieves a relation F1 score of 89.8. In coreference resolution, TANL performs similarly to previous approaches that employ a BERT-base model, except for CorefQA . To the best of our knowledge, ours is the first endto-end approach to coreference resolution not requiring a separate mention proposal module and not enforcing a maximum mention length.</p><p>For other datasets, we obtain a competitive performance within a few points of the best baselines. We highlight that our approach uses a single model architecture that can be trained to perform any of the tasks without model modification. This is in stark contrast with typical discriminative models, which tend to be task-specific, as can be seen from <ref type="table" target="#tab_8">Table 1</ref>.</p><p>In fact, under this unified framework, a single model can be trained to perform multiple or all tasks at once, with the performance being on par or even better than the single-task setting. In particular, when the dataset sizes are small such as in ADE or CoNLL04, we obtain sizable improvements and SpERT <ref type="bibr" target="#b14">(Eberts &amp; Ulges, 2019)</ref> 88.9 71.5 89.3 78.8 DyGIE <ref type="bibr" target="#b54">(Luan et al., 2019)</ref> 88.4 63.2 MRC4ERE <ref type="bibr">(Zhao et al., 2020)</ref> 88.9 71.9 85.5 62.1 RSAN <ref type="bibr" target="#b59">(Yuan et al., 2020)</ref> 84 ). The only case where our multi-task model has notably lower scores is coreference resolution, where the input documents are much longer than in the other tasks. Since the maximum sequence length in the multi-task experiment (512 tokens) is smaller than in the singledataset coreference experiment (1,536 tokens for input and 2,048 for output), the input documents need to be split into smaller chunks, and this hurts the model's ability to connect multiple mentions of the same entity across different chunks. From the multi-task experiment, we leave out all datasets based on ACE2005 except for event extraction due to overlap between train and test splits for different tasks. We discuss our experiments in more detail in Appendix A.</p><p>All results presented in this paper are obtained from a pre-trained T5-base model. In principle, any pre-trained generative language model can be used, such as BART <ref type="bibr" target="#b32">(Lewis et al., 2020)</ref> or <ref type="bibr">GPT-2 (Radford et al., 2019)</ref>. It would be interesting to check whether these models are as capable as T5 (or even better) at learning to translate between our augmented languages. We leave this as a direction for future investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">LOW-RESOURCE SETTINGS</head><p>Multiple experiments suggest that TANL is data-efficient compared to other baselines. On the FewRel dataset, a benchmark for few-shot relation classification, our model outperforms the best baselines BERT EM and BERT EM +MTB <ref type="bibr" target="#b10">(Devlin et al., 2019;</ref><ref type="bibr" target="#b48">Soares et al., 2019)</ref>, where the MTB version uses a large entity-linked text corpus for pre-training. On the TACRED relation classification dataset, our model also improves upon the best baselines (from 71.5 to 71.9). While TACRED is not specifically a few-shot dataset, we observe that there are many label types that rarely appear in the training set, some of them having less than 40 appearances out of approximately 70,000 training label instances. We show the occurrence statistics for all label types in the appendix (Table 3), demonstrating that the dataset is highly imbalanced. Nonetheless, we find that our model performs well, even on instances involving scarce label types. This ability distinguishes our models from other few-shot approaches such as prototypical networks <ref type="bibr" target="#b47">(Snell et al., 2017)</ref> or matching networks <ref type="bibr" target="#b52">(Vinyals et al., 2016)</ref>, which are designed only for few-shot scenarios but do not scale well on real-world data which often contains a mix of high and low-resource label types.</p><p>Our low-resource study on the joint entity and relation extraction task also confirms that our approach is more data-efficient compared to other methods. We experiment on the CoNLL04 dataset, using only 0.8% (9 sentences) to 6% (72 sentences) of the training data. Our approach outperforms SpERT (a state-of-the-art discriminative model for joint entity and relation extraction) in this low-resource regime, whereas the performance is similar when using the full training set.</p><p>Thanks to the unified framework, we can easily train on a task, potentially with larger resources, and adapt to other low-resource end tasks (transfer learning). To show this, we train a model with a large dataset from joint entity and relation extraction (NYT) and fine-tune it on a limited portion of the CoNLL04 dataset <ref type="figure" target="#fig_0">(Figure 2)</ref>, obtaining a significant increase in performance (up to +9 relation F1).</p><p>Finally, in Appendix C we analyze how the size of the training dataset affects the number of generation errors of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ABLATION STUDIES</head><p>We conduct ablation studies to demonstrate that label semantics, augmented natural language format, and optimal alignment all contribute to the effectiveness of TANL <ref type="figure" target="#fig_0">(Figure 2b</ref>). Further details on these ablation studies can be found in Appendix B.</p><p>Numeric labels: To prevent the model from understanding the task through label semantics, we use numeric labels. This substantially hurts the performance, especially in a low-resource setting where transfer learning is more important. Abridged output: Second, to determine the impact of the augmented natural language format outlined in Section 4, we experiment with a format which does not repeat the entire input sentence. We find that this abridged format consistently hurts model performance, especially in low-resource scenarios. In other tasks, we generally find that a more natural-looking format usually performs better (see Appendix A.3). No DP alignment: We use exact word matching instead of the dynamic programming alignment described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION AND CONCLUSION</head><p>We have demonstrated that our unified text-to-text approach to structured prediction can handle all the considered tasks within a simple framework and offers additional benefits in low-resource settings. Unlike discriminative models common in the literature, TANL is generative as it translates from an input to an output in augmented natural languages. These augmented languages are flexible and can be designed to handle a variety of tasks, some of which are complex and previously required sophisticated prediction modules. By streamlining all tasks to be compatible with a single model, multi-task learning becomes seamless and yields state-of-the-art performance for many tasks.</p><p>Generative models, and in particular sequence-to-sequence models, have been used successfully in many NLP problems such as machine translation, text summarization, etc. These tasks involve mappings from one natural language input to another natural language output. However, the use of sequence modeling for structured prediction has received little consideration. This is perhaps due to the perception that the generative approach is too unconstrained and that it would not be a robust way to generate a precise output format that corresponds to structured objects, or that it may add an unnecessary layer of complexity with respect to discriminative models. We demonstrate that this is quite the opposite. The generative approach can easily handle disparate tasks, even at the same time, by outputting specific structures appropriate for each task with little, if any, format error.</p><p>We note that one drawback of the current generative approach is that the time complexity for each token generation is O(L 2 ) where L is the sentence length. However, there have been recent advances in the attention mechanism that reduce the complexity to O(L log L) as in Reformer <ref type="bibr" target="#b28">(Kitaev et al., 2020)</ref>, or to O(L) as in Linformer . Incorporating these techniques in the future can significantly reduce computation time and allow us to tackle more complex tasks, as well as improve on datasets with long input sequences such as in coreference resolution.</p><p>Based on our findings, we believe that generative modeling is highly promising but has been an understudied topic in structured prediction. Our findings corroborate a recent trend where tasks typically treated with discriminative methods have been successfully solved using generative approaches <ref type="bibr">(Brown et al., 2020;</ref><ref type="bibr" target="#b23">Izacard &amp; Grave, 2020;</ref><ref type="bibr" target="#b44">Schick &amp; Sch?tze, 2020)</ref>. We hope our results will foster further research in the generative direction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NAACL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EXPERIMENTAL SETUP, DATASETS, AND BASELINES</head><p>In all experiments, we fine-tune a pre-trained T5-base model <ref type="bibr">(Raffel et al., 2019)</ref>, to exploit prior knowledge of the natural language. The family of T5 models was specially designed for downstream text-to-text tasks, making them suitable for our needs. The T5-base model has about 220 million parameters. For comparison, both encoder and decoder are similar in size to BERT-base <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>. We use the implementation of HuggingFace's Transformers library <ref type="bibr" target="#b57">(Wolf et al., 2019)</ref>.</p><p>To keep our framework as simple as possible, hyperparameters are the same across the majority of our experiments. We use: 8 V100 GPUs with a batch size of 8 per GPU; the AdamW optimizer <ref type="bibr" target="#b27">(Kingma &amp; Ba, 2015;</ref><ref type="bibr" target="#b39">Loshchilov &amp; Hutter, 2019)</ref>; linear learning rate decay starting from 0.0005; maximum input/output sequence length equal to 256 tokens at training time (longer sequences are truncated), except for relation classification, coreference resolution, and dialogue state tracking (see below). The number of fine-tuning epochs is adjusted depending on the size of the dataset, as described later. With these settings, one fine-tuning step takes approximately 0.8 seconds. This translates into 15 seconds per epoch for the (relatively small) CoNLL04 dataset (joint entity-relation extraction) and 16 minutes per epoch for the (much larger) OntoNotes dataset (NER). At inference time, we employ beam search with 8 beams, and we adjust the maximum sequence length depending on the length of the sentences in each dataset. Note that beam search is not an essential part of our framework, as we find that greedy decoding gives almost identical results.</p><p>In the rest of this section, we describe datasets and baselines for each structured prediction task, as well as additional insights on particular experiments. Results of all experiments are given in <ref type="table" target="#tab_8">Table 1</ref>. Unless otherwise specified, micro-F1 scores are reported. Most experiments are run more than once, as described below, and the average result is reported. <ref type="table" target="#tab_16">Table 5</ref> shows input-output examples from different datasets.</p><p>For the multi-task experiment, we train for 50 epochs on 80 GPUs, with a batch size of 3 per GPU. The maximum input/output sequence length is set to 512 for all tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 JOINT ENTITY-RELATION EXTRACTION</head><p>Datasets. We experiment on the following datasets: CoNLL04 <ref type="bibr" target="#b41">(Roth &amp; Yih, 2004)</ref>, ADE (Gurulingappa et al., 2012), <ref type="bibr">NYT (Riedel et al., 2010)</ref>, and ACE2005 <ref type="bibr" target="#b55">(Walker et al., 2006</ref>).</p><p>? The CoNLL04 dataset consists of sentences extracted from news articles, with four entity types (location, organization, person, other) and five relation types (work for, kill, organization based in, live in, located in). As in previous work, we use the training (922 sentences), validation (231 sentences), and test (288 sentences) split by <ref type="bibr" target="#b19">Gupta et al. (2016)</ref>. We train for 200 epochs and report our test results averaged over 10 runs.</p><p>? The ADE dataset consists of 4, 272 sentences extracted from medical reports, with two entity types (drug, disease) and a single relation type (effect). This dataset has sentences with nested entities. As in previous work, we conduct a 10-fold cross-validation and report the average macro-F1 results across all 10 splits (except for the multi-task experiment, which is carried out once and uses the first split of the ADE dataset). We train for 200 epochs.</p><p>? The NYT dataset <ref type="bibr" target="#b62">(Zeng et al., 2018)</ref> is based on the New York Times corpus and was automatically labeled with distant supervision by <ref type="bibr">Riedel et al. (2010)</ref>. We use the preprocessed version of <ref type="bibr" target="#b60">Yu et al. (2019)</ref>. This dataset has three entity types (location, organization, person) and 24 relation types (such as place of birth, nationality, company). It consists of 56,195 sentences for training, 5,000 for validation, and 5,000 for testing. We train for 50 epochs and report our test results averaged over 5 runs.</p><p>? The ACE2005 dataset is derived from the ACE2005 corpus <ref type="bibr" target="#b55">(Walker et al., 2006)</ref> and consists of sentences from a variety of domains, including news and online forums. We use the processing code of <ref type="bibr" target="#b54">Luan et al. (2019)</ref>. After filtering out the sentences without entities, we get 7,477 sentences for training, 1789 for validation, and 1517 for testing. It has seven entity types (location, organization, person, vehicle, geographical entity, weapon, facility) and six relation types (PHYS, ART, ORG-AFF, GEN-AFF, PER-SOC, PART-WHOLE).  The natural labels we use for the relation types are: physical, artifact, employer, affiliation, social, part of. We train for 100 epochs and report our test results averaged over 10 runs.</p><p>For all single-dataset experiments, <ref type="table" target="#tab_11">Table 2</ref> shows the number of training epochs, the number of runs, and the standard deviations, in addition to the average results, which are already reported in <ref type="table" target="#tab_8">Table 1</ref>.</p><p>Baselines. SpERT <ref type="bibr" target="#b14">(Eberts &amp; Ulges, 2019</ref>) is a BERT-based model which performs span classification and then relation classification. Multi-turn QA <ref type="bibr" target="#b37">(Li et al., 2019c)</ref> casts the problem as a multi-turn question answering task. ETL-Span <ref type="bibr" target="#b60">(Yu et al., 2019)</ref> uses BiLSTM and decomposes the problem into two tagging sub-problems: head entity extraction, and tail entity and relation extraction. WDec (Nayak &amp; Ng, 2020) uses an encoder-decoder architecture to directly generate a list of relation tuples. MRC4ERE <ref type="bibr">(Zhao et al., 2020)</ref> improves on the question answering approach by leveraging a diverse set of questions. RSAN <ref type="bibr" target="#b59">(Yuan et al., 2020</ref>) is a sequence labeling approach which utilizes a relation-aware attention mechanism.</p><p>Low-resource experiments. As outlined in Section 5.2, we experiment on the CoNLL04 dataset with only a limited portion of the training set available and plot our results in <ref type="figure" target="#fig_1">Figure 3</ref>. Comparison is made with SpERT <ref type="figure" target="#fig_0">(Eberts &amp; Ulges, 2019)</ref>, a state-of-the-art discriminative model. TANL performs better than SpERT with fewer data, especially on the more complex task of relation extraction (right plot). We also show our method's performance with preliminary fine-tuning on the NYT dataset for one epoch, which significantly improves the performance on both entity and relation extraction. To account for the small dataset size, we fine-tune on CoNLL04 for 2,000 epochs (10? the number of epochs we use to train on the full CoNLL04 dataset). For a fair comparison, we train SpERT for 20, 200, and 2000 epochs (respectively 1?, 10?, and 100? the number of epochs suggested in the paper), and report the best result among the three, which is always obtained with 200 epochs. We plot mean and standard deviation over 10 runs (each model being fine-tuned on the same 10 subsets of the training set and evaluated on the entire test set). For reference, the smallest training set has only 9 sentences (0.8% of the total), effectively consisting in a few-shot learning scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-dataset experiments.</head><p>We train a single model on all four datasets for 20 epochs and report the average over 10 runs. We use a different split of the ADE dataset in each run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 NAMED ENTITY RECOGNITION</head><p>Datasets. We experiment on two flat NER datasets, CoNLL03 <ref type="bibr" target="#b42">(Sang &amp; Meulder, 2003)</ref> and <ref type="bibr">OntoNotes (Pradhan et al., 2013)</ref>, and two nested NER datasets, <ref type="bibr">GENIA (Ohta et al., 2002)</ref> and ACE2005 <ref type="bibr" target="#b55">(Walker et al., 2006)</ref>.</p><p>? For the CoNLL03 dataset <ref type="bibr" target="#b42">(Sang &amp; Meulder, 2003)</ref> we use the same processing and splits as <ref type="bibr" target="#b35">Li et al. (2019a)</ref>, resulting in 14,041 sentences for training, 3,250 for validation, and 3,453 for testing. This dataset has four entity types <ref type="bibr">(location, organization, person, miscellaneous)</ref>. We train for 50 epochs and report our test results averaged over 10 runs.</p><p>? The English OntoNotes dataset <ref type="bibr">(Pradhan et al., 2013)</ref> consists of 59,924 sentences for training, 8,528 for validation, and 8,262 for testing. It has 18 entity types (such as person, organization, date, percent). We train for 20 epochs and report our test results averaged over 10 runs. ? The GENIA dataset <ref type="bibr">(Ohta et al., 2002)</ref> consists of sentences from the molecular biology domain. As in previous work, we use the processing and splits of <ref type="bibr" target="#b16">Finkel &amp; Manning (2009)</ref> resulting in 14,824 sentences for training, 1,855 for validation, and 1,854 for testing. There are five entity types (protein, DNA, RNA, cell line, cell type). We train for 50 epochs and report our test results averaged over 10 runs. ? The ACE2005 dataset for nested NER is based on the ACE2005 corpus <ref type="bibr" target="#b55">(Walker et al., 2006)</ref>, but is different from the one used for joint entity-relation extraction. We use the same processing and splits of <ref type="bibr" target="#b35">Li et al. (2019a)</ref>, resulting in 7,299 sentences for training, 971 for validation, and 1,060 for testing. It has the same seven entity types as the ACE2005 dataset used for joint entity-relation extraction. We train for 50 epochs and report our test results averaged over 10 runs.</p><p>As for joint entity-relation extraction, <ref type="table" target="#tab_11">Table 2</ref> summarizes our setup and results (with standard deviations) for the single-dataset experiments.</p><p>Baselines. State-of-the-art results on popular NER datasets are mostly detained by BERT-MRC <ref type="bibr" target="#b35">(Li et al., 2019a)</ref> and BERT-MRC + DSC <ref type="bibr" target="#b36">(Li et al., 2019b)</ref>, which formulate the problem as a machine reading comprehension task, solved by asking multiple questions. ClozeCNN <ref type="bibr" target="#b1">(Baevski et al., 2019)</ref> leverages a cloze-driven pre-training. Seq2seq-BERT <ref type="bibr" target="#b49">(Strakov? et al., 2019</ref>) uses a seq2seq model to output the list of entity types. Second-best learning and decoding <ref type="bibr" target="#b46">(Shibuya &amp; Hovy, 2019)</ref> iteratively decodes nested entities starting from the outermost ones, using the Viterbi algorithm. For flat NER, our approach is similar to <ref type="bibr">GSL (Athiwaratkun et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-dataset experiments.</head><p>We train a single model on all four datasets for 10 epochs and report our results averaged over 5 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 RELATION CLASSIFICATION</head><p>Datasets. We experiment on FewRel <ref type="bibr" target="#b21">(Han et al., 2018)</ref> and TACRED <ref type="bibr" target="#b63">(Zhang et al., 2017)</ref>.</p><p>? FewRel consists of 100 relations with 7 instances for each relation. The standard evaluation for this benchmark uses few-shot N -way K-shot settings, which we follow. The entire dataset is split into train (64 relations), validation (16 relations) and test set (20 relations). We train our model on the meta-training set, which has no overlapping classes with the evaluation set. At evaluation time, given a support set and a query set on a new task, we fine-tune the model on the support set to learn the new task and evaluate on the query set. ? TACRED is a large-scale relation classification dataset with 106,344 examples (68,164 for training, 22,671 for validation, and 15,509 for testing), covering 41 relation types. We train for 5 epochs and report our test results averaged over 5 runs. The maximum input sequence length is set to 300, whereas the maximum output sequence length is set to 64 during training and 128 during inference.</p><p>Baselines. We compare our approach with the following two models in the literature. The first is BERT-pair <ref type="bibr" target="#b18">(Gao et al., 2019)</ref>, a sequence classification model based on BERT, which learns to optimize the scores indicating the relation between a query instance and other supporting instances for the same relation. The second is BERT EM + Matching the Blanks (MTB) <ref type="bibr" target="#b48">(Soares et al., 2019)</ref>. BERT EM uses entity markers indicating the start and the end of the head and tail entities in the input sentence. MTB is a pre-training based on an additional large corpus of relation data. Nevertheless, our model is able to outperform BERT EM +MTB in certain cases, such as the 5-way 1-shot setting on FewRel.</p><p>Augmented natural language formats. We experiment with many augmented natural language formats, as shown below: Carmen Melis" is the head entity. However, this format also does not perform as well, possibly because the meaning of the words head and tail are not fully understood in this context. Overall, the chosen format sounds the most natural out of all options and is closer to natural language, which we use as our guiding principle to design our augmented natural language formats.</p><p>TACRED results and label sparsity. A major factor for our state-of-the-art result on the TACRED dataset is the shared semantics across different labels, which is particularly beneficial in the case of sparse labels. In <ref type="table" target="#tab_13">Table 3</ref> we show the relation types in natural words, the number of training examples, which can be quite small, and the test recall (i.e., out of all ground truth relations for a given type, how many we predict correctly). We can see that even though some relation types such as date of birth have as little as 64 labels in the training set (less than 0.1% of the entire set), our model is able to correctly predict this relation type with recall 77.8%.</p><p>The ability to handle few-shot cases allows our approach to perform well in real-world data such as TACRED, where the labels can be highly imbalanced. As seen in <ref type="table" target="#tab_13">Table 3</ref>, only a few instances such as employee of, top members employees, title, and no relation dominate the majority of the training set (approximately 60,000 out of 68,000), where the rest can be considered scarce. Our model is different from other approaches specifically designed for few-shot scenarios in that it scales across different levels of data.</p><p>Few-shot experiments. For the FewRel dataset, we perform meta-training by training the model on the training set of FewRel for 1 epoch. During evaluation, we fine-tune the model on the support set for each episode for 2,500 epochs in the 1-shot cases, and for 500 epochs in the 5-shot cases.</p><p>Likelihood-based prediction. In relation classification, we aim to predict one class out of a pre-defined set of classes, so we can perform prediction by using sequence likelihoods as class scores. This helps improve the performance particularly in the case of few-shot scenarios, where the generation of label types can be imperfect since the model has seen only one or few instances of each type. With the likelihood prediction, we obtain a slight improvement across the board. For instance, we improve from an F1 score of 95.6 ? 4.8 to 96.4 ? 4.2 for the 5-way 5-shot case of FewRel. For TACRED, using the likelihood approach yields a smaller improvement, possibly due to the fact that the model can generate exact label types given enough training resources, unlike in the few-shot case. All our reported numbers on the FewRel and TACRED datasets are obtained by using this approach. Datasets. We use <ref type="bibr">CoNLL-2005</ref><ref type="bibr" target="#b4">(Carreras &amp; M?rquez, 2005</ref> and the CoNLL-2012 English subset of OntoNotes 5.0 <ref type="bibr">(Pradhan et al., 2013)</ref> in our experiments. See also <ref type="bibr" target="#b4">Carreras &amp; M?rquez (2005)</ref>; Pradhan et al. <ref type="bibr">(2012)</ref>. These tasks have highly specific label types, and their natural words might be cumbersome for training. Therefore, we use the raw label types from the original datasets as presented below.</p><p>? CoNLL-2005 focuses on the semantic roles given verb predicates. The argument notation is the following. V: verb; A0: acceptor; A1: thing accepted; A2: accepted from; A3: attribute; AM-MOD: modal; AM-NEG: negation.</p><p>? CoNLL-2012. The argument notation, taken from Pradhan et al. <ref type="formula">(2012)</ref>, is as follows.</p><p>Numbered arguments (A0-A5, AA): Arguments defining verb-specific roles. Their semantics depends on the verb and the verb usage in a sentence, or verb sense. The most frequent roles are A0 and A1. Commonly, A0 stands for the agent, and A1 corresponds to the patient or theme of the proposition. However, no consistent generalization can be made across different verbs or different senses of the same verb. PropBank takes the definition of verb senses from VerbNet, and for each verb and each sense defines the set of possible roles for that verb usage, called the roleset. The definition of rolesets is provided in the PropBank Frames files, made available for the shared task as an official resource to develop systems.</p><p>Adjuncts <ref type="formula">(</ref> References (R-): Arguments representing arguments realized in other parts of the sentence. The role of a reference is the same as the role of the referenced argument. The label is an R-tag prefixed to the label of the referent, e.g., R-A1.</p><p>Baselines. We compare our results with Dependency and Span SRL <ref type="bibr" target="#b38">(Li et al., 2019d)</ref>, which uses a Bi-LSTM with highway connection and biaffine scorers, and BERT-SRL <ref type="bibr" target="#b45">(Shi &amp; Lin, 2019)</ref>, BERT-based model which predicts the spans based on the contextual and positional embeddings.</p><p>Multi-dataset experiments. We train a single model on all datasets for 50 epochs and report our results averaged over 5 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 EVENT EXTRACTION</head><p>Datasets. We use the ACE2005 English event data <ref type="bibr" target="#b55">(Walker et al., 2006)</ref> in our experiments, following standard event extraction literature. We use the same split as previous work <ref type="bibr" target="#b24">(Ji &amp; Grishman, 2008;</ref><ref type="bibr" target="#b34">Li et al., 2013)</ref> with 529 documents for training, 30 for validation, and 40 for testing. Since the majority of event triggers and their corresponding arguments are within the same sentence, we perform the event extraction task only at the sentence level. We fine-tune our model for 50 epochs on this dataset.</p><p>Baselines. We compare our method with the following two baseline models in the literature. The first is J3EE (Nguyen &amp; Nguyen, 2019), a Bi-GRU based model that jointly performs event trigger detection, event mention detection, and event argument classification. J3EE performs event trigger detection and event mention detection as sequence tagging problems, and event argument classification as a classification problem, given any trigger and candidate argument pair. The second baseline is DyGIE++ <ref type="bibr" target="#b54">(Wadden et al., 2019)</ref>, a BERT based multi-task learning framework for the tasks of coreference resolution, relation extraction, named entity recognition, and event extraction. DyGIE++ enumerates all possible phrases within a sentence and predicts the best entity type and trigger type for each of these phrases. Argument roles are then predicted for each trigger and entity pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 COREFERENCE RESOLUTION</head><p>Datasets. We use the standard OntoNotes benchmark defined in the CoNLL-2012 shared task <ref type="bibr">(Pradhan et al., 2012)</ref>. It consists of 2,802 documents for training, 343 for validation, and 348 for testing, for a total of about one million words. Since documents can be large (up to 4,000 words), we split each document into partially overlapping chunks up to 1,024 words long <ref type="bibr">(and 128 or 196</ref> words for the multi-task experiment, respectively for training and for testing). At test time, we merge groups from different chunks if they have at least one mention in common in order to obtain document-level predictions. As in prior work, evaluation is done by computing the average F1 score of the three standard metrics for coreference resolution: MUC, B 3 , CEAF ?4 . We train for 100 epochs, with a maximum sequence length equal to 1,536 tokens for input and 2,048 for output, and a batch size of 1 per GPU.</p><p>Baselines. The e2e-coref model <ref type="bibr" target="#b30">(Lee et al., 2017)</ref> is among the first end-to-end approaches to coreference resolution. It considers all spans as potential mentions and learns a distribution over possible antecedents for each span. Higher-order c2f-coref <ref type="bibr" target="#b31">(Lee et al., 2018)</ref> iteratively refines span representations taking into account higher-order relations between mentions. BERT + c2f-coref <ref type="bibr" target="#b25">(Joshi et al., 2019)</ref> combines the previous approach with BERT. SpanBERT <ref type="bibr" target="#b26">(Joshi et al., 2020)</ref> introduces a new pretraining method which is designed to better represent and predict spans of text.</p><p>CorefQA  generate queries for each mention from a mention proposal network and uses a question answering framework to extract text spans of coreferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 DIALOGUE STATE TRACKING</head><p>Datasets. We use the MultiWOZ 2.1 (Eric et al., 2020) task oriented dialogue dataset in our experiments. It consists of 8,420 conversations for training, 1,000 for validation, and 999 for testing. We follow the pre-processing procedure put forward in <ref type="bibr" target="#b58">(Wu et al., 2019)</ref> for dialogue state tracking.</p><p>In addition, we remove the "police" and "hospital" domains from the training set since they are not present in the test set. Removing these two domains reduces the training set size from 8,420 to 7,904. We fine-tune for 100 epochs, with maximum sequence length set to 512 tokens. We train a single generative model that predicts the dialogue state for the entire dialogue history up to the current turn. Following prior work, we report the joint accuracy.</p><p>Baselines. We compare our performance on MultiWOZ 2.1 against SimpleTOD (Hosseini-Asl et al., 2020), the current state of the art for MultiWOZ dialogue state tracking. SimpleTOD uses a sequence to sequence approach based on the GPT-2 (Radford et al., 2019) language model. Unlike our approach, SimpleTOD is trained to jointly generate actions and responses as well as dialogue states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ABLATION STUDIES</head><p>As outlined in Section 5.3, we conduct ablation studies on the CoNLL04 dataset (joint entity and relation extraction) to demonstrate the importance of label semantics, natural output format, and optimal alignment. We compare TANL with the following three variations.</p><p>? Numeric labels: we use numbers (1, 2, 3, . . . ) to indicate entity and relation types in the output sentences, as in the following example. ? No alignment: we process output sentences without the alignment module. For each predicted entity or relation, we look for the first exact match in the input sentence (the entity or relation is discarded if no exact match is found). Results show that all three components (label semantics, natural output format, and alignment) positively contribute to the effectiveness of TANL. The impact of label semantics is not noticeable when using the full CoNLL04 training dataset (natural and numeric labels give similar F1 scores), but it becomes statistically relevant when using 50% of the training data, or less. On the other hand, the impact of alignment is higher when the training dataset is larger. Interestingly, for entity extraction (left plot of <ref type="figure">Figure 4</ref>), repeating the input sentence is more important than using natural labels, whereas the opposite is true for relation extraction (right plot).</p><p>From these experiments, we deduce that: (1) the model indeed uses latent knowledge about label semantics, especially when the amount of training data is low; (2) using a "natural" output format (which replicates the input sentence as much as possible) allows the model to make more accurate predictions, likely by encouraging the use of the entire input as context; (3) alignment helps in locating the correct entity spans in the input sentence, and in correcting mistakes made by the model when replicating the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ANALYSIS OF GENERATION ERRORS</head><p>The performance of TANL crucially depends on the quality of the generated output sentences. <ref type="figure" target="#fig_3">Figure 5</ref> shows how often the following kinds of generation errors occur on the CoNLL04 dataset.</p><p>? Reconstruction errors: the output sentence does not exactly replicate the input sentence. ? Format errors: the augmented natural language format is invalid. ? Entity errors: there is at least one relation whose predicted tail entity does not match any predicted entity. ? Label errors: there is at least one predicted entity or relation type that does not exactly match any of the dataset's possible types.</p><p>Reconstruction errors are by far the most common, but they are mitigated by our alignment step. When using the full CoNLL04 training dataset, other errors appear very infrequently; therefore, it is not necessary to add further post-processing steps to mitigate them. We perform this generation error analysis on the CoNLL04 because it is the smallest of the benchmarks we consider, and as a result, the generation errors on CoNLL04 are likely to be the most significant. Yet when training on only a limited portion of the training data, format and entity errors do occur. In this low-resource setting, TANL would benefit from additional post-processing. We leave the investigation of such post-processing strategies aimed at low-resource scenarios for future work. , this morning, about 20 workers wore or carried red union bandannas and held placards with messages like, "The Mayor Lied, There Goes Your Ride" and "On Strike." ACE2005 (entity-rel extraction) that is the very joyous town of palestine, west virginia, on the news that jessica lynch is eventually going to come home.</p><p>[ that that that that that that that that that that that that that that that that that | geographical entity ] is the very joyous </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Experiments on the CoNLL04 dataset. (a) Our model outperforms the previous state-of-the-art model SpERT, in low-resource scenarios. (b) Ablation studies where we remove label semantics (numeric labels), augmented natural language format (abridged output) or dynamic programming alignment (no DP alignment), and plot the score difference with the non-ablated TANL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Low-resource experiments on the CoNLL04 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>AM-): General arguments that any verb may take optionally. The following are the 13 types of adjuncts. AM-ADV: general-purpose; AM-CAU: cause; AM-DIR: direction; AM-DIS: discourse marker; AM-EXT: extent; AM-LOC: location; AM-MNR: manner; AM-MOD: modal verb; AM-NEG: negation marker; AM-PNC: purpose; AM-PRD: predication; AM-REC: reciprocal; AM-TMP: temporal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Percentage of output sentences presenting different kinds of errors, when training with a variable portion of the CoNLL04 training dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>c novel [ The Lord of the Rings ] was published in 1954-1955, book was completed. e Lord of the Rings | subject] e ] [ in 1954-1955 | temporal], book was completed. novel [ The Lord of the Rings | 54-1955, years after the [ book Rings ] was completed. person ]'s epic novel [ The Lord of the Rings | book | author = Tolkien ] was published in 1954-1955, years after the book was completed. entity and relation extraction Tolkien's epic novel [ The Lord of the Rings | subject] [ was published | predicate ] [ in 1954-1955 | temporal], years after the book was completed. Role Labeling [ Tolkien | head ]'s epic novel [ The Lord of the Rings | head ] was published in 1954-1955, years after the [ book | The Lord of the Rings ] was completed.</figDesc><table><row><cell>nguage Translation</cell><cell></cell><cell></cell><cell></cell><cell>Graph Extraction</cell></row><row><cell>[ Tolkien | Joint Semantic Coreference resolution</cell><cell cols="3">Graph Extraction Decoding</cell><cell>Tolkien person The Lord of the Ring author was published The Lord of the Ring book In 1954-1955 subject temporal</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Tolkien</cell><cell>The Lord of the Ring</cell></row><row><cell></cell><cell>Tolkien person</cell><cell>author</cell><cell>The Lord of the Ring book</cell><cell>refers to</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>book</cell></row><row><cell></cell><cell></cell><cell cols="2">was published</cell></row><row><cell>Decoding</cell><cell>subject</cell><cell></cell><cell>temporal</cell></row><row><cell></cell><cell cols="2">The Lord of the Ring</cell><cell>In 1954-1955</cell></row><row><cell></cell><cell>Tolkien</cell><cell></cell><cell>The Lord of the Ring</cell></row><row><cell></cell><cell></cell><cell></cell><cell>refers to</cell></row><row><cell></cell><cell></cell><cell>book</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1 arXiv:2101.05779v3 [cs.LG] 2 Dec 2021 Published as a conference paper at ICLR 2021 TANL Tolkien's epic novel The Lord of the Rings was published in 1954-1955, years after the book was completed. entity and relation extraction [ Tolkien | person ]'s epic novel [ The Lord of the Rings | book | author = Tolkien ] was published in 1954-1955, years after the book was completed. entity and relation extraction Tolkien's epic novel The Lord of the Rings [ was published ] in 1954-1955, years after the book was completed. Tolkien's epic novel [ The Lord of the Rings | subject ] [ was published | predicate ] [ in 1954-1955 | temporal ], years after the book was completed. Tolkien's epic novel The Lord of the Rings was published in 1954-1955, years after the book was completed. resolution [ Tolkien ]'s epic novel [ The Lord of the Rings ] was published in 1954-1955, years after the [ book | The Lord of the Rings ] was completed.</figDesc><table><row><cell>Augmented Natural Language Translation</cell><cell cols="3">Structure Extraction</cell></row><row><cell>Semantic role labeling Joint Joint Semantic role labeling</cell><cell>person Tolkien</cell><cell>author</cell><cell>book The Lord of the Rings</cell></row><row><cell></cell><cell></cell><cell cols="2">was published</cell></row><row><cell>Decoding</cell><cell>subject</cell><cell></cell><cell>temporal</cell></row><row><cell></cell><cell cols="2">The Lord of the Rings</cell><cell>In 1954-1955</cell></row><row><cell>Coreference Coreference resolution</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Tolkien</cell><cell></cell><cell>The Lord of the Rings</cell></row><row><cell></cell><cell></cell><cell></cell><cell>refers to</cell></row><row><cell></cell><cell></cell><cell>book</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Tolkien's epic novel The Lord of the Rings was published in 1954-1955, years after the book was completed. Tolkien Tolkien Tolkien | person ]'s epic novel [ The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings The Lord of the Rings | book | author = Tolkien ] was published in 1954-1955, years after the book was completed.</figDesc><table><row><cell>Output: [ Tolkien Tolkien Tolkien Tolkien Tolkien Tolkien Tolkien Tolkien Tolkien Tolkien Tolkien Tolkien Tolkien Tolkien</cell></row></table><note>Input:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Six days after starting acyclovir she exhibited signs of lithium toxicity. effect = acyclovir | effect = lithium ].</figDesc><table><row><cell>Output: Six days after starting [ acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir acyclovir | drug ] she exhibited signs of [ [ lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium lithium | drug ] toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity toxicity |</cell></row><row><cell>disease |</cell></row></table><note>Input:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Trigger extraction input: Two soldiers were attacked and injured yesterday. Trigger extraction output: Two soldiers were [ attacked attacked attacked attacked attacked yesterday yesterday | time | attack time = attacked ].</figDesc><table><row><cell>attacked attacked attacked attacked attacked attacked attacked attacked attacked attacked attacked attacked | attack ] and [ injured injured injured injured injured injured injured injured injured injured injured injured injured injured injured injured injured | injury ] yesterday.</cell></row><row><cell>Argument extraction input: Two soldiers were [ attacked attacked attacked attacked attacked attacked attacked attacked attacked attacked attacked attacked attacked attacked attacked attacked attacked | attack ] and injured yesterday.</cell></row><row><cell>Argument extraction output: [ Two soldiers Two soldiers Two soldiers Two soldiers Two soldiers Two soldiers Two soldiers Two soldiers Two soldiers Two soldiers Two soldiers Two soldiers Two soldiers Two soldiers Two soldiers Two soldiers Two soldiers | individual | target = attacked ] were attacked and injured [</cell></row><row><cell>yesterday yesterday yesterday yesterday yesterday yesterday yesterday yesterday yesterday yesterday yesterday yesterday yesterday yesterday yesterday</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Barack Obama nominated Hillary Rodham Clinton as his secretary of state on Monday. He chose her because she had foreign affairs experience as a former First Lady. Hillary Rodham Clinton ] on Monday. [ He He He He He Hillary Rodham Clinton ] had foreign affairs experience as a former [</figDesc><table><row><cell>his his his his his his his his his his his his his his Output: [ Barack Obama Barack Obama Barack Obama Barack Obama Barack Obama Barack Obama Barack Obama Barack Obama Barack Obama Barack Obama Barack Obama Barack Obama Barack Obama Barack Obama Barack Obama Barack Obama Barack Obama ] nominated [ Hillary Rodham Clinton Hillary Rodham Clinton Hillary Rodham Clinton Hillary Rodham Clinton Hillary Rodham Clinton Hillary Rodham Clinton Hillary Rodham Clinton Hillary Rodham Clinton Hillary Rodham Clinton Hillary Rodham Clinton Hillary Rodham Clinton Hillary Rodham Clinton Hillary Rodham Clinton Hillary Rodham Clinton Hillary Rodham Clinton Hillary Rodham Clinton Hillary Rodham Clinton ] as [ his his his his his his his his his his his his his his his his his | Barack Obama ] [</cell></row><row><cell>secretary of state secretary of state secretary of state secretary of state secretary of state secretary of state secretary of state secretary of state secretary of state secretary of state secretary of state secretary of state secretary of state secretary of state secretary of state secretary of state secretary of state | He He He He He He He He He He He He | Barack Obama ] chose [ her her her her her her her her her her her her her her her her her | Hillary</cell></row><row><cell>Rodham Clinton ] because [ she she she she she she she she she she she she she she she she she | First Lady First Lady First Lady First Lady First Lady First Lady First Lady First Lady First Lady First Lady First Lady First Lady First Lady First Lady First Lady First Lady</cell></row></table><note>his his his | Barack Obama ] in the output.Input:First Lady | Hillary Rodham Clinton ].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 1 :</head><label>1</label><figDesc>Results on all tasks. All numbers indicate F1 scores except noted otherwise. Datasets marked with an asterisk (*) have nested entities. Entity Rel. Entity Rel. Entity Rel. Entity Rel.</figDesc><table><row><cell>CoNLL04</cell><cell>ADE*</cell><cell>NYT</cell><cell>ACE2005</cell></row><row><cell>Entity Relation Extr.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>become the new state of the art (from 80.6 to 83.7 for ADE relation F1, and from 89.4 to 90.6 for CoNLL04 entity F1</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.6</cell><cell></cell></row><row><cell></cell><cell>TANL</cell><cell>89.4</cell><cell>71.4</cell><cell>90.2</cell><cell>80.6</cell><cell>94.9</cell><cell>90.8</cell><cell>88.9</cell><cell>63.7</cell></row><row><cell></cell><cell>TANL (multi-dataset)</cell><cell>89.8</cell><cell>72.6</cell><cell>90.0</cell><cell>80.0</cell><cell>94.7</cell><cell>90.5</cell><cell>88.2</cell><cell>62.5</cell></row><row><cell></cell><cell>TANL (multi-task)</cell><cell>90.3</cell><cell>70.0</cell><cell>91.2</cell><cell>83.8</cell><cell>94.7</cell><cell>90.7</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">CoNLL03</cell><cell cols="2">OntoNotes</cell><cell cols="2">GENIA*</cell><cell cols="2">ACE2005*</cell></row><row><cell></cell><cell>BERT-MRC (Li et al., 2019a)</cell><cell cols="2">93.0</cell><cell cols="2">91.1</cell><cell cols="2">83.8</cell><cell cols="2">86.9</cell></row><row><cell></cell><cell>BERT-MRC+DSC (Li et al., 2019b)</cell><cell cols="2">93.3</cell><cell cols="2">92.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NER</cell><cell>Cloze-CNN (Baevski et al., 2019) GSL (Athiwaratkun et al., 2020)</cell><cell cols="2">93.5 90.7</cell><cell cols="2">90.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>TANL</cell><cell cols="2">91.7</cell><cell cols="2">89.8</cell><cell cols="2">76.4</cell><cell cols="2">84.9</cell></row><row><cell></cell><cell>TANL (multi-dataset)</cell><cell cols="2">92.0</cell><cell cols="2">89.8</cell><cell cols="2">75.9</cell><cell cols="2">84.4</cell></row><row><cell></cell><cell>TANL (multi-task)</cell><cell cols="2">91.7</cell><cell cols="2">89.4</cell><cell cols="2">76.4</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">FewRel 1.0 (validation)</cell><cell></cell></row><row><cell></cell><cell cols="3">TACRED</cell><cell>5-way 1-shot</cell><cell cols="2">5-way 5-shot</cell><cell>10-way 1-shot</cell><cell cols="2">10-way 5-shot</cell></row><row><cell>Relation Class.</cell><cell>BERT-EM (Soares et al., 2019) BERTEM+MTB (Soares et al., 2019) DG-SpanBERT (Chen et al., 2020) BERT-PAIR (Gao et al., 2019) TANL TANL (multi-task)</cell><cell>70.1 71.5 71.5 71.9 69.1</cell><cell cols="7">88.9 90.1 85.7 94.0 ? 4.1 96.4 ? 4.2 82.6 ? 4.5 88.2 ? 5.9 82.8 83.4 89.5 76.8 81.8</cell></row><row><cell></cell><cell></cell><cell cols="3">CoNLL05 WSJ</cell><cell cols="2">CoNLL05 Brown</cell><cell cols="3">CoNLL2012</cell></row><row><cell></cell><cell>Dep and Span (Li et al., 2019d)</cell><cell></cell><cell>86.3</cell><cell></cell><cell cols="2">76.4</cell><cell></cell><cell>83.1</cell></row><row><cell></cell><cell>BERT SRL (Shi &amp; Lin, 2019)</cell><cell></cell><cell>88.8</cell><cell></cell><cell cols="2">82.0</cell><cell></cell><cell>86.5</cell></row><row><cell>SRL</cell><cell>TANL TANL (multi-dataset)</cell><cell></cell><cell>89.3 89.4</cell><cell></cell><cell cols="2">82.0 84.3</cell><cell></cell><cell>87.7 87.6</cell></row><row><cell></cell><cell>TANL (multi-task)</cell><cell></cell><cell>89.1</cell><cell></cell><cell cols="2">84.1</cell><cell></cell><cell>87.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ACE2005</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Trigger Id.</cell><cell cols="2">Trigger Cl.</cell><cell cols="4">Argument Id. Argument Cl.</cell></row><row><cell>Event Extr.</cell><cell>J3EE (Nguyen &amp; Nguyen, 2019) DyGIE++ (Wadden et al., 2019) TANL TANL (multi-task)</cell><cell cols="2">72.5 72.9 71.8</cell><cell cols="2">69.8 69.7 68.4 68.5</cell><cell cols="2">59.9 55.4 50.1 48.5</cell><cell cols="2">52.1 52.5 47.6 48.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">CoNLL-2012* (BERT-base BERT-large)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MUC</cell><cell></cell><cell>B 3</cell><cell cols="2">CEAF ? 4</cell><cell cols="2">Avg. F1</cell></row><row><cell>Coreference Res.</cell><cell cols="9">Higher-order c2f-coref (Lee et al., 2018) 80.4 SpanBERT (Joshi et al., 2020) BERT+c2f-coref (Joshi et al., 2019) 81.4 83.5 71.7 75.3 68.8 71.9 73.9 76.9 70.8 67.6 73.0 85.3 78.1 75.3 79.6 CorefQA+SpanBERT (Wu et al., 2020) 86.3 88.0 77.6 82.2 75.8 79.1 79.9 83.1 TANL 81.0 69.0 68.4 72.8 TANL (multi-task) 78.7 65.7 63.8 69.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">MultiWOZ 2.1 (Joint Accuracy)</cell></row><row><cell></cell><cell>TRADE (Wu et al., 2019)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>45.6</cell><cell></cell><cell></cell></row><row><cell>DST</cell><cell>SimpleTOD (Hosseini-Asl et al., 2020) TANL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>55.7 50.5</cell><cell></cell><cell></cell></row><row><cell></cell><cell>TANL (multi-task)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>51.4</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 3036-3046. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1308. URL https://doi.org/10.18653/v1/n19-1308. Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. CoRR, abs/1806.08730, 2018. URL http: //arxiv.org/abs/1806.08730. Tapas Nayak and Hwee Tou Ng. Effective modeling of encoder-decoder architecture for joint entity and relation extraction. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 8528-8535. AAAI Press, 2020. URL https:// aaai.org/ojs/index.php/AAAI/article/view/6374. Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR, abs/1910.10683, 2019. URL http://arxiv.org/abs/1910.10683.</figDesc><table><row><cell>Saul B Needleman and Christian D Wunsch. A general method applicable to the search for similar-</cell></row><row><cell>ities in the amino acid sequence of two proteins. Journal of molecular biology, 48(3):443-453,</cell></row><row><cell>1970.</cell></row><row><cell>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language</cell></row><row><cell>models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</cell></row><row><cell>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi</cell></row><row><cell>Zhou,</cell></row></table><note>Trung Minh Nguyen and Thien Huu Nguyen. One for all: Neural joint modeling of entities and events. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty- First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 -February 1, 2019, pp. 6851-6858. AAAI Press, 2019. doi: 10.1609/aaai. v33i01.33016851. URL https://doi.org/10.1609/aaai.v33i01.33016851. Tomoko Ohta, Yuka Tateisi, Jin-Dong Kim, Hideki Mima, and Junichi Tsujii. The genia corpus: An annotated research abstract corpus in molecular biology domain. In Proceedings of the second international conference on Human Language Technology Research, pp. 82-86, 2002. Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayandeh, Lars Liden, and Jianfeng Gao.SOLOIST: few-shot task-oriented dialog with A single pre-trained auto-regressive model. CoRR, abs/2005.05298, 2020. URL https://arxiv.org/abs/2005.05298. Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. Conll- 2012 shared task: Modeling multilingual unrestricted coreference in ontonotes. In Sameer Pradhan, Alessandro Moschitti, and Nianwen Xue (eds.), Joint Conference on Empirical Meth- ods in Natural Language Processing and Computational Natural Language Learning -Pro- ceedings of the Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes, EMNLP-CoNLL 2012, July 13, 2012, Jeju Island, Korea, pp. 1-40. ACL, 2012. URL https: //www.aclweb.org/anthology/W12-4501/. Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Bj?rkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. Towards robust linguistic analysis using ontonotes. In Julia Hockenmaier and Sebastian Riedel (eds.), Proceedings of the Seventeenth Conference on Computational Natural Language Learning, CoNLL 2013, Sofia, Bulgaria, August 8-9, 2013, pp. 143-152. ACL, 2013. URL https://www.aclweb.org/anthology/W13-3516/.Sebastian Riedel, Limin Yao, and Andrew McCallum. Modeling relations and their mentions without labeled text. In Jos? L. Balc?zar, Francesco Bonchi, Aristides Gionis, and Mich?le Sebag (eds.), Machine Learning and Knowledge Discovery in Databases, European Confer- ence, ECML PKDD 2010, Barcelona, Spain, September 20-24, 2010, Proceedings, Part III, vol- ume 6323 of Lecture Notes in Computer Science, pp. 148-163. Springer, 2010. doi: 10.1007/ 978-3-642-15939-8\_10. URL https://doi.org/10.1007/978-3-642-15939-8_ 10.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 2 :</head><label>2</label><figDesc>Details about the single-dataset experiments in joint entity-relation extraction and named entity recognition.</figDesc><table><row><cell cols="2">Dataset</cell><cell></cell><cell></cell><cell cols="2"># Epochs</cell><cell></cell><cell># Runs</cell><cell>Results</cell></row><row><cell cols="4">Joint entity-relation extraction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Entity F1</cell><cell>Relation F1</cell></row><row><cell cols="2">CoNLL04</cell><cell></cell><cell></cell><cell>200</cell><cell></cell><cell></cell><cell>10</cell><cell>89.4 ? 0.3</cell><cell>71.4 ? 1.1</cell></row><row><cell cols="2">ADE</cell><cell></cell><cell></cell><cell>200</cell><cell></cell><cell></cell><cell>10</cell><cell>90.2 ? 0.7</cell><cell>80.6 ? 1.5</cell></row><row><cell cols="2">NYT</cell><cell></cell><cell></cell><cell>50</cell><cell></cell><cell></cell><cell>5</cell><cell>94.9 ? 0.1</cell><cell>90.8 ? 0.1</cell></row><row><cell cols="2">ACE2005</cell><cell></cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell>10</cell><cell>88.9 ? 0.1</cell><cell>63.7 ? 0.7</cell></row><row><cell cols="4">Named entity recognition</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Entity F1</cell></row><row><cell cols="2">CoNLL03</cell><cell></cell><cell></cell><cell>50</cell><cell></cell><cell></cell><cell>10</cell><cell>91.7 ? 0.1</cell></row><row><cell cols="2">OntoNotes</cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell>10</cell><cell>89.8 ? 0.1</cell></row><row><cell cols="2">GENIA</cell><cell></cell><cell></cell><cell>50</cell><cell></cell><cell></cell><cell>10</cell><cell>76.4 ? 0.4</cell></row><row><cell cols="2">ACE2005</cell><cell></cell><cell></cell><cell>50</cell><cell></cell><cell></cell><cell>10</cell><cell>84.9 ? 0.2</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell></row><row><cell>Entity F1</cell><cell>40 50 60</cell><cell></cell><cell></cell><cell>TANL TANL (with NYT)</cell><cell>Relation F1</cell><cell>20 30 10</cell><cell></cell><cell>TANL TANL (with NYT)</cell></row><row><cell></cell><cell>30</cell><cell>1%</cell><cell>2% Percentage of training data</cell><cell>4% SpERT</cell><cell></cell><cell>0</cell><cell>1%</cell><cell>2% Percentage of training data</cell><cell>4% SpERT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>The alternative 2 version annotates the head vs. tail information for the entities directly in the input, instead of using a phrase such as "relationship between [ Carmen Melis</figDesc><table><row><cell>Input (chosen): Born in Bologna, Orlandi was a student of the famous Italian [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ] and voice</cell></row><row><cell>teacher [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis ] in Milan. The relationship between [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis ] and [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ] is</cell></row><row><cell>Output (chosen) : relationship between [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis ] and [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ] = voice type</cell></row><row><cell>Input (alternative 1): Born in Bologna, Orlandi was a student of the famous Italian [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ] and voice</cell></row><row><cell>teacher [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis ] in Milan. The relationship between [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis ] and [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ] is</cell></row><row><cell>Output (alternative 1): voice type</cell></row><row><cell>Input (alternative 2): Born in Bologna, Orlandi was a student of the famous Italian [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano | tail ] and</cell></row><row><cell>voice teacher [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis | head ] in Milan.</cell></row><row><cell>Output (alternative 2): relationship between [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis ] and [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ] = voice type</cell></row><row><cell>The alternative 1 version has a shorted output which only produces the keyword such as voice type</cell></row><row><cell>corresponding to the predicted relation. However, we find that it does not perform as well as the</cell></row><row><cell>chosen format. We hypothesize that it is due to the rich semantics of the sentence "relationship</cell></row><row><cell>between [ Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis ] and [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ]", and possibly softer gradient information on the longer</cell></row><row><cell>sequence which improves training.</cell></row><row><cell>Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis ] and [ soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano soprano ]" to specify</cell></row><row><cell>that "Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis Carmen Melis</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 3 :</head><label>3</label><figDesc>TACRED recall by relation type (in one of our 5 runs), with number of training, validation, and test examples.</figDesc><table><row><cell>Relation type</cell><cell># Train</cell><cell># Dev</cell><cell cols="2"># Test Test recall</cell></row><row><cell>country of death</cell><cell>7</cell><cell>47</cell><cell>9</cell><cell>44.4</cell></row><row><cell>dissolved</cell><cell>24</cell><cell>9</cell><cell>2</cell><cell>50.0</cell></row><row><cell>country of birth</cell><cell>29</cell><cell>21</cell><cell>5</cell><cell>0.0</cell></row><row><cell>state or province of birth</cell><cell>39</cell><cell>27</cell><cell>8</cell><cell>50.0</cell></row><row><cell>state or province of death</cell><cell>50</cell><cell>42</cell><cell>14</cell><cell>64.3</cell></row><row><cell>religion</cell><cell>54</cell><cell>54</cell><cell>47</cell><cell>48.9</cell></row><row><cell>date of birth</cell><cell>64</cell><cell>32</cell><cell>9</cell><cell>77.8</cell></row><row><cell>city of birth</cell><cell>66</cell><cell>34</cell><cell>5</cell><cell>20.0</cell></row><row><cell>charges</cell><cell>73</cell><cell>106</cell><cell>103</cell><cell>85.4</cell></row><row><cell>number of employees members</cell><cell>76</cell><cell>28</cell><cell>19</cell><cell>63.2</cell></row><row><cell>shareholders</cell><cell>77</cell><cell>56</cell><cell>13</cell><cell>0.0</cell></row><row><cell>city of death</cell><cell>82</cell><cell>119</cell><cell>28</cell><cell>32.1</cell></row><row><cell>founded</cell><cell>92</cell><cell>39</cell><cell>37</cell><cell>83.8</cell></row><row><cell>political religious affiliation</cell><cell>106</cell><cell>11</cell><cell>10</cell><cell>40.0</cell></row><row><cell>website</cell><cell>112</cell><cell>87</cell><cell>26</cell><cell>88.5</cell></row><row><cell>cause of death</cell><cell>118</cell><cell>169</cell><cell>52</cell><cell>36.5</cell></row><row><cell>member of</cell><cell>123</cell><cell>32</cell><cell>18</cell><cell>0.0</cell></row><row><cell>founded by</cell><cell>125</cell><cell>77</cell><cell>68</cell><cell>82.4</cell></row><row><cell>date of death</cell><cell>135</cell><cell>207</cell><cell>54</cell><cell>55.6</cell></row><row><cell>schools attended</cell><cell>150</cell><cell>51</cell><cell>30</cell><cell>73.3</cell></row><row><cell>siblings</cell><cell>166</cell><cell>31</cell><cell>55</cell><cell>76.4</cell></row><row><cell>members</cell><cell>171</cell><cell>86</cell><cell>31</cell><cell>48.4</cell></row><row><cell>other family</cell><cell>180</cell><cell>81</cell><cell>60</cell><cell>46.7</cell></row><row><cell>children</cell><cell>212</cell><cell>100</cell><cell>37</cell><cell>67.6</cell></row><row><cell>state or province of headquarters</cell><cell>230</cell><cell>71</cell><cell>51</cell><cell>80.4</cell></row><row><cell>spouse</cell><cell>259</cell><cell>160</cell><cell>66</cell><cell>69.7</cell></row><row><cell>subsidiaries</cell><cell>297</cell><cell>114</cell><cell>44</cell><cell>45.5</cell></row><row><cell>origin</cell><cell>326</cell><cell>211</cell><cell>132</cell><cell>62.1</cell></row><row><cell>state or provinces of residence</cell><cell>332</cell><cell>73</cell><cell>81</cell><cell>54.3</cell></row><row><cell>cities of residence</cell><cell>375</cell><cell>180</cell><cell>189</cell><cell>58.7</cell></row><row><cell>city of headquarters</cell><cell>383</cell><cell>110</cell><cell>82</cell><cell>70.7</cell></row><row><cell>age</cell><cell>391</cell><cell>244</cell><cell>200</cell><cell>96.0</cell></row><row><cell>parents</cell><cell>439</cell><cell>153</cell><cell>150</cell><cell>64.7</cell></row><row><cell>countries of residence</cell><cell>446</cell><cell>227</cell><cell>148</cell><cell>37.8</cell></row><row><cell>country of headquarters</cell><cell>469</cell><cell>178</cell><cell>108</cell><cell>60.2</cell></row><row><cell>alternate names</cell><cell>913</cell><cell>377</cell><cell>224</cell><cell>89.3</cell></row><row><cell>employee of</cell><cell>1,525</cell><cell>376</cell><cell>264</cell><cell>70.1</cell></row><row><cell>top members employees</cell><cell>1,891</cell><cell>535</cell><cell>346</cell><cell>84.4</cell></row><row><cell>title</cell><cell>2,444</cell><cell>920</cell><cell>500</cell><cell>86.8</cell></row><row><cell>no relation</cell><cell cols="3">55,113 17,196 12,184</cell><cell>93.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Output: [ Boston University | 2 ]'s [ Michael D. Papagiannis | 3 | 1 = Boston University ] said he believes the crater was created [ 100 million years | 4 ] ago when a 50-mile-wide meteorite slammed into the [ Earth | 1 ].? Abridged output: here, the output consists of a list of entities, enclosed between [ ] tokens, without text between them.</figDesc><table /><note>Output: [ Boston University | organization ] [ Michael D. Papagiannis | person | works for = Boston University ] [ 100 million years | other ] [ Earth | location ]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on the CoNLL04 dataset (using the full training set, and using only 50% of the training sentences). We report mean and standard deviation over 10 runs.? 0.30 71.44 ? 1.15 87.15 ? 1.08 68.30 ? 1.47 TANL (numeric labels) 89.13 ? 0.45 71.57 ? 0.89 86.59 ? 0.94 66.12 ? 1.31 TANL (abridged output) 88.42 ? 0.67 70.98 ? 1.12 86.11 ? 0.55 67.18 ? 1.18 TANL (no alignment) 87.88 ? 0.31 69.72 ? 1.31 85.56 ? 1.01 66.64 ? 1.54 Ablation studies on CoNLL04, using different portions of the training dataset. The outcomes of these experiments are shown in Figures 2 and 4, and Table 4. We run all experiments using a variable amount of training data, from 100% (1,153 sentences) down to 0.8% (9 sentences), and always evaluate on the entire test set (288 sentences). To account for the variable size of the training dataset, we adjust the number of training epochs as follows: 200 epochs when using all training data; 400 epochs for 50% of the training data; 800 epochs for 25%; 1,600 epochs for 12.5%; 2,000 epochs for all remaining cases (6.3%, 3.1%, 1.6%, 0.8%).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">CoNLL04</cell><cell></cell><cell>CoNLL04 (50%)</cell></row><row><cell></cell><cell></cell><cell>Model</cell><cell>Entity F1</cell><cell cols="3">Relation F1</cell><cell>Entity F1</cell><cell>Relation F1</cell></row><row><cell>Entity F1</cell><cell>50 60 70 80 90 40</cell><cell cols="2">TANL 89.44 1% 3% 10% 30% Percentage of training data TANL TANL (numeric labels) 100% TANL (abridged output) TANL (no DP alignment)</cell><cell>Relation F1</cell><cell>20 30 40 50 60 70 10</cell><cell>1%</cell><cell>3% Percentage of training data 10% 30% TANL TANL (numeric labels) 100% TANL (abridged output) TANL (no DP alignment)</cell></row><row><cell></cell><cell></cell><cell>Figure 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 5 :</head><label>5</label><figDesc>Input-output examples for all structured prediction datasets. Michael D. Papagiannis said he believes the crater was created 100 million years ago when a 50-mile-wide meteorite slammed into the Earth. Michael D. Papagiannis | person | works for = Boston University ] said he believes the crater was created Queens Queens Queens | location | contains = East Elmhurst ]</figDesc><table><row><cell>Dataset</cell><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Output</cell></row><row><cell>CoNLL04</cell><cell cols="6">Boston University's [ 100 million years 100 million years 100 million years 100 million years 100 million years 100 million years 100 million years 100 million years 100 million years 100 million years 100 million years 100 million years 100 million years 100 million years 100 million years 100 million years 100 million years | other ] ago when a 50-mile-wide meteorite</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>slammed into the [ Earth Earth Earth Earth Earth Earth Earth Earth Earth Earth Earth Earth Earth Earth Earth Earth Earth | location ].</cell></row><row><cell>ADE</cell><cell cols="5">Progressive hypoxemia mandated endotracheal</cell><cell>[ Progressive hypoxemia Progressive hypoxemia Progressive hypoxemia Progressive hypoxemia Progressive hypoxemia Progressive hypoxemia Progressive hypoxemia Progressive hypoxemia Progressive hypoxemia Progressive hypoxemia Progressive hypoxemia Progressive hypoxemia Progressive hypoxemia Progressive hypoxemia Progressive hypoxemia Progressive hypoxemia Progressive hypoxemia | disease | effect = rituximab ] mandated</cell></row><row><cell></cell><cell>intubation</cell><cell>1</cell><cell>week</cell><cell>after</cell><cell>rituximab</cell><cell>endotracheal intubation 1 week after [ rituximab rituximab rituximab rituximab rituximab rituximab rituximab rituximab rituximab rituximab rituximab rituximab rituximab rituximab rituximab rituximab rituximab | drug ] administration</cell></row><row><cell></cell><cell cols="5">administration and led to death 4 weeks</cell><cell>and led to death 4 weeks after admission.</cell></row><row><cell></cell><cell cols="2">after admission.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NYT</cell><cell cols="5">At the Triboro Coach depot in East Elmhurst,</cell><cell>At the Triboro Coach depot in [ East Elmhurst East Elmhurst East Elmhurst East Elmhurst East Elmhurst East Elmhurst East Elmhurst East Elmhurst East Elmhurst East Elmhurst East Elmhurst East Elmhurst East Elmhurst East Elmhurst East Elmhurst East Elmhurst East Elmhurst | location | neighborhood</cell></row><row><cell></cell><cell cols="5">Queens, this morning, about 20 workers wore</cell><cell>of = Queens ], [ Queens Queens Queens Queens Queens Queens Queens Queens Queens Queens Queens Queens Queens Queens</cell></row><row><cell></cell><cell cols="5">or carried red union bandannas and held</cell></row><row><cell></cell><cell cols="5">placards with messages like, "The Mayor Lied,</cell></row><row><cell></cell><cell cols="5">There Goes Your Ride" and "On Strike."</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>person | located in = home ] is eventually going to come [ home home home home home</figDesc><table><row><cell>[ town town town town town town town town town town town town town town town town town | geographical</cell></row><row><cell>entity ] of [ palestine palestine palestine palestine palestine palestine palestine palestine palestine palestine palestine palestine palestine palestine palestine palestine palestine | geographical entity | part of = west virginia ],</cell></row><row><cell>[ west virginia west virginia west virginia west virginia west virginia west virginia west virginia west virginia west virginia west virginia west virginia west virginia west virginia west virginia west virginia west virginia west virginia | geographical entity ], on the news that [ jessica lynch jessica lynch jessica lynch jessica lynch jessica lynch jessica lynch jessica lynch jessica lynch jessica lynch jessica lynch jessica lynch jessica lynch jessica lynch jessica lynch jessica lynch jessica lynch jessica lynch</cell></row><row><cell>| home home home home home home home home home home home home |</cell></row><row><cell>geographical entity ].</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We are grateful to Wenxuan Zhou and Tianyu Gao for pointing out an inconsistency in computing results on the TACRED dataset, which have been corrected inTable 1.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>OntoNotes The eventual court decision could become a landmark in Dutch corporate law because the lawsuit ASKO plans to file would be the first to challenge the entire principle and practice of companies issuing voting preferred shares to management -controlled trusts to dilute voting power of common stockholders. first first first first first | ordinal ] to challenge the entire principle and practice of companies issuing voting preferred shares to management -controlled trusts to dilute voting power of common stockholders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GENIA</head><p>Activation of CD4 positive T cells is a primary requirement for human immunodeficiency virus (HIV) entry, efficient HIV replication, and progression to AIDS, Utilizing CD4 positive T cell lines and purified T cells from normal individuals, we have demonstrated that native envelope glycoproteins of HIV, gp 160, can induce activation of transcription factor, activated protein -1 (AP -1).   transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 transcription factor, activated protein -1 | protein ] ([ AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 AP-1 | protein ]).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activation of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells | cell type</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells T cells | cell type</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACE2005 (NER)</head><p>While [ belief ] hotel area not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given, hotel book day not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given, hotel book people not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given, hotel book stay not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given, hotel internet not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given, hotel name not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given, hotel parking yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes, hotel price range cheap cheap cheap cheap cheap cheap cheap cheap cheap cheap cheap cheap cheap cheap cheap cheap cheap, hotel stars not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given not given, hotel type hotel hotel hotel hotel hotel hotel hotel hotel hotel hotel hotel hotel hotel hotel hotel hotel hotel [ belief ]</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Augmented natural language for generative sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nogueira</forename><surname>Ben Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-main.27" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="375" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cloze-driven pretraining of self-attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1539</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1539" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<editor>Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5359" to="5368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<editor>Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gray</surname></persName>
		</author>
		<title level="m">Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
		<meeting><address><addrLine>Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2005 shared task: Semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llu?s</forename><surname>M?rquez</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W05-0620/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on Computational Natural Language Learning</title>
		<editor>Ido Dagan and Daniel Gildea</editor>
		<meeting>the Ninth Conference on Computational Natural Language Learning<address><addrLine>CoNLL; Ann Arbor, Michigan, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06-29" />
			<biblScope unit="page" from="152" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-task learning for sequence tagging: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/C18-1251/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018</title>
		<editor>Emily M. Bender, Leon Derczynski, and Pierre Isabelle</editor>
		<meeting>the 27th International Conference on Computational Linguistics, COLING 2018<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2965" to="2977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Efficient long-distance relation extraction with dg-spanbert. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Hoehndorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.03636" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parsing as language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kook</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D16-1257" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2331" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.3115/1118693.1118694</idno>
		<ptr target="https://www.aclweb.org/anthology/W02-1001" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
		<idno>1532-4435</idno>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>Jill Burstein, Christy Doran, and Thamar Solorio</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Beyond [CLS] through ranking by generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-main.134" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1722" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1033</idno>
		<ptr target="https://www.aclweb.org/anthology/P15-1033" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N16-1024" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Span-based joint entity and relation extraction with transformer pre-training. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Eberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1909.07755" />
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MultiWOZ 2.1: A consolidated multidomain dialogue dataset with state corrections and state tracking baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shachi</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanchit</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<idno>979-10-95546-34-4</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.lrec-1.53" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
		<meeting>The 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05" />
			<biblScope unit="page" from="422" to="428" />
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08-07" />
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<ptr target="https://www.aclweb.org/anthology/D09-1015/" />
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fewrel 2.0: Towards more challenging few-shot relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1649</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1649" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<editor>Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="6249" to="6254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Table filling multi-task recurrent neural network for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Andrassy</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/C16-1239/" />
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<editor>Nicoletta Calzolari, Yuji Matsumoto, and Rashmi Prasad</editor>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2537" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsha</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdul</forename><forename type="middle">Mateen</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angus</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliane</forename><surname>Fluck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hofmann-Apitius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Toldo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2012.04.008</idno>
		<ptr target="https://doi.org/10.1016/j.jbi.2012.04.008" />
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Informatics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="885" to="892" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1514</idno>
		<ptr target="https://doi.org/10.18653/v1/d18-1514" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun&apos;ichi Tsujii</editor>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11-04" />
			<biblScope unit="page" from="4803" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A simple language model for task-oriented dialogue. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.00796" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Refining event extraction through cross-document inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P08-1030" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="254" to="262" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BERT for coreference resolution: Baselines and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1588</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1588" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<editor>Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="5802" to="5807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/1853" />
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Yoshua Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkgNKkHtvB" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
	<note>ISBN 1558607781</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1018</idno>
		<ptr target="https://doi.org/10.18653/v1/d17-1018" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Martha Palmer, Rebecca Hwa, and Sebastian Riedel</editor>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Higher-order coreference resolution with coarseto-fine inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2108</idno>
		<ptr target="https://www.aclweb.org/anthology/N18-2108" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.703/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Downstream model design of pre-trained language model for relation extraction task. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Tian</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.03786" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P13-1008" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A unified MRC framework for named entity recognition. CoRR, abs/1910.11476</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingrong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.11476" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Dice loss for dataimbalanced NLP tasks. CoRR, abs/1911.02855</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1911.02855" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Entity-relation extraction as multi-turn question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1129</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1129" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<editor>Anna Korhonen, David R. Traum, and Llu?s M?rquez</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1340" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dependency or span, end-to-end uniform semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shexia</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016730</idno>
		<ptr target="https://doi.org/10.1609/aaai.v33i01.33016730" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-02-01" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="6730" to="6737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bkg6RiCqY7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Subendhu Rongali, Luca Soldaini, Emilio Monti, and Wael Hamza. Don&apos;t parse, generate! A sequence to sequence architecture for task-oriented semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380064</idno>
		<ptr target="http://dx.doi.org/10.1145/3366423.3380064" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<editor>Jill Burstein, Christy Doran, and Thamar Solorio</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Proceedings of The Web Conference 2020</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W04-2401/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Conference on Computational Natural Language Learning</title>
		<editor>Hwee Tou Ng and Ellen Riloff</editor>
		<meeting>the Eighth Conference on Computational Natural Language Learning<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-05-06" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Held in cooperation with HLT-NAACL 2004</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning</title>
		<editor>Walter Daelemans and Miles Osborne</editor>
		<meeting>the Seventh Conference on Natural Language Learning<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05-31" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
	<note>Held in cooperation with HLT-NAACL 2003</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
		<ptr target="https://www.aclweb.org/anthology/W03-0419/" />
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno>abs/2009.07118</idno>
		<ptr target="https://arxiv.org/abs/2009" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Simple BERT models for relation extraction and semantic role labeling. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.05255" />
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Nested named entity recognition via second-best sequence learning and decoding. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Shibuya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1909.02250" />
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning. CoRR, abs/1703.05175</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1703.05175" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1279</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1279" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<editor>Anna Korhonen, David R. Traum, and Llu?s M?rquez</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Neural architectures for nested NER through linearization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Strakov?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1527</idno>
		<ptr target="https://www.aclweb.org/anthology/P19-1527" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="5326" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Support vector machine learning for interdependent and structured output spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
		<idno type="DOI">10.1145/1015330.1015341</idno>
		<ptr target="https://doi.org/10.1145/1015330.1015341" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First International Conference on Machine Learning, ICML &apos;04</title>
		<meeting>the Twenty-First International Conference on Machine Learning, ICML &apos;04<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">104</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2015/file/277281aada22045c03945dcb2ca6f2ec-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Exploring and predicting transferability across NLP tasks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mattarella-Micke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1585</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1585" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<editor>Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5783" to="5788" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Ace 2005 multilingual training corpus. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Medero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">45</biblScope>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.04768" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.03771" />
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Transferable multi-domain state generator for task-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Corefqa: Coreference resolution as querybased span prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.622/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6953" to="6963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Joint extraction of entities and relations based on a novel decomposition strategy. CoRR, abs/1909.04273</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1909.04273" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A relation-specific attention network for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiannan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeliang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/561</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2020/561" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<editor>Christian Bessiere</editor>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1047</idno>
		<ptr target="https://www.aclweb.org/anthology/P18-1047/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<editor>Iryna Gurevych and Yusuke Miyao</editor>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1004</idno>
		<ptr target="https://doi.org/10.18653/v1/d17-1004" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Martha Palmer, Rebecca Hwa, and Sebastian Riedel</editor>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Asking effective and diverse questions: A machine reading comprehension based framework for joint entity-relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/546</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2020/546" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<editor>Christian Bessiere</editor>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Papagiannis CoNLL03 Charlton, 61, and his wife, Peggy, became citizens of Ireland when they formally received Irish passports from deputy Prime Minister Dick Spring who said the honour had been made in recognition of Charlton&apos;s achievements as the national soccer manager</title>
		<editor>Charlton Charlton Charlton Charlton Charlton Charlton Charlton Charlton Charlton Charlton Charlton Charlton Charlton Charlton Charlton Charlton Charlton | person</editor>
		<imprint>
			<biblScope unit="page">61</biblScope>
		</imprint>
		<respStmt>
			<orgName>Ireland Ireland Ireland Ireland Ireland Ireland Ireland Ireland Ireland Ireland Ireland Ireland Ireland Ireland Ireland Ireland</orgName>
		</respStmt>
	</monogr>
	<note>] who said the honour had been made in recognition of. s achievements as the national soccer manager</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
