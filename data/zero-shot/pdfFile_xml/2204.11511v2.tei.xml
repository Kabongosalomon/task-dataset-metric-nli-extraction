<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Spatio-Temporal Multilayer Perceptron for Gesture Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Holzbock</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Tsaregorodtsev</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Dawoud</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
						</author>
						<title level="a" type="main">A Spatio-Temporal Multilayer Perceptron for Gesture Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/IV51971.2022.9827054</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gesture recognition is essential for the interaction of autonomous vehicles with humans. While the current approaches focus on combining several modalities like image features, keypoints and bone vectors, we present neural network architecture that delivers state-of-the-art results only with body skeleton input data. We propose the spatio-temporal multilayer perceptron for gesture recognition in the context of autonomous vehicles. Given 3D body poses over time, we define temporal and spatial mixing operations to extract features in both domains. Additionally, the importance of each time step is re-weighted with Squeeze-and-Excitation layers. An extensive evaluation of the TCG and Drive&amp;Act datasets is provided to showcase the promising performance of our approach. Furthermore, we deploy our model to our autonomous vehicle to show its realtime capability and stable execution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Gestures are necessary for the interaction between autonomous vehicles and humans. For example, traffic control officers can request an autonomous vehicle to stop or turn with specific hand gestures. Similarly, the driver can control it from the inside of a car, e.g. pointing to a desired parking spot. Because of its high importance, the problem of gesture recognition for autonomous vehicles is not new to the community <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. The current state-of-the-art for indoor <ref type="bibr" target="#b3">[4]</ref> and outdoor <ref type="bibr" target="#b4">[5]</ref> gesture recognition builds on deep neural networks. A popular approach is to extract the 2D or 3D body poses from images <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b8">[8]</ref>, which is considered a more robust representation for gesture recognition <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>. Then a neural network can learn from the skeleton-based data over time <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[11]</ref>. Also, convolutional neural networks are often used to directly recognize gestures from image data <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>. Alternatively, multiple streams of neural network models are considered for processing the spatial and temporal dimensions of the body pose, as well as the image data over time <ref type="bibr" target="#b14">[14]</ref>. In this work, we show that only using the body pose representation over time is sufficient for the gesture recognition task in the context of autonomous vehicles. We present the spatio-temporal multilayer perceptron (st-MLP) for autonomous vehicle gesture recognition. A visual overview of our approach is given in <ref type="figure" target="#fig_0">Fig. 1</ref>. Unlike prior work, we process the spatial and temporal domain of the 3D body pose sequence simultaneously with a single neural network. To design our model, we derive our motivation from MLP-Mixers <ref type="bibr" target="#b15">[15]</ref>, which were designed for image classification. An MLP-Mixer performs feature mixing operations between the two image dimensions. Based on this observation, we define the temporal and spatial mixing operations to process 3D body poses over the time and space dimensions. Moreover, the advantage of the MLP-Mixers, instead of 3D convolutional neural networks, is the reduction of required computational power, as each dimension is mixed separately. First, our spatial-mixing extracts features in the spatial domain. Next, our temporal-mixing processes the data in the temporal domain. In addition, we introduce the Squeeze-and-Excitation (SE) block <ref type="bibr" target="#b16">[16]</ref> after each mixing operation to re-calibrate the feature representation and to shift attention. Our model learns a joint feature representation based on several blocks of interchangeable temporal and spatial mixing operations.</p><p>The design of our architecture aims at a low-latency deep neural network dedicated to automated driving gesture recognition. For that reason, we pick the TCG <ref type="bibr" target="#b4">[5]</ref> and Drive&amp;Act <ref type="bibr" target="#b3">[4]</ref> datasets for evaluation which contains typical gestures in the field of autonomous driving. In our experiments, we demonstrate state-of-the-art results when comparing with related work on both datasets. In addition, we compress our model and deploy it to our autonomous vehicle to show the real-time capabilities of our approach.</p><p>In summary, we propose an efficient approach for gesture recognition in automated driving, while we are the first to use an MLP-Mixer architecture for temporal data processing in the field of autonomous driving. Finally, we demonstrate real-time performance in our autonomous vehicle. Our code and pre-trained models are publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Gesture Recognition for Autonomous Driving</head><p>The capability to understand gestures is a key enabler for automated driving applications where gestures involve body parts motion like hands, arms, head, and/or the entire body <ref type="bibr" target="#b17">[17]</ref>. Over the past few years, the gesture recognition problem in the vicinity of automated driving has been wellstudied in the context of diverse human-vehicle interaction applications <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>. This includes interactions inside the vehicle <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>, e.g. control of infotainment systems, as well as interactions outside the vehicle, e.g. traffic control officers and pedestrians <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>. Further studies address other aspects of the problem, such as the lack of public datasets on traffic control gestures <ref type="bibr" target="#b4">[5]</ref>, relying on robust motion capture sensors <ref type="bibr" target="#b25">[25]</ref>, driver behavior prediction <ref type="bibr" target="#b26">[26]</ref>, and pedestrian intention prediction <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>. These works shed light on the importance of an accurate understanding of the human body language for automated driving. Furthermore, they make use of deep neural networks as their gateway towards achieving state-of-the-art results <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b32">[32]</ref>. Unlike prior work in neural networks, we present a model based on MLP-Mixers <ref type="bibr" target="#b15">[15]</ref> using no recurrent structures or convolutional layers and instead only relying on linear layers and a transpose operation for skeletonbased gesture recognition. This enables our approach to processes 3D skeleton data in the spatial and temporal domain using only a single stream, similar to <ref type="bibr" target="#b11">[11]</ref>. In contrast, an LSTM <ref type="bibr" target="#b33">[33]</ref> can only process the data in the temporal dimension. We compare the results of our approach with results of temporal models e.g. Bi-LSTMs [34] on the TCG <ref type="bibr" target="#b4">[5]</ref> and Drive&amp;Act <ref type="bibr" target="#b3">[4]</ref> datasets and show significant improvements.</p><p>In autonomous vehicles, real-time performance is also important for a safe and user-friendly experience, motivated by certain applications like driver behaviour prediction for handover control <ref type="bibr" target="#b35">[35]</ref> or pedestrian behaviour interpretation at crosswalks <ref type="bibr" target="#b36">[36]</ref>. Although existing works often claim realtime capabilities with deep neural networks <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b38">[38]</ref>, the execution time of the complete pipeline is more than 100 ms, which is reported in <ref type="bibr" target="#b39">[39]</ref> as a sufficient planning frequency of an autonomous vehicle. Our st-MLP is a lightweight network that can run in less than 1 ms on our autonomous vehicle, while the complete pipeline including pose estimation takes around 42 ms. Attention-based models are common in natural language processing <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b42">[42]</ref>, object detection <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref> and image classification <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr" target="#b46">[46]</ref>, among others. A class of neural networks that use the attention concept among layer normalization and linear layers are Transformers. Recently, MLP-Mixers have been proposed by <ref type="bibr" target="#b15">[15]</ref> as an efficient alternative to computationally demanding models like CNNs <ref type="bibr" target="#b47">[47]</ref> and self-attention-based Transformers <ref type="bibr" target="#b40">[40]</ref>, while achieving similar performance on popular image classification benchmarks. The MLP-Mixer architecture is inspired by both CNNs and Transformers since it processes image patches instead of the whole image. It contains channel-mixing blocks for each token (image patch) to capture spatial and per-channel features. Although it has shown competitive performance for image classification, it has not been explored for other data types. Our work is the first to present a model similar to an MLP-Mixer that works on 3D body pose skeletons overtime on the task of gesture recognition in automated driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>We focus on skeleton-based gesture recognition where the 3D body pose of an individual serves as input to our approach, as it outperforms image-based methods. We define the spatio-temporal multilayer perceptron (st-MLP) that receives a sequence of 3D body poses as input to perform gesture prediction as a classification task. An overview of our method is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><formula xml:id="formula_0">Let D = {X i , y i } N i=1</formula><p>be the train set where each sample consists of T body skeletons X i = {x i,0 , . . . , x i,T } and the associated ground-truth gesture as one-hot vector labels y i ? {0, 1} C , such that C c=1 y i (c) = 1 for a C-category classification problem. At each time step t ? T , the corresponding 3D body skeleton x i,t ? R 3?K is represented by K body joints. Based on the train set D, our objective is to learn to predict the gesture category, i.e. y i , for the 3D skeleton input sequence X i . We define this mapping as</p><formula xml:id="formula_1">y i = f (X i ; ?),</formula><p>where ? is a set of learnable parameters. To approximate the mapping function, we propose the st-MLP below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatio-Temporal MLP</head><p>Our motivation comes from the concept of MLP-Mixers <ref type="bibr" target="#b15">[15]</ref>, which is an efficient attention mechanism for image data <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b45">[45]</ref>. The MLP-Mixer is designed to learn and mix features from images for the classification tasks. To this end, the images are divided into smaller patches of the same size and fed into the mixer model. As the MLP-Mixer performs mixing across all dimensions, the attention is shifted from only considering single patches to also learning the relation between the patches. By mixing a specific dimension, we introduce new features in this dimension. At first, all dimensions are individually mixed and enriched with features. Subsequent mixing operations are then applied to these aggregated features. This results in cross-dimensionally mixed features, similarly to separable convolutions for CNNs, with the added ability of MLPs to aggregate data across the entire dimension instead of a small kernel window. However, unlike images, we deal with a different type of data and therefore present a new formulation to classify a sequence of 3D human skeletons. Additionally, we integrate the Squeeze-and-Excitation (SE) block into our model to re-weight the influence of each time step as mentioned in Sec. III-C.</p><p>In our problem, our model learns to mix features from 3D body skeletons, which are defined in space and time. We define the mixing operation in the time domain as temporalmixing (shown on the right side of <ref type="figure" target="#fig_2">Fig. 2</ref>) and mixing across the body joints as the spatial-mixing operation (shown on the left side of <ref type="figure" target="#fig_2">Fig. 2)</ref>. Given a single input sequence X divided over T time steps, the skeleton of each time step x t ? R K?3 is flattened into a vector of length k = 3 * K. This results in a two-dimensional input R T ?k containing one temporal dimension and one spatial dimension. Flattening is needed to reduce the two-dimensional joints into one dimension in order to enable spatial mixing. Next, the flattened dimension k is projected into a hidden dimension S using a convolutional layer. The transformed input can be represented as a matrixX ? R T ?S . The matrix is passed through each of the L layers of the st-MLP.</p><p>Each layer l ? [1 . . . L] consists of a mixing block containing the spatial-mixing unit, followed by the temporalmixing unit. We illustrate a mixing block in <ref type="figure" target="#fig_2">Fig. 2</ref>. Both types of units contain fully connected layers, i.e. MLPs, and expand the input to a hidden dimension, namely to D S for the spatial-mixing units and D T for the temporal-mixing units. The values of both hidden dimensions can be selected independently of the number of joints and sequence length. In addition, each layer contains skip-connections, non-linear activation functions, and Layer Normalization <ref type="bibr" target="#b48">[48]</ref> to obtain meaningful gradients during backpropagation and prevent overfitting. The Layer Normalization is similar to regular Batch Normalization <ref type="bibr" target="#b49">[49]</ref>, however, it normalizes the input over the channel dimension instead of the batch dimension. In our case, this provides normalization over the time dimension. The first unit (spatial-mixing) operates on the rows ofX. Therefore, the first unit is applied to the transposed matrixX T . The second unit (temporal-mixing) operates on the columns of U, which is the output from the spatialmixing block. The input to the first layer is the informatio? X 0 extracted from 3D skeletons, while V 0 is the output. The inputX l of the following layer l is the output V l?1 of the previous layer l ? 1, i.e. V l?1 =X l . For the sake of simplicity, we use a single sample to define U and V as:</p><formula xml:id="formula_2">U s,l =X s,l + (W 2,l (g(W 1,l h(X T s,l )))) T , s ? [1 . . . S],<label>(1)</label></formula><formula xml:id="formula_3">V j,l = U j,l + W 4,l (g(W 3,l h(U j,l ))), j ? [1 . . . T ],<label>(2)</label></formula><p>where g(.) is the non-linear (GeLU) activation and h(.) is the layer normalization operator. Finally, the output from the last layer is passed through a global average pooling layer followed by a linear classifier to predict the gesture class?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Squeeze-and-Excitation Block</head><p>The Squeeze-and-Excitation (SE) block <ref type="bibr" target="#b16">[16]</ref> takes the distinct influence of each channel of a feature tensor into account and assigns a weight to each channel depending on its importance. Since the influence of the time steps can differ between the various gestures, the weights are not fixed and can vary between gestures. This helps the network to take the higher importance of recent time steps compared to earlier time steps into account, which is similar to LSTMs weighting its hidden states and its current input to focus on either history or current values more. An overview of the SEblock is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. The SE-block uses global average pooling to condense every feature layer into a single value. In a second step, the compressed feature tensor is processed with a linear layer network which contains two linear layers as well as ReLU and Softmax activation functions. Finally, each feature layer of the original tensor is multiplied with its corresponding weight of the linear layer network output to assign the calculated importance to each channel.</p><p>In gesture recognition, each time step has a different influence on the predicted gesture. The most recent time step T of the skeleton sequence X has a higher influence on the ground truth y than the oldest time step 0. To take this into account, we use the SE-block to weight the importance of each time step in the model. In <ref type="figure" target="#fig_2">Fig. 2</ref> we show the integration of the SE-block after the temporal-mixing and spatial-mixing units into the mixing block but before the addition of the skip connection. Combining Eq. 1 and Eq. 2 with the SE-block, the importance weighting of the time domain can be included in each layer l. For better understanding, we reformulate the new output for a single sample as:</p><formula xml:id="formula_4">A s,l = W 2,l (g(W 1,l h(X T s,l ))), U s,l =X s,l + ?(W 2SE r(W 1SE A T s,l )), s ? [1 . . . S],<label>(3)</label></formula><formula xml:id="formula_5">B j,l = W 4,l (g(W 3,l h(U j,l ))), V j,l = U j,l + ?(W 2SE r(W 1SE B j,l )), j ? [1 . . . T ],<label>(4)</label></formula><p>where r(.) and ?(.) are the ReLU and the Softmax activation functions respectively. A s,l and B j,l are the results of the spatial-mixing and temporal-mixing before the addition of the skip connection. The time steps in A s,l and B j,l are weighted by the included SE-block. At both locations, we use the same SE-block module resulting in shared SE-block weights W 1SE and W 2SE across both mixer units. This leads to similar time step weights at both SE-block locations, improving model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Complete Model</head><p>In <ref type="figure" target="#fig_2">Fig. 2</ref> we show the structure of one mixing block, where the SE-block is already included right before the skipconnection. The st-MLP consists of several mixing blocks stacked one after another. The number of mixing blocks is defined by the number of layers L.</p><p>The aim is to minimize the cross-entropy between the predictions? i and ground truth y i . This requires the optimization of the model weights by backpropagation and an optimization algorithm, hence the optimal model weights ? * should satisfy the following condition:  </p><formula xml:id="formula_6">? * = arg min ? L(y i , f (X i ; ?)).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. AUTONOMOUS VEHICLE IMPLEMENTATION</head><p>We evaluate gesture recognition based on st-MLPs in a real-world scenario on an autonomous vehicle. In the following section, we describe our implementation for the gesture recognition task and the hardware specification of the autonomous vehicle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Gesture Recognition</head><p>The st-MLP processes 3D human body skeletons to predict the presented gesture. However, there are no sensors available for autonomous vehicles that directly return human body skeletons. Therefore, we use an RGB camera mounted on a vehicle to perform gesture recognition. An overview of the full approach is shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. To extract a 2D human body skeleton from the camera image, a recent deep learning approach for human pose estimation called OpenPifPaf <ref type="bibr" target="#b50">[50]</ref> is used. The pose extraction step provides 2D body pose estimates which are lifted using the VideoPose3D-CNN <ref type="bibr" target="#b51">[51]</ref> to obtain 3D body pose estimates. The lifting procedure helps to resolve ambiguities coming from certain 2D poses, as well as the orientation variance of specific gestures. We accumulate and stack the 3D human body poses x of the last T time steps to get a sequence of poses x 0 , . . . , x T . This sequence is fed into the st-MLP to predict the gestur? Y T performed at time step T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Specification of the Autonomous Vehicle</head><p>The test vehicle is equipped with an Intel Xeon E5-2640 CPU and NVIDIA 2080Ti GPU, running Ubuntu 18.04 in a docker container. To interact in urban environments the autonomous vehicle must be able to react in real-time to changes in the environment. As most of the time is used for neural network inference, we rely on the NVIDIA TensorRT framework <ref type="bibr" target="#b52">[52]</ref> to speed up execution. The main advantage of TensorRT is the provided execution optimization which improves the memory latency and allocation. Furthermore, it is used to convert the model parameters from 32-bit to 16-bit floating-point precision to speed up calculations. By using our st-MLP with fixed input sizes, we achieve quicker and more stable execution times using TensorRT compared to standard LSTMs, which is highly important in a real-time environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>We first perform evaluations on two standard benchmarks and then on real-world settings. Furthermore, we introduce the evaluation datasets, provide the training details of our st-MLP, and show the effect of different components of our approach with ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>For the evaluation, we use two gesture recognition datasets for autonomous driving. The first dataset is the TCG dataset <ref type="bibr" target="#b4">[5]</ref>. The TCG dataset contains the standard European traffic control gestures, namely Stop, Go and Clear. Additionally, a fourth class named Idle is added. The Idle gesture resembles the situation where none of the previous three classes is present. The dataset contains gestures from different scenarios at 4-way-intersections and T-intersections as well as different camera views of the traffic controller. These different viewpoints affect the label, e.g. the label of the stop gesture differs if the traffic controller is seen from the front or the side. The gestures are performed by 5 different subjects which are represented by 17 human body joints stacked into a time sequence. The evaluation is performed on the test set following the TCG protocol in a cross-view and cross-subject manner based on the accuracy, Jaccard index, and F1-score metrics.</p><p>The second dataset in the evaluation is the Drive&amp;Act dataset <ref type="bibr" target="#b3">[4]</ref>. This dataset is recorded inside of a car and contains gestures and actions that can be performed while driving, e.g. eating/drinking, or actions becoming possible with an increasing amount of automated driving like reading a newspaper. The gestures in the dataset are executed twice by 15 different subjects. The subjects are represented by a skeleton consisting of 13 joints (4 fewer joints compared to TCG), as the lower part of the body has no influence on the performed gesture, and leg detection with cameras is impossible due to occluded perspectives. For the evaluation, we follow the protocol for coarse classes on the validation and test set as defined in the Drive&amp;Act dataset and use three different splits for each of the training, validation, and test sets. The mean per-class accuracy is the evaluation metric.</p><p>The main differences between these datasets are the different number of classes, base skeletons, and action locations, as the actions contained in Drive&amp;Act are performed inside of a car while the gestures from the TCG dataset are outdoors. Furthermore, we classify the entire sequence with one label for Drive&amp;Act, while for TCG an action is predicted for each time step.</p><p>In addition to the comparison of st-MLP with other gesture recognition approaches, we show the real-time capability of our st-MLP on the autonomous vehicle described in Sec. IV-B. Therefore, we rely on a labeled sequence recorded with our autonomous vehicle. We annotate this sequence with the labels of the TCG dataset. Note that we report the mean inference time and gesture recognition performance over 167 frames at 15 fps based on a 1024 ? 768 image resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatio-Temporal MLP Implementation</head><p>The st-MLP is implemented using PyTorch <ref type="bibr" target="#b53">[53]</ref>. At the input, a 1D convolution of kernel size 1 ? k is applied. Further processing is done with linear layers, GeLU activation functions, transpose operations, and the SE-block. The output of the last linear layer goes through the softmax activation function for gesture classification.</p><p>The TCG dataset is trained for 70 epochs with a balanced batch (i.e. equal class probability for each sample) of size 1024. For the optimization, the Ranger optimizer <ref type="bibr" target="#b54">[54]</ref> with a flat and cosine annealing learning rate scheduler is used. After the first 50 epochs, the learning rate of 0.001 is reduced to 0.0001 with the cosine annealing function. As for the internal hyperparameters of our st-MLP model, the number of layers L is set to 4, the hidden dimension S to 512, the sequence length T to 24, the hidden dimension for the spatial-mixing D S to 32, and the hidden dimension for the temporal-mixing D T to 256.</p><p>The optimization on the Act&amp;Drive dataset is done with different hyperparameters due to varying settings, e.g. the number of joints and number of classes. The training takes place for 80 epochs with a balanced batch of size 2048. We use the Adam optimizer <ref type="bibr" target="#b55">[55]</ref> with a learning rate of 0.001 and a cosine annealing learning rate scheduler to reduce the learning rate by a factor of 0.1 at the end of training. We use the same model hyperparameters for the st-MLP as in the TCG dataset, only changing the number of layers L to 2, the sequence length T to 90, and the hidden dimension for the spatial-mixing D S to 64. Additionally, we convert the world coordinates of the dataset into camera coordinates.</p><p>The aforementioned hyperparameters controlling architecture depth/hidden dimensions, learning rate, and training epochs are optimized using grid search for each dataset separately. Furthermore, for each method the best performing parameters have been selected for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation of the Spatio-Temporal MLP</head><p>We present separately below the evaluation on the TCG and Drive&amp;Act datasets. a) TCG Dataset: We compare our st-MLP with other models <ref type="bibr" target="#b4">[5]</ref> for gesture recognition in the cross-subject and the cross-view protocol on the TCG dataset. The results are shown in Tab. I for the cross-subject evaluation and in Tab. II for the cross-view evaluation. In both tables, we provide the mean and standard deviation over 3 runs, similar to <ref type="bibr" target="#b4">[5]</ref>. The accuracy of the st-MLP in both evaluation protocols is slightly below the best accuracy of the other models. In contrast, the Jaccard index and the F1-score of the st-MLP are better than the other models. To understand the drop in the accuracy and the increase in the other metrics, we plot the confusion matrices of our st-MLP and the confusion matrix of the best working model from <ref type="bibr" target="#b4">[5]</ref>, namely the Bi-LSTM in <ref type="figure" target="#fig_4">Fig 4.</ref> Compared to the Bi-LSTM, shown in <ref type="figure" target="#fig_4">Fig. 4a and Fig. 4b</ref>, the st-MLP shown in <ref type="figure" target="#fig_4">Fig. 4c and Fig. 4d</ref>, has a lower accuracy in the Idle class but improves the detection performance on the other classes. Due to the high imbalance of the dataset (many samples belong to the idle class, while fewer samples belong to the other classes), the accuracy drops with lower accuracy in the idle class. This is caused by the weighted mean calculation of the accuracy metric. To this end, relying on accuracy alone is insufficient to assess the model performance on TCG, hence, F1-score and Jaccard index are additionally calculated per label and then summarized by an unweighted mean. This leads to higher detection performance for underrepresented classes and therefore better results for the F1score and Jaccard index. Furthermore, our approach has a smaller standard deviation during three different evaluation runs compared to the baseline as shown in Tab. I and Tab. II. This indicates that our approach delivers more consistent results.</p><p>b) Drive&amp;Act Dataset: To examine the generalization abilities of our st-MLP, we evaluate on a different autonomous driving dataset, namely the Drive&amp;Act dataset <ref type="bibr" target="#b3">[4]</ref>. In Tab. III, we compare our results on the evaluation protocol of the Drive&amp;Act dataset with the results of the paper's temporal models. In the columns Validation and Test, the best mean per-class accuracy results on the validation and test sets  are shown. We only compare with the Pose method of the Drive&amp;Act dataset, as only in this method the same input data is used. In the Two-Stream approach, additional spatial dependencies between the joints are employed. Comparing the st-MLP with the Pose approach, we gain 1.65% accuracy. Furthermore, we reach similar results to the Two-Stream method by only using the same data as the Pose method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>Each component of the st-MLP has a different effect on the model performance. We show the effect of spatial-mixing and temporal-mixing as well as the influence of the SE-block. a) Influence of Temporal-Mixing and Spatial-Mixing: In each layer of the st-MLP, we perform spatial-mixing and temporal-mixing. We determine the influence of each mixing type on the overall performance by training models with only one mixing type, either spatial-mixing or temporal-mixing. Furthermore, we evaluate the combination of temporalmixing and spatial-mixing in two separate streams, which we call two-stream model.</p><p>The performance of the single mixing type models is compared with the performance of our st-MLP on the Drive&amp;Act dataset. The st-MLP has a mean per-class accuracy of 34.61%. This is around 2.5% better than the performance of 32.08% of the model using only temporal-mixing, while the model using only spatial-mixing achieved 30.98%. This shows that the combination of both mixing types improves the model. Furthermore, we implement a two-stream model with the temporal-mixing stream and spatial-mixing stream going through separate neural networks. Before classification, both streams are combined using global average pooling. When comparing the two-stream model with our st-MLP, i.e. single stream, we find that the alternating st-MLP with a mean per-class accuracy of 34.61% is superior to the performance of the two-stream model with only 30.09% accuracy.</p><p>b) Influence of the SE-Block: We use the SE-block in our st-MLP to weight different time steps by their importance as described in Sec. III-C. The st-MLP shares the SEblock module between the spatial-mixing and the temporalmixing to reach mean per-class accuracy of 34.61% on the Drive&amp;Act dataset. Without a SE-block using the standard MLP-Mixer architecture the st-MLP reaches 0.79% less mean per-class accuracy. When using two separate SE-blocks behind the spatial-mixing and temporal-mixing, we are still 0.46% mean per-class accuracy behind the performance of the st-MLP with shared SE-blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Evaluation of the Real World Scenario</head><p>In Sec. IV-A, we present a method to extract 3D human body skeletons from a stream of camera images and perform gesture recognition. To improve real-time execution and achieve constant run-time on our autonomous vehicle, we utilize TensorRT <ref type="bibr" target="#b52">[52]</ref> for execution. In Tab. IV, we present the inference time in milliseconds (ms) for each element of the method. All results are based on the hardware specified in Sec.IV-B and the evaluation sequence from Sec. V-A. As it can be observed in Tab. IV, the inference time of our implementation on the autonomous vehicle is about 42 ms. This means that the prediction of the presented gestures can be easily performed inside of one planning cycle of the vehicle that takes 100 ms according to <ref type="bibr" target="#b39">[39]</ref>. Therefore, the prediction arrives in time for being considered in the current planning cycle. Without using TensorRT, the inference constantly exceeded 100 ms, while achieving the same accuracy of 83.23% on our test sequence. Moreover, TensorRT does not impact the model output. One can also see, that the st-MLP part of the pipeline is extremely fast on its own, achieving sub-1ms inference and thus making it suitable for timing-critical environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>We presented the spatio-temporal multilayer perceptron for skeleton-based gesture recognition in the context of autonomous vehicles. We introduced the spatial-mixing and temporal-mixing of the 3D body pose overtime to train a gesture classification model. In our evaluations, we reached state-of-the-art performance on the TCG and Drive&amp;Act datasets. Finally, we deployed our model on our autonomous </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of our proposed spatio-temporal MLP (st-MLP). Features are extraced from the time sequence of 3D human body poses and processed with the following N mixing blocks. The squares represent the matrices where T are the number of time steps and S the dimension of the encoded skeleton features. The output of the mixing blocks is aggregated over time and is used for the gesture classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1</head><label></label><figDesc>https://github.com/holzbock/st_mlp B. Attention-Based Networks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The figure depicts one mixing block, of which the st-MLP can contain several. The mixing block takes an interval of encoded 3D human body skeletons and predicts the presented gesture. The rows of the matrices represent the different time steps, the columns the different joints. In the lower right corner, the architecture of the SE-block is shown. The weight for each channel is calculated with global average pooling (GAP), linear layers, and activation functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Our proposed gesture recognition pipeline on the autonomous vehicle. Our input consists of sequences of camera images. The images are fed into a pose estimation framework to obtain sequences of 2D poses. These poses are then lifted with another deep neural network to 3D poses and then processed with our st-MLP to predict the gesture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>(a) Cross-subject; Bi-LSTM (b) Cross-view; Bi-LSTM (c) Cross-subject; st-MLP (d) Cross-view; st-MLP Confusion matrices of the Bi-LSTM used from [5] and our spatiotemporal MLP (st-MLP) for the cross-subject and cross-view task of the TCG dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I</head><label>I</label><figDesc>CROSS-SUBJECT EVALUATION OF OUR SPATIO-TEMPORAL MLP (ST-MLP) ON THE TEST SET OF THE TCG DATASET. RESULTS OF OTHERMETHODS USED FROM<ref type="bibr" target="#b4">[5]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell>Metric</cell><cell></cell></row><row><cell>Method</cell><cell>Accuracy</cell><cell>Jaccard Index</cell><cell>F1-Score</cell></row><row><cell>GRU</cell><cell>84.44(?2.0)</cell><cell>58.16(?4.2)</cell><cell>70.45(?3.1)</cell></row><row><cell>Att-LSTM</cell><cell>85.67(?2.1)</cell><cell>50.70(?9.9)</cell><cell>61.87(?10.6)</cell></row><row><cell>Bi-GRU</cell><cell>86.80(?1.6)</cell><cell>57.25(?7.4)</cell><cell>68.95(?6.4)</cell></row><row><cell>Bi-LSTM</cell><cell>87.24(?1.8)</cell><cell>67.00(?2.1)</cell><cell>78.48(?1.8)</cell></row><row><cell>st-MLP (ours)</cell><cell>85.99(?0.11)</cell><cell>67.88(?0.06)</cell><cell>80.05(?0.03)</cell></row><row><cell></cell><cell cols="2">TABLE II</cell><cell></cell></row><row><cell cols="4">CROSS-VIEW EVALUATION OF OUR SPATIO-TEMPORAL MLP (ST-MLP)</cell></row><row><cell cols="4">ON THE TEST SET OF THE TCG DATASET. RESULTS OF OTHER METHODS</cell></row><row><cell></cell><cell cols="2">USED FROM [5].</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Metric</cell><cell></cell></row><row><cell>Method</cell><cell>Accuracy</cell><cell>Jaccard Index</cell><cell>F1-Score</cell></row><row><cell>GRU</cell><cell>83.47(?1.4)</cell><cell>56.25(?7.6)</cell><cell>68.59(?7.4)</cell></row><row><cell>Att-LSTM</cell><cell>85.30(?1.1)</cell><cell>59.87(?12.7)</cell><cell>71.20(?12.3)</cell></row><row><cell>Bi-GRU</cell><cell>87.37(?0.3)</cell><cell>55.55(?2.8)</cell><cell>67.68(?2.2)</cell></row><row><cell>Bi-LSTM</cell><cell>86.66(?1.2)</cell><cell>65.95(?4.7)</cell><cell>77.14(?4.3)</cell></row><row><cell>st-MLP (ours)</cell><cell>86.90(?0.08)</cell><cell>70.83(?0.28)</cell><cell>82.48(?0.21)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III EVALUATION</head><label>III</label><figDesc>OF THE SPATIO-TEMPORAL MLP (ST-MLP) ON THE DRIVE&amp;ACT DATASET. RESULTS OF THE OTHER METHODS WERE OBTAINED FROM [4].</figDesc><table><row><cell>Method</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>Pose [4]</cell><cell>37.18</cell><cell>32.96</cell></row><row><cell>Two-Stream [4]</cell><cell>39.37</cell><cell>34.81</cell></row><row><cell>st-MLP (ours)</cell><cell>40.56</cell><cell>34.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV WE</head><label>IV</label><figDesc>MEASURE INFERENCE TIMES OF OUR OPTIMIZED APPROACH ON THE AUTONOMOUS VEHICLE DESCRIBED IN SEC. IV-B. THE EXECUTION TIME OF EACH PART OF OUR RECOGNITION SYSTEM IS REPORTED.</figDesc><table><row><cell>Part</cell><cell>Time (ms)</cell></row><row><cell>2D Pose</cell><cell>36.06</cell></row><row><cell>3D Pose Lifting</cell><cell>5.13</cell></row><row><cell>Gesture Recognition</cell><cell>0.66</cell></row><row><cell>Total</cell><cell>41.85</cell></row><row><cell cols="2">vehicle to show its real-time capability and stable execution,</cell></row><row><cell cols="2">which are both important features for operation on an au-</cell></row><row><cell>tonomous vehicle.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real time gesture to automotive control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dhuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Current Trends towards Converging Technologies (ICCTCT)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time system for monitoring driver vigilance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nuevo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="77" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time detection of driver cognitive distraction using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on intelligent transportation systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="340" to="350" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Drive&amp;act: A multi-modal dataset for fine-grained driver behavior recognition in autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roitberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haurilet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rei?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Voit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Traffic control gesture recognition for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiederer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouazizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kressel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="10" to="676" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning temporal 3d human pose estimation with pseudo-labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouazizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kressel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 17th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Selfsupervised 3d human pose estimation with multiple-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouazizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiederer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kressel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Holistic human pose estimation with regression forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Amann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Articulated Motion and Deformable Objects</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="20" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey on unsupervised outlier detection in high-dimensional numerical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Analysis and Data Mining</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="363" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Body pose and context information for driver secondary task detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Popp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Anneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Voit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2015" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gesture recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Acharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="324" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vision based hand gesture recognition for human computer interaction: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Rautaray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence review</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autonomous vehicles that interact with pedestrians: A survey of theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on intelligent transportation systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="900" to="918" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Free-hand gesture recognition with 3d-cnns for in-car infotainment control in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sachara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kopinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gepperth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Handmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="959" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A research study of hand gesture recognition technologies and applications for human vehicle interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Pickering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Burnham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 3rd Institution of Engineering and Technology Conference on Automotive Electronics. IET</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">In-vehicle hand activity recognition using integration of regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1034" to="1039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hand gesture recognition in automotive human-machine interaction using depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zengeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kopinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Handmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">59</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conventionalized gestures for the interaction of people in traffic with autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vasardani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM SIGSPATIAL international workshop on computational transportation science</title>
		<meeting>the 9th ACM SIGSPATIAL international workshop on computational transportation science</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using deep learning in infrared images to enable human gesture recognition for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="88" to="227" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Drive&amp;act: A multi-modal dataset for fine-grained driver behavior recognition in autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roitberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haurilet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rei?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Voit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2801" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pedestrian intention recognition by means of a hidden markov model and body language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lorenzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fern?ndez-Llorca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sotelo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 20th International Conference on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pedestrian path, pose, and intention prediction through gaussian process dynamical models and pedestrian activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quintero M?nguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Parra Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fern?ndez-Llorca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sotelo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1803" to="1814" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic analysis of pedestrian&apos;s body language in the interaction with autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Morales-?lvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>G?mez-Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fern?ndez-L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Garc?a-Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olaverri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional neural networks and long short-term memory for skeleton-based human activity and hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Nunez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cabido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Pantrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Montemayor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="80" to="94" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learned hand gesture classification through synthetically generated training samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lindgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalavakonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hannaford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3937" to="3942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hand gesture recognition with 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep stacked bidirectional lstm neural network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image and Graphics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="676" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Looking at the driver/rider in autonomous vehicles to predict take-over readiness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Vehicles</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="52" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Investigating pedestrian suggestions for external features on fully autonomous vehicles: A virtual reality experiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Strawderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Carruth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transportation research part F: traffic psychology and behaviour</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="135" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Realtime gesture recognition with shallow convolutional neural networks employing an ultra low cost radar system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Ehrnsperger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Siart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Eibert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 German Microwave Conference (GeMiC)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="88" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Short-range radar based real-time hand gesture recognition using lstm encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="33" to="610" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Autonomous vehicles control in the vislab intercontinental autonomous challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Broggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Medici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Panciroli</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1367578812000132" />
	</analytic>
	<monogr>
		<title level="j">Annual Reviews in Control</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="171" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01933</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Visual transformers: Tokenbased image representation and processing for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03677</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR, 07-09</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ser. Proceedings of Machine Learning</title>
		<editor>Research, F. Bach and D. Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning, ser. Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tensorrt</surname></persName>
		</author>
		<idno>7.2.2.3</idno>
		<ptr target="https://developer.nvidia.com/tensorrt" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Ranger -a synergistic optimizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wright</surname></persName>
		</author>
		<ptr target="https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
