<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PAPER UNDER REVIEW 1 2BiVQA: Double Bi-LSTM based Video Quality Assessment of UGC Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Telili</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sid</forename><forename type="middle">Ahmed</forename><surname>Fezza</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Wassim</forename><surname>Hamidouche</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanene</forename><forename type="middle">F Z Brachemi</forename><surname>Meftah</surname></persName>
						</author>
						<title level="a" type="main">PAPER UNDER REVIEW 1 2BiVQA: Double Bi-LSTM based Video Quality Assessment of UGC Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Blind video quality assessment</term>
					<term>user-generated content</term>
					<term>deep learning</term>
					<term>Bi-LSTM</term>
					<term>spatial pooling</term>
					<term>temporal pool- ing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, with the growing popularity of mobile devices as well as video sharing platforms (e.g., YouTube, Facebook, TikTok, and Twitch), User-Generated Content (UGC) videos have become increasingly common and now account for a large portion of multimedia traffic on the internet. Unlike professionally generated videos produced by filmmakers and videographers, typically, UGC videos contain multiple authentic distortions, generally introduced during capture and processing by naive users. Quality prediction of UGC videos is of paramount importance to optimize and monitor their processing in hosting platforms, such as their coding, transcoding, and streaming. However, blind quality prediction of UGC is quite challenging because the degradations of UGC videos are unknown and very diverse, in addition to the unavailability of pristine reference. Therefore, in this paper, we propose an accurate and efficient Blind Video Quality Assessment (BVQA) model for UGC videos, which we name 2BiVQA for double Bi-LSTM Video Quality Assessment. 2BiVQA metric consists of three main blocks, including a pretrained Convolutional Neural Network (CNN) to extract discriminative features from image patches, which are then fed into two Recurrent Neural Networks (RNNs) for spatial and temporal pooling. Specifically, we use two Bi-directional Long Short Term Memory (Bi-LSTM) networks, the first is used to capture shortrange dependencies between image patches, while the second allows capturing long-range dependencies between frames to account for the temporal memory effect. Experimental results on recent large-scale UGC video quality datasets show that 2BiVQA achieves high performance at a lower computational cost than state-of-the-art models. The source code of our 2BiVQA metric is made publicly available at: https://github.com/atelili/2BiVQA. Index Terms-Blind video quality assessment, user-generated content, deep learning, Bi-LSTM, spatial pooling, temporal pooling.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Currently, video represents the majority of Internet traffic. According to Cisco's recent report <ref type="bibr" target="#b0">[1]</ref>, it now accounts for around 82% of global Internet traffic. Some of this traffic is generated by streaming video providers like Netflix, Amazon Prime Video, etc. Usually, the content they provide has been created by experts using professional capture devices and in a controlled environment, known as Professionally Generated Content (PGC). PGC videos are pristine high-quality videos that reach a certain level of perfection, making them suitable candidates as references in Video Quality Assessment (VQA) process. On the other hand, User-Generated Content (UGC) accounts for a significant portion of video traffic, which A. Telili and W. Hamidouche are with Univ. Rennes, INSA Rennes, CNRS, IETR -UMR 6164, Rennes, France (e-mail: atelili@insa-rennes.fr and whamidou@insa-rennes.fr ).</p><p>SA. Fezza and H. F. Z. Brachemi Meftah are with National Higher School of Telecommunications and ICT, Oran, Algeria (e-mail: sfezza@ensttic.dz). is collected and shared over social media and other videosharing platforms, such as Facebook, Youtube, TikTok and Twitch. This content is typically captured by nonprofessional users using their own capture devices (e.g., smartphones) and under different shooting conditions. Unlike PGC videos, UGC videos may suffer from multiple authentic distortions that can be introduced during acquisition. Moreover, compression and transmission distortions are still introduced before uploading to the hosting platform. UGC distortions are unpredictable, more diverse, intermixed, and the unavailability of a pristine reference makes the prediction of UGC video quality very challenging. Thus, there is a great need to accurately assess the quality of UGC videos in order to optimize and monitor their processing in hosting platforms, such as their coding, transcoding and streaming.</p><p>For VQA, the most reliable technique is to perform a subjective quality evaluation. In subjective tests, a panel of human viewers is asked to rate the quality of stimuli displayed and assessed under a particular protocol and viewing conditions <ref type="bibr" target="#b1">[2]</ref>. However, subjective tests are time-consuming, costly, and they cannot be used in real-time applications. As an alternative, objective quality measures have been developed to automatically predict the quality of videos. Depending on the required amount of the reference information, objective VQA metrics can be divided into three categories: Full Reference Video Quality Assessment (FR-VQA), Reduced Reference Video Quality Assessment (RR-VQA) and No Reference Video Quality Assessment (NR-VQA). FR-VQA quality metrics require the presence of the entire pristine video frames to compare against in order to compute the quality score. However, adopting such a strategy for UGC videos is not consistent, since the videos uploaded to the hosting platform have already undergone distortions due to acquisition and compression, making them not suitable as reference videos. Thus, no reference or blind VQA metrics remain the obvious solution that solves the UGC-VQA issue. Although most recent Blind Image Quality Assessment (BIQA)/Blind Video Quality Assessment (BVQA) methods achieve good performance on synthetic distortion datasets <ref type="bibr" target="#b2">[3]</ref>, their performance on UGC videos remains far from satisfactory <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b8">[8]</ref>, and predicting the quality of UGC videos is still challenging and unsolved problem.</p><p>Recently, with the massive growth in social media, attention has moved more towards building an accurate and efficient BVQA model suitable for UGC content, which allows achieving more intelligent analysis and processing in various applications. Hence, in recent years, researchers have deployed considerable efforts into the development of in-thewild UGC datasets such as KoNViD-1k <ref type="bibr" target="#b4">[5]</ref>, LIVE-VQC <ref type="bibr" target="#b6">[6]</ref>, arXiv:2208.14774v1 [eess.IV] 31 Aug 2022</p><p>and YouTube-UGC <ref type="bibr" target="#b7">[7]</ref>, to cite a few examples. These UGC datasets differ significantly from synthetic distortion datasets by a varied type and a wide range of distortions, but also by the fact that the distortion is not uniformly distributed over the spatial and temporal domains, resulting in fluctuating video quality.</p><p>The existing metrics do not consider or consider insufficiently this last aspect. They do not take into account how non-uniform distortions affect the overall frame quality score and how adjacent frames, from past and future, impact the perceived quality of the current frame. Typically, existing metrics use the mean as a pooling strategy, which is not a good representation of the spatial-temporal quality distributions. In this regard, we use a data-driven deep-learning approach in the proposed metric to enhance the VQA.</p><p>It is obviously desirable to have accurate video quality metrics for the UGC videos. Thus, in this work, we propose an efficient model for UGC-VQA, termed 2BiVQA for double Bi-LSTM Video Quality Assessment. The main contributions of this paper can be summarized as follows:</p><p>? We propose an accurate and efficient BVQA metric for UGC that performs the quality assessment in line with the Human Visual System (HVS). The components of 2BiVQA include a Convolutional Neural Network (CNN) for spatial feature extraction and two Recurrent Neural Networks (RNNs) for capturing spatial-temporal dependencies. We show that pre-training the features extraction module on an in-the-wild Image Quality Assessment (IQA) dataset significantly improves the performance of 2BiVQA. ? We leverage two RNNs, namely Bi-directional Long Short Term Memory (Bi-LSTM) networks, for both spatial and temporal pooling, which allows our model to take into account the characteristics of UGC videos as well as the HVS behavior. <ref type="bibr">?</ref> We conduct experiments on three widely-used UGC-VQA datasets to demonstrate the effectiveness of 2BiVQA. Experimental results show that the proposed 2BiVQA metric achieves competitive performance with State-Of-The-Art (SOTA) methods, provides the best generalization capability, and even at a low computational cost. The rest of this paper is organized as follows. Section II reviews related work, then Section III presents the proposed 2BiVQA model. The performance of our model is assessed and analyzed in Section IV. Finally, Section V concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Given the unavailability of pristine sources, FR-VQA metrics cannot well predict the perceptual quality of UGC videos. Thus, in this section, we focus on BVQA methods, as these methods are the most suitable for providing UGC video quality estimation. BVQA methods can be grouped into two categories, whether their relevant features are extracted from the input video based on conventional handcrafted techniques or deep learning-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Handcrafted feature-based BVQA models</head><p>The earliest BIQA/BVQA methods were mostly distortionspecific QA algorithms, which addressed a specific type of distortion, such as blur <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>, blockiness <ref type="bibr" target="#b11">[11]</ref>, ringing <ref type="bibr" target="#b12">[12]</ref>, banding <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref> and noise <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref> or targeted multiple types of distortion <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b18">[18]</ref>. Later on, the most successful handcrafted features-based BVQA models mainly rely on learning approaches, using a set of relevant perceptual features combined with a regression model to predict quality scores <ref type="bibr" target="#b19">[19]</ref>- <ref type="bibr" target="#b22">[22]</ref>.</p><p>The most popular BIQA/BVQA algorithms are based on Natural Scene Statistics (NSS) <ref type="bibr" target="#b23">[23]</ref>, extracted from either spatial domain or transform domain. NSS refer to the hypothesis that the natural scenes form a minor subspace within the space of all conceivable signals. These NSS are altered by the presence of distortions, so they were widely used to blindly measure the quality of images/videos. Successful models relying on NSS are derived from the spatial domain (NIQE <ref type="bibr" target="#b24">[24]</ref>, BRISQUE <ref type="bibr" target="#b24">[24]</ref>), Discrete Cosine Transform (DCT) (BLIINDS <ref type="bibr" target="#b25">[25]</ref> and BLIINDS-II <ref type="bibr" target="#b26">[26]</ref>) and Discrete Wavelet Transform (DWT) (BIQI <ref type="bibr" target="#b27">[27]</ref>, DIIVINE <ref type="bibr" target="#b19">[19]</ref>, C-DIIVINE <ref type="bibr" target="#b28">[28]</ref>). These metrics have been expanded to the VQA task using the space-time natural video statistics models <ref type="bibr" target="#b29">[29]</ref>- <ref type="bibr" target="#b32">[31]</ref>. Other extensions of NSS have been proposed including in log-derivative and log-Gabor spaces (DESIQUE <ref type="bibr" target="#b33">[32]</ref>), the joint statistics of the gradient magnitude and Laplacian of Gaussian responses in the spatial domain (GM-LOG <ref type="bibr" target="#b34">[33]</ref>) and the gradient domain of LAB color transforms (HIGRADE <ref type="bibr" target="#b20">[20]</ref>). HIGRADE is based on both gradient orientation information extracted from the gradient structure tensor and gradient magnitude. Once these features are extracted, a mapping is performed from the feature space to the Mean Opinion Score (MOS) scores using the Support Vector Regression (SVR). In <ref type="bibr" target="#b21">[21]</ref>, the authors proposed a bag of feature-maps approach. In which several feature maps are derived from multiple color spaces and transform domains, then scene statistics from each of these maps are extracted. The obtained results demonstrated the relevance of the extracted features for the quality prediction of images corrupted by complex mixtures of authentic distortions. Motivated by the success of unsupervised feature learning for BIQA in CORNIA <ref type="bibr" target="#b35">[34]</ref>, the authors proposed to extend it to video signal V-CORNIA <ref type="bibr" target="#b36">[35]</ref>. In V-CORNIA, frame-level features are extracted by unsupervised feature learning, and a SVR is subsequently used to learn a mapping from feature space to frame-level quality scores. Finally, an overall video quality score is derived via a hysteresis temporal pooling.</p><p>To leverage these rich IQA metrics for VQA context, a straightforward approach is to compute the quality score of each frame and then pool them into an overall video quality score. The most adopted temporal pooling method is the average, however, this approach does not take into account the temporal change and quality fluctuation. This is why more advanced temporal pooling strategies have been proposed <ref type="bibr" target="#b36">[35]</ref>- <ref type="bibr" target="#b38">[37]</ref>.</p><p>However, performing VQA cannot be based solely on spatial information, i.e., based only on IQA metrics, since temporal information such as motion plays a crucial role in the perception of quality/distortion in the video and must be taken into account. Therefore, unlike simply extending IQA methods to assess video quality using a pooling strategy, other methods have attempted to include temporal information directly in their models. For instance, a completely blind metric called Video Intrinsic Integrity and Distortion Evaluation Oracle (VIIDEO) was proposed in <ref type="bibr" target="#b39">[38]</ref>. VIIDEO is based on a set of perceptually relevant temporal video statistic models of video frame difference signals. Inter-subband correlations over local and global time spans were used to quantify the degree of distortion present in the video and thus predict the video quality score. Manasa et al. <ref type="bibr" target="#b40">[39]</ref> proposed to estimate perceptual quality by estimating statistical irregularities in optical flow using features at the patch level and the frame level. V-BLIINDS has been proposed in <ref type="bibr" target="#b32">[31]</ref>, which includes a spatio-temporal NSS model of DCT coefficient statistics, as well as a motion model that quantifies motion coherency in the video. Li et al. <ref type="bibr" target="#b29">[29]</ref> proposed a BVQA based on the spatiotemporal statistics of videos in the 3D-DCT domain, which allows to simultaneously describe the spatial and temporal regularities of local space-time regions.</p><p>Two-Level Approach for No-Reference Consumer Video Quality Assessment (TLVQM) <ref type="bibr" target="#b41">[40]</ref> is another handcrafted features-based BVQA method relying on a two-level approach for features extraction. First, Low Complexity Features (LCF) are calculated at a rate of one frame per second over the entire video sequence, then the LCF are utilized to select a set of representative subset of frames for calculating High Complexity Features (HCF). Finally, both low and high complexity features are aggregated as a single feature vector representing the whole video sequence by using SVR as a regression model. A more recent fusion-based BVQA model is VIDeo quality EVALuator (VIDEVAL) <ref type="bibr" target="#b3">[4]</ref>, which is based on features selection among top-performing BIQA/BVQA models such as BRISQUE, HIGRADE, TLVQM, etc. To select the most relevant features, Random Forest (RF) is used to remove the less significant features. Finally, a Support Vector Machine (SVM) with a linear kernel is trained to regress the final features vector into a quality score. Kancharla et al. <ref type="bibr" target="#b42">[41]</ref> proposed a BVQA method, which includes a bandpass filter model of the visual system to evaluate the temporal quality and a weighted NIQE module to estimate the frame-level spatial quality. Finally, the global video quality score is computed by the average of the spatial quality and the temporal quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep learning-based BVQA models</head><p>In recent years, deep CNNs have shown outstanding performance in a wide range of computer vision tasks such as image classification <ref type="bibr" target="#b43">[42]</ref>, <ref type="bibr" target="#b44">[43]</ref>, object detection <ref type="bibr" target="#b45">[44]</ref>, <ref type="bibr" target="#b46">[45]</ref> and image segmentation <ref type="bibr" target="#b47">[46]</ref>, <ref type="bibr" target="#b48">[47]</ref>. Recently, with the release of several larger IQA/VQA datasets <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b49">[48]</ref>, <ref type="bibr" target="#b50">[49]</ref>, deep CNNs have been extensively explored to solve image/video quality assessment problem. Yet, due to the lack of large-scale IQA/VQA datasets, it is quite challenging to train a deep CNN from scratch to reach a competitive performance. To overcome the limitation of small data size, two solutions have been used in the literature: 1) performing a patch-wise training to increase data samples <ref type="bibr" target="#b51">[50]</ref>, <ref type="bibr" target="#b52">[51]</ref>, or 2) leveraging pre-trained deep CNNs on large datasets like ImageNet <ref type="bibr" target="#b53">[52]</ref>, then performing fine-tuning on target IQA/VQA datasets.</p><p>The first adoption of a CNN model to the problem of IQA was made by Kang et al. in <ref type="bibr" target="#b51">[50]</ref>, where a CNN was used to blindly predict the image quality score. They combined feature learning and regression in end-to-end optimization without using handcrafted features. Following this work, considerable deep learning-based BIQA methods have been proposed <ref type="bibr" target="#b54">[53]</ref>, which achieved quite good performance. For video, on the other hand, very few methods based on deep learning have been dedicated to BVQA.</p><p>For instance, Ahn et al. <ref type="bibr" target="#b55">[54]</ref> proposed a BVQA metric based on a deep CNN model named DeepBVQA, which includes various spatial and temporal cues. In DeepBVQA, a pretrained CNN model for IQA is used to extract spatial features from each frame, and temporal sharpness variation is exploited to extract temporal features. Then, these spatial and temporal features are combined into a video feature to be regressed to a final quality score. Another deep learning-based VQA model was proposed in <ref type="bibr" target="#b56">[55]</ref>, which consists of a 3D-CNN to extract spatio-temporal features followed by a Long Short Term Memory (LSTM) to predict the perceived video quality. A multi-task CNN framework, named V-MEON, was proposed in <ref type="bibr" target="#b57">[56]</ref> that predicts both the quality score and codec type of a video. V-MEON is based on 3D-CNN network to extract spatio-temporal features from a video, followed by the codec classifier and the quality predictor that are jointly optimized. VSFA <ref type="bibr" target="#b58">[57]</ref> model also uses a CNN, pre-trained on image classification tasks, as a features extraction module, and then it uses Gated Recurrent Unit (GRU) and a subjectively-inspired temporal pooling layer to output the video quality score. Next, an improved version of VSFA, named MDVSFA, was proposed in <ref type="bibr" target="#b59">[58]</ref>. MDVSFA uses a mixed dataset training strategy for training a single VQA model with multiple datasets. Yi et al. <ref type="bibr" target="#b60">[59]</ref> proposed a modified VGG-16 network with non-local layers to learn the global relationship of spatial features, which can be regarded as a kind of attention mechanism. In addition, they combined GRU and a temporal pooling layer to model the temporal-memory effects.</p><p>More  consists of three stages: a multiscale feature extraction network that extracts spatio-temporal features, a hierarchical spatiotemporal fusion network that integrates intermediate feature information, and finally, a quality regression network that predicts the video quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED DOUBLE BI-LSTM VIDEO QUALITY ASSESSMENT METHOD</head><p>Let us consider a video sequence V as a set of T consecutive frames: V = {x 1 , x 2 , . . . , x T }. The problem of UGC-VQA consists in predicting a quality scoreq from a video sequence V as follows:</p><formula xml:id="formula_0">q = m(x 1 , x 2 , . . . , x T ).<label>(1)</label></formula><p>To address this problem, we propose a BVQA metric called 2BiVQA for double Bi-LSTM Video Quality Assessment. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the framework of the proposed 2BiVQA is composed of four main modules: features extraction, spatial pooling, temporal pooling, and finally, a quality regression module. These four modules are integrated to form an end-toend BVQA metric. Each of the four modules will be described in detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Features extraction</head><p>CNN features have been shown to correlate well with perceptual judgments <ref type="bibr" target="#b64">[63]</ref> and represent good candidates for human perception-related applications <ref type="bibr" target="#b64">[63]</ref>- <ref type="bibr" target="#b68">[67]</ref>. The performance of CNN strongly depends on the number of training samples. However, existing UGC-VQA datasets are much smaller compared to the typical computer vision datasets with millions of samples. Thus, it is very difficult to train a deep CNN from scratch while achieving competitive quality prediction performance, since the model can be prone to over-fitting problem. Nevertheless, the authors of <ref type="bibr" target="#b3">[4]</ref> showed that well-known deep CNN feature descriptors (e.g., ResNet-50 <ref type="bibr" target="#b44">[43]</ref>, VGG-16 <ref type="bibr" target="#b69">[68]</ref>, etc.) pre-trained on other vision tasks like image classification are transferable to UGC IQA/VQA problems, and they can achieve outstanding performance. In our contribution, we opted for the ResNet-50 pre-trained on ImageNet <ref type="bibr" target="#b53">[52]</ref> as the backbone model to extract spatial features. Even though several CNN models can be used, we obtained the best performance with ResNet-50 (see <ref type="table" target="#tab_1">Tables II  and III</ref>). In the following, we first introduce the backbone model, and then we detail the feature extraction process.</p><p>ResNet-50: is a variant of ResNet model that introduced a concept allowing to train ultra-deep neural networks that can include hundreds and even thousands of layers. In fact, theoretically, a deeper neural network is able to learn more complex features, which should lead to better performance. However, in practice, the ultimate effect of adding more and more layers is increasing the training error. This problem is known as the degradation problem <ref type="bibr" target="#b44">[43]</ref>. Residual blocks introduced in ResNet aim to solve this issue. It includes shortcut connections to perform identity mapping, which allows a deeper model to have no higher training error than its shallower counterpart.</p><p>Features extraction process: one issue with using pretrained models is their standard input shape. Two solutions can be envisaged to overcome this constraint, either resizing the input frame or dividing it into several patches. The first technique can affect the perceptual quality or attenuate the intensity of pre-existing artifacts. Therefore, we opted for the second solution, which solves the problem of a standard size input on the one hand, and avoids over-fitting with the limited dataset on another hand. Thus, for each frame x i a sliding window is used to extract N patches x j i , ?j ? {1, . . . , N }, ?i ? {1, . . . , T }, with a stride slightly smaller than the patch size (224 ? 224). Then, these patches are fed into the ResNet-50 model pre-trained on ImageNet for spatial features extraction as follows:</p><formula xml:id="formula_1">y j i = h ? (x j i ),<label>(2)</label></formula><p>where h ? represents the parametric function of the feature </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatial pooling</head><p>Once the features have been extracted from each patch, we need to aggregate them into one vector per frame. To do this, it is essential to take into account that UGC distortions are not uniformly distributed. In addition, the local distortion visibility is influenced by its surrounding regions, which can either emphasize or mask the perception of distortion. Moreover, the perceptual quality of HVS varies over the spatial domain <ref type="bibr" target="#b70">[69]</ref>.</p><p>Thus, to mimic the HVS behavior as well as to account for UGC distortion features, unlike processing each patch independently, we consider the entire sequence of patches (x 1 i , x 2 i , ..., x N i ) T of a frame i at once. To achieve this, we use a sequence model that can efficiently capture the dependencies between the patches of a frame. Specifically, we design our spatial pooling using Bi-LSTM network <ref type="bibr" target="#b71">[70]</ref> as the sequence model, which provides the ability to deal with dependencies across patches.</p><p>In the following, we first explain the internal mechanisms of LSTM <ref type="bibr" target="#b72">[71]</ref>, then we introduce Bi-LSTM, and finally, the learning strategy of the proposed spatial pooling module is described.</p><p>LSTM (Long short term memory) <ref type="bibr" target="#b72">[71]</ref>: is one of the most popular RNNs, designed to deal with long time-dependencies. It allows solving the diminishing and exploding gradient problems in long structures <ref type="bibr" target="#b73">[72]</ref>. Each LSTM cell consists of an input gate i t , a forget gate f t , an output gate o t , a candidate cell statec t , a cell state c t , and a hidden state h t , as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. i t is used to determine the information to store in the current cell state c t , while f t determines the thrown away information. o t decides the information to be passed to the current hidden state h t , which is computed as follows: where ? is the sigmoid function and ? is the concatenation operator. + and ? are the element-wise addition and product operations, respectively. W (x) and b (x) represent the weight matrix and the bias vector of gate x, respectively. Bi-LSTM (Bi-directional long short term memory) <ref type="bibr" target="#b71">[70]</ref>: is a stack of two independent LSTMs. This structure allows the network to take both backward and forward information in consideration. It has been proved that Bi-LSTM is far better than regular LSTM in many fields, like forecasting time series <ref type="bibr" target="#b74">[73]</ref>, phoneme classification <ref type="bibr" target="#b75">[74]</ref>, speech recognition <ref type="bibr" target="#b76">[75]</ref>, etc. However, to the best of our knowledge, Bi-LSTM has not yet been considered in IQA/VQA problems.</p><formula xml:id="formula_2">i t = ?(W (i) ? (h t?1 ? x t ) + b (i) ), f t = ?(W (f ) ? (h t?1 ? x t ) + b (f ) ), o t = ?(W (o) ? (h t?1 ? x t ) + b (o) ), c t = tanh(W (c) ? (h t?1 ? x t ) + b (c) ), c t = f t ? c t?1 + i t ?c t , h t = o t ? tanh(c t ),<label>(3)</label></formula><p>The architecture of the spatial pooling module is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The module is composed of two Bi-LSTM layers with K cells each, followed by a FC layer with 256 nodes. We have found that using this architecture with K = 64 leads to the best results in our experiments. The feature vector (y 1 i , y 2 i , . . . , y N i ) T of a frame i is fed into the spatial pooling module, expressed as:</p><formula xml:id="formula_3">y i = g ?s (y 1 i , y 2 i , . . . , y N i ), ?i ? {1, . . . , T },<label>(4)</label></formula><p>where g ?s is the parametric function of the spatial pooling module with the training parameters ? s . Pre-training technique: transfer learning is a powerful machine learning technique. Here, we perform pre-training followed by fine-tuning, which is a widely-used transfer learning paradigm. Pre-training refers to training a model in a specific source domain D S with learning task T S to initialize its parameters before fine-tuning it for a new learning task T T in the target domain D T , where a domain D consists of a feature space X and a marginal probability distribution P (X), where X = {s 1 , . . . , s n } ? X .</p><p>In our approach, the spatial pooling module is trained in this way in two separate stages. It is first pre-trained using the KonIQ-10k dataset <ref type="bibr" target="#b77">[76]</ref>, which is a large-scale in-the-wild IQA dataset, regardless of the remaining module as an IQA model. We assume that previously described complex behaviors and characteristics of HVS and UGC distortions, respectively, are embedded in the subjective quality dataset. Thus, we aim to transfer the knowledge acquired by the model after training on the KonIQ-10k dataset, encoded in the weights of the model, to exploit it for the target UGC-VQA task. Moreover, this pre-training step has the advantage of presenting to the model more diverse content by leveraging the larger authentic IQA dataset.</p><p>Finally, the spatial pooling module is fine-tuned using the subjective video quality scores with the rest of the modules in the second stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Temporal pooling</head><p>Aggregating quality features of frames into an overall video score is one of the main still unresolved challenges in VQA <ref type="bibr" target="#b37">[36]</ref>, <ref type="bibr" target="#b38">[37]</ref>, <ref type="bibr" target="#b41">[40]</ref>, <ref type="bibr" target="#b59">[58]</ref>, <ref type="bibr" target="#b78">[77]</ref>. In fact, the human quality judgment at a late frame can definitely be affected by the previous frames, which is widely known as the temporal-memory effect <ref type="bibr" target="#b36">[35]</ref>, <ref type="bibr" target="#b78">[77]</ref>- <ref type="bibr" target="#b80">[79]</ref>. According to this temporal behavior of the HVS, low-quality frames leave more impressions on the viewer. For instance, if most of the frames are of high quality, and only a few frames are of low quality, humans generally determine that the video is of low quality. Most of the previously developed VQA metrics focus much more on the accuracy of quality scores at the frame level, disregarding the impact of adjacent frames, from past and future, on the subjective quality of the current frame.</p><p>Therefore, in order to take into account the temporal variation of distortions as well as the temporal-memory effects of human perception, we propose a novel temporal pooling method using Bi-LSTM network to aggregate framelevel features (? 1 ,? 2 , . . . ,? T ) into a global feature vector y for the entire video sequence. As described previously, Bi-LSTM networks have the ability to take both backward and forward information into consideration, which makes it possible to capture long-range dependencies between frames like the temporal-memory effect. Similar to spatial pooling, this module is composed of two Bi-LSTM layers with 64 cells each, followed by a FC layer with 256 nodes. The temporal pooling module is defined as:</p><formula xml:id="formula_4">y = g ?t (? 1 ,? 2 , . . . ,? T ),<label>(5)</label></formula><p>where g ?t is the parametric function of the temporal pooling module with the training parameters ? t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Quality regression</head><p>After extracting quality-aware features and aggregating them into a single vector?, we need to map these features to the final video quality score q. Here, we used one node FC as a regression model with a linear activation function. Therefore, we obtain the final video quality score as follows:</p><formula xml:id="formula_5">q = ?(?),<label>(6)</label></formula><p>where ? denotes the FC layer.  <ref type="bibr" target="#b7">[7]</ref> 1380 360p-4k 20s MOS+? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> IV. EXPERIMENTAL RESULTS</p><p>In this section, we first define the experimental setups, including the description of datasets, the evaluation methods and the implementation details. Then, we present the results of ablation studies, the comparison with SOTA, the generalization capability, and finally, the complexity evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>To train, fine-tune and test the proposed model, three UGC-VQA datasets are considered, including KoNViD-1K <ref type="bibr" target="#b4">[5]</ref>, LIVE-VQC <ref type="bibr" target="#b6">[6]</ref> and YouTube-UGC <ref type="bibr" target="#b7">[7]</ref>. The features of these three datasets are summarized in <ref type="table" target="#tab_1">Table I</ref>. For YouTube-UGC, we excluded 57 grayscale videos for a fair comparison as in <ref type="bibr" target="#b3">[4]</ref>. We also used these three datasets to create a fourth dataset, which is the union of them after MOS calibration using the Iterative Nested Least Squares Algorithm (INSLA) <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b81">[80]</ref>:</p><formula xml:id="formula_6">q = 5 ? 4 ? ((5 ? q)/4 ? 1.1241 ? 0.0993) , (7) q = 5 ? 4 ? ((100 ? q)/100 ? 0.7132 + 0.0253) ,<label>(8)</label></formula><p>where Eqs. <ref type="formula">(7)</ref> and <ref type="formula" target="#formula_6">(8)</ref> are used for calibrating KoNViD-1K and LIVE-VQC, respectively, while YouTube-UGC is selected as the anchor dataset. q and q denote the adjusted and the original scores, respectively. The formed dataset is referred to in the following as All-Combined. In addition, the KonIQ-10k IQA dataset <ref type="bibr" target="#b77">[76]</ref> is used to train the spatial pooling module separately. This dataset contains 10,073 in-the-wild images with different resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation methods</head><p>Since there is no defined size of the training (or test) set for KoNViD-1k, LIVE-VQA, and YouTube-UGC, and as convened, we randomly split each dataset into two nonoverlapping subsets, 80% for training and 20% for testing. We performed k fold iterations, and the median performance on the test sets is reported.</p><p>We considered four standard measures to assess the performance of the proposed model, including Spearman Rank Order Correlation Coefficient (SROCC) and Kendall Rank Correlation Coefficient (KRCC), which are prediction monotonicity measures, and Pearson Linear Correlation Coefficient (PLCC) and Root Mean Squared Error (RMSE), which are prediction accuracy measures. Before calculating PLCC and RMSE, we perform a nonlinear four-parametric logistic regression to match the predicted score to the subject score as follows:  <ref type="figure">Fig. 4</ref>: The architecture of the spatial pooling module for pretraining.</p><formula xml:id="formula_7">L g (x) = ? + ? ? ? 1 + exp(?x + ?/|?|) .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training process</head><p>The training is conducted in two steps: first, a pre-training of the spatial pooling module, followed by an end-to-end training of the model.</p><p>For the pre-training step, each image from KonIQ-10k dataset is divided into N patches. Then, the features extracted from the patches (y 1 , y 2 , . . . , y N ) T are fed into the spatial pooling module for training. A FC layer with an identity activation function is added as a regressor head to predict the image quality scoreq I , as illustrated in <ref type="figure">Figure 4</ref>.</p><p>The pre-training process is performed with 200 epochs using Adam optimizer <ref type="bibr" target="#b82">[81]</ref> with an initial learning rate of 1e ? 4, batch size of 16 and the Mean Squared Error (MSE) as loss function 2 :</p><formula xml:id="formula_8">2 (q I ,q I ) = 1 L L l=1 q I l ?q I l 2 ,<label>(10)</label></formula><p>where q I ,q I and L represent the ground truth, the predicted image quality score and the batch size, respectively. The second stage of the training process is to map the framelevel feature vectors into the global quality score. For this purpose, the spatial pooling module is fine-tuned at the same time as the temporal pooling module on the target UGC-VQA datasets. This is done using the same hyper-parameters as the pre-training: 200 epochs with an initial learning rate 1e ? 4 and MSE as loss function 2 (q,q) computed between q and q, which represent the ground truth and the predicted video quality scores, respectively.</p><p>Note that during fine-tuning and training, the weights of the backbone model are frozen. The proposed model was implemented using the TensorFlow framework 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation studies</head><p>To justify the choice and highlight the contribution of each component in the proposed model, we conducted ablation studies on the following aspects. The first study aims to select the best backbone model to extract reliable perceptual features. We considered four well-known deep CNN models: VGG16 <ref type="bibr" target="#b69">[68]</ref>, Densenet169 <ref type="bibr" target="#b83">[82]</ref>, ResNet-50 <ref type="bibr" target="#b44">[43]</ref> and Efficient-NetB7 <ref type="bibr" target="#b84">[83]</ref>. Also, we tested four spatial pooling methods: simple concatenation, arithmetic mean, LSTM, and Bi-LSTM. The result of this first study is reported in <ref type="table" target="#tab_1">Table II</ref>, where some interesting observations can be made. First, ResNet-50 is able to extract the most significant perceptual features and achieves better performance than the other CNN models. Second, RNN models (LSTM and Bi-LSTM) are the pooling methods that obtained the highest correlation scores allowing a 6.09% improvement in terms of SROCC compared to the classical pooling methods, including concatenation and arithmetic mean. Finally, this study shows that the best combination for the IQA is ResNet-50 as a features extraction model and Bi-LSTM as a spatial pooling method.</p><p>In the second study, we investigated the effect of pretraining the spatial pooling module, and we also tested several temporal pooling methods, including arithmetic mean, harmonic mean, geometric mean, LSTM, and Bi-LSTM. We depict the results of this second study in <ref type="table" target="#tab_1">Table III</ref>. It is important to note that this study is conducted on KoNViD-1K dataset with a randomly 80%-20% split over only one iteration to avoid a huge training time. The results show that pre-training the spatial pooling module on KonIQ-10k dataset significantly improves the prediction performance, for instance in terms of SROCC by 3.04%. Moreover, the results indicate that Bi-LSTM is the best-performing temporal pooling method, showing its effectiveness in capturing long-range dependencies between frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Performance evaluation and comparison</head><p>To assess the performance of the proposed 2BiVQA metric, we compared it with ten BIQA models (BRISQUE <ref type="bibr" target="#b24">[24]</ref>, NIQE <ref type="bibr" target="#b85">[84]</ref>, ILNIQE <ref type="bibr" target="#b86">[85]</ref>, GM-LOG <ref type="bibr" target="#b34">[33]</ref>, HIGRADE <ref type="bibr" target="#b20">[20]</ref>, CORNIA <ref type="bibr" target="#b35">[34]</ref>, HOSA <ref type="bibr" target="#b87">[86]</ref>, FRIQUEE <ref type="bibr" target="#b21">[21]</ref>, PaQ-2-PiQ <ref type="bibr" target="#b88">[87]</ref> and KonCept512 <ref type="bibr" target="#b77">[76]</ref>), and five BVQA models (V-BLIINDS <ref type="bibr" target="#b32">[31]</ref>, VIIDEO <ref type="bibr" target="#b39">[38]</ref>, TLVQM <ref type="bibr" target="#b41">[40]</ref>, VIDEVAL <ref type="bibr" target="#b3">[4]</ref> and RAPIQUE <ref type="bibr" target="#b61">[60]</ref>). In addition, two deep CNN models (VGG-19 <ref type="bibr" target="#b43">[42]</ref> and ResNet-50 <ref type="bibr" target="#b44">[43]</ref>) using transfer learning were benchmarked. Among these methods, NIQE, ILNIQE, and VIIDEO are completely blind because they don't require any training. The rest of the methods were trained and tested under the same conditions as our proposed model. For VGG-19 and ResNet-50 models, the frame-level scores are obtained using two FC layers with 256 and 1 nodes, respectively. For all considered BIQA models, we extend them for VQA by averaging the separate frame quality scores to obtain the overall video quality score. <ref type="table" target="#tab_6">Table V</ref> shows the performance of these methods in the four considered datasets. We can notice that most of the BIQA metrics, except those CNN-based, provide low performance, which indicates that the temporal-related features are substantial for VQA, and using a simple average pooling is not sufficient to achieve high performance. We can also observe that CNN-based BIQA approaches, i.e., VGG-19 and ResNet-50, perform well on larger datasets (KoNViD-1k, YouTube-UGC and All-Combined), showing the superiority of the datadriven deep-learning approaches over handcrafted featurebased ones when trained with sufficient dataset size.</p><p>On KoNViD-1K, BVQA methods generally provide acceptable results, while our 2BiVQA model achieves the best results, even outperforming RAPIQUE, one of the most recent SOTA models. On LIVE-VQC, which contains many mobile videos showing huge camera motions, 2BiVQA also achieves the best performance on three of the four evaluation metrics. TLVQM method also yields competitive scores on this dataset, thanks to its many heavily designed motion-relevant features.</p><p>On YouTube-UGC, 2BiVQA and VIDEVAL metrics achieve the best correlation scores, outperforming by fare the other BVQA models. Finally, for the largest dataset (All-Combined), 2BiVQA delivers the second-best performance, slightly outperformed by RAPIQUE. <ref type="figure" target="#fig_4">Figure 5</ref> shows the MOS versus the prediction scores and nonlinear logistic fitted curves for the three best performing models (VIDEVAL, RAPIQUE and 2BiVQA) on the four evaluated datasets. These figures illustrate visually that the performance of 2BiVQA remains stable over the different datasets. Its scatter points are more densely clustered around the fitted curves, which are also more linear, especially for KoNViD-1k, YouTube-UGC, and All-Combined datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Cross dataset generalization</head><p>A good VQA metric is supposed to generalize to unseen samples. Accordingly, we perform a cross dataset evaluation by training the three best performing BVQA models on one dataset and testing them on the other datasets. The results are shown in <ref type="table" target="#tab_1">Table IV</ref>. From this table, we can observe that the proposed model generalizes well to unseen datasets, and its performance does not depend on the dataset, which represents an essential feature for UGC-BVQA. This good generalization of the proposed method, which we believe is primarily due to the separation of the training into two stages, first the pretraining on KonIQ-10k dataset and then the fine-tuning on the target UGC-VQA dataset. The training on this diverse content allows our model to learn a rich feature representation suitable for UGC video quality score prediction. It can also be noted   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Complexity and runtime comparison</head><p>Computational efficiency is crucial for VQA algorithms, especially in practical deployments. In this regard, we performed runtime comparisons of our model as well as several methods on the same desktop computer equipped with an Intel? Xeon W-2145 CPU @ 3.70GHz ?16, 64G RAM, and GeForce RTX 2080 Ti graphics card under Ubuntu 20.04 Long Term Support (LTS) operating system. We used the initially released implementation in MATLAB R2018b and python 3.8.8 for GM-LOG, VIDEVAL, and RAPIQUE metrics. For BRISQUE and NIQE, we used scikit-video python library implementation. The remaining models, namely VGG19 and 2BiVQA, were implemented in TensorFlow. All BIQA models extract features at one frame per second, and then an average pooling was used to get the overall video quality score. We consider videos from YouTube-UGC at HD resolution (1920 ? 1080), then we recorded the average runtime in seconds, as shown in <ref type="table" target="#tab_1">Table  VI</ref>. For better illustration, <ref type="figure">Figure 6</ref> shows the scatter plots of SROCC versus runtime. It may be observed that BIQA models are faster than other methods, while VGG19, RAPIQUE, and 2BiVQA are relatively comparable.  V. CONCLUSION</p><p>In this paper, we proposed an effective BVQA metric for UGC videos, named 2BiVQA for double Bi-LSTM Video Quality Assessment. Our contribution relies on a deep CNNbased model to extract frame-level features and two Bi-LSTM networks for spatial and temporal pooling. Specifically, the first Bi-LSTM network is used to efficiently capture the shortterm dependencies between neighboring patches, while the second Bi-LSTM network is exploited to capture long-range dependencies between frames over the entire video. In this way, the proposed 2BiVQA can take into account the features of UGC videos and mimic the behavior of the HVS.</p><p>In addition, the training was carried out in two stages to avoid over-fitting with the limited dataset. This training strategy improved feature representation, which significantly increased accuracy performance.</p><p>We conducted comprehensive tests on four UGC-VQA datasets. Results showed that 2BiVQA outperforms SOTA methods on two of the considered datasets (KonViD-1k and LIVE-VQC) and achieves competitive performance on YouTube-UGC and All-Combined. We further showed that the performance of the proposed solution is independent of the training dataset and generalizes better on unseen datasets than other BVQA methods, which is a key feature of the UGC VQA problem. Finally, since computational efficiency is crucial for BVQA algorithms, especially in real-time applications such as streaming, 2BiVQA has achieved a good trade-off between inference runtime and prediction performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The overall framework of the proposed 2BiVQA metric. The features extraction module is used to extract spatial features from patches x j i . The spatial and temporal pooling modules are used to aggregate features into a final vector? while accounting for HVS behavior. Finally, the regression module uses the final vector? to predict the quality scoreq.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The internal structure of the LSTM cell. extraction model with training parameters ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The architecture of the spatial pooling module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Scatter plots and nonlinear logistic fitted curves of VIDEVAL, RAPIQUE, and 2BiVQA models versus MOS using kfold cross-validation on KoNViD-1k, LIVE-VQC, YouTube-UGC, and All-Combined datasets. The logistic model coefficients are given for the three objective metrics tested on the four datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>recently, Tu et al.<ref type="bibr" target="#b61">[60]</ref> proposed a hybrid method, named Rapid and Accurate Video Quality Prediction Evaluator (RAPIQUE), which uses both handcrafted and deep CNNbased high-level features. RAPIQUE is based on two modules, a NSS features extractor module, which extracts both spatial and temporal features, and a deep CNN features extractor (ResNet-50) which extracts deep high-level features. Finally, a regressor model is used to map the extracted features to a quality score. In<ref type="bibr" target="#b62">[61]</ref>, the authors proposed to hierarchically add the feature maps from intermediate layers into the final feature maps and calculate their global mean and standard deviation as the feature representation. Thus, covering the full range of visual features from low-level to high-level. Subsequently, Fully Connected (FC) and temporal pooling are used for the quality regression. In the same way, Shen et al.<ref type="bibr" target="#b63">[62]</ref> proposed a BVQA method with spatio-temporal feature fusion and hierarchical information integration. Their metric</figDesc><table><row><cell>. . . f r a m e s</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Features Extraction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>. . .</cell><cell>N patches</cell><cell>7?7 Conv, 64</cell><cell>3?3 Max Pool</cell><cell>1?1 Conv, 64</cell><cell>3?3 Conv, 64</cell><cell>1?1 Conv, 256</cell><cell>1?1 Conv, 128</cell><cell>3?3 Conv, 128</cell><cell>1?1 Conv, 512</cell><cell>1?1 Conv, 256</cell><cell>3?3 Conv, 256</cell><cell>1?1 Conv, 1024</cell><cell>1?1 Conv, 512</cell><cell>3?3 Conv, 512</cell><cell>1?1 Conv, 2048</cell><cell>GAP</cell><cell>. . .</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Spatial Pooling</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Temporal Pooling</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>64 BI-LSTM</cell><cell>64 BI-LSTM</cell><cell>256 FC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64 BI-LSTM</cell><cell>64 BI-LSTM</cell><cell>256 FC</cell><cell></cell><cell>FC</cell><cell></cell><cell></cell></row></table><note>. . .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Summary of the considered UGC-VQA datasets.</figDesc><table><row><cell cols="2">Database #Video</cell><cell>Resolution</cell><cell>Time</cell><cell>Label</cell><cell>Range</cell></row><row><cell>KoNViD-1k [5]</cell><cell>1200</cell><cell>540p</cell><cell>8s</cell><cell>MOS+?</cell><cell>[1,5]</cell></row><row><cell>LIVE-VQC [6]</cell><cell>585</cell><cell>240p-1080p</cell><cell>10s</cell><cell>MOS</cell><cell>[0,100]</cell></row><row><cell>YouTube-UGC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Performance of the ablation study for the spatial pooling module on the KonIQ-10k dataset. Bold entries indicate the top three performing methods, while the best method is underlined.</figDesc><table><row><cell>Model</cell><cell>#Features per patch</cell><cell>Spatial pooling</cell><cell>SROCC ?</cell><cell>PLCC ?</cell><cell cols="2">KRCC ? RMSE ?</cell></row><row><cell>VGG16</cell><cell>512</cell><cell>Concatenate Mean LSTM</cell><cell>0.821 0.818 0.875</cell><cell>0.829 0.829 0.894</cell><cell>0.628 0.622 0.689</cell><cell>0.378 0.381 0.249</cell></row><row><cell></cell><cell></cell><cell>Bi-LSTM</cell><cell>0.876</cell><cell>0.900</cell><cell>0.693</cell><cell>0.242</cell></row><row><cell>Densenet169</cell><cell>1664</cell><cell>Concatenate Mean LSTM Bi-LSTM</cell><cell>0.833 0.825 0.871 0.878</cell><cell>0.850 0.839 0.898 0.902</cell><cell>0.639 0.623 0.691 0.698</cell><cell>0.293 0.373 0.245 0.240</cell></row><row><cell>ResNet50</cell><cell>2048</cell><cell>Concatenate Mean LSTM</cell><cell>0.856 0.856 0.906</cell><cell>0.871 0.858 0.924</cell><cell>0.669 0.664 0.737</cell><cell>0.275 0.275 0.214</cell></row><row><cell></cell><cell></cell><cell>Bi-LSTM</cell><cell>0.910</cell><cell>0.925</cell><cell>0.742</cell><cell>0.213</cell></row><row><cell>EfficientNetB7</cell><cell>2560</cell><cell>Concatenate Mean LSTM Bi-LSTM</cell><cell>0.763 0.791 0.851 0.853</cell><cell>0.741 0.816 0.873 0.878</cell><cell>0.570 0.593 0.664 0.668</cell><cell>0.414 0.391 0.272 0.266</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Performance of the ablation study on the KoNViD-1k dataset. Each entry is presented as spatial pooling without/with pre-training on the KonIQ-10k dataset. Bold entries indicate the top three performing methods, while the best method is underlined.</figDesc><table><row><cell>Model</cell><cell>Temporal pooling</cell><cell>SROCC ?</cell><cell>PLCC ?</cell><cell>KRCC ?</cell><cell>RMSE ?</cell><cell>#parameters</cell></row><row><cell></cell><cell>Mean</cell><cell cols="2">0.776 / 0.771 0.773 / 0.767</cell><cell cols="2">0.588 / 0.575 0.424 / 0.451</cell><cell>1,213,953</cell></row><row><cell></cell><cell>Harmonic</cell><cell cols="2">0.776 / 0.773 0.774 / 0.769</cell><cell cols="2">0.588 / 0.576 0.424 / 0.453</cell><cell>1,213,953</cell></row><row><cell>VGG16</cell><cell>Geometric</cell><cell cols="2">0.777 / 0.772 0.774 / 0.768</cell><cell cols="2">0.588 / 0.576 0.424 / 0.452</cell><cell>1,213,953</cell></row><row><cell></cell><cell>LSTM</cell><cell cols="2">0.790 / 0.809 0.799 / 0.829</cell><cell cols="2">0.594 / 0.613 0.461 / 0.383</cell><cell>1,820,929</cell></row><row><cell></cell><cell>Bi-LSTM</cell><cell cols="2">0.797 / 0.819 0.806 / 0.833</cell><cell cols="2">0.606 / 0.622 0.458 / 0.380</cell><cell>2,529,281</cell></row><row><cell></cell><cell>Mean</cell><cell cols="2">0.799 / 0.815 0.806 / 0.811</cell><cell cols="2">0.601 / 0.624 0.414 / 0.404</cell><cell>1,803,777</cell></row><row><cell></cell><cell>Harmonic</cell><cell cols="2">0.800 / 0.816 0.807 / 0.812</cell><cell cols="2">0.603 / 0.624 0.415 / 0.403</cell><cell>1,803,777</cell></row><row><cell>DenseNet169</cell><cell>Geometric</cell><cell cols="2">0.799 / 0.815 0.807 / 0.811</cell><cell cols="2">0.602 / 0.624 0.415 / 0.404</cell><cell>1,803,777</cell></row><row><cell></cell><cell>LSTM</cell><cell cols="2">0.805 / 0.825 0.821 / 0.839</cell><cell cols="2">0.618 / 0.623 0.392 / 0.373</cell><cell>2,410,753</cell></row><row><cell></cell><cell>Bi-LSTM</cell><cell cols="2">0.810 / 0.825 0.824 / 0.842</cell><cell cols="2">0.621 / 0.626 0.385 / 0.370</cell><cell>3,050,241</cell></row><row><cell></cell><cell>Mean</cell><cell cols="2">0.795 / 0.829 0.802 / 0.816</cell><cell cols="2">0.601 / 0.631 0.411 / 0.398</cell><cell>2,000,385</cell></row><row><cell></cell><cell>Harmonic</cell><cell cols="2">0.796 / 0.828 0.802 / 0.815</cell><cell cols="2">0.603 / 0.631 0.409 / 0.400</cell><cell>2,000,385</cell></row><row><cell>ResNet50</cell><cell>Geometric</cell><cell>0.795 / 0.829</cell><cell>0.802 / 0.815</cell><cell cols="2">0.602 / 0.631 0.410 / 0.399</cell><cell>2,000,385</cell></row><row><cell></cell><cell>LSTM</cell><cell cols="2">0.825 / 0.827 0.821 / 0.819</cell><cell>0.625 / 0.633</cell><cell>0.394 / 0.384</cell><cell>2,607,361</cell></row><row><cell></cell><cell>Bi-LSTM</cell><cell>0.830 / 0.846</cell><cell>0.820 / 0.840</cell><cell>0.634 / 0.652</cell><cell>0.382 / 0.362</cell><cell>3,312,385</cell></row><row><cell></cell><cell>Mean</cell><cell cols="2">0.746 / 0.782 0.766 / 0.780</cell><cell cols="2">0.557 / 0.594 0.458 / 0.432</cell><cell>2,262,529</cell></row><row><cell></cell><cell>Harmonic</cell><cell cols="2">0.749 / 0.785 0.770 / 0.783</cell><cell cols="2">0.559 / 0.597 0.457 / 0.432</cell><cell>2,262,529</cell></row><row><cell>EfficientNetB7</cell><cell>Geometric</cell><cell cols="2">0.747 / 0.784 0.768 / 0.782</cell><cell cols="2">0.558 / 0.596 0.457 / 0.432</cell><cell>2,262,529</cell></row><row><cell></cell><cell>LSTM</cell><cell cols="2">0.752 / 0.800 0.773 / 0.809</cell><cell cols="2">0.561 / 0.602 0.451 / 0.398</cell><cell>2,869,505</cell></row><row><cell></cell><cell>Bi-LSTM</cell><cell cols="2">0.759 / 0.801 0.776 / 0.814</cell><cell cols="2">0.567 / 0.605 0.448 / 0.398</cell><cell>3,508,993</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Cross dataset generalization in terms of SROCC.</figDesc><table><row><cell>Model</cell><cell>Train \Test</cell><cell cols="3">KoNViD-1K LIVE-VQC YouTube-UGC</cell></row><row><cell></cell><cell>KoNViD-1K</cell><cell>-</cell><cell>0.604</cell><cell>0.392</cell></row><row><cell>VIDEVAL</cell><cell>LIVE-VQC</cell><cell>0.644</cell><cell>-</cell><cell>0.277</cell></row><row><cell></cell><cell>YouTube-UGC</cell><cell>0.594</cell><cell>0.388</cell><cell>-</cell></row><row><cell></cell><cell>KoNViD-1K</cell><cell>-</cell><cell>0.546</cell><cell>0.318</cell></row><row><cell>RAPIQUE</cell><cell>LIVE-VQC</cell><cell>0.656</cell><cell>-</cell><cell>0.352</cell></row><row><cell></cell><cell>YouTube-UGC</cell><cell>0.582</cell><cell>0.623</cell><cell>-</cell></row><row><cell></cell><cell>KoNViD-1K</cell><cell>-</cell><cell>0.770</cell><cell>0.428</cell></row><row><cell>2BiVQA</cell><cell>LIVE-VQC</cell><cell>0.753</cell><cell>-</cell><cell>0.416</cell></row><row><cell></cell><cell>YouTube-UGC</cell><cell>0.647</cell><cell>0.674</cell><cell>-</cell></row></table><note>that the cross-domain BVQA methods generalization using YouTube-UGC is the best on average.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Performance comparison of evaluated BVQA models on the four UGC-VQA datasets. The underlined and boldfaced entries indicate the best and top three performers on each dataset for each performance measure, respectively. PLCC ? KRCC ? RMSE ? SROCC ? PLCC ? KRCC ? RMSE ?</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">KonViD-1k</cell><cell></cell><cell></cell><cell cols="2">LIVE-VQC</cell><cell></cell></row><row><cell>Model</cell><cell cols="4">SROCC ? PLCC ? KRCC ? RMSE ?</cell><cell cols="3">SROCC ? PLCC ? KRCC ?</cell><cell>RMSE?</cell></row><row><cell>BRISQUE [24]</cell><cell>0.656</cell><cell>0.657</cell><cell>0.476</cell><cell>0.481</cell><cell>0.592</cell><cell>0.638</cell><cell>0.416</cell><cell>13.100</cell></row><row><cell>NIQE [84]</cell><cell>0.541</cell><cell>0.553</cell><cell>0.379</cell><cell>0.533</cell><cell>0.595</cell><cell>0.628</cell><cell>0.425</cell><cell>13.110</cell></row><row><cell>ILNIQE [85]</cell><cell>0.526</cell><cell>0.540</cell><cell>0.369</cell><cell>0.540</cell><cell>0.503</cell><cell>0.543</cell><cell>0.355</cell><cell>14.148</cell></row><row><cell>VIIDEO [38]</cell><cell>0.298</cell><cell>0.300</cell><cell>0.203</cell><cell>0.610</cell><cell>0.033</cell><cell>0.023</cell><cell>0.214</cell><cell>16.654</cell></row><row><cell>GM-LOG [33]</cell><cell>0.657</cell><cell>0.663</cell><cell>0.477</cell><cell>0.481</cell><cell>0.588</cell><cell>0.621</cell><cell>0.418</cell><cell>13.223</cell></row><row><cell>HIGRADE [20]</cell><cell>0.720</cell><cell>0.726</cell><cell>0.531</cell><cell>0.439</cell><cell>0.610</cell><cell>0.633</cell><cell>0.439</cell><cell>13.027</cell></row><row><cell>FRIQUEE [21]</cell><cell>0.747</cell><cell>0.748</cell><cell>0.550</cell><cell>0.425</cell><cell>0.657</cell><cell>0.700</cell><cell>0.477</cell><cell>12.198</cell></row><row><cell>CORNIA [34]</cell><cell>0.716</cell><cell>0.713</cell><cell>0.523</cell><cell>0.448</cell><cell>0.671</cell><cell>0.718</cell><cell>0.484</cell><cell>11.832</cell></row><row><cell>HOSA [86]</cell><cell>0.765</cell><cell>0.766</cell><cell>0.569</cell><cell>0.414</cell><cell>0.687</cell><cell>0.741</cell><cell>0.503</cell><cell>11.353</cell></row><row><cell>VGG-19 [42]</cell><cell>0.774</cell><cell>0.784</cell><cell>0.584</cell><cell>0.395</cell><cell>0.656</cell><cell>0.716</cell><cell>0.472</cell><cell>11.783</cell></row><row><cell>ResNet-50 [43]</cell><cell>0.801</cell><cell>0.810</cell><cell>0.610</cell><cell>0.374</cell><cell>0.663</cell><cell>0.720</cell><cell>0.478</cell><cell>11.591</cell></row><row><cell>KonCept512 [76]</cell><cell>0.734</cell><cell>0.748</cell><cell>0.542</cell><cell>0.426</cell><cell>0.664</cell><cell>0.727</cell><cell>0.479</cell><cell>11.626</cell></row><row><cell>PaQ-2-PiQ [87]</cell><cell>0.613</cell><cell>0.601</cell><cell>0.433</cell><cell>0.514</cell><cell>0.643</cell><cell>0.668</cell><cell>0.456</cell><cell>12.619</cell></row><row><cell>V-BLIINDS [31]</cell><cell>0.710</cell><cell>0.703</cell><cell>0.518</cell><cell>0.459</cell><cell>0.693</cell><cell>0.717</cell><cell>0.507</cell><cell>11.765</cell></row><row><cell>TLVQM [40]</cell><cell>0.772</cell><cell>0.768</cell><cell>0.577</cell><cell>0.410</cell><cell>0.798</cell><cell>0.802</cell><cell>0.608</cell><cell>10.145</cell></row><row><cell>VIDEVAL [4]</cell><cell>0.783</cell><cell>0.780</cell><cell>0.584</cell><cell>0.402</cell><cell>0.752</cell><cell>0.751</cell><cell>0.563</cell><cell>11.100</cell></row><row><cell>RAPIQUE [60]</cell><cell>0.807</cell><cell>0.815</cell><cell>0.618</cell><cell>0.364</cell><cell>0.741</cell><cell>0.765</cell><cell>0.557</cell><cell>10.665</cell></row><row><cell>2BiVQA</cell><cell>0.815</cell><cell>0.835</cell><cell>0.629</cell><cell>0.352</cell><cell>0.761</cell><cell>0.832</cell><cell>0.621</cell><cell>9.979</cell></row><row><cell>Dataset</cell><cell></cell><cell cols="2">YouTube-UGC</cell><cell></cell><cell></cell><cell cols="2">All-Combined</cell><cell></cell></row><row><cell cols="2">Model SROCC ? BRISQUE [24] 0.382</cell><cell>0.395</cell><cell>0.263</cell><cell>0.591</cell><cell>0.569</cell><cell>0.586</cell><cell>0.403</cell><cell>0.561</cell></row><row><cell>NIQE [84]</cell><cell>0.237</cell><cell>0.277</cell><cell>0.160</cell><cell>0.617</cell><cell>0.462</cell><cell>0.477</cell><cell>0.322</cell><cell>0.611</cell></row><row><cell>ILNIQE [85]</cell><cell>0.291</cell><cell>0.330</cell><cell>0.198</cell><cell>0.605</cell><cell>0.459</cell><cell>0.474</cell><cell>0.321</cell><cell>0.611</cell></row><row><cell>VIIDEO [38]</cell><cell>0.058</cell><cell>0.153</cell><cell>0.038</cell><cell>0.635</cell><cell>0.103</cell><cell>0.162</cell><cell>0.068</cell><cell>0.680</cell></row><row><cell>GM-LOG [33]</cell><cell>0.367</cell><cell>0.392</cell><cell>0.251</cell><cell>0.589</cell><cell>0.565</cell><cell>0.594</cell><cell>0.399</cell><cell>0.558</cell></row><row><cell>HIGRADE [20]</cell><cell>0.737</cell><cell>0.721</cell><cell>0.547</cell><cell>0.447</cell><cell>0.739</cell><cell>0.736</cell><cell>0.547</cell><cell>0.467</cell></row><row><cell>FRIQUEE [21]</cell><cell>0.765</cell><cell>0.757</cell><cell>0.568</cell><cell>0.416</cell><cell>0.756</cell><cell>0.755</cell><cell>0.565</cell><cell>0.454</cell></row><row><cell>CORNIA [34]</cell><cell>0.597</cell><cell>0.605</cell><cell>0.421</cell><cell>0.513</cell><cell>0.676</cell><cell>0.697</cell><cell>0.484</cell><cell>0.494</cell></row><row><cell>HOSA [86]</cell><cell>0.602</cell><cell>0.604</cell><cell>0.425</cell><cell>0.513</cell><cell>0.695</cell><cell>0.708</cell><cell>0.503</cell><cell>0.489</cell></row><row><cell>VGG-19 [42]</cell><cell>0.702</cell><cell>0.699</cell><cell>0.509</cell><cell>0.456</cell><cell>0.732</cell><cell>0.748</cell><cell>0.539</cell><cell>0.461</cell></row><row><cell>ResNet-50 [43]</cell><cell>0.718</cell><cell>0.709</cell><cell>0.522</cell><cell>0.453</cell><cell>0.755</cell><cell>0.774</cell><cell>0.561</cell><cell>0.438</cell></row><row><cell>KonCept512 [76]</cell><cell>0.587</cell><cell>0.594</cell><cell>0.410</cell><cell>0.513</cell><cell>0.660</cell><cell>0.676</cell><cell>0.475</cell><cell>0.509</cell></row><row><cell>PaQ-2-PiQ [87]</cell><cell>0.265</cell><cell>0.293</cell><cell>0.177</cell><cell>0.615</cell><cell>0.472</cell><cell>0.482</cell><cell>0.324</cell><cell>0.608</cell></row><row><cell>V-BLIINDS [31]</cell><cell>0.559</cell><cell>0.555</cell><cell>0.389</cell><cell>0.535</cell><cell>0.654</cell><cell>0.6599</cell><cell>0.473</cell><cell>0.520</cell></row><row><cell>TLVQM [40]</cell><cell>0.669</cell><cell>0.659</cell><cell>0.481</cell><cell>0.484</cell><cell>0.727</cell><cell>0.734</cell><cell>0.534</cell><cell>0.470</cell></row><row><cell>VIDEVAL [4]</cell><cell>0.778</cell><cell>0.773</cell><cell>0.583</cell><cell>0.404</cell><cell>0.796</cell><cell>0.793</cell><cell>0.603</cell><cell>0.426</cell></row><row><cell>RAPIQUE [60]</cell><cell>0.761</cell><cell>0.762</cell><cell>0.561</cell><cell>0.406</cell><cell>0.808</cell><cell>0.818</cell><cell>0.614</cell><cell>0.407</cell></row><row><cell>2BiVQA</cell><cell>0.771</cell><cell>0.790</cell><cell>0.581</cell><cell>0.404</cell><cell>0.800</cell><cell>0.794</cell><cell>0.608</cell><cell>0.421</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI :</head><label>VI</label><figDesc>Average runtime comparison evaluated on 1080p videos from YouTube-UGC.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Time (Sec.)</cell></row><row><cell>Method</cell><cell>Deep Learning Framework</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>CPU</cell><cell>GPU</cell></row><row><cell>BRISQUE</cell><cell>MATLAB</cell><cell>1.45</cell><cell></cell></row><row><cell>NIQE</cell><cell>MATLAB</cell><cell>1.68</cell><cell></cell></row><row><cell>GM-LOG</cell><cell>MATLAB</cell><cell>1.77</cell><cell></cell></row><row><cell>VIDEVAL</cell><cell>MATLAB</cell><cell>217.2</cell><cell></cell></row><row><cell>RAPIQUE</cell><cell>MATLAB</cell><cell>12.6</cell><cell></cell></row><row><cell>VGG19</cell><cell>TensorFlow</cell><cell>9.26</cell><cell>7.81</cell></row><row><cell>2BiVQA</cell><cell>TensorFlow</cell><cell>16.2</cell><cell>13.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">An implementation of the 2BiVQA metric is made publicly available: https://github.com/atelili/2BiVQA.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">) white paper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Cisco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cisco annual internet report</title>
		<meeting><address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Methodology for the subjective assessment of the quality of television pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">R</forename><surname>Bt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Telecommunication Union</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="500" to="513" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Study of subjective and objective quality assessment of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1427" to="1441" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ugcvqa: Benchmarking blind video quality assessment for user generated content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4449" to="4464" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The konstanz natural video database (konvid-1k)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jenadeleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Szir?nyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Ninth international conference on quality of multimedia experience (QoMEX)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<title level="m">Scatter plots of SRCC (on YouTube-UGC) of selected BVQA methods versus runtime (on 1080p</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale study of perceptual video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="612" to="627" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Youtube ugc dataset for video compression research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inguva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Perceptual quality assessment of internet videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1248" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A no-reference perceptual blur metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marziliano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dufaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. International conference on image processing</title>
		<meeting>International conference on image processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>III-III</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Blind image quality assessment for measuring image blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Congress on Image and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="467" to="470" />
			<date type="published" when="2008" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Blind measurement of blocking artifacts in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Evan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2000 International Conference on Image Processing</title>
		<meeting>2000 International Conference on Image Processing</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="981" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Measurement of ringing artifacts in jpeg images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Publishing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6076</biblScope>
			<biblScope unit="page">60760</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A perceptual visibility metric for banding artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-U</forename><surname>Kum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kokaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2067" to="2071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bband index: a no-reference banding artifact predictor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2712" to="2716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast and reliable structure-oriented video noise estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="113" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Film grain synthesis for av1 video codec</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Norkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 Data Compression Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hybrid no-reference quality metric for singly and multiply distorted images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Broadcasting</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="567" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">No reference quality assessment for multiply-distorted images based on an improved bagof-words model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1811" to="1815" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: From natural scene statistics to perceptual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3350" to="3364" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">No-reference quality assessment of tone-mapped hdr pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2957" to="2971" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Perceptual quality prediction on authentically distorted images using a bag of features approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="32" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image quality assessment using human visual dog model fused with random forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3282" to="3292" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Statistics of natural images: Scaling in the woods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Ruderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">814</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A dct statistics-based blind image quality index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="583" to="586" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: A natural scene statistics approach in the dct domain</title>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3339" to="3352" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A two-step framework for constructing blind image quality indices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="516" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">C-diivine: No-reference image quality assessment based on local magnitude and phase statistics of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal processing: image communication</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="725" to="747" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spatiotemporal statistics for video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3329" to="3342" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spatio-temporal measures of naturalness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1750" to="1754" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Blind prediction of natural video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1352" to="1365" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment based on log-derivative statistics of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">43025</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Blind image quality assessment using joint statistics of gradient magnitude and laplacian features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4850" to="4862" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning framework for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1098" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">No-reference video quality assessment via feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="491" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal hysteresis model of time varying subjective video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1153" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A comparative evaluation of temporal pooling methods for blind video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="141" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A completely blind video integrity oracle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An optical flow-based no-reference video quality assessment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Manasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Channappayya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2400" to="2404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Two-level approach for no-reference consumer video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5923" to="5938" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Completely blind quality assessment of user generated video content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kancharla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Channappayya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="263" to="274" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio and Y. LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.10934" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Parsenet: Looking wider to see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno>abs/1506.04579</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Massive online crowdsourced study of subjective and objective picture quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="372" to="387" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Konvid-150k: A dataset for no-reference video quality assessment of videos in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>G?tz-Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="72" to="139" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1733" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep convolutional neural models for picture-quality prediction: Challenges and solutions to data-driven image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="130" to="141" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A survey of dnn methods for blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="123" to="788" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep blind video quality assessment based on temporal human perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="619" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep neural networks for no-reference video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2349" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">End-to-end blind quality assessment of compressed videos using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="546" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Quality assessment of in-the-wild videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2351" to="2359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Unified quality assessment of in-the-wild videos with mixed datasets training</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1238" to="1257" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Attention based network for no-reference ugc video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1414" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Rapique: Rapid and accurate video quality prediction of user generated content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Open Journal of Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="425" to="440" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep learning based fullreference and no-reference quality assessment models for compressed ugc videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">An end-to-end no-reference video quality assessment method with hierarchical spatiotemporal feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Broadcasting</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deepsim: Deep similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">257</biblScope>
			<biblScope unit="page" from="104" to="114" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep feature importance awareness based no-reference image quality prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="page" from="209" to="223" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Image quality assessment by comparing cnn features between images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Amirshahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="60" to="410" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Accelerating very deep convolutional networks for classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1943" to="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Digital images and human vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Null</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronic Imaging Science and Technology Conference</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The performance of lstm and bilstm in forecasting time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Siami-Namini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Namin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3285" to="3292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE workshop on automatic speech recognition and understanding</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sziranyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4041" to="4056" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Video quality pooling adaptive to perceptual distortion severity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="610" to="620" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Deep video quality assessor: From spatio-temporal visual sensitivity to a convolutional neural aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="219" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Considering temporal variations of spatial visual distortions in video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ninassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="253" to="265" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">An objective method for combining multiple subjective data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Pinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Communications and Image Processing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5150</biblScope>
			<biblScope unit="page" from="583" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio and Y. LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A feature-enriched completely blind image quality evaluator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2579" to="2591" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Blind image quality assessment based on high order statistics aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4444" to="4457" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">From patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3575" to="3585" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
