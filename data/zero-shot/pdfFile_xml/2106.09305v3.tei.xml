<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SCINet: Time Series Modeling and Forecasting with Sample Convolution and Interaction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Liu</surname></persName>
							<email>*mhliu@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">CUhk REliable Computing (CURE) Lab. Dept. of Computer Science &amp; Egnineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CUhk REliable Computing (CURE) Lab. Dept. of Computer Science &amp; Egnineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muxi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CUhk REliable Computing (CURE) Lab. Dept. of Computer Science &amp; Egnineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CUhk REliable Computing (CURE) Lab. Dept. of Computer Science &amp; Egnineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuxia</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CUhk REliable Computing (CURE) Lab. Dept. of Computer Science &amp; Egnineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingna</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CUhk REliable Computing (CURE) Lab. Dept. of Computer Science &amp; Egnineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CUhk REliable Computing (CURE) Lab. Dept. of Computer Science &amp; Egnineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SCINet: Time Series Modeling and Forecasting with Sample Convolution and Interaction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One unique property of time series is that the temporal relations are largely preserved after downsampling into two sub-sequences. By taking advantage of this property, we propose a novel neural network architecture that conducts sample convolution and interaction for temporal modeling and forecasting, named SCINet. Specifically, SCINet is a recursive downsample-convolve-interact architecture. In each layer, we use multiple convolutional filters to extract distinct yet valuable temporal features from the downsampled sub-sequences or features. By combining these rich features aggregated from multiple resolutions, SCINet effectively models time series with complex temporal dynamics. Experimental results show that SCINet achieves significant forecasting accuracy improvements over both existing convolutional models and Transformer-based solutions across various real-world time series forecasting datasets. Our codes and data are available at https://github.com/cure-lab/SCINet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Time series forecasting (TSF) enables decision-making with the estimated future evolution of metrics or events, thereby playing a crucial role in various scientific and engineering fields such as healthcare <ref type="bibr" target="#b0">[1]</ref>, energy management <ref type="bibr" target="#b41">[42]</ref>, traffic flow <ref type="bibr" target="#b41">[42]</ref>, and financial investment <ref type="bibr" target="#b9">[10]</ref>, to name a few.</p><p>There are mainly three kinds of deep neural networks used for sequence modeling, and they are all applied for time series forecasting <ref type="bibr" target="#b23">[24]</ref>: (i). recurrent neural networks (RNNs) <ref type="bibr" target="#b12">[13]</ref>; (ii). Transformerbased models <ref type="bibr" target="#b36">[37]</ref>; and (iii). temporal convolutional networks (TCN) <ref type="bibr" target="#b3">[4]</ref>.</p><p>Despite the promising results of TSF methods based on these generic models, they do not consider the specialty of time series data during modeling. For example, one unique property of time series is that the temporal relations (e.g., the trend and the seasonal components of the data) are largely preserved after downsampling into two sub-sequences. Consequently, by recursively downsampling the time series into sub-sequences, we could obtain a rich set of convolutional filters to extract dynamic temporal features at multiple resolutions.</p><p>Motivated by the above, in this paper, we propose a novel neural network architecture for time series modeling and forecasting, named sample convolution and interaction network (SCINet). The main contributions of this paper are as follows:</p><p>? We propose SCINet, a hierarchical downsample-convolve-interact TSF framework that effectively models time series with complex temporal dynamics. By iteratively extracting and exchanging information at multiple temporal resolutions, an effective representation with enhanced predictability can be learned, as verified by its comparatively lower permutation entropy (PE) <ref type="bibr" target="#b15">[16]</ref>.  ? We design the basic building block, SCI-Block, for constructing SCINet, which downsamples the input data/feature into two sub-sequences, and then extracts features of each subsequence using distinct convolutional filters. To compensate for the information loss during the downsampling procedure, we incorporate interactive learning between the two convolutional features within each SCI-Block.</p><p>Extensive experiments on various real-world TSF datasets show that our model consistently outperforms existing TSF approaches by a considerable margin. Moreover, while SCINet does not explicitly model spatial relations, it achieves competitive forecasting accuracy on spatial-temporal TSF tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work and Motivation</head><p>The time series forecasting problem is defined as: Given a long time series X * and a look-back window of fixed length T , at timestamp t, time series forecasting is to predictX t+1:t+? = {x t+1 , ..., x t+? } based on the past T steps X t?T +1:t = {x t?T +1 , ..., x t }. Here, ? is the length of the forecast horizon, x t ? R d is the value at time step t, and d is the number of variates. For simplicity, in the following we will omit the subscripts, and use X andX to represent the historical data and the forecasted data, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related Work</head><p>Traditional time series forecasting methods such as the autoregressive integrated moving average (ARIMA) model <ref type="bibr" target="#b7">[8]</ref> and Holt-Winters seasonal method <ref type="bibr" target="#b13">[14]</ref> have theoretical guarantees. However, they are mainly applicable for univariate forecasting problems, restricting their applications to complex time series data. With the increasing data availability and computing power in recent years, it is shown that deep learning-based TSF techniques have the potential to achieve better forecasting accuracy than conventional approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Earlier RNN-based TSF methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> summarize the past information compactly in the internal memory states that are recursively updated with new inputs at each time step, as shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. The gradient vanishing/exploding problems and the inefficient training procedure greatly restrict the application of RNN-based models.</p><p>In recent years, Transformer-based models <ref type="bibr" target="#b36">[37]</ref> have taken the place of RNN models in almost all sequence modeling tasks, thanks to the effectiveness and efficiency of the self-attention mechanisms. Various Transformer-based TSF methods (see <ref type="figure" target="#fig_0">Fig. 1</ref>(b)) are proposed in the literature <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b24">25]</ref>. These works typically focus on the challenging long-term time series forecasting problem, taking advantage of their remarkable long sequence modeling capabilities.</p><p>Another popular type of TSF model is the so-called temporal convolutional network <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b26">27]</ref>, wherein convolutional filters are used to capture local temporal features (see <ref type="figure" target="#fig_0">Fig. 1(c)</ref>). The proposed SCINet is also constructed based on temporal convolution. However, our method has several key differences compared with the TCN model based on dilated causal convolution, as discussed in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Rethinking Dilated Causal Convolution for Time Series Modeling and Forecasting</head><p>The local correlation of time series data is reflected in the continuous changes within a time slot, and convolutional filters can effectively capture such local features. Consequently, convolutional neural networks are explored in the literature for time series modeling and forecasting. In particular, dilated causal convolution (DCS) is the current de facto method used in this respect.</p><p>DCS was first proposed for generating raw audio waveforms in WaveNet <ref type="bibr" target="#b27">[28]</ref>. Later, <ref type="bibr" target="#b3">[4]</ref> simplifies the WaveNet architecture to the so-called temporal convolutional networks (see <ref type="figure" target="#fig_0">Fig. 1</ref> (c)). TCN consists of a stack of causal convolutional layers with exponentially enlarged dilation factors, which can achieve a large receptive field with just a few convolutional layers. Over the years, TCN has been widely used in all kinds of time series forecasting problems and achieve promising results <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33]</ref>. Moreover, convolutional filters can work seamlessly with graph neural networks (GNNs) to solve various spatial-temporal TSF problems.</p><p>With causal convolutions in the TCN architecture, an output i is convolved only with the i th and earlier elements in the previous layer. While causality should be kept in forecasting tasks, the potential "future information leakage" problem exists only when the output and the input have temporal overlaps. In other words, causal convolutions should be applied only in autoregressive forecasting, wherein the previous output serves as the input for future prediction. When the predictions are completely based on the known inputs in the look-back window, there is no need to use causal convolutions. We can safely apply normal convolutions on the look-back window for forecasting.</p><p>More importantly, the dilated architecture in TCN has two inherent limitations:</p><p>? A single convolutional filter is shared within each layer. Such a unified convolutional kernel tends to extract the average temporal features from the data/features in the previous layer. However, complex time series may contain substantial temporal dynamics. Hence, it is essential to extract distinct yet valuable features with a rich set of convolutional filters. ? While the final layer of the TCN model has the global view of the entire look-back window, the effective receptive fields of the intermediate layers (especially those close to the inputs) are limited, causing temporal relation loss during feature extraction.</p><p>The above limitations of the TCN architecture motivate the proposed SCINet design, as detailed in the following section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCI-Block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCI-Block SCI-Block</head><formula xml:id="formula_0">SCI-Block SCI-Block SCI-Block ? ? ? ? ? Input: ! !"# ( if = 1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SCINet: Sample Convolution and Interaction Network</head><p>SCINet adopts an encoder-decoder architecture. The encoder is a hierarchical convolutional network that captures dynamic temporal dependencies at multiple resolutions with a rich set of convolutional filters. As shown in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>, the basic building block, SCI-Block (Section 3.1), downsamples the input data or feature into two sub-sequences and then processes each sub-sequence with a set of convolutional filters to extract distinct yet valuable temporal features from each part. To compensate for the information loss during downsampling, we incorporate interactive learning between the two sub-sequences. Our SCINet (Section 3.2) is constructed by arranging multiple SCI-Blocks into a binary tree structure ( <ref type="figure" target="#fig_1">Fig. 2(b)</ref>). A distinctive advantage of such design is that each SCI-Block has both local and global views of the entire time series, thereby facilitating the extraction of useful temporal features. After all the downsample-convolve-interact operations, we realign the extracted features into a new sequence representation and add it to the original time series for forecasting with a fully-connected network as the decoder. To facilitate extracting complicated temporal patterns, we could further stack multiple SCINets and apply intermediate supervision to get a Stacked SCINet (Section 3.3), as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>(c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SCI-Block</head><p>The SCI-Block ( <ref type="figure" target="#fig_1">Fig. 2(a)</ref>) is the basic module of the SCINet, which decomposes the input feature F into two sub-features F odd and F even through the operations of Spliting and Interactive-learning.</p><p>The Splitting procedure downsamples the original sequence F into two sub-sequences F even and F odd by separating the even and the odd elements, which are of coarser temporal resolution but preserve most information of the original sequence.</p><p>Next, we use different convolutional kernels to extract features from F even and F odd . As the kernels are separate, the extracted features from them would contain distinct yet valuable temporal relations with enhanced representation capabilities. To compensate for potential information loss with downsampling, we propose a novel interactive-learning strategy to allow information interchange between the two sub-sequences by learning affine transformation parameters from each other. As shown in <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>, the interactive learning procedure consists of two steps.</p><p>First, F even and F odd are projected to hidden states with two different 1D convolutional modules ? and ?, respectively, and transformed to the formats of exp and interact to the F even and F odd with the element-wise product (see Eq. <ref type="formula">(1)</ref>). This can be viewed as performing scaling transformation on F even and F odd , where the scaling factors are learned from each other using neural network modules. Here, is the Hadamard product or element-wise production. F s odd = F odd exp(?(Feven)), F s even = Feven exp(?(F odd )).</p><p>(1)</p><formula xml:id="formula_1">F odd = F s odd ? ?(F s even ), F even = F s even ? ?(F s odd ).<label>(2)</label></formula><p>Second, as shown in Eq. (11), the two scaled features F s even and F s odd are further projected to another two hidden states with the other two 1D convolutional modules ? and ?, and then added to or subtracted from 1 F s even and F s odd . The final outputs of the interactive learning module are two updated sub-features F even and F odd . The default architectures of ?, ?, ? and ? are shown in the Appendix C.</p><p>Compared to the dilated convolutions used in the TCN architecture, the proposed downsampleconvolve-interact architecture achieves an even larger receptive field at each convolutional layer. More importantly, unlike TCN that employs a single shared convolutional filter at each layer, significantly restricting its feature extraction capabilities, SCI-Block aggregates essential information extracted from the two downsampled sub-sequences that have both local and global views of the entire time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SCINet</head><p>With the SCI-Blocks presented above, we construct the SCINet by arranging multiple SCI-Blocks hierarchically and get a tree-structured framework, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref> </p><formula xml:id="formula_2">(b).</formula><p>There are 2 l SCI-Blocks at the l-th level, where l = 1, . . . , L is the index of the level, and L is the total number of levels. Within the k-th SCINet of the stacked SCINet (Section 3.3), the input time series X (for k = 1) or feature vectorX</p><formula xml:id="formula_3">k?1 = {x k?1 1 , ...,x k?1 ? } (for k &gt; 1)</formula><p>is gradually downsampled and processed by SCI-Blocks through different levels, which allows for effective feature learning of different temporal resolutions. In particular, the information from previous levels will be gradually accumulated, i.e., the features of the deeper levels would contain extra finer-scale temporal information transmitted from the shallower levels. In this way, we can capture both short-term and long-term temporal dependencies in the time series.</p><p>After going through L levels of SCI-Blocks, we rearrange the elements in all the sub-features by reversing the odd-even splitting operation and concatenate them into a new sequence representation. It is then added to the original time series through a residual connection <ref type="bibr" target="#b11">[12]</ref> to generate a new sequence with enhanced predictability. Finally, a simple fully-connected network is used to decode the enhanced sequence representation intoX k = {x k 1 , ...,x k ? }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Stacked SCINet</head><p>When there are sufficient training samples, we could stack K layers of SCINets to achieve even better forecasting accuracy (see <ref type="figure" target="#fig_1">Fig. 2</ref> (c)), at the cost of a more complex model structure.</p><p>Specifically, we apply intermediate supervision <ref type="bibr" target="#b4">[5]</ref> on the output of each SCINet using the groundtruth values, to ease the learning of the intermediate temporal features. The output of the k-th intermediate SCINet,X k with length ? , is concatenated with part of the input X t?(T ?? )+1:t to recover the length to the original input and feeded as input into the (k + 1)-th SCINet, where k = 1, . . . , K ?1, and K is the total number of the SCINets in the stacked structure. The output of the K-th SCINet,X K , is the final forecasting results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Function</head><p>To train a stacked SCINet with K (K ? 1) SCINets, the loss of the k-th prediction results is calculated as the L1 loss between the output of the k-th SCINet and the ground-truth horizontal window to be predicted:</p><formula xml:id="formula_4">L k = 1 ? ? i=0 x k i ? x i<label>(3)</label></formula><p>The total loss of the stacked SCINet can be written as:</p><formula xml:id="formula_5">L = K k=1 L k .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Complexity Analysis</head><p>Thanks to the downsampling procedure, the neurons at each convolutional layer of SCINet have a larger receptive field than those of TCN. More importantly, the set of rich convolutional filters in SCINet enable flexible extraction of temporal features from multiple resolutions. Consequently, SCINet usually does not require downsampling the original sequence to the coarsest level for effective forecasting. Given the look-back window size T , TCN generally requires log 2 T layers when the dilation factor is 2, while the number of layers L in SCINet could be much smaller than log 2 T . Our empirical study shows that the best forecasting accuracy is achieved with L ? 5 in most cases even with large T (e.g., 168). As for the number of stacks K, our empirical study also shows that K ? 3 would be sufficient.</p><p>Consequently, the computational cost of SCINet is usually on par with that of the TCN architecture. The worst-case time complexity is O(T log T ), much less than that of vanilla Transformer-based solutions: O(T 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we show the quantitative and qualitative comparisons with the state-of-the-art models for time series forecasting. We also present a comprehensive ablation study to evaluate the effectiveness of different components in SCINet. More details on datasets, evaluation metrics, data pre-processing, experimental settings, network structures and their hyper-parameters are shown in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We conduct experiments on 11 popular time series datasets: (1) Electricity Transformer Temperature <ref type="bibr" target="#b41">[42]</ref> (ETTh) (2) Traffic (3) Solar-Energy (4) Electricity (5) Exchange-Rate (6) PeMS (PEMS03, PEMS04, PEMS07 and PEMS08). A brief description of these datasets is listed in <ref type="table" target="#tab_1">Table 1</ref>. All the experiments on these datasets in this section are conducted under multi-variate TSF setting.</p><p>To make a fair comparison, we follow existing experimental settings, and use the same evaluation metrics as the original publications <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b18">19]</ref> in each dataset.  The experimental setting is the same as <ref type="bibr" target="#b18">[19]</ref>, which uses the input length of 168 to forecast different future horizons{3, 6, 12, 24}.</p><p>As can be seen in  Long-term Time Series Forecasting: many real-world applications also require to predict longterm events. Therefore, we conduct the experiments on Exchange Rate, Electricity ,Traffic and ETT datasets to evaluate the performance of SCINet on long-term TSF tasks. In this experiment, we only compare SCINet with Transformer-based methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b24">25]</ref>, since they are more popular in recent long-term TSF research.</p><p>As can be seen from <ref type="table" target="#tab_5">Table 3</ref>, the SCINet achieves state-of-the-art performances in most benchmarks and prediction length settings. Overall, SCINet yields 39.89% average improvements on MSE among the above settings. In particular, for Exchange-Rate, compared to previous state-of-the-art results, SCINet gives average 65% improvements on MSE. We attribute it to that the proposed SCINet can better capture both short (local temporal dynamics)and long (trend, seasonality)-term temporal dependencies to make an accurate prediction in long-term TSF. Besides, compared with the vanilla Transformer-based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b41">42]</ref>, the newly-proposed Transformer-based forecasting model Autoformer <ref type="bibr" target="#b37">[38]</ref> achieves the second best performance in all experimental settings and also surpasses the SCINet in Traffic(96). This is because, Autoformer incorporates more prior knowledge about the time series data. It focuses on modelling seasonal patterns and conducts self-attention at the sub-series level (instead of the raw data), which is much better in extracting long-term temporal patterns than vanilla Transformer-based methods.   <ref type="table" target="#tab_6">Table 4</ref> and <ref type="table" target="#tab_7">Table 5</ref>, respectively.</p><p>Multivariate Time-series Forecasting on ETT: as can be seen from <ref type="table" target="#tab_6">Table 4</ref>, compared with RNNbased methods such as LSTMa <ref type="bibr" target="#b1">[2]</ref> and LSTnet <ref type="bibr" target="#b18">[19]</ref>, Transformer-based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b41">42]</ref> are better at capturing the long-term latent patterns in the entire historical data for predicting the future, leading to lower prediction errors. However, TCN further outperforms such vanilla Transformer-based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b41">42]</ref>, because the stacked convolutional layers allow for more effective local-to-global temporal relation learning for multivariate time series. It is worth noting that SCINet outperforms all the above models by a large margin. <ref type="figure" target="#fig_2">Fig. 3</ref> presents the qualitative results on some randomly selected sequences of the ETTh1 dataset, which clearly demonstrate the capability of SCINet in obtaining the trend and seasonality of time series for TSF.</p><p>Univariate Time-series Forecasting on ETT: in this experimental setting, we bring several strong baseline methods for univariate forecasting into comparison, including ARIMA, Prophet <ref type="bibr" target="#b35">[36]</ref>, DeepAR <ref type="bibr" target="#b31">[32]</ref> and N-Beats <ref type="bibr" target="#b28">[29]</ref>. In <ref type="table" target="#tab_7">Table 5</ref>, we can observe that N-Beats is superior to other baseline methods in most cases. In fact, N-Beats also takes the unique properties of time series into consideration and directly learns a trend and a seasonality model using a very deep stack of fully-connected layers with residuals, which is a departure from the predominant architectures, such as RNNs, CNNs and Transformers. Nevertheless, the performance of SCINet is still much better than N-Beats.</p><p>We attribute the significant performance improvements of SCINet on the ETT datasets to: (i) SCINet effectively captures temporal dependencies from multiple temporal resolutions; (ii) ETT datasets are publicly available recently and domain-specific solutions tuned specifically for these datasets do not exist yet. Spatial-temporal Time Series Forecasting: besides the general TSF tasks, there is also a large category of data related to spatial-temporal forecasting. For example, traffic datasets PeMS <ref type="bibr" target="#b8">[9]</ref> (PEMS03, PEMS04, PEMS07 and PEMS08) are complicated spatial-temporal time series for public traffic network and they have been investigated for decades. Most recent approaches: DCRNN <ref type="bibr" target="#b21">[22]</ref>, STGCN <ref type="bibr" target="#b40">[41]</ref>, ASTGCN <ref type="bibr" target="#b10">[11]</ref>, GraphWaveNet <ref type="bibr" target="#b38">[39]</ref>, STSGCN <ref type="bibr" target="#b34">[35]</ref>, AGCRN <ref type="bibr" target="#b2">[3]</ref>, LSGCN <ref type="bibr" target="#b14">[15]</ref> and STFGNN <ref type="bibr" target="#b19">[20]</ref> use graph neural networks to capture spatial relations while modeling temporal dependencies via conventional TCN or LSTM architectures. We follow the same experimental settings as the above works. As shown in <ref type="table" target="#tab_8">Table 6</ref>, these GNN-based methods generally perform better than pure RNN or TCN-based methods. However, SCINet still achieves better performance without sophisticated spatial relation modelling, which further proves the superb temporal modeling capabilities of SCINet. Predictability estimation: inspired by <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30]</ref>, we use permutation entropy (PE) <ref type="bibr" target="#b5">[6]</ref> to measure the predictability of the original input and the enhanced representation learnt by SCINet. Time series with lower PE values are regarded as less complex, thus theoretically easier to predict 2 . The PE values of the original time series and the corresponding enhanced representations are shown in <ref type="table" target="#tab_9">Table 7</ref>. As can be observed, the enhanced representations learnt by SCINet indeed have lower PE values compared with the original inputs, which indicates that it is easier to predict the future from the enhanced representations using the same forecaster. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation studies</head><p>To evaluate the impact of each main component used in SCINet, we experiment on several model variants on two datasets: ETTh1 and PEMS08.</p><p>SCIBlock: we first set the number of stacks K = 1 and the number of SCINet levels L = 3 . For the SCI-Block design, to validate the effectiveness of the interactive learning and the distinct convolution weights for processing the sub-sequences, we experiment on two variants, namely w/o. InterLearn and WeightShare. The w/o. InterLearn is obtained by removing the interactive-learning procedure described in Eq. (1) and <ref type="bibr" target="#b10">(11)</ref>. In this case, the two sub-sequences would be updated using F odd = ?(?(F odd )) and F even = ?(?(F even )). For WeightShare, the modules ?, ?, ?, and ? share the same weight.</p><p>The evaluation results in <ref type="figure" target="#fig_3">Fig. 4</ref> show that both interactive learning and distinct weights are essential, as they improve the prediction accuracies of both datasets at various prediction horizons. At the same time, comparing <ref type="figure" target="#fig_3">Fig. 4(a)</ref> with <ref type="figure" target="#fig_3">Fig. 4(b)</ref>, we can observe that interactive learning is more effective for cases with longer look-back window sizes. This is because, intuitively, we can extract more effective features by exchanging information between the downsampled sub-sequences when there are longer look-back windows for such interactions.</p><p>SCINet: for the design of SCINet with multiple levels of SCI-Blocks, we also experiment on two variants. The first variant w/o. ResConn is obtained by removing the residual connection from the complete SCINet. The other variant w/o. Linear removes the decoder (i.e., the fully-connected layer) from the complete model. As can be observed in <ref type="figure" target="#fig_3">Fig. 4</ref>, removing the residual connection leads to a significant performance drop. Besides the general benefit in facilitating the model training, more importantly, the predictability of the original time series is enhanced with the help of residuals. The fully-connected layer is also critical for prediction accuracy, indicating the effectiveness of the decoder in extracting and fusing the most relevant temporal information according to the given supervision for prediction.</p><p>We also conduct comprehensive ablation studies on the impact of K (number of stacks) and L (number of levels), and the selection of operator in the interact learning mechanism. These results are shown in the Appendix B.2 due to space limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations and Future Work</head><p>In this paper, we mainly focus on TSF problem for the regular time series collected at even intervals of time and ordered chronologically. However, in real-world applications, the time series might contain noisy data, missing data or collected at irregular time intervals, which is referred to as irregular time series. The proposed SCINet is relatively robust to the noisy data thanks to the progressive downsampling and interactive learning procedure, but it might be affected by the missing data if the ratio exceeds a certain threshold, wherein the downsampling-based multi-resolution sequence representation in SCINet may introduce biases, leading to poor prediction performance. The proposed downsampling mechanism may also have difficulty handling data collected at irregular intervals. We plan to take the above issues into consideration in the future development of SCINet.</p><p>Moreover, this work focuses on the deterministic time series forecasting problem. Many application scenarios require probabilistic forecasts, and we plan to revise SCINet to generate such prediction results.</p><p>Finally, while SCINet generates promising results for spatial-temporal time series without explicitly modeling spatial relations, the forecasting accuracy could be further improved by incorporating dedicated spatial models. We plan to investigate such solutions in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel neural network architecture, sample convolution and interaction network (SCINet) for time series modeling and forecasting, motivated by the unique properties of time series data compared to generic sequence data. The proposed SCINet is a hierarchical downsample-convolve-interact structure with a rich set of convolutional filters. It iteratively extracts and exchanges information at different temporal resolutions and learns an effective representation with enhanced predictability. Extensive experiments on various real-world TSF datasets demonstrate the superiority of our model over state-of-the-art methods.</p><p>Squared Error (RSE) and Empirical Correlation Coefficient (CORR) to evaluate the performance of the TSF models on these datasets following <ref type="bibr" target="#b18">[19]</ref>, which are calculated as follows:</p><formula xml:id="formula_6">RSE = ? i=0 (xi ? xi) 2 ? i=0 (xi ? mean(X)) 2 ,<label>(9)</label></formula><formula xml:id="formula_7">CORR = 1 d d j=0 ? i=0 (xi,j ? mean(Xj ))(xi,j ? mean(Xj )) ? i=0 (xi,j ? mean(Xj )) 2 (xi,j ? mean(Xj )) 2 ,<label>(10)</label></formula><p>where X andX are the ground-truth and model's prediction, respectively. d is the number of variates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Extra Experimental Results</head><p>In this section, we first add error bars on different forecasting steps T, and also conduct empirical studies on ETTh1 and PEMS datasets to show the impact of different parameter and operator combinations in SCI-Block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Error Bars Evaluation</head><p>Since deep models for time series forecasting may be influenced by different random initialization, we report our results with 5 runs on the ETTh1 dataset. From <ref type="table" target="#tab_10">Table 8</ref>, we show the standard deviation (Std.) is basically 2% to 3% of the mean values, indicating SCINet is robust towards different initialization. As can be observed from <ref type="table" target="#tab_11">Table 9</ref>, when fixing K = 1, larger L leads to better prediction accuracy for the cases with larger T (T = 128 or 192). This is because we could further extract essential information from coarser temporal resolutions with deeper levels in the SCINet when T is large. As for the number of stacks K, when fixing L = 3, if T is small (e.g. T = 24 or 48), we find that increasing K would improve prediction accuracy. This is because, under such circumstances, the information extracted from a single SCINet is insufficient. By stacking more SCINets, we effectively increase the representation learning capability of the model, which facilitates extracting more robust temporal relations for the forecasting task. However, when T is large (e.g., 192), a shallow stack can already well capture the temporal dependencies for the time series. Under such circumstances, using deeper stacks may suffer from overfitting issues with the increase of parameters, which degrades the performance in the inference stage.</p><p>From <ref type="table" target="#tab_11">Table 9</ref>, we can observe a clear trade-off between L and K. Moreover, the performance variation under different T also indicates the importance of the look-back window selection for forecasting tasks. While T is typically pre-determined based on domain knowledge about the time series data, based on our empirical study, L ? 5 and K ? 3 are usually sufficient and tuning these hyperparameters does not incur much effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Empirical Study on Operator Selection</head><p>In interactive-learning equation,</p><formula xml:id="formula_8">F odd = F s odd ? ?(F s even ), F even = F s even ? ?(F s odd ).<label>(11)</label></formula><p>the operators can be either "addition" or "subtraction". Although the model can learn the operation adaptively during training, the parameter initialization would affect the final performance. As shown in the following table the impact of operator settings is minor. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Reproducibility</head><p>Our code is implemented with PyTorch. All the experiments are conducted on an Nvidia Tesla V100 SXM2 GPU (32GB memory), which is sufficient for all our experiments.</p><p>Structure of the network modules ?, ?, ?, and ? in SCI-Block: As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, ?, ?, ?, and ? use the same network architecture. First, the replication padding is used to keep the border shrunk caused by the convolution operation. Then, a 1d convolutional layer with kernel size k is applied to extend the input channel C to h*C and followed with LeakyRelu and Dropout. h means a scale of the hidden size. Next, the second 1d convolutional layer with kernel size k is to recover the channel h*C to the input channel C. The stride of all the convolutions is 1. We use a LeakyRelu activation after the first convolutional layer because of its sparsity properties and a reduced likelihood of vanishing gradient. We apply a Tanh activation after the second convolutional layer since it can keep both positive and negative features into [-1, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>To enhance the performance in single-step (short-term time series forecasting Sec. 4.2) forecasting, we revise the loss function of the last SCINet in the stacked SCINet with K(K ? 1). The loss function contains two parts: For the last stack K, we introduce a balancing parameter ? ? (0, 1) for the value of the last time-step 9 :</p><formula xml:id="formula_9">L k = 1 ? ? i=0 x k i ? x i , k = K.<label>(12)</label></formula><formula xml:id="formula_10">L K = 1 ? ? 1 ? ?1 i=0 x K i ? x i + ? x K ? ? x ? .<label>(13)</label></formula><p>Therefore, the total loss of the stacked SCINet can be written as:</p><formula xml:id="formula_11">L = K?1 k=1 L k + L K .<label>(14)</label></formula><p>Training details: For all datasets, we fix the random seed to be 4321, and train the model for 150 epochs at most. The reported results on the test set are based on the model that achieves the best performance on the validation set.</p><p>Hyper-parameter tuning: We conduct a grid search over all the essential hyper-parameters on the held-out validation set of the datasets. The detailed hyper-parameter configurations of ETT are shown in <ref type="table" target="#tab_1">Table 11</ref>  <ref type="bibr" target="#b9">10</ref> . Besides, the parameters of the four datasets in PeMS are presented in Table13. The Traffic, Solar-Energy, Electricity and Exchange-rate are shown in <ref type="table" target="#tab_1">Table 12</ref>. Notably, we only apply the weighted loss to the Solar and Exchange-rate data since they show less auto-correlation <ref type="bibr" target="#b18">[19]</ref>, which indicates the temporal correlation of the distant time-stamp cannot be well modelled by a general L1 loss. Moreover, to build a non-causal TCN <ref type="bibr" target="#b10">11</ref> in the paper, we only need to remove the chomps in the code and make the padding equal to the dilation.  <ref type="table" target="#tab_1">Hyperparameter   Horizon  24  48  168 336 720  24  48  168 336 720  24  48  96  288 672  Look-back window  48  96  336 336 736  48  96  336 336 736  48  96  384 672 672  Batch size  8  16  32  512 256  16  4  16  128 128  32  16  32  32  32  Learning rate  3e-3 9e-3 5e-4 1e-4 5e-5 7e-3 7e-3 5e-5 5e-5 1e-5 5e-3 1e-3</ref>    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Existing sequence modeling architectures for time series forecasting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The overall architecture of Sample Convolution and Interaction Network (SCINet).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The prediction results (Horizon = 48) of SCINet, Autoformer, Informer, and TCN on randomly-selected sequences from ETTh1 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Component analysis of SCINet on two datasets. Smaller values are better. See Section 4.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>ReplicationPad1dConv1dFigure 5 :</head><label>5</label><figDesc>(in_channels = C, out_channels = h*C, Kernel =k) LeakyRelu (negative_slop = 0.01) Dropout (d) Tanh Conv1d (in_channels =h*C, out_channels = C, Kernel =k) Input : X (C, L) Output: ? ' (C, L) The structure of ?, ?, ?, and ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The overall information of the 11 datasets.</figDesc><table><row><cell>Datasets</cell><cell>ETTh(1,2)</cell><cell>ETTm1</cell><cell>Traffic</cell><cell>Solar-Energy</cell><cell>Electricity</cell><cell>Exchange-Rate</cell><cell>PEMS03</cell><cell>PEMS04</cell><cell>PEMS07</cell><cell>PEMS08</cell></row><row><cell>Variants</cell><cell>7</cell><cell>7</cell><cell>862</cell><cell>137</cell><cell>321</cell><cell>8</cell><cell>358</cell><cell>307</cell><cell>883</cell><cell>170</cell></row><row><cell>Timesteps</cell><cell>17,420</cell><cell>69,680</cell><cell>17,544</cell><cell>52,560</cell><cell>26,304</cell><cell>7,588</cell><cell>26,209</cell><cell>16,992</cell><cell>28,224</cell><cell>17,856</cell></row><row><cell>Granularity</cell><cell>1hour</cell><cell>15min</cell><cell>1hour</cell><cell>10min</cell><cell>1hour</cell><cell>1day</cell><cell>5min</cell><cell>5min</cell><cell>5min</cell><cell>5min</cell></row><row><cell>Start time</cell><cell>7/1/2016</cell><cell>7/1/2016</cell><cell>1/1/2015</cell><cell>1/1/2006</cell><cell>1/1/2012</cell><cell>1/1/1990</cell><cell>5/1/2012</cell><cell>7/1/2017</cell><cell>5/1/2017</cell><cell>3/1/2012</cell></row><row><cell>Task type</cell><cell>Multi-step</cell><cell>Multi-step</cell><cell>Single-step</cell><cell>Single-step</cell><cell>Single-step</cell><cell>Single-step</cell><cell>Multi-step</cell><cell>Multi-step</cell><cell>Multi-step</cell><cell>Multi-step</cell></row><row><cell>Data partition</cell><cell cols="2">Follow [42]</cell><cell></cell><cell cols="3">Training/Validation/Testing: 6/2/2</cell><cell cols="4">Training/Validation/Testing: 6/2/2</cell></row><row><cell cols="3">4.2 Results and Analyses</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 ,</head><label>2</label><figDesc><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6</ref> provide the main experimental results of SCINet. We observe that SCINet shows superior performance than other TSF models on various tasks, including short-term, long-term and spatial-temporal time series forecasting.Short-term Time Series Forecasting: we evaluate the performance of the SCINet in short-term TSF tasks with other baseline methods on Traffic, Solar-Energy, Electricity and Exchange-Rate datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Note that, TCN ? denotes a variant of TCN wherein causal convolutions are replaced by normal convolutions, and improves the original TCN across all the datasets, which supports our claim in Sec. 2.2. Moreover, we can also observe that the Transformer-based methods have poor performance in this task. For short-term forecasting, the recent data points are typically more important for accurate forecasting. However, the permutationinvariant self-attention mechanisms used in Transformer-based methods do not pay much attention to such critical information. In contrast, the general sequential models (RNN/TCN) can formulate it easily, showing quite competitive results in short-term forecasting.</figDesc><table /><note>, the proposed SCINet outperforms existing RNN/TCN-based (LSTNet [19], TPA-LSTM [34], TCN [4], TCN ? ) and Transformer-based [38, 42, 37] TSF solutions in most cases, especially for the Solar-Energy and Exchange-Rate datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Short-term forecasting performance comparison on the four datasets. The best results are shown in bold and second best results are highlighted with underlined blue font. IMP shows the improvement of SCINet over the best model.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">SCINet</cell><cell cols="2">Autoformer [40]</cell><cell cols="2">Informer [42]</cell><cell cols="2">Transformer [37]</cell><cell cols="2">*TCN [4]</cell><cell cols="2">*TCN  ?</cell><cell cols="2">LSTNet [19]</cell><cell cols="2">TPA-LSTM [34]</cell><cell>IMP</cell></row><row><cell>Metric</cell><cell>?</cell><cell>RSE</cell><cell>CORR</cell><cell>RSE</cell><cell>CORR</cell><cell>RSE</cell><cell>CORR</cell><cell>RSE</cell><cell>CORR</cell><cell>RSE</cell><cell>CORR</cell><cell>RSE</cell><cell>CORR</cell><cell>RSE</cell><cell>CORR</cell><cell>RSE</cell><cell>CORR</cell><cell>RSE</cell></row><row><cell></cell><cell>3</cell><cell>0.1775</cell><cell>0.9853</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>0.1940</cell><cell>0.9835</cell><cell>0.1900</cell><cell>0.9848</cell><cell>0.1843</cell><cell>0.9843</cell><cell>0.1803</cell><cell>0.9850</cell><cell>1.55%</cell></row><row><cell>Solar-Energy</cell><cell>6 12</cell><cell>0.2301 0.2997</cell><cell>0.9739 0.9550</cell><cell>N/A N/A</cell><cell>N/A N/A</cell><cell>N/A N/A</cell><cell>N/A N/A</cell><cell>N/A N/A</cell><cell>N/A N/A</cell><cell>0.2581 0.3512</cell><cell>0.9602 0.9321</cell><cell>0.2382 0.3353</cell><cell>0.9612 0.9432</cell><cell>0.2559 0.3254</cell><cell>0.9690 0.9467</cell><cell>0.2347 0.3234</cell><cell>0.9742 0.9487</cell><cell>1.96% 7.33%</cell></row><row><cell></cell><cell>24</cell><cell>0.4081</cell><cell>0.9112</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>0.4732</cell><cell>0.8812</cell><cell>0.4676</cell><cell>0.8851</cell><cell>0.4643</cell><cell>0.8870</cell><cell>0.4389</cell><cell>0.9081</cell><cell>7.02%</cell></row><row><cell></cell><cell>3</cell><cell>0.4216</cell><cell>0.8920</cell><cell>0.5368</cell><cell>0.8268</cell><cell>0.5175</cell><cell>0.8515</cell><cell>0.5122</cell><cell>0.8555</cell><cell>0.5459</cell><cell>0.8486</cell><cell>0.5361</cell><cell>0.8540</cell><cell>0.4777</cell><cell>0.8721</cell><cell>0.4487</cell><cell>0.8812</cell><cell>6.04%</cell></row><row><cell>Traffic</cell><cell>6 12</cell><cell>0.4414 0.4495</cell><cell>0.8809 0.8772</cell><cell>0.5462 0.5623</cell><cell>0.8191 0.8082</cell><cell>0.5258 0.5533</cell><cell>0.8465 0.8279</cell><cell>0.5455 0.5485</cell><cell>0.8388 0.8317</cell><cell>0.6061 0.6367</cell><cell>0.8205 0.8048</cell><cell>0.5992 0.6061</cell><cell>0.8197 0.8205</cell><cell>0.4893 0.4950</cell><cell>0.8690 0.8614</cell><cell>0.4658 0.4641</cell><cell>0.8717 0.8717</cell><cell>5.24% 3.15%</cell></row><row><cell></cell><cell>24</cell><cell>0.4453</cell><cell>0.8825</cell><cell>0.6020</cell><cell>0.7757</cell><cell>0.5883</cell><cell>0.8033</cell><cell>0.5934</cell><cell>0.8048</cell><cell>0.6586</cell><cell>0.7921</cell><cell>0.6456</cell><cell>0.7982</cell><cell>0.4973</cell><cell>0.8588</cell><cell>0.4765</cell><cell>0.8629</cell><cell>6.55%</cell></row><row><cell></cell><cell>3</cell><cell>0.0740</cell><cell>0.9494</cell><cell>0.1458</cell><cell>0.9032</cell><cell>0.1524</cell><cell>0.8858</cell><cell>0.1182</cell><cell>0.9055</cell><cell>0.0892</cell><cell>0.9232</cell><cell>0.0852</cell><cell>0.9293</cell><cell>0.0864</cell><cell>0.9283</cell><cell>0.0823</cell><cell>0.9439</cell><cell>10.09%</cell></row><row><cell>Electricity</cell><cell>6 12</cell><cell>0.0845 0.0929</cell><cell>0.9387 0.9305</cell><cell>0.1555 0.1541</cell><cell>0.8957 0.8907</cell><cell>0.1932 0.1748</cell><cell>0.8660 0.8585</cell><cell>0.1328 0.1375</cell><cell>0.8962 0.8849</cell><cell>0.0974 0.1053</cell><cell>0.9121 0.9017</cell><cell>0.0924 0.0993</cell><cell>0.9235 0.9173</cell><cell>0.0931 0.1007</cell><cell>0.9135 0.9077</cell><cell>0.0916 0.0964</cell><cell>0.9337 0.9250</cell><cell>7.75% 3.63%</cell></row><row><cell></cell><cell>24</cell><cell>0.0967</cell><cell>0.9270</cell><cell>0.1754</cell><cell>0.8732</cell><cell>0.2110</cell><cell>0.8347</cell><cell>0.1461</cell><cell>0.8774</cell><cell>0.1091</cell><cell>0.9101</cell><cell>0.0989</cell><cell>0.9101</cell><cell>0.1007</cell><cell>0.9119</cell><cell>0.1006</cell><cell>0.9133</cell><cell>3.88%</cell></row><row><cell></cell><cell>3</cell><cell>0.0171</cell><cell>0.9787</cell><cell>0.0400</cell><cell>0.9458</cell><cell>0.1392</cell><cell>0.9473</cell><cell>0.0689</cell><cell>0.9759</cell><cell>0.0217</cell><cell>0.9693</cell><cell>0.0202</cell><cell>0.9712</cell><cell>0.0226</cell><cell>0.9735</cell><cell>0.0174</cell><cell>0.979</cell><cell>1.72%</cell></row><row><cell>Exchange</cell><cell>6</cell><cell>0.0240</cell><cell>0.9704</cell><cell>0.0481</cell><cell>0.9197</cell><cell>0.1548</cell><cell>0.9207</cell><cell>0.0806</cell><cell>0.9671</cell><cell>0.0263</cell><cell>0.9633</cell><cell>0.0257</cell><cell>0.9628</cell><cell>0.0280</cell><cell>0.9658</cell><cell>0.0241</cell><cell>0.9709</cell><cell>0.41%</cell></row><row><cell>Rate</cell><cell>12</cell><cell>0.0331</cell><cell>0.9553</cell><cell>0.0638</cell><cell>0.9054</cell><cell>0.1793</cell><cell>0.8817</cell><cell>0.0893</cell><cell>0.9476</cell><cell>0.0393</cell><cell>0.9531</cell><cell>0.0352</cell><cell>0.9501</cell><cell>0.0356</cell><cell>0.9511</cell><cell>0.0341</cell><cell>0.9564</cell><cell>2.93%</cell></row><row><cell></cell><cell>24</cell><cell>0.0436</cell><cell>0.9396</cell><cell>0.0651</cell><cell>0.8952</cell><cell>0.1998</cell><cell>0.7715</cell><cell>0.1127</cell><cell>0.9213</cell><cell>0.0492</cell><cell>0.9223</cell><cell>0.0487</cell><cell>0.9314</cell><cell>0.0449</cell><cell>0.9354</cell><cell>0.0444</cell><cell>0.9381</cell><cell>1.80%</cell></row></table><note>-Autoformer, Informer and Transformer achieved by Autoformer [40] requires pre-prossessed datasets for training. -N/A denotes no pre-prossessed dataset for training. - * denotes re-implementation. ? denotes the variant with normal convolutions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Long-term forecasting performance comparison with Transformer-based models.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">SCINet</cell><cell cols="2">Autoformer [38]</cell><cell cols="2">* Pyraformer [25]</cell><cell cols="2">Informer [42]</cell><cell cols="2">Transformer [37]</cell><cell cols="2">LogTrans [21]</cell><cell cols="2">Reformer [18]</cell><cell>IMP</cell></row><row><cell>Metric</cell><cell></cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell></row><row><cell></cell><cell>96</cell><cell>0.061</cell><cell>0.188</cell><cell>0.197</cell><cell>0.323</cell><cell>1.748</cell><cell>1.105</cell><cell>0.847</cell><cell>0.752</cell><cell>0.559</cell><cell>0.587</cell><cell>0.968</cell><cell>0.812</cell><cell>1.065</cell><cell>0.829</cell><cell>68.98%</cell></row><row><cell>Exchange</cell><cell>192</cell><cell>0.106</cell><cell>0.244</cell><cell>0.300</cell><cell>0.369</cell><cell>1.874</cell><cell>1.151</cell><cell>1.204</cell><cell>0.895</cell><cell>1.168</cell><cell>0.835</cell><cell>1.040</cell><cell>0.851</cell><cell>1.188</cell><cell>0.906</cell><cell>64.70%</cell></row><row><cell>Rate</cell><cell>336</cell><cell>0.181</cell><cell>0.323</cell><cell>0.509</cell><cell>0.524</cell><cell>1.943</cell><cell>1.172</cell><cell>1.672</cell><cell>1.036</cell><cell>1.423</cell><cell>0.949</cell><cell>1.659</cell><cell>1.081</cell><cell>1.357</cell><cell>0.976</cell><cell>64.36%</cell></row><row><cell></cell><cell>720</cell><cell>0.525</cell><cell>0.571</cell><cell>1.447</cell><cell>0.941</cell><cell>2.085</cell><cell>1.206</cell><cell>2.478</cell><cell>2.478</cell><cell>2.160</cell><cell>1.150</cell><cell>1.941</cell><cell>1.127</cell><cell>1.510</cell><cell>1.016</cell><cell>63.72%</cell></row><row><cell></cell><cell>96</cell><cell>0.168</cell><cell>0.253</cell><cell>0.201</cell><cell>0.317</cell><cell>0.386</cell><cell>0.449</cell><cell>0.274</cell><cell>0.368</cell><cell>0.263</cell><cell>0.359</cell><cell>0.258</cell><cell>0.357</cell><cell>0.312</cell><cell>0.402</cell><cell>16.42%</cell></row><row><cell>Electricity</cell><cell>192 336</cell><cell>0.175 0.189</cell><cell>0.262 0.278</cell><cell>0.222 0.231</cell><cell>0.334 0.338</cell><cell>0.378 0.376</cell><cell>0.443 0.443</cell><cell>0.296 0.300</cell><cell>0.296 0.394</cell><cell>0.273 0.277</cell><cell>0.374 0.373</cell><cell>0.266 0.280</cell><cell>0.368 0.380</cell><cell>0.348 0.350</cell><cell>0.433 0.433</cell><cell>21.17% 18.19%</cell></row><row><cell></cell><cell>720</cell><cell>0.231</cell><cell>0.316</cell><cell>0.254</cell><cell>0.361</cell><cell>0.376</cell><cell>0.445</cell><cell>0.373</cell><cell>0.439</cell><cell>0.290</cell><cell>0.378</cell><cell>0.283</cell><cell>0.376</cell><cell>0.340</cell><cell>0.420</cell><cell>9.06%</cell></row><row><cell></cell><cell>96</cell><cell>0.613</cell><cell>0.395</cell><cell>0.613</cell><cell>0.388</cell><cell>0.867</cell><cell>0.468</cell><cell>0.719</cell><cell>0.391</cell><cell>0.638</cell><cell>0.354</cell><cell>0.684</cell><cell>0.384</cell><cell>0.732</cell><cell>0.423</cell><cell>0.00%</cell></row><row><cell>Traffic</cell><cell>192 336</cell><cell>0.535 0.540</cell><cell>0.355 0.359</cell><cell>0.616 0.622</cell><cell>0.382 0.337</cell><cell>0.869 0.881</cell><cell>0.467 0.469</cell><cell>0.696 0.777</cell><cell>0.379 0.420</cell><cell>0.647 0.669</cell><cell>0.354 0.364</cell><cell>0.685 0.733</cell><cell>0.390 0.408</cell><cell>0.733 0.742</cell><cell>0.420 0.420</cell><cell>13.15% 13.18%</cell></row><row><cell></cell><cell>720</cell><cell>0.620</cell><cell>0.394</cell><cell>0.660</cell><cell>0.408</cell><cell>0.896</cell><cell>0.473</cell><cell>0.864</cell><cell>0.472</cell><cell>0.707</cell><cell>0.386</cell><cell>0.717</cell><cell>0.396</cell><cell>0.755</cell><cell>0.423</cell><cell>6.06%</cell></row><row><cell cols="4">- *  denotes re-implementation.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Multivariate time-series forecasting results on the ETT datasets. 11% 24.48% 17.24% 2.14% -9.02% 30.77% 25.81% 26.61% 22.67% 1.04% 38.71% 22.83% 21.40% 49.59% 40.18% - * denotes re-implementation.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ETTh1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ETTh2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ETTm1</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Metrics</cell><cell></cell><cell></cell><cell>Horizon</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Horizon</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Horizon</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>24</cell><cell>48</cell><cell>168</cell><cell>336</cell><cell>720</cell><cell>24</cell><cell>48</cell><cell>168</cell><cell>336</cell><cell>720</cell><cell>24</cell><cell>48</cell><cell>96</cell><cell>288</cell><cell>672</cell></row><row><cell>LogTrans [21]</cell><cell>MSE MAE</cell><cell>0.686 0.604</cell><cell>0.766 0.757</cell><cell>1.002 0.846</cell><cell>1.362 0.952</cell><cell>1.397 1.291</cell><cell>0.828 0.750</cell><cell>1.806 1.034</cell><cell>4.070 1.681</cell><cell>3.875 1.763</cell><cell>3.913 1.552</cell><cell>0.419 0.412</cell><cell>0.507 0.583</cell><cell>0.768 0.792</cell><cell>1.462 1.320</cell><cell>1.669 1.461</cell></row><row><cell>Reformer [18]</cell><cell>MSE MAE</cell><cell>0.991 0.754</cell><cell>1.313 0.906</cell><cell>1.824 1.138</cell><cell>2.117 1.280</cell><cell>2.415 1.520</cell><cell>1.531 1.613</cell><cell>1.871 1.735</cell><cell>4.660 1.846</cell><cell>4.028 1.688</cell><cell>5.381 2.015</cell><cell>0.724 0.607</cell><cell>1.098 0.777</cell><cell>1.433 0.945</cell><cell>1.820 1.094</cell><cell>2.187 1.232</cell></row><row><cell>LSTMa [2]</cell><cell>MSE MAE</cell><cell>0.650 0.624</cell><cell>0.702 0.675</cell><cell>1.212 0.867</cell><cell>1.424 0.994</cell><cell>1.960 1.322</cell><cell>1.143 0.813</cell><cell>1.671 1.221</cell><cell>4.117 1.674</cell><cell>3.434 1.549</cell><cell>3.963 1.788</cell><cell>0.621 0.629</cell><cell>1.392 0.939</cell><cell>1.339 0.913</cell><cell>1.740 1.124</cell><cell>2.736 1.555</cell></row><row><cell>LSTNet [19]</cell><cell>MSE MAE</cell><cell>1.293 0.901</cell><cell>1.456 0.960</cell><cell>1.997 1.214</cell><cell>2.655 1.369</cell><cell>2.143 1.380</cell><cell>2.742 1.457</cell><cell>3.567 1.687</cell><cell>3.242 2.513</cell><cell>2.544 2.591</cell><cell>4.625 3.709</cell><cell>1.968 L1700</cell><cell>1.999 1.215</cell><cell>2.762 1.542</cell><cell>1.257 2.076</cell><cell>1.917 2.941</cell></row><row><cell>Informer [42]</cell><cell>MSE MAE</cell><cell>0.577 0.549</cell><cell>0.685 0.625</cell><cell>0.931 0.752</cell><cell>1.128 0.873</cell><cell>1.215 0.896</cell><cell>0.720 0.665</cell><cell>1.457 1.001</cell><cell>3.489 1.515</cell><cell>2.723 1.340</cell><cell>3.467 1.473</cell><cell>0.323 0.369</cell><cell>0.494 0.503</cell><cell>0.678 0.614</cell><cell>1.056 0.786</cell><cell>1.192 0.926</cell></row><row><cell>*TCN [4]</cell><cell>MSE MAE</cell><cell>0.511 0.549</cell><cell>0.515 0.529</cell><cell>0.694 0.617</cell><cell>0.814 0.682</cell><cell>0.944 0.778</cell><cell>0.444 0.478</cell><cell>0.617 0.615</cell><cell>2.405 1.266</cell><cell>2.486 1.312</cell><cell>2.608 1.276</cell><cell>0.229 0.282</cell><cell>0.239 0.360</cell><cell>0.260 0.363</cell><cell>0.768 0.646</cell><cell>2.732 1.371</cell></row><row><cell>*Pyraformer [25]</cell><cell>MSE MAE</cell><cell>0.479 0.499</cell><cell>0.518 0.520</cell><cell>0.758 0.665</cell><cell>0.891 0.738</cell><cell>0.963 0.782</cell><cell>0.477 0.537</cell><cell>0.934 0.764</cell><cell>3.913 1.557</cell><cell>0.907 0.747</cell><cell>0.963 0.783</cell><cell>0.332 0.383</cell><cell>0.492 0.475</cell><cell>0.543 0.510</cell><cell>0.656 0.598</cell><cell>0.901 0.720</cell></row><row><cell>Autoformer [38]</cell><cell>MSE MAE</cell><cell>0.406 0.440</cell><cell>0.478 0.462</cell><cell>0.493 0.481</cell><cell>0.515 0.492</cell><cell>0.499 0.500</cell><cell>0.260 0.339</cell><cell>0.311 0.372</cell><cell>0.466 0.458</cell><cell>0.472 0.478</cell><cell>0.480 0.488</cell><cell>0.408 0.424</cell><cell>0.499 0.464</cell><cell>0.540 0.489</cell><cell>0.636 0.533</cell><cell>0.699 0.564</cell></row><row><cell>SCINet</cell><cell>MSE MAE</cell><cell>0.300 0.342</cell><cell>0.361 0.388</cell><cell>0.408 0.417</cell><cell>0.504 0.495</cell><cell>0.544 0.527</cell><cell>0.180 0.263</cell><cell>0.230 0.303</cell><cell>0.342 0.380</cell><cell>0.365 0.409</cell><cell>0.475 0.488</cell><cell>0.106 0.202</cell><cell>0.136 0.230</cell><cell>0.165 0.252</cell><cell>0.253 0.315</cell><cell>0.346 0.376</cell></row><row><cell>IMP</cell><cell>MSE</cell><cell>26.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Besides, ETT datasets [42] are originally used to evaluate the performance of long-sequence TSF tasks, which are conducted on two experimental settings, Multivariate Time-series Forecasting and Univariate Time-series Forecasting. For a fair comparison, we keep all input lengths T the same as Informer. The results are shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Univariate time-series forecasting results on the ETT datasets. 12% 60.19% 21.11% 20.75% 17.50% 40.90% 24.39% 15.96% 26.22% -11.28% 24.00% -15.38% -12.28% -7.76% -50.00%</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ETTh1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ETTh2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ETTm1</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Metrics</cell><cell></cell><cell></cell><cell>Horizon</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Horizon</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Horizon</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>24</cell><cell>48</cell><cell>168</cell><cell>336</cell><cell>720</cell><cell>24</cell><cell>48</cell><cell>168</cell><cell>336</cell><cell>720</cell><cell>24</cell><cell>48</cell><cell>96</cell><cell>288</cell><cell>672</cell></row><row><cell>ARIMA</cell><cell>MSE MAE</cell><cell>0.108 0.284</cell><cell>0.175 0.424</cell><cell>0.396 0.504</cell><cell>0.468 0.593</cell><cell>0.659 0.766</cell><cell>3.554 0.445</cell><cell>3.190 0.474</cell><cell>2.800 0.595</cell><cell>2.753 0.738</cell><cell>2.878 1.044</cell><cell>0.090 0.206</cell><cell>0.179 0.306</cell><cell>0.272 0.399</cell><cell>0.462 0.558</cell><cell>0.639 0.697</cell></row><row><cell>Prophet [36]</cell><cell>MSE MAE</cell><cell>0.115 0.275</cell><cell>0.168 0.330</cell><cell>1.224 0.763</cell><cell>1.549 1.820</cell><cell>2.735 3.253</cell><cell>0.199 0.381</cell><cell>0.304 0.462</cell><cell>2.145 1.068</cell><cell>2.096 2.543</cell><cell>3.355 4.664</cell><cell>0.120 0.290</cell><cell>0.133 0.305</cell><cell>0.194 0.396</cell><cell>0.452 0.574</cell><cell>2.747 1.174</cell></row><row><cell>DeepAR [32]</cell><cell>MSE MAE</cell><cell>0.107 0.280</cell><cell>0.162 0.327</cell><cell>0.239 0.422</cell><cell>0.445 0.552</cell><cell>0.658 0.707</cell><cell>0.098 0.263</cell><cell>0.163 0.341</cell><cell>0.255 0.414</cell><cell>0.604 0.607</cell><cell>0.429 0.580</cell><cell>0.091 0.243</cell><cell>0.219 0.362</cell><cell>0.364 0.496</cell><cell>0.948 0.795</cell><cell>2.437 1.352</cell></row><row><cell>N-Beats [29]</cell><cell>MSE MAE</cell><cell>0.042 0.156</cell><cell>0.065 0.200</cell><cell>0.106 0.255</cell><cell>0.127 0.284</cell><cell>0.269 0.422</cell><cell>0.078 0.210</cell><cell>0.123 0.271</cell><cell>0.244 0.393</cell><cell>0.270 0.418</cell><cell>0.281 0.432</cell><cell>0.031 0.117</cell><cell>0.056 0.168</cell><cell>0.095 0.234</cell><cell>0.157 0.311</cell><cell>0.207 0.370</cell></row><row><cell>Informer [42]</cell><cell>MSE MAE</cell><cell>0.098 0.247</cell><cell>0.158 0.319</cell><cell>0.183 0.346</cell><cell>0.222 0.387</cell><cell>0.269 0.435</cell><cell>0.093 0.240</cell><cell>0.155 0.314</cell><cell>0.232 0.389</cell><cell>0.263 0.417</cell><cell>0.277 0.431</cell><cell>0.030 0.137</cell><cell>0.069 0.203</cell><cell>0.194 0.372</cell><cell>0.401 0.554</cell><cell>0.512 0.644</cell></row><row><cell>Autoformer [38]</cell><cell>MSE MAE</cell><cell>0.057 0.188</cell><cell>0.103 0.257</cell><cell>0.090 0.235</cell><cell>0.106 0.254</cell><cell>0.120 0.277</cell><cell>0.110 0.259</cell><cell>0.123 0.271</cell><cell>0.188 0.340</cell><cell>0.225 0.376</cell><cell>0.257 0.402</cell><cell>0.025 0.122</cell><cell>0.039 0.156</cell><cell>0.057 0.184</cell><cell>0.103 0.253</cell><cell>0.110 0.261</cell></row><row><cell>SCINet</cell><cell>MSE MAE</cell><cell>0.029 0.127</cell><cell>0.041 0.154</cell><cell>0.071 0.210</cell><cell>0.084 0.234</cell><cell>0.099 0.250</cell><cell>0.065 0.183</cell><cell>0.093 0.227</cell><cell>0.158 0.311</cell><cell>0.166 0.329</cell><cell>0.286 0.429</cell><cell>0.019 0.084</cell><cell>0.045 0.138</cell><cell>0.064 0.183</cell><cell>0.111 0.252</cell><cell>0.165 0.316</cell></row><row><cell>IMP</cell><cell>MSE</cell><cell>49.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison of different approaches on the PeMS datasets.</figDesc><table><row><cell cols="15">Methods Datasets Metrics *LSTM *TCN *TCN  ? DCRNN STGCN ASTGCN(r) GraphWaveNet STSGCN STFGNN AGCRN LSGCN SCINet MAE IMP</cell></row><row><cell></cell><cell>MAE</cell><cell>21.33</cell><cell>19.32</cell><cell>18.87</cell><cell>18.18</cell><cell>17.49</cell><cell>17.69</cell><cell>19.85</cell><cell>17.48</cell><cell>16.77</cell><cell>*15.98</cell><cell>-</cell><cell>14.98</cell><cell>6.26%</cell></row><row><cell>PEMS03</cell><cell>MAPE</cell><cell>21.33</cell><cell>19.93</cell><cell>18.63</cell><cell>18.91</cell><cell>17.15</cell><cell>19.40</cell><cell>19.31</cell><cell>16.78</cell><cell>16.30</cell><cell>*15.23</cell><cell>-</cell><cell>14.11</cell><cell>7.36%</cell></row><row><cell></cell><cell>RMSE</cell><cell>35.11</cell><cell>33.55</cell><cell>32.24</cell><cell>30.31</cell><cell>30.12</cell><cell>29.66</cell><cell>32.94</cell><cell>29.21</cell><cell>28.34</cell><cell>*28.25</cell><cell>-</cell><cell>24.08</cell><cell>8.37%</cell></row><row><cell></cell><cell>MAE</cell><cell>25.14</cell><cell>23.22</cell><cell>22.81</cell><cell>24.70</cell><cell>22.70</cell><cell>22.93</cell><cell>25.45</cell><cell>21.19</cell><cell>19.83</cell><cell>19.83</cell><cell>21.53</cell><cell>18.95</cell><cell>4.44%</cell></row><row><cell>PEMS04</cell><cell>MAPE</cell><cell>20.33</cell><cell>15.59</cell><cell>14.31</cell><cell>17.12</cell><cell>14.59</cell><cell>16.56</cell><cell>17.29</cell><cell>13.90</cell><cell>13.02</cell><cell>12.97</cell><cell>13.18</cell><cell>11.86</cell><cell>8.56%</cell></row><row><cell></cell><cell>RMSE</cell><cell>39.59</cell><cell>37.26</cell><cell>36.87</cell><cell>38.12</cell><cell>35.55</cell><cell>35.22</cell><cell>39.70</cell><cell>33.65</cell><cell>31.88</cell><cell>32.30</cell><cell>33.86</cell><cell>30.89</cell><cell>4.40%</cell></row><row><cell></cell><cell>MAE</cell><cell>29.98</cell><cell>32.72</cell><cell>30.53</cell><cell>28.30</cell><cell>25.38</cell><cell>28.05</cell><cell>26.85</cell><cell>24.26</cell><cell>22.07</cell><cell>*22.37</cell><cell>-</cell><cell>21.19</cell><cell>5.27%</cell></row><row><cell>PEMS07</cell><cell>MAPE</cell><cell>15.33</cell><cell>14.26</cell><cell>13.88</cell><cell>11.66</cell><cell>11.08</cell><cell>13.92</cell><cell>12.12</cell><cell>10.21</cell><cell>9.21</cell><cell>*9.12</cell><cell>-</cell><cell>8.83</cell><cell>3.18%</cell></row><row><cell></cell><cell>RMSE</cell><cell>42.84</cell><cell>42.23</cell><cell>41.02</cell><cell>38.58</cell><cell>38.78</cell><cell>42.57</cell><cell>42.78</cell><cell>39.03</cell><cell>35.80</cell><cell>*36.55</cell><cell>-</cell><cell>34.03</cell><cell>6.89%</cell></row><row><cell></cell><cell>MAE</cell><cell>22.20</cell><cell>22.72</cell><cell>21.42</cell><cell>17.86</cell><cell>18.02</cell><cell>18.61</cell><cell>19.13</cell><cell>17.13</cell><cell>16.64</cell><cell>15.95</cell><cell>17.73</cell><cell>15.72</cell><cell>1.44%</cell></row><row><cell>PEMS08</cell><cell>MAPE</cell><cell>15.32</cell><cell>14.03</cell><cell>13.09</cell><cell>11.45</cell><cell>11.40</cell><cell>13.08</cell><cell>12.68</cell><cell>10.96</cell><cell>10.60</cell><cell>10.09</cell><cell>11.20</cell><cell>9.80</cell><cell>2.87%</cell></row><row><cell></cell><cell>RMSE</cell><cell>32.06</cell><cell>35.79</cell><cell>34.03</cell><cell>27.83</cell><cell>27.83</cell><cell>28.16</cell><cell>31.05</cell><cell>26.80</cell><cell>26.22</cell><cell>25.22</cell><cell>26.76</cell><cell>24.76</cell><cell>1.82%</cell></row></table><note>-dash denotes that the methods do not implement on this dataset. * denotes re-implementation or re-training. ? denotes the variant with normal convolutions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Permutation entropy comparison before and after SCINet. m (embedding dimension) and ? (time-lag) are two parameters used for calculating PE, and the values are selected following<ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b15">16]</ref>.</figDesc><table><row><cell cols="2">Permutation Entropy</cell><cell cols="9">Datasets ETTh1 Traffic Solar-Energy Electricity Exc-rate PEMS03 PEMS04 PEMS07 PEMS08</cell></row><row><cell>Parameters</cell><cell>m (? = 1)  *</cell><cell>6</cell><cell>6</cell><cell>7</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>6</cell></row><row><cell>Value</cell><cell cols="3">Original Input Enhanced Representation 0.7096 0.8832 0.8878 0.9371</cell><cell>0.4739 0.3537</cell><cell>0.9489 0.8901</cell><cell>0.8260 0.7836</cell><cell>0.9649 0.8377</cell><cell>0.9203 0.8749</cell><cell>0.9148 0.8330</cell><cell>0.9390 0.8831</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>The error bars of SCINet with 5 runs on the ETTh1 dataset. Evaluation on the Impact of K and LWe conduct experiments on ETTh1 dataset (with the multivariate experimental setting) to evaluate the impact of K (number of stacks) and L (number of levels), under various look-back window sizes T . The prediction horizon is fixed to be 24.</figDesc><table><row><cell>T</cell><cell cols="2">Metrics Seed 1 Seed 2 Seed 3 Seed 4 Seed 5 Mean</cell><cell>Std.</cell></row><row><cell>24</cell><cell>MSE</cell><cell cols="2">0.3346 0.3381 0.3541 0.3370 0.3370 0.3402 0.0079</cell></row><row><cell></cell><cell>MAE</cell><cell cols="2">0.3699 0.3742 0.3826 0.3719 0.3722 0.3742 0.0050</cell></row><row><cell>48</cell><cell>MSE</cell><cell cols="2">0.4148 0.4259 0.3899 0.3830 0.3856 0.3998 0.0193</cell></row><row><cell></cell><cell>MAE</cell><cell cols="2">0.4370 0.4520 0.4139 0.4108 0.4173 0.4262 0.0177</cell></row><row><cell cols="2">168 MSE</cell><cell cols="2">0.4490 0.5038 0.4433 0.4493 0.4432 0.4577 0.0259</cell></row><row><cell></cell><cell>MAE</cell><cell cols="2">0.4526 0.4985 0.4466 0.4501 0.4476 0.4591 0.0222</cell></row><row><cell cols="2">336 MSE</cell><cell cols="2">0.5288 0.5935 0.5230 0.5308 0.5373 0.5427 0.0289</cell></row><row><cell></cell><cell>MAE</cell><cell cols="2">0.5131 0.5486 0.5114 0.5150 0.5166 0.5209 0.0156</cell></row><row><cell cols="2">720 MSE</cell><cell cols="2">0.5607 0.5923 0.5855 0.5582 0.5678 0.5729 0.0152</cell></row><row><cell></cell><cell>MAE</cell><cell cols="2">0.5469 0.5653 0.5630 0.5418 0.5502 0.5534 0.0103</cell></row><row><cell>B.2</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>The impact of L and K on MSE.</figDesc><table><row><cell>Number of</cell><cell>Horizon</cell><cell></cell><cell></cell><cell>24</cell><cell></cell><cell></cell></row><row><cell>Levels &amp; Stacks</cell><cell>T</cell><cell>24</cell><cell>48</cell><cell>96</cell><cell>128</cell><cell>192</cell></row><row><cell></cell><cell>2</cell><cell cols="5">0.411 0.348 0.347 0.334 0.384</cell></row><row><cell>Level L</cell><cell>3</cell><cell cols="5">0.405 0.346 0.316 0.418 0.330</cell></row><row><cell>(K =1)</cell><cell>4</cell><cell>-</cell><cell cols="4">0.360 0.340 0.331 0.325</cell></row><row><cell></cell><cell>5</cell><cell>-</cell><cell>-</cell><cell cols="3">0.354 0.323 0.356</cell></row><row><cell></cell><cell>1</cell><cell cols="5">0.405 0.346 0.316 0.418 0.330</cell></row><row><cell>Stack K</cell><cell>2</cell><cell cols="5">0.423 0.344 0.344 0.339 0.375</cell></row><row><cell>(L = 3)</cell><cell>3</cell><cell cols="5">0.374 0.341 0.345 0.353 0.363</cell></row><row><cell></cell><cell>4</cell><cell cols="5">0.390 0.342 0.335 0.356 0.388</cell></row><row><cell cols="3">-Dash denotes the input cannot be further splitted.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>The impact of different operators</figDesc><table><row><cell>Operators</cell><cell cols="4">PEMS03 PEMS04 PEMS07 PEMS08 MAE</cell></row><row><cell>+, +</cell><cell>15.08</cell><cell>19.27</cell><cell>21.69</cell><cell>15.72</cell></row><row><cell>-, -</cell><cell>15.06</cell><cell>19.21</cell><cell>21.63</cell><cell>15.78</cell></row><row><cell>+, -</cell><cell>15.09</cell><cell>19.31</cell><cell>21.77</cell><cell>15.84</cell></row><row><cell>-, +</cell><cell>15.30</cell><cell>19.32</cell><cell>21.72</cell><cell>15.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>The hyperparameters in ETT datasets (Multivariate)</figDesc><table><row><cell>Model configurations</cell><cell>ETTh1</cell><cell>ETTh2</cell><cell>ETTm1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>The hyperparameters in Traffic, Solar-energy, Electricity and Exchange-rate datasets</figDesc><table><row><cell cols="2">Model configurations</cell><cell></cell><cell cols="2">Solar</cell><cell></cell><cell>Electricity</cell><cell></cell><cell cols="3">Traffic</cell><cell></cell><cell>Exc-Rate</cell></row><row><cell></cell><cell>Horizon</cell><cell>3</cell><cell>6</cell><cell>12</cell><cell cols="2">24 3 6 12 24</cell><cell>3</cell><cell>6</cell><cell cols="2">12</cell><cell cols="2">24 3 6 12</cell><cell>24</cell></row><row><cell>Hyperparameter</cell><cell>Look-back window Batch size</cell><cell cols="4">160 256 256 1024 256</cell><cell>32</cell><cell></cell><cell></cell><cell>168 16</cell><cell></cell><cell></cell><cell>4</cell></row><row><cell></cell><cell>Learning rate</cell><cell></cell><cell></cell><cell>1e-4</cell><cell></cell><cell>9e-3</cell><cell></cell><cell></cell><cell>5e-4</cell><cell></cell><cell></cell><cell>5e-3</cell><cell>7e-3</cell></row><row><cell></cell><cell>h</cell><cell>1</cell><cell>0.5</cell><cell>2</cell><cell>1</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell cols="2">0.5</cell><cell>2</cell><cell>0.125</cell></row><row><cell>SCI Block</cell><cell>k</cell><cell></cell><cell></cell><cell>5</cell><cell></cell><cell>5</cell><cell></cell><cell></cell><cell>5</cell><cell></cell><cell></cell><cell>5</cell></row><row><cell></cell><cell>Dropout</cell><cell></cell><cell></cell><cell>0.25</cell><cell></cell><cell>0</cell><cell cols="5">0.5 0.25 0.25 0.5</cell><cell>0.5</cell></row><row><cell>SCINet</cell><cell>L (level)</cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell>3</cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell>2</cell><cell>3</cell></row><row><cell>Stacked SCINet</cell><cell>K (stack) Loss weight (?)</cell><cell></cell><cell>2</cell><cell>0.5</cell><cell>1</cell><cell>2 ?</cell><cell>2</cell><cell>1</cell><cell>?</cell><cell>2</cell><cell>2</cell><cell>1 0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>The hyperparameters in PeMS datasets</figDesc><table><row><cell cols="2">Model configurations</cell><cell cols="4">PEMS03 PEMS04 PEMS07 PEMS08</cell></row><row><cell></cell><cell>Horizon</cell><cell></cell><cell>12</cell><cell></cell></row><row><cell>Hyperparameter</cell><cell>Look-back window Batch size</cell><cell></cell><cell>12 8</cell><cell></cell></row><row><cell></cell><cell>Learning rate</cell><cell></cell><cell cols="2">1e-3</cell></row><row><cell></cell><cell>h</cell><cell>0.0625</cell><cell>0.0625</cell><cell>0.03125</cell><cell>1</cell></row><row><cell>SCI Block</cell><cell>k</cell><cell></cell><cell>5</cell><cell></cell></row><row><cell></cell><cell>Dropout</cell><cell>0.25</cell><cell>0</cell><cell>0.25</cell><cell>0.5</cell></row><row><cell>SCINet</cell><cell>L (level)</cell><cell></cell><cell>2</cell><cell></cell></row><row><cell>Stacked SCINet</cell><cell>K (stack)</cell><cell></cell><cell>1</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The selection of the operators in Eq.(2) affects the parameter initialization of our model and we show its impact in the Appendix B.3.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Please note that PE is only a quantitative measurement based on complexity. It would not be proper to say that a time series with lower PE value will be always easier to predict than a different type of time series with a higher PE value because the prediction accuracy also depends on many other factors, such as the available data for training, the trend and seasonality elements of the time series data, as well as the predictive model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/zhouhaoyi/ETDataset 4 http://pems.dot.ca.gov 5 http://www.nrel.gov/grid/solar-power-data.html 6 https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014 7 https://github.com/laiguokun/multivariate-time-series-data 8 https://pems.dot.ca.gov</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">This is slightly different from other practice for single-step forecasting<ref type="bibr" target="#b18">[19]</ref>, because we choose to use all the available values in the prediction window as supervision signal.<ref type="bibr" target="#b9">10</ref> The results on ETTh2 and ETTm1 datasets can be referred to: https://github.com/cure-lab/SCINet 11 https://github.com/locuslab/TCN/issues/45</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>This work was supported in part by Alibaba Group Holding Ltd. under Grant No. TA2015393. We thank the anonymous reviewers for their constructive comments and suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Checklist</head><p>The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [TODO] to [Yes] , <ref type="bibr">[No]</ref> , or [N/A] . You are strongly encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description. For example:</p><p>? Did you include the license to the code and datasets? <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this appendix, we first introduce the datasets and evaluation metrics used in the experiments in Section A. Then, we provide extra experimental results in Section B. In Section C, we present details of network design, training scheme, and hyper-parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets and Evaluation Metrics</head><p>We conduct experiments on 11 popular time series datasets: (1) Electricity Transformer Temperature <ref type="bibr" target="#b41">[42]</ref>  <ref type="figure">(ETTh(1,2)</ref>,ETTm1) 3 consists of 2 year electric power data collected from two separated counties of China. Each data point includes an "oil temperature" value and 6 power load features.</p><p>(2) Traffic 4 contains the hourly data describing the road occupancy rates (ranging from 0 to 1) that are recorded by the sensors on San Francisco Bay area freeways from 2015 to 2016 (48 months in total). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Electricity Transformer Temperature (ETT)</head><p>For data pre-processing, we perform zero-mean normalization, i.e., X = (X ? mean(X))/std(X), where mean(X) and std(X) are the mean and the standard deviation of historical time series, respectively. We use Mean Absolute Errors (MAE) <ref type="bibr" target="#b16">[17]</ref> and Mean Squared Errors (MSE) <ref type="bibr" target="#b25">[26]</ref> for model comparison. Besides, the train, validation and test sets contain 12, 4 and 4 months data, respectively.</p><p>wherex i is the model's prediction, and x i is the ground-truth. ? is the length of the prediction horizon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 PeMS</head><p>Following <ref type="bibr" target="#b16">[17]</ref>, the data is pre-processed using zero-mean normalization and we use Root Mean Squared Errors (RMSE) and Mean Absolute Percentage Errors (MAPE) as evaluation metrics on this dataset.</p><p>M AP E = 1 ? ? i=0 |(xi ? xi)/xi|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Traffic, Solar-Energy, Electricity and Exchange-Rate</head><p>In our experiments, the length of the look-back window T for these datasets is 168, and we trained independent models for different length of future horizon (i.e., ? = 3, 6, 12, 24). We use Root Relative</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taha</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">Chase</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12206</idno>
		<title level="m">Temporal-clustering invariance in irregular healthcare time series</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional recurrent network for traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17804" to="17815" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Trellis networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Permutation entropy: a natural complexity measure for time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Pompe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page">174102</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Borovykh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Bohte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelis W</forename><surname>Oosterlee</surname></persName>
		</author>
		<title level="m">Conditional time series forecasting with convolutional neural networks. stat</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Some recent advances in forecasting and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Box</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gwilym</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jenkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="109" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Freeway performance measurement system: mining loop detector data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Petty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Skabardonis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pravin</forename><surname>Varaiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanfeng</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Record</title>
		<imprint>
			<biblScope unit="volume">1748</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="102" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Trimmed fuzzy clustering of financial time series based on dynamic time warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livia</forename><forename type="middle">De</forename><surname>Pierpaolo D&amp;apos;urso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Massari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of operations research</title>
		<imprint>
			<biblScope unit="volume">299</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1379" to="1395" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention based spatial-temporal graph convolutional networks for traffic flow forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youfang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaiyu</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Forecasting seasonals and trends by exponentially weighted moving averages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Holt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="5" to="10" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lsgcn: Long short-term traffic prediction with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuyin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2355" to="2361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enhanced time series predictability with well-defined structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuntao</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical and Applied Climatology</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Another look at measures of forecast accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><forename type="middle">B</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of forecasting</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="679" to="688" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling long-and short-term temporal patterns with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In SIGIR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial-temporal fusion graph neural networks for traffic flow forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengzhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal fusion transformers for interpretable multi-horizon time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Sercan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ar?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Loeff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Time-series forecasting with deep learning: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Zohren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="page">20200209</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pyraformer: Lowcomplexity pyramidal attention for long-range time series modeling and forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schahram</forename><surname>Dustdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The accuracy of extrapolation (time series) methods: Results of a forecasting competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Makridakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Carbone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fildes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Hibon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Newton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Parzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of forecasting</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="153" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Temporal latent auto-encoder: A method for probabilistic multivariate time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Quanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio</title>
		<meeting><address><addrLine>Alex Graves</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Oriol Vinyals</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">N-beats: Neural basis expansion analysis for interpretable time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitri</forename><surname>Carpo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The intrinsic predictability of ecological time series and its potential to guide forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Pennekamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><forename type="middle">C</forename><surname>Iles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgina</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Brose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Gaedke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ute</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kratina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Munch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological Monographs</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1359</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep state space models for time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><forename type="middle">W</forename><surname>Syama Sundar Rangapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyang</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Januschowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepar: Probabilistic forecasting with autoregressive recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Flunkert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Januschowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1181" to="1191" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Think globally, act locally: A deep neural network approach to high-dimensional time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal pattern attention for multivariate time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shun-Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan-Keng</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yi Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spatial-temporal synchronous graph convolutional networks: A new framework for spatial-temporal network data forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youfang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengnan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaiyu</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="914" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Forecasting at scale. The American Statistician</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Letham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Autoformer: Decomposition transformers with autocorrelation for long-term series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiehui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph wavenet for deep spatial-temporal graph modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiehui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoteng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Informer: Beyond efficient transformer for long sequence time-series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wancai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
