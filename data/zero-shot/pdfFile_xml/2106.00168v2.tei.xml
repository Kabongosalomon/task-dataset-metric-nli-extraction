<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Pseudo Labels for Semi-Supervised Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengduo</forename><surname>Li</surname></persName>
							<email>hdli@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
							<email>zxwu@fudan.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
							<email>abhinav@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Pseudo Labels for Semi-Supervised Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in semi-supervised object detection (SSOD) are largely driven by consistency-based pseudo-labeling methods for image classification tasks, producing pseudo labels as supervisory signals. However, when using pseudo labels, there is a lack of consideration in localization precision and amplified class imbalance, both of which are critical for detection tasks. In this paper, we introduce certainty-aware pseudo labels tailored for object detection, which can effectively estimate the classification and localization quality of derived pseudo labels. This is achieved by converting conventional localization as a classification task followed by refinement. Conditioned on classification and localization quality scores, we dynamically adjust the thresholds used to generate pseudo labels and reweight loss functions for each category to alleviate the class imbalance problem. Extensive experiments demonstrate that our method improves state-of-the-art SSOD performance by 1-2% AP on COCO and PASCAL VOC while being orthogonal and complementary to most existing methods. In the limited-annotation regime, our approach improves supervised baselines by up to 10% AP using only 1-10% labeled data from COCO.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The astounding performance of deep neural networks on various computer vision tasks can be largely attributed to the availability of large-scale datasets that are manually labeled. However, collecting human annotations is labor-intensive and time-consuming, particularly for visual understanding tasks, like object detection <ref type="bibr" target="#b19">(Lin et al. 2014;</ref><ref type="bibr">Kuznetsova et al. 2020</ref>) and semantic segmentation <ref type="bibr" target="#b9">(Everingham et al. 2010;</ref><ref type="bibr" target="#b4">Cordts et al. 2016)</ref>. To remedy this, there is an ever-growing interest in semi-supervised learning (SSL), which learns feature representations with limited supervision by exploring the massive amount of unlabeled images that are readily available. While extensive studies have been conducted on SSL for image classification tasks <ref type="bibr" target="#b1">(Berthelot et al. 2019;</ref><ref type="bibr" target="#b35">Xie et al. 2020a;</ref><ref type="bibr" target="#b26">Sohn et al. 2020a;</ref><ref type="bibr" target="#b1">Berthelot et al. 2019;</ref><ref type="bibr" target="#b36">Xie et al. 2020b;</ref><ref type="bibr" target="#b16">Laine and Aila 2017;</ref><ref type="bibr" target="#b22">Miyato et al. 2018;</ref><ref type="bibr" target="#b0">Bachman, Alsharif, and Precup 2014)</ref>, relatively limited effort has been made to address object detection, for which annotations are more expensive to obtain.</p><p>Most recent semi-supervised object detection (SSOD) approaches <ref type="bibr" target="#b27">(Sohn et al. 2020b;</ref><ref type="bibr" target="#b21">Liu et al. 2021;</ref><ref type="bibr" target="#b13">Jeong et al. 2019;</ref><ref type="bibr" target="#b41">Zhou et al. 2021</ref>) are direct extensions of SSL methods designed for image classification using a teacher-student training paradigm <ref type="bibr" target="#b28">(Tarvainen and Valpola 2017;</ref><ref type="bibr" target="#b26">Sohn et al. 2020a;</ref><ref type="bibr" target="#b1">Berthelot et al. 2019)</ref>. In particular, the teacher model is first trained in a supervised manner with a limited number of labeled samples. Then, given an unlabeled image, the teacher model produces pseudo bounding boxes together with their corresponding class predictions, which are further used as ground-truth labels for the student model. To ensure effective distillation, the teacher and the student models typically operate on two augmented views of the same image <ref type="bibr">(Sohn et al. 2020a,b;</ref><ref type="bibr" target="#b21">Liu et al. 2021;</ref><ref type="bibr" target="#b41">Zhou et al. 2021)</ref>.</p><p>The use of a teacher-student model at its core aims to produce reliable pseudo labels in lieu of human annotations. While effective, we argue that pseudo labels, in the form of bounding boxes associated with class predictions, are suboptimal for SSOD. The reasons are twofold: (1) In image classification, prediction scores naturally represent the likelihood of an object appearing in an image, and thus setting a threshold to select highly confident predictions is reasonable. However, as detection requires localizing and classifying objects using two separate branches through regression and classification, the resulting classification scores of pseudo boxes are unaware of the localization quality. Therefore, while widely adopted, filtering out boxes based on class predictions on top of non-maximum suppression is not appropriate; (2) Pseudo labels produced by the teacher model amplifies class imbalance which results from the long-tailed nature in detection tasks. For example, there are only 9 toasters but 12,343 persons in 5% of the COCO <ref type="bibr" target="#b19">(Lin et al. 2014)</ref> training set even though they are both common 1 classes! As a result, lower-confidence predictions from underrepresented classes are oftentimes filtered out with a threshold that works well for top-performing classes.</p><p>To mitigate these issues, we propose certainty-aware pseudo labels together with dynamic thresholding and reweighting mechanisms tailored for SSOD. In particular, the certainty-aware pseudo labels are designed to reflect localization quality and classification confidence at the same time. Conditioned on these certainty measurements, we dy-  <ref type="figure">Figure 1</ref>: A conceptual overview of our approach. Left: We first train the teacher model on labeled images to generate pseudo labels (boxes) on unlabeled images. The student model is then trained with pseudo labels. Right: We propose to generate certainty-aware pseudo labels conditioned on both classification and localization confidence scores, for improved localization, by formulating localization as a classification problem. The scores are then used to derive dynamic thresholds and re-weight losses in a class-wise manner to mitigate class imbalance.</p><p>namically adjust the thresholds used to produce pseudo labels and reweight loss functions on a per-category basis to combat class imbalance. While conceptually appealing, it is challenging to have an in-vitro metric in mainstream detection frameworks that reflects localization quality to complement classification accuracy due to the design that performs localization with regression. Motivated by a few recent studies that replace regression with classification for better localization <ref type="bibr" target="#b23">(Qiu et al. 2020;</ref><ref type="bibr" target="#b32">Wang et al. 2020</ref>), we formulate localization as a classification problem to obtain an estimation of localization quality. More specifically, for each side of a candidate box, we introduce a line segment that is perpendicular to it. The line is split into consecutive intervals, each of which is associated with a prediction score through classification, indicating the probability of the side intersects with the interval. We then average the maximal classification scores from all four sides of a candidate box as its localization quality metric. To ensure accurate localization, we further refine locations within intervals. The pseudo labels are now certainty-aware, measuring both localization precision and classification confidence, and can be readily used to generate better labels. In particular, for each category, conditioned on the localization and classification confidence, we dynamically determine a threshold to generate pseudo labels and reweight loss functions such that underrepresented classes are emphasized during training to mitigate class imbalance.</p><p>We conduct extensive experiments on COCO <ref type="bibr" target="#b19">(Lin et al. 2014</ref>) and PASCAL VOC <ref type="bibr" target="#b9">(Everingham et al. 2010</ref>) un-der common semi-supervised settings, and demonstrate that our method improves SOTA performance by 1-2% on COCO and PASCAL VOC respectively using various training recipes while being orthogonal and complementary to most recent methods (which will be shown empirically), and improves the supervised baseline by up to 10% AP when using only 1/2/5/10 % annotations of COCO. We further show that our method is complementary to existing approaches resorting to orthogonal techniques like co-teaching <ref type="bibr" target="#b11">(Han et al. 2018</ref>) and model ensemble. Extensive ablation experiments are conducted to validate the effectiveness of different components of our method, and demonstrate that our approach is relatively robust to hyper-parameter selections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>Our goal is to address semi-supervised object detection where a set of labeled images with box-level annotations and a set of unlabeled images are used for training. Built upon consistency-based pseudo labeling, our method produces certainty-aware pseudo labels for both classification and localization. This is achieved by formulating box localization as a classification problem and injecting localization confidence to guide pseudo label generation. Conditioned on classification and localization certainty, we dynamically adjust the thresholds to generate pseudo labels and re-weight the loss function for different classes. An overview of our method is shown in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary</head><p>Our approach is built upon consistency-based pseudo labeling, which has proven effective for both semi-supervised image classification and object detection. Below, we briefly introduce the teacher-student training paradigm which serves as the basis for current consistency-based approaches. Overall, a teacher model is firstly trained on labeled images, and then it is used to produce pseudo labels (boxes) on unlabeled images to supervise the training of a student model. Formally, given a set of labeled images S and a set of unlabeled images U, an object detector is trained on S in a standard supervised manner:</p><formula xml:id="formula_0">L s (I, p, t, p * , t * ) = E I?S E i?B L s (I, p i , t i ) = E I?S E i?B [ cls (p i , p i * ) + loc (t i , t i * )]<label>(1)</label></formula><p>where I is an input image with a set of candidate boxes B, and p i , t i denote the prediction of class probability and bounding box coordinates for the i-th candidate box. Each candidate box is associated with a one-hot label p i * and a ground-truth box location t i * as supervisory signals, and the losses for classification and localization are often instantiated as a weighted sum of a standard cross-entropy loss and a smooth L 1 .</p><p>The teacher model trained on S then generates pseudo boxes on all unlabeled images in U through standard inference. These pseudo boxes are further filtered by a predefined threshold ? conditioned on the prediction confidence p i ; the remaining boxes are used to train a student model whose weights are initialized from the teacher model:</p><formula xml:id="formula_1">L = L s (I s , p * , t * ) + ? u L u (I u , p u * , t u * )<label>(2)</label></formula><p>where p u * and t u * denote pseudo class labels and box coordinates derived from the teacher model. The loss is a weighted sum of supervised loss L s on labeled images and unsupervised loss L u on unlabeled samples controlled by ?.</p><p>Following <ref type="bibr" target="#b27">(Sohn et al. 2020b;</ref><ref type="bibr" target="#b21">Liu et al. 2021;</ref><ref type="bibr" target="#b41">Zhou et al. 2021;</ref><ref type="bibr" target="#b37">Yang et al. 2021)</ref>, given an unlabeled image, when generating pseudo labels, we only use horizontal flipping as a weak augmentation; when training the student model, we use strong augmentations including color jitter, Gaussian blur and Cutout (2017) for the same image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Certainty-aware Pseudo Labels</head><p>Recall that existing approaches typically form bounding boxes through coordinate regression, and then predict the objects within boxes through classification. To generate pseudo boxes used as ground-truth by the student model, it is a common practice to apply a threshold ? to filter out boxes with low classification scores. While straightforward, such a localization-agnostic strategy fails to model how well boxes are localized. To address this issue, we formulate localization as classification, producing certainty-aware boxes, such that the quality of both localization and classification are explicitly considered to guide the generation of pseudo labels. Formally, given an unlocalized candidate box (x 1 , y 1 , x 2 , y 2 ) with its top left corner at (x 1 , y 1 ) and its bottom right corner at (x 2 , y 2 ), its corresponding ground-truth locations are denoted as (x g1 , y g1 , x g2 , y g2 ). Each side of the candidate is independently localized to the corresponding side of ground-truth through classification. Taking the left side of the candidate box as an example, we first obtain a line segment l which is perpendicular to the side, then split l evenly into K consecutive intervals and predict which interval the unlocalized side belongs to according to the ground-truth position x g1 through a K-way classification. In particular, if the left side of the ground-truth box is perpendicular to the k-th interval, we mark that the side belongs to the k-th interval for training (see <ref type="figure">Figure 1</ref> for an illustration). Then the loss function for localization given an image can be written as:</p><formula xml:id="formula_2">seg (T , Y ) = E i?B s=4 s=1 k=K k=1 ?y i s,k log(sigmoid(t i s,k ))<label>(3)</label></formula><p>where the superscript i denotes the i-th candidate box sampled from the box set, and t i k,s is the unnormalized prediction score from the k-th interval in the s-th side, and the label y i k,s = 1 if the side belongs to the k-th interval otherwise y i k,s = 0. To measure the localization quality for the i-th box, we first obtain the maximal class score along each side and then compute the mean of these scores:</p><formula xml:id="formula_3">v i = 1 4 s=4 s=1 max 1?k?K (t i s,k ).<label>(4)</label></formula><p>The localization quality score v i , indicating how well boxes are localized, together with the classification confidence p i are two complementary metrics measuring the certainty of localization and classification, respectively. They are further used for post-processing like non-maximum suppression and pseudo label generation, which will be described below.</p><p>Thus far we have formulated box localization in a classification manner to obtain quality measurement, yet the localization performance could be largely hindered by discretizing the problem of deriving continuous bounding box coordinates. In particular, the membership of an interval is a rough estimation of location particularly when the interval size is large. To obtain the precise location of a side within the interval, we further perform regression from the center line x k of the k-th interval to the ground-truth line x g for finer localization. We use a smooth L 1 loss for fine regression, and the overall localization loss becomes:</p><formula xml:id="formula_4">loc = seg (T , Y ) + reg (Y , X g ) = E i?B K k y k [ ?log(sigmoid(t i k,s )) + SmoothL1(x k , x g ) ]<label>(5)</label></formula><p>Finally, we replace the localization loss in Eqn. 1 used by both the teacher model and the student model with Eqn. 5. Consequently, the trained teacher model produces pseudo labels that are aware of both localization and classification quality.  Dynamic Thresholding and Re-weighting</p><p>As discussed above, class imbalance exists in object detection especially when annotations are scarce. The imbalance is further enlarged in semi-supervised settings since the teacher model produces relatively lower confidence scores for underrepresented classes <ref type="bibr" target="#b7">(Dave et al. 2021)</ref>, which hardly survive the often large threshold ? . On the other hand, simply lowering ? introduces more noisy pseudo labels in common classes. With this in mind, we propose to dynamically adjust the threshold and re-weight losses in a classwise manner conditioned on classification and localization confidence scores for each category. For each category m, the classification and localization confidence score p j m and v j m for each foreground candidate box (indexed by j) are accumulated online to produce an unnormalized frequency score c = j p j m v j m , which not only approximates the detector's current overall confidence level for the category but also counts the number of foreground instances. The class-specific threshold ? m and re-weighting coefficient ? m are then derived as follows:</p><formula xml:id="formula_5">? m = j p j m v j m E m?M j 1 ?1 ?, ? m = E m?M j 1 j p j m v j m ?2<label>(6)</label></formula><p>where E m?M j 1 denotes the average number of foreground instances from all categories and ? is the original manually chosen threshold. The class-specific ? m is then applied to filter pseudo labels, and ? m is multiplied to losses (Eqn. 2) of all foreground instances in each category. Two factors ? 1 and ? 2 control the degree of focus on underrepresented classes; when set to 0, dynamic thresholding and re-weighting are disabled. By keeping more pseudo labels for underrepresented classes, as well as promoting their importance during training through re-weighting the losses, the bias towards head classes is mitigated. It is worth pointing out that ? m needs to be bounded as it is applied on predicted probabilities, and we find clipping it into [0.4, 0.9] works well empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments Experimental Setup</head><p>Datasets. We evaluate our method on two standard object detection datasets, COCO <ref type="bibr" target="#b19">(Lin et al. 2014</ref>) and PASCAL VOC <ref type="bibr" target="#b9">(Everingham et al. 2010</ref>), under semi-supervised settings following <ref type="bibr" target="#b13">(Jeong et al. 2019;</ref><ref type="bibr" target="#b27">Sohn et al. 2020b;</ref><ref type="bibr" target="#b21">Liu et al. 2021;</ref><ref type="bibr" target="#b41">Zhou et al. 2021;</ref><ref type="bibr" target="#b37">Yang et al. 2021)</ref>. In particular, four settings are used: (1) COCO-full: the COCO train2017 set containing ?118k images is used as the labeled set, and the additional ?123k unlabeled images are used as unlabeled set; (2) COCO-partial: we follow <ref type="bibr" target="#b27">(Sohn et al. 2020b</ref>) and randomly sample 1%/2%/5%/10% images from COCO train2017 set as the labeled set, and use the remaining images in train2017 as the unlabeled set; (3) PASCAL VOC: the VOC07 trainval set is used as labeled set and the VOC12 trainval is used as unlabeled set; (4) PASCAL VOC + COCO-20: following <ref type="bibr" target="#b27">(Sohn et al. 2020b)</ref>, images from COCO containing the 20 classes in PASCAL VOC are used as an additional unlabeled set. For evaluation, the val2017 set of COCO and the VOC07 test set of PASCAL VOC are used. Training and Testing Configuration. Since existing methods for SSOD use various different setups for training and testing, we evaluate our method under multiple settings for fair comparison. In all settings, the teacher model is firstly trained on the labeled set, and the student model is trained on the combination of labeled and unlabeled images. We report mean Average Precision (mAP) at different IoU thresholds (e.g. AP 50 , AP 75 and AP 50:95 which is denoted as AP) to measure the effectiveness. Implementation Details. Our implementation follows existing approaches for fair comparison, and thus we use For more details of the implementation such as choices of hyper parameters and training recipes, we refer readers to Appendix.  <ref type="table">Table 2</ref>: Results (AP) on COCO-partial. ? denotes using a lower final score threshold to improve recall as in <ref type="bibr" target="#b41">(Zhou et al. 2021)</ref>. ? denotes using ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1% COCO</head><note type="other">2% COCO 5% COCO 10% COCO Supervised</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2% COCO AP</head><p>Overall 21.6 ?? 22.5 Rarest 10 Classes 23.9 ?? 26.0   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>We first report the results on four settings, and compare with supervised baselines as well as various state-of-theart approaches for semi-supervised object detection, such as CSD <ref type="formula" target="#formula_0">(2019)</ref>  <ref type="table" target="#tab_2">Table 1 and Table 2</ref>. COCO-full and PASCAL VOC. As shown in <ref type="table" target="#tab_2">Table 1</ref>(ac), our approach outperforms state-of-the-art methods by at least 1 ? 2% AP on COCO and PASCAL VOC. For example, when trained under 3? schedule, our method obtains 43.3% AP and outperforms Unbiased Teacher <ref type="formula" target="#formula_0">(2021)</ref> and Humble teacher (2021) by 2.0% and 1.0% respectively. In the short schedule (1?) setting, our approach obtains 41.0% AP, which outperforms methods using long schedules like CSD (2019) and STAC (2020b). On PASCAL VOC, we obtain 52.4% AP and 54.6% AP with single-scale training and multi-scale training respectively. Notably, large improvements are obtained by our method when precise localization is needed (e.g. AP 75 ), indicating that our approach improves the localization quality for SSOD. COCO-partial. We then evaluate our method under the limited-annotation regime on COCO-partial. As demonstrated in <ref type="table">Table 2</ref>, our method improves supervised baselines by up to 10%. When 10% annotations are available, our method achieves 32.23% AP and is ? 2% higher than Instant-Teaching <ref type="bibr" target="#b41">(Zhou et al. 2021</ref>) even though model ensemble is used in their method. With only 1%/2%/5% an-notations available, our method is able to achieve state-ofthe-art 19.02%, 23.34% and 28.40% APs respectively. Improvements for underrepresented classes. To validate the effectiveness of our method on improving the detection performance for underrepresented classes, we also show results <ref type="table" target="#tab_4">(Table 3)</ref> on the rarest 10 classes in terms of number of annotations in the training set. We can see after adding dynamic pseudo label thresholding and loss re-weighting methods described in Sec. 2, the overall performance is improve by 0.9% AP (from 21.6% to 22.5%) and the performance on rare classes is improved by 2.1% AP (from 23.9% to 26.0%). This confirms that our method indeed promotes the performance for underrepresented classes.</p><p>Compatibility to other methods. It is worth pointing out that our method is orthogonal to many useful techniques explored in existing approaches mentioned above. For example, when using a simplified model ensemble method from <ref type="bibr" target="#b41">(Zhou et al. 2021</ref>), a further performance improvement is observed as in <ref type="table" target="#tab_5">Table 4</ref>. In particular, we train two teacher models separately and use the ensemble of them to generate pseudo labels, which are then used to train two student models. Finally, the ensemble of two student models is evaluated. As can be seen, AP 50 and overall AP are improved by 0.8% and 0.5% respectively. We also show in <ref type="table" target="#tab_6">Table 5</ref> that sampling labeled and unlabeled images with 1:1 ratio during training as in <ref type="bibr">Tang et al. 2021)</ref> further improves performance of our method. Other methods like Mean Teacher (2017), <ref type="bibr">Coteaching (2018)</ref>, input ensemble and soft labels have also been utilized in <ref type="bibr" target="#b41">Zhou et al. 2021;</ref><ref type="bibr" target="#b37">Yang et al. 2021;</ref><ref type="bibr">Tang et al. 2021)</ref> but not in our method, for which we believe our work could be complementary to many current state-of-the-art methods for semi-supervised object detection -and thus the performance could be further improved when combining these methods with ours. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improvement on localization performance.</head><p>Having demonstrated the overall efficacy of our approach, we now evaluate the localization performance. We first compare our method against the baseline without the proposed components at different IoU criteria. As shown in <ref type="figure">Figure 2</ref>, our method improves baseline by a larger margin when higher localization precision is required: the performances are similar at 0.5 IoU threshold, whereas our approach obtains more than 6% higher mAP at 0.85 IoU threshold. We further evaluate the performance of teacher models on the withheld unlabeled images and see whether pseudo labels produced by our method are better localized. Similar trends in <ref type="figure">Figure 3</ref> confirm that pseudo labels produced by our method are localized more precisely, and thus improve the detection performance for semi-supervised object detection.</p><p>Qualitative results. In addition to the quantitative analysis presented above, we provide some qualitative results in <ref type="figure" target="#fig_3">Figure 4</ref>. As can be observed, our method produces more precise localization results than the baseline without proposed components in Sec. 2. In particular, our method is better at localizing boundaries of irregular-posed objects like the bear and person in <ref type="figure" target="#fig_3">Figure 4</ref>.  <ref type="table">Table 6</ref>: Effectiveness of proposed components including certainty-aware pseudo labels (CA), loss re-weighting (RE) and dynamic thresholding (DT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CA RE DT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>Effectiveness of different components. We validate the effectiveness of proposed components and summarize the results in <ref type="table">Table 6</ref>. We can see by adding the certainty-aware pseudo labels, class-specific loss re-weighting and dynamic thresholding, the performance is improved by 1.7%, 0.8% and 0.5% respectively. When all the components are added, our approach improves the baseline by 2.6% AP and 4.3% AP 75 , confirming the proposed components are effective and especially useful for improving localization quality.</p><p>Data augmentations. We also study the usefulness of different data augmentation techniques. <ref type="table" target="#tab_9">Table 7</ref> summarizes the results. When no data augmentation is applied for training the student model, the performance degrades from 22.5% to 20.3% AP, indicating that data augmentation is critical. Adding color jittering and Gaussian blurring improves the performance by 1.2%, and applying Cutout further boosts AP by 1%.</p><p>Hyper-parameter Sensitivity. We experiment with different hyper-parameters and summarize the results in <ref type="table" target="#tab_11">Table 8</ref> and 9. For localization, our method is robust to hyperparameter selection as long as K is large enough to produce fine-grained localization intervals. However, when K is set to be a small number like 4, the intervals are too coarse and the localization branch degenerates to a similar form of pure regression method, resulting in a degraded perfor-   mance of 19.7% AP. For dynamic thresholding and loss reweighting, larger ? 1 and ? 2 leads to more emphasis on infrequent classes during training, and we find using ? 1 = 0.05 and ? 2 = 0.6 gives the best result, as shown in <ref type="table" target="#tab_12">Table 9</ref>. When set as 0, the corresponding method is disabled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Object Detection. As a fundamental computer vision task, object detection has been extensively studied for decades <ref type="bibr" target="#b31">(Viola and Jones 2001;</ref><ref type="bibr" target="#b10">Felzenszwalb et al. 2009;</ref><ref type="bibr" target="#b25">Ren et al. 2015;</ref><ref type="bibr" target="#b24">Redmon et al. 2016;</ref><ref type="bibr" target="#b3">Carion et al. 2020)</ref>. Modern object detectors have evolved from anchorbased detectors like Faster RCNN <ref type="formula" target="#formula_0">(2015)</ref>, <ref type="bibr">YOLO (2016)</ref> and <ref type="bibr">SSD (2016)</ref>, to anchor-free <ref type="bibr" target="#b29">(Tian et al. 2019;</ref><ref type="bibr" target="#b42">Zhou, Wang, and Kr?henb?hl 2019)</ref> and transformer-based detectors <ref type="bibr" target="#b3">(Carion et al. 2020)</ref> in pursuit of simpler formulation and stronger performance. Various directions have also been actively explored on improving localization precision <ref type="bibr" target="#b14">(Jiang et al. 2018;</ref><ref type="bibr" target="#b32">Wang et al. 2020;</ref><ref type="bibr" target="#b23">Qiu et al. 2020)</ref>, inference efficiency <ref type="bibr">(Najibi, Singh, and Davis 2019;</ref><ref type="bibr" target="#b30">Uzkent, Yeh, and Ermon 2020)</ref>, training paradigms <ref type="bibr" target="#b17">Li et al. 2020)</ref>, to name a few. While powering a wide range of applications, standard object detectors require box annotations for all objects-of-interest in images during training, which are time-consuming and labour-intensive to obtain. Semi-Supervised Learning. Semi-supervised learning (SSL) for visual understanding leverages unlabeled images for improved performance in various tasks <ref type="bibr" target="#b1">(Berthelot et al. 2019;</ref><ref type="bibr" target="#b35">Xie et al. 2020a;</ref><ref type="bibr" target="#b26">Sohn et al. 2020a;</ref><ref type="bibr" target="#b22">Miyato et al. 2018;</ref><ref type="bibr" target="#b0">Bachman, Alsharif, and Precup 2014;</ref><ref type="bibr" target="#b36">Xie et al. 2020b;</ref><ref type="bibr" target="#b16">Laine and Aila 2017)</ref>. Recent advances on SSL mostly resort to consistency-based methods with data augmentation and have significantly improved performance for image classification. Specifically, the model is incentivized to produce consistent predictions across different views of an in-    <ref type="bibr" target="#b5">(Cubuk et al. 2019</ref><ref type="bibr" target="#b6">(Cubuk et al. , 2020</ref> and adversarially generated ones <ref type="bibr" target="#b22">(Miyato et al. 2018</ref>  <ref type="formula" target="#formula_0">(2021)</ref> to combat the noise in pseudo labels. While semi-supervised object detection performance has been steadily improved, most current approaches directly leverage recent advances on semi-supervised image classification for object detection.</p><p>In contrast, we investigate and address the unique challenge of semi-supervised object detection-injecting localization precision to generate better boxes and dynamically adjusting pseudo label threshold to combat class imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we rethink the use of pseudo labels for semisupervised object detection (SSOD), and equip pseudo labels to be certainty-aware so as to address the lack of localization confidence when generating pseudo labels and the amplified class imbalance. We presented certainty-aware pseudo labeling considering both classification and localization quality by formulating box localization as a classification problem. Conditioned on the quality scores, the pseudo labels are filtered by dynamically derived thresholds and the losses are re-weighted in a class-specific manner, in pursuit of improved localization quality and balanced network learning for SSOD. Extensive experiments under multiple settings demonstrated the efficacy of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Training and Testing Configuration</head><p>More details of training and testing on different experimental settings are provided as below:</p><p>? (1) COCO-full: we report results under 1? and 3? training schedules, which are roughly equivalent to 12 and 36 epochs respectively. The teacher models are trained for 180k / 540k iterations, and the student models are trained using the same schedule. ? (2) COCO-partial: for 1/2/5/10% settings, we train teacher models for 6k/12k/30k/60k iterations, and then train student models for 180k iterations. During testing, we report results under two score thresholds, 0.05 and 0.001, that are applied on final detection predictions. A lower threshold generally improves the recall through keeping more predicted boxes and thus results in slightly better performance. ? (3) PASCAL VOC: we train teacher models for 10k iterations, then train student models for 90k iterations. Both single-scale training and multi-scale training results are reported.</p><p>For ablation study and analysis, we use the 2% COCO setting and a shorter 0.5? schedule due to the limited computational resources, if not mentioned otherwise.</p><p>It is worth pointing out that existing methods typically use a larger batch size than ours (16/32/64 v.s. 8), and thus our training schedule is equal to -sometimes shorter thanother state-of-the-art SSOD approaches <ref type="bibr" target="#b27">(Sohn et al. 2020b;</ref><ref type="bibr" target="#b21">Liu et al. 2021;</ref><ref type="bibr">Tang et al. 2021;</ref><ref type="bibr" target="#b41">Zhou et al. 2021;</ref><ref type="bibr" target="#b37">Yang et al. 2021</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>The ResNet-50 <ref type="bibr" target="#b12">(He et al. 2016</ref>) backbone network we use is initialized from ImageNet pre-trained weights. We set ? u = 1.0 and ? = 0.7. For the localization branch, we set K = 30. For dynamic thresholding and re-weighting, we set ? 1 = 0.05 and ? 2 = 0.6. We train the models with 4 Nvidia 1080 Ti GPUs, using a total batch size of 8. We use SGD with an initial learning rate of 0.01, a weight decay of 1e ? 4, a momentum of 0.9. Learning rate is divided by 10 at the 120k/160k iteration for the 180k schedule, and likewise for other schedules. For single-scale training, the short side of image is resized to 600 for PASCAL VOC and 800 for COCO; for multi-scale training, the short side size is sampled from (640, 800). The long side is kept no more than 1, 333 after resizing. Other details are the same as in Detectron2 <ref type="bibr" target="#b34">(Wu et al. 2019)</ref>, which is used for our implementation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Faster-RCNN with FPN (Lin et al. 2017) as our detector using a ResNet-50 (He et al. 2016) as its backbone network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, STAC (2020b), ISMT (2021), Instant-Teaching (2021), Multi-Phase Learning (2021), Unbiased Teacher (2021) and Humble teacher (2021). For approaches using ensemble techniques like (Zhou et al. 2021; Wang et al. 2021), we report their single-model results for fair comparison. For Unbiased Teacher (Liu et al. 2021) which uses larger batch size and longer training schedules, we retrain it under our training schedules with their official implementation. Results are summarized in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Performance at different IoU criteria under 2% COCO setting. Evaluating the localization precision of pseudo boxes from teacher model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of localization quality. Our method (Green) localizes objects more precisely than the baseline (Blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Teacher Student TeacherSupervised Learning Stage Teacher-Student Learning Stage</head><label></label><figDesc></figDesc><table><row><cell>Labeled</cell><cell></cell><cell></cell><cell cols="2">Interval Classification</cell></row><row><cell>Weak Aug.</cell><cell>Loc. Branch</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>candidate</cell></row><row><cell>Unlabeled</cell><cell>Cls. Branch</cell><cell>p</cell><cell>v</cell><cell>ground-truth</cell></row><row><cell>Weak Aug.</cell><cell cols="2">Certainty-Aware Pesudo Labels</cell><cell cols="2">Dynamic Thresholding and Re-weighting</cell></row><row><cell>Initialize</cell><cell>Pseudo Boxes</cell><cell></cell><cell cols="2">underrepresented classes</cell></row><row><cell>Strong Aug.</cell><cell></cell><cell></cell><cell cols="2">class-specific thresholds</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">and loss re-weighting</cell></row><row><cell></cell><cell>keep</cell><cell>discard</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison with state-of-the-art approaches on COCO-full, PASCAL VOC, and PASCAL VOC + COCO-20 settings. * denotes the use of longer training schedule (3?). ? denotes multi-scale training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>? 0.15 (+1.15) 13.60 ? 0.10 (+0.90) 18.90 ? 0.10 (+0.43) 24.50 ? 0.15 (+0.64) STAC (2020b) 13.97 ? 0.35 (+4.92) 18.25 ? 0.25 (+5.55) 24.38 ? 0.12 (+5.91) 28.64 ? 0.21 (+4.78) Unbiased Teacher (2021) 17.84 ? 0.12 (+8.79) 21.98 ? 0.07 (+9.28) 26.30 ? 0.11 (+7.83) 29.64 ? 0.10 (+5.78) Humble teacher ? (2021) 16.96 ? 0.38 (+7.91) 21.72 ? 0.24 (+9.02) 27.70 ? 0.15 (+9.23) 31.61 ? 0.28 (+7.74) Instant-Teaching ? (2021) 16.00 ? 0.20 (+6.95) 20.70 ? 0.30 (+8.00) 25.50 ? 0.05 (+7.03) 29.45 ? 0.15 (+5.59)</figDesc><table><row><cell></cell><cell>9.05 ? 0.16</cell><cell>12.70 ? 0.15</cell><cell>18.47 ? 0.22</cell><cell>23.86 ? 0.81</cell></row><row><cell cols="5">CSD (2019) 10.20 Instant-Teaching  ?  ? (2021) 18.05 ? 0.15 (+9.00) 22.45 ? 0.15 (+9.75) 26.75 ? 0.05 (+8.28) 30.40 ? 0.05 (+6.54)</cell></row><row><cell>Ours</cell><cell cols="4">18.21 ? 0.31 (+9.16) 22.62 ? 0.24 (+9.92) 27.78 ? 0.17 (+9.31) 31.67 ? 0.18 (+7.81)</cell></row><row><cell>Ours  ?</cell><cell cols="4">19.02 ? 0.25 (+9.97) 23.34 ? 0.18 (+10.64) 28.40 ? 0.15 (+9.93) 32.23 ? 0.14 (+8.37)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>2% COCO</cell><cell cols="2">AP 50 AP 75 AP</cell></row><row><cell cols="2">Single Model 37.1</cell><cell>23.7 22.5</cell></row><row><cell>Ensemble</cell><cell>37.9</cell><cell>24.1 23.0</cell></row><row><cell>: Performance improvement</cell><cell></cell><cell></cell></row><row><cell>on the rarest 10 classes.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">VOC + COCO-20 AP 50 AP 75 AP</cell></row><row><cell>Original</cell><cell>79.6</cell><cell>61.2 56.1</cell></row><row><cell>1:1 Sampling</cell><cell>79.8</cell><cell>62.1 56.9</cell></row><row><cell>: Performance of our method</cell><cell></cell><cell></cell></row><row><cell>with model ensemble.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Performance with 1:1 la- beled:unlabeled image sampling ratio.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Effectiveness of different data augmentations applied when training the student model, including color jitter</figDesc><table /><note>(Color), Gaussian blur (Blur) and Cutout.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>? 1</cell><cell>? 2</cell><cell cols="2">AP 50 AP 75 AP</cell></row><row><cell cols="2">0.05 0.4</cell><cell>36.6</cell><cell>23.2 22.2</cell></row><row><cell cols="2">0.05 0.6</cell><cell>37.1</cell><cell>23.7 22.5</cell></row><row><cell cols="2">0.05 0.8</cell><cell>37.0</cell><cell>23.7 22.5</cell></row><row><cell cols="2">0.03 0.6</cell><cell>36.8</cell><cell>23.6 22.3</cell></row><row><cell cols="2">0.07 0.6</cell><cell>36.7</cell><cell>23.3 22.3</cell></row><row><cell>: Hyper parameter</cell><cell></cell><cell></cell></row><row><cell>sensitivity on number of</cell><cell></cell><cell></cell></row><row><cell>intervals K.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>: Hyper-parameter sen-</cell></row><row><cell>sitivity on variance controlling</cell></row><row><cell>factors ? 1 and ? 2 .</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning with pseudo-ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exponential Moving Average Normalization for Self-supervised and Semi-supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluating Large-Vocabulary Object Detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01066</idno>
	</analytic>
	<monogr>
		<title level="m">The Devil is in the Details</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>IJCV</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Consistencybased Semi-supervised Learning for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>et al. 2020. The open images dataset v4. IJCV</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning from noisy anchors for one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unbiased teacher for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9745" to="9755" />
		</imprint>
	</monogr>
	<note>Autofocus: Efficient multi-scale inference</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Offset bin classification network for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster rcnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Humble Teachers Teach Better Students for Semi-Supervised Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04757</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Tang, Y</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A simple semi-supervised learning framework for object detection</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">FCOS: Fully Convolutional One-Stage Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient object detection in large images using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uzkent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Side-aware boundary localization for more precise object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Data-Uncertainty Guided Multi-Phase Learning for Semi-Supervised Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Selftraining with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Interactive Self-Training with Mean Teachers for Semi-supervised Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Instant-Teaching: An End-to-End Semi-Supervised Object Detection Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as Points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
