<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PRIME: A Few Primitives Can Boost Robustness to Common Corruptions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apostolos</forename><surname>Modas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique F?d?rale de Lausanne (EPFL)</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Rade</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ETH Z?rich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Ortiz-Jim?nez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique F?d?rale de Lausanne (EPFL)</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique F?d?rale de Lausanne (EPFL)</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PRIME: A Few Primitives Can Boost Robustness to Common Corruptions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite their impressive performance on image classification tasks, deep networks have a hard time generalizing to unforeseen corruptions of their data. To fix this vulnerability, prior works have built complex data augmentation strategies, combining multiple methods to enrich the training data. However, introducing intricate design choices or heuristics makes it hard to understand which elements of these methods are indeed crucial for improving robustness. In this work, we take a step back and follow a principled approach to achieve robustness to common corruptions. We propose PRIME, a general data augmentation scheme that relies on simple yet rich families of max-entropy image transformations. PRIME outperforms the prior art in terms of corruption robustness, while its simplicity and plug-and-play nature enable combination with other methods to further boost their robustness. We analyze PRIME to shed light on the importance of the mixing strategy on synthesizing corrupted images, and to reveal the robustness-accuracy trade-offs arising in the context of common corruptions. Finally, we show that the computational efficiency of our method allows it to be easily used in both on-line and off-line data augmentation schemes 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep image classifiers do not work well in the presence of various types of distribution shifts <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">18,</ref><ref type="bibr">42]</ref>. Most notably, their performance can severely drop when the input images are affected by common corruptions that are not contained in the training data, such as digital artefacts, low contrast, or blurs <ref type="bibr">[21,</ref><ref type="bibr">29]</ref>. In general, "common corruptions" is an umbrella term coined to describe the set of all possible distortions that can happen to natural images during their acquisition, storage, and processing lifetime, which can be very diverse. Nevertheless, while the space of possible perturbations is huge, the term "common corruptions" is generally used to refer to image transformations that, while degrading the quality of the images, still preserve their semantic information.</p><p>The first two authors contributed equally to this work. <ref type="bibr" target="#b0">1</ref> Our code is available at https://github.com/amodas/PRIME-augmentations PRIME <ref type="figure">Fig. 1</ref>. Images generated with PRIME, a simple method that uses a family of maxentropy transformations in different visual domains to create diverse augmentations.</p><p>Building classifiers that are robust to common corruptions is far from trivial. A naive solution is to include data with all sorts of corruptions during training, but the sheer scale of all possible types of typical perturbations that might affect an image is simply too large. Moreover, the problem is per se ill-defined since there exists no formal description of all possible common corruptions.</p><p>To overcome this issue, the research community has recently favoured increasing the "diversity" of the training data via data augmentation schemes <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">22,</ref><ref type="bibr">20]</ref>. Intuitively, the hope is that showing very diverse augmentations of an image to a network would increase the chance that the latter becomes invariant to some common corruptions. Still, covering the full space of common corruptions is hard. Hence, current literature has mostly resorted to increasing the diversity of augmentations by designing intricate data augmentation pipelines, e.g., introducing DNNs for generating varied augmentations <ref type="bibr">[20,</ref><ref type="bibr" target="#b4">5]</ref>, or coalescing multiple techniques <ref type="bibr">[44]</ref>, and thus achieve good performance on different benchmarks. This strategy, though, leaves a big range of unintuitive design choices, making it hard to pinpoint which elements of these methods meaningfully contribute to the overall robustness. Meanwhile, the high complexity of recent methods <ref type="bibr">[44,</ref><ref type="bibr" target="#b4">5]</ref> makes them impractical for large-scale tasks. Whereas, some methods are tailored to particular datasets and might not be general enough. Nonetheless, the problem of building robust classifiers is far from completely solved, and the gap between robust and standard accuracy is still large.</p><p>In this work, we take a step back and provide a systematic way for designing a simple, yet effective data augmentation scheme. By focusing on first principles, we formulate a new mathematical model for semantically-preserving corruptions, and build on basic concepts to characterize the notions of transformation strength and diversity using a few transformation primitives. Relying on this model, we propose PRIME, a data augmentation scheme that draws transformations from a max-entropy distribution to efficiently sample from a large space of possible distortions (see <ref type="figure">Fig. 1</ref>). The performance of PRIME, alone, already tops the current baselines on different common corruption datasets, whilst it can also be combined with other methods to further boost their performance. Moreover, the simplicity and flexibility of PRIME allows to easily understand how each of its components contributes to improving robustness.</p><p>Altogether, the main contributions of our work include:</p><p>? We introduce PRIME, a simple method that is built on a few guiding principles, which efficiently boosts robustness to common corruptions. ? We experimentally show that PRIME, despite its simplicity, achieves stateof-the-art robustness on multiple corruption benchmarks. ? Last, our thorough ablation study sheds light on the necessity of having diverse transformations, on the role of mixing in the success of current methods, on the potential robustness-accuracy trade-off, and on the importance of online augmentations.</p><p>Overall, PRIME is a simple model-based scheme that can be easily understood, ablated, and tuned. Our work is an important step in the race for robustness against common corruptions, and we believe that it has the potential to become the new baseline for learning robust classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">General model of visual corruptions</head><p>In this work, motivated by the "semantically-preserving" nature of common corruptions, we define a new model of typical distortions. Specifically, we leverage the long tradition of image processing in developing techniques to manipulate images while retaining their semantics and construct a principled framework to characterize a large space of visual corruptions.</p><p>Let x : [0, 1] 2 ? [0, 1] 3 be a continuous image 2 mapping pixel coordinates r = (r 1 , r 2 ) to RGB values. We define our model of common corruptions as the action on x of the following additive subgroup of the near-ring of transformations <ref type="bibr" target="#b3">[4]</ref> </p><formula xml:id="formula_0">T x = n i=1 ? i g i 1 ? ? ? ? ? g i m (x) : g i j ? {?, ?, ?}, ? i ? R ,<label>(1)</label></formula><p>where ?, ? and ? are random primitive transformations which distort x along the spectral (?), spatial (? ), and color (?) domains. As we will see, defining each of these primitives in a principled and coherent fashion will be enough to construct a set of perturbations which covers most types of visual corruptions.</p><p>To guarantee as much diversity as possible in our model, we follow the principle of maximum entropy to define our distributions of transformations <ref type="bibr" target="#b7">[8]</ref>. Note that using a set of augmentations that guarantees maximum entropy comes naturally when trying to optimize the sample complexity derived from certain information-theoretic generalization bounds, both in the clean [45] and corrupted settings <ref type="bibr">[28]</ref>. Specifically, the principle of maximum entropy postulates favoring those distributions that are as unbiased as possible given the set of constraints that define a family of distributions. In our case, these constraints are given in the form of an expected strength ? 2 , some boundary conditions, e.g., the displacement field must be zero at the borders of an image, and finally the desired smoothness level K. The principle of smoothness helps formalize the notion of physical plausibility, as most naturally occurring processes are smooth.</p><p>Formally, let I denote the space of all images, and let f : I ? I be a random image transformation distributed according to the law ?. Further, let us define a set of constraints C ? F, which restricts the domain of applicability of f , i.e., f ? C, and where F denotes the space of functions I ? I. The principle of maximum entropy postulates using the distribution ? which has maximum entropy given the constraints:</p><formula xml:id="formula_1">maximize ? H(?) = ? F d?(f ) log(?(f )) (2) subject to f ? C ?f ? supp(?),</formula><p>where H(?) represents the entropy of the distribution ? <ref type="bibr" target="#b7">[8]</ref>. In its general form, solving Eq. (2) for any set of constraints C is intractable. In Appendix A, we formally derive the analytical expressions for the distributions of each of our family of transformations, by leveraging results from statistical physics <ref type="bibr" target="#b0">[1]</ref>.</p><p>In what follows, we describe the analytical solutions to Eq. (2) for each of our basic primitives. In general, these distributions are governed by two parameters: K to control smoothness, and ? 2 to control strength. These transformations fall back to identity mappings when ? 2 = 0, independently of K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spectral domain</head><p>We parameterize the distribution of random spectral transformations using random filters ?(r), such that the transformation output follows</p><formula xml:id="formula_2">?(x)(r) = (x * (? + ? )) (r),<label>(3)</label></formula><p>where, * is the convolution operator, ?(r) represents a Dirac delta, i.e., identity filter, and ? (r) is implemented in the discrete grid as an FIR filter of size K ? ? K ? with i.i.d random entries distributed according to N (0, ? 2 ? ). Here, ? 2 ? governs the transformation strength, while larger K ? yields filters of higher spectral resolution. The bias ?(r) retains the output close to the original image. Spatial domain We model our distribution of random spatial transformations, which apply random perturbations over the coordinates of an image, as ? (x)(r) = x(r + ? (r)).</p><p>This model has been recently proposed in [34] to define a distribution of random smooth diffeomorphisms in order to study the stability of neural networks to small spatial transformations. To guarantee smoothness but preserve maximum entropy, the authors propose to parameterize the vector field ? as</p><formula xml:id="formula_4">? (r) = i 2 +j 2 ?K 2 ? ? i,j sin(?ir 1 ) sin(?jr 2 ),<label>(5)</label></formula><p>where ? i,j ? N (0, ? 2 ? /(i 2 + j 2 )). Such choice guarantees that the resulting mapping is smooth according to the cut frequency K ? , while ? 2 ? determines its strength.</p><p>Algorithm 1: PRIME</p><formula xml:id="formula_5">Input: Image x, primitives G = {Id, ?, ? ?}, where Id is the identity operator Output: Augmented imagex 1x0 ? x 2 for i ? {1, . . . , n} do 3xi ? x 4 for j ? {1, . . . , m} do 5 g ? U(G) Strength ? ? U (?min, ?max) 6xi ? g(xi) 7 end 8 end 9 ? ? Dir(1) Random Dirichlet convex coefficients 10x ? n i=0 ?ixi</formula><p>Color domain Following a similar approach, we define the distribution of random color transformations as random mappings ? between color spaces</p><formula xml:id="formula_6">?(x)(r) = x(r) + K? n=0 ? n sin (?n x(r)) ,<label>(6)</label></formula><p>where ? n ? N (0, ? 2 ? I 3 ), with denoting elementwise multiplication. Again, K ? controls the smoothness of the transformations and ? 2 ? their strength. Compared to Eq. (5), the coefficients in Eq. (6) are not weighted by the inverse of the frequency, and have constant variance. In practice, we observe that reducing the variance of the coefficients for higher frequencies creates color mappings that are too smooth and almost imperceptible, so we decided to drop this dependency.</p><p>Finally, we note that PRIME is very flexible with respect to its core primitives. In particular, PRIME can be easily extended to include other distributions of maximum entropy transformations that suit an objective task. For example, one might add the distribution of maximum entropy additive perturbations given by ?(x)(r) = x(r) + ? (r), where ? (r) ? N (0, ? 2 ? ). Nonetheless, since most benchmarks of visual corruptions disallow the use of additive perturbations during training [21], we do not include an additive perturbation category.</p><p>Overall, as demonstrated by our results in Secs. 4.2 and 5.2, our model is very flexible and can cover a large part of the semantic-preserving distortions. It also allows to easily control the strength and style of the transformations with just a few parameters. Moreover, changing the transformation strength enables to control the trade-off between corruption robustness and standard accuracy, as shown in Sec. 5.3. In what follows, we use this model to design an efficient augmentation scheme to build classifiers robust to common corruptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRIME: A simple augmentation scheme</head><p>We now introduce PRIME, a simple yet efficient augmentation scheme that uses our PRImitives of Maximum Entropy to confer robustness against common corruptions. The pseudo-code of PRIME is given in Algorithm 1, which draws a random sample from Eq. (1) using a convex combination of a composition of basic primitives. Below we describe the main implementation details.</p><p>Parameter selection It is important to ensure that the semantic information of an image is preserved after it goes through PRIME. As measuring semantic preservation quantitatively is not simple, we subjectively select each primitive's parameters based on visual inspection, ensuring maximum permissible distortion while retaining the semantic content of the image. However, to avoid relying on a specific strength for each transformation, PRIME stochastically generates augmentations of different strengths by sampling ? from a uniform distribution, with different minimum and maximum values for each primitive. For the color primitive, we observed that fairly large values for K ? (in the order of 500) are important for covering a large space of visual distortions. Unfortunately, implementing such a transformation can be memory inefficient. To avoid this issue, PRIME uses a slight modification of Eq. (6) and combines a fixed number ? of consecutive frequencies randomly chosen in the range [0, K ? ].</p><p>Mixing transformations The concept of mixing has been a recurring theme in the augmentation literature <ref type="bibr">[48,</ref><ref type="bibr">47,</ref><ref type="bibr">22,</ref><ref type="bibr">44]</ref> and PRIME follows the same trend. In particular, Algorithm 1 uses a convex combination of n basic augmentations consisting of the composition of m of our primitive transformations. In general, the convex mixing procedure (i) broadens the set of possible training augmentations, and (ii) ensures that the augmented image stay close to the original one. We later provide empirical results which underline the efficacy of mixing in Sec. 5.2. Overall, the exact mixing parameters are provided in Appendix B. Note that, the basic skeleton of PRIME is similar to that of AugMix. However, as we will see next, incorporating our maximum entropy transformations leads to significant gains in common corruptions robustness over AugMix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Performance analysis</head><p>In this section, we compare the classification performance of our method on multiple datasets with that of two current approaches: AugMix and DeepAugment (DA). We illustrate that PRIME significantly advances the corruption robustness over that of AugMix and DeepAugment on all the benchmarks. We also show that our method yields additional benefits when employed in concert with unsupervised domain adaptation [39].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training setup</head><p>We consider the CIFAR-10 (C-10), CIFAR-100 (C-100) <ref type="bibr">[</ref> Detailed training setup appears in Appendix C. We evaluate our trained models on the common corrupted versions (C-10-C, C-100-C, IN-100-C, IN-C) of the aforementioned datasets. The common corruptions [21] constitute 15 image distortions each applied with 5 different severity levels. These corruptions can be grouped into four categories, viz. noise, blur, weather and digital.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Robustness to common corruptions</head><p>In order to assess the effectiveness of PRIME, we evaluate its performance against C-10, C-100, IN-100 and IN common corruptions. The results are summarized in Tab. 1 3 . Amongst individual methods, PRIME yields superior results compared to those obtained by AugMix and DeepAugment alone and advances the baseline performance on the corrupted counterparts of the four datasets. As listed, PRIME pushes the corruption accuracy by 1.2% and 3.3% on C-10-C and C-100-C respectively over AugMix. On IN-100-C, a more complicated dataset, we observe significant improvements wherein PRIME outperforms AugMix by 10.9%. In fact, this increase in performance hints that our primitive transformations are actually able to cover a larger space of image corruptions, compared to the restricted set of AugMix. Interestingly, the random transformations in PRIME also lead to a 3.9% boost in corruptions accuracy over DeepAugment despite the fact that DeepAugment leverages additional knowledge to augment the training data via its use of pre-trained architectures. Moreover, PRIME provides cumulative gains when combined with DeepAugment, reducing the mean corruption error (mCE) of prior art (DA+AugMix) by 2.7% on IN-100-C. Lastly, we also evaluate the performance of PRIME on full IN-C. However, we do not use JSD in order to reduce computational complexity. Yet, even without the JSD loss, PRIME outperforms, in terms of corruption accuracy, both AugMix (with JSD) and DeepAugment by 6.7% and 2.4% respectively, while the mCE is reduced by 7.8% and 2.9%. And last, when PRIME is combined with DeepAugment, it also surpasses the performance of DA+AugMix (with JSD), reaching a corruption accuracy of almost 60% and an mCE of 51.3%. Note here, that, not only PRIME achieves superior robustness, but it does so efficiently. Compared to standard training on IN-100, AugMix requires 1.20x time and PRIME requires 1.27x. In contrast, DA is tedious and we do not measure its runtime since it also requires the training of two large image-to-image networks for producing augmentations, and can only be applied offline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Unsupervised domain adaptation</head><p>Recently, robustness to common corruptions has also been of significant interest in the field of unsupervised domain adaptation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">39]</ref>. The main difference is that, in domain adaptation, one exploits the limited access to test-time corrupted samples to adjust certain network parameters. Hence, it would be interesting to investigate the utility of PRIME under the setting of domain adaption. To that end, we combine our method with the adaption trick of <ref type="bibr">[39]</ref>. Specifically, we adjust the batch normalization (BN) statistics of our models using a few corrupted samples. Suppose z s ? {? s , ? s } are the BN mean and variance estimated from the training data, and z t ? {? t , ? t } are the corresponding statistics computed from n unlabelled, corrupted test samples, then we re-estimate the BN statistics as follows.?</p><formula xml:id="formula_7">= N N + n z s + n N + n z t .<label>(7)</label></formula><p>We consider three adaptation scenarios: single sample (n = 1, N = 16), partial (n = 8, N = 16) and full (n = 400, N = 0) adaptation. Here, we do not perform parameter tuning for N . As shown in Tab. 2, simply correcting BN statistics using as little as 8 corrupted samples pushes the corruption accuracy of PRIME from 71.6% to 75.3%. In general, PRIME yields cumulative gains in combination with adaptation and has the best IN-100-C accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Robustness insights using PRIME</head><p>In this section, we exploit the simplicity and the controllable nature of PRIME to investigate different aspects behind robustness to common corruptions. We first analyze how each transformation domain contributes to the overall robustness of the network. Then, we empirically locate and justify the benefits of mixing the transformations of each domain. Moreover, we demonstrate the existence of a robustness-accuracy trade-off, and, finally, we comment on the low-complexity benefits of PRIME in different data augmentation settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Contribution of transformations</head><p>We want to understand how the transformations in each domain of Eq. (1) contribute to the overall robustness. To that end, we conduct an ablation study on IN-100-C by training a ResNet-18 with the max-entropy transformations of PRIME individually or in combination. As shown in Tab. 3, spectral transformations mainly help against blur, weather and digital corruptions. Spatial operations also improve on blurs, but on elastic transforms as well (digital). On the <ref type="table">Table 3</ref>. Impact of the different max-entropy primitives (?: spectral, ?: color, ? : spatial) in PRIME on common corruption accuracy (?) of a ResNet-18. All the transformations are essential for the performance of PRIME. The JSD loss is not used. contrary, color transformations excel on noises and certain high frequency digital distortions, e.g., pixelate and JPEG artefacts, and have minor effect on weather changes. Besides, incrementally combining the transformations lead to cumulative gains e.g., spatial+color help on both noises and blurs. Yet, for obtaining the best results, the combination of all transformations is required. This means that each transformation increases the coverage over the space of possible distortions and the increase in robustness comes from their cumulative contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The role of mixing</head><p>In most data augmentation methods, besides the importance of the transformations themselves, mixing has been claimed as an essential module for increasing diversity in the training process <ref type="bibr">[48,</ref><ref type="bibr">47,</ref><ref type="bibr">22,</ref><ref type="bibr">44]</ref>. In our attempt to provide insights on the role of mixing in the context of common corruptions, we found out that it is capable of constructing augmented images that look perceptually similar to their corrupted counterparts. In fact, the improvements on specific corruption types observed in Tab. 3 can be largely attributed to mixing. As exemplified in <ref type="figure" target="#fig_2">Fig. 3</ref>, careful combinations of spectral transformations with the clean image introduce brightness and contrast-like artefacts that look similar to the corresponding corruptions in IN-C. Also, combining spatial transformations creates blur-like artefacts that look identical to zoom blur in IN-C. Finally, notice how mixing color transformations helps fabricate corruptions of the "noise" category. This means that the max-entropy color model of PRIME enables robustness to different types of noise without explicitly adding any during training. Note that one of the main goals of data augmentation is to achieve maximum coverage of the space of possible distortions using a limited transformation budget, i.e., within a few training epochs. The principle of max-entropy guarantees this within each primitive, but the effect of mixing on the overall space is harder to quantify. In this regard, we can use the distance in the embedding space, ?, of a SimCLRv2 <ref type="bibr" target="#b6">[7]</ref> model as a proxy for visual similarity <ref type="bibr">[49,</ref><ref type="bibr">30]</ref>. We are interested in measuring how mixing the base transformations changes the likelihood spectral spectral PRIME spectral spectral PRIME PRIME PRIME <ref type="figure" target="#fig_2">Fig. 3</ref>. Mixing produces images that are visually similar to the test-time corruptions. Each example shows the clean image, the PRIME image and the corresponding common corruption that resembles the image produced by mixing. We also report the mixing combination used for recreating the corruption. See Appendix D for additional examples. that an augmentation scheme generates some sample during training that is visually similar to some of the common corruptions. To that end, we randomly select N = 1000 training images {x n } N n=1 from IN, along with their C = 75 (15 corruptions of 5 severity levels) associated common corruptions {x c n } C c=1 , and generate for each of the clean images another T = 100 transformed samples {x t n } T t=1 using each augmentation scheme. Moreover, for each corruptionx c n we find its closest neighborx t n from the set of generated samples using the cosine distance in the embedding space. Our overall measure of fitness is <ref type="table" target="#tab_4">Table 4</ref> shows the values of this measure applied to AugMix and PRIME, with and without mixing. For reference, we also report the values of the clean (no transform) images {x n } N n=1 . More percentile scores can be found in Appendix F. Clearly, mixing helps reduce the distance between the common corruptions and the augmented samples from both methods. We also observe that PRIME, even with only 100 augmentations per image -in the order of the number of training  epochs -can generate samples that are twice as close to the common corruptions as AugMix. In fact, the feature similarity between training augmentations and test corruptions was also studied in <ref type="bibr">[29]</ref>, with an attempt to justify the good performance of AugMix on C-10. Yet, we see that the fundamental transformations of AugMix are not enough to span a broad space guaranteeing high perceptual similarity to IN-C. The significant difference in terms of perceptual similarity in Tab. 4 between AugMix and PRIME may explain the superior performance of PRIME on IN-100-C and IN-C (cf. Tab. 1) 4 .</p><formula xml:id="formula_8">1 N C N n=1 C c=1 min t 1 ? ?(x c n ) ?(x t n ) ?(x c n ) 2 ?(x t n ) 2 .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Robustness vs. accuracy trade-off</head><p>An important phenomenon observed in the literature of adversarial robustness is the so-called robustness-accuracy trade-off <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">43,</ref><ref type="bibr">35]</ref>, where technically adversarial training [27] with smaller perturbations (typically smaller ?) results in models with higher standard but lower adversarial accuracy, and vice versa.</p><p>In this sense, we want to understand if the strength of the image transformations introduced through data augmentations in PRIME can also cause such phenomenon in the context of robustness to common corruptions. As described in Sec. 2, each of the transformations of PRIME has a strength parameter ?, which can be seen as the analogue of ? in adversarial robustness. Hence, we can easily reduce or increase the strength of the transformations by setting? = ??, where ? ? R + . Then, by training a network for different values of ? we can monitor its accuracy on the clean and the corrupted datasets. We train a ResNet-18 on C-10 and IN-100 using the setup of Sec. 4.1. For reducing complexity, we do not use the JSD loss and train for 30 epochs. This sub-optimal setting could cause some performance drop compared to the results of Tab. 1, but we expect the overall trends in terms of accuracy and robustness to be preserved. Regarding the scaling of the parameters' strength, for C-10 we set ? ? [10 ?3 , 10 2 ] and sample 100 values spaced evenly on a log-scale, while for IN-100 we set ? ? [10 ?2 , 10 2 ] and we sample 20 values.</p><p>The results are presented in <ref type="figure" target="#fig_3">Fig. 4</ref>. For both C-10 and IN-100, it seems that there is a sweet spot for the scale around ? = 0.2 and ? = 1 respectively, where the accuracy on common corruptions reaches its maximum. For ? smaller than these values, we observe a clear trade-off between validation and robust accuracy. While the robustness to common corruptions increases, the validation accuracy decays. However, for ? greater than the sweet-spot values, we observe that the trade-off ceases to exist since both the validation and robust accuracy present similar behaviour (slight decay). In fact, these observations indicate that robust and validation accuracies are not always positively correlated and that one might have to slightly sacrifice validation accuracy in order to achieve robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Sample complexity</head><p>Finally, we investigate the necessity of performing augmentation during training (on-line augmentation), compared to statically augmenting the dataset before training (off-line augmentation). On the one hand, on-line augmentation is useful when the dataset is huge and storing augmented versions requires a lot of memory. Besides, there are cases where offline augmentation is not feasible as it relies on pre-trained or generative models which are unavailable in certain scenarios, e.g., <ref type="bibr">DeepAugment [20]</ref> or AdA <ref type="bibr" target="#b4">[5]</ref> cannot be applied on C-100. On the other hand, off-line augmentation may be necessary to avoid the computational cost of generating augmentations during training.</p><p>To this end, for each of the C-10 and IN-100 training sets, we augment them off-line with k = 1, 2, . . . , 10 i.i.d. PRIME transformed versions. Afterwards, for different values of k, we train a ResNet-18 on the corresponding augmented dataset and report the accuracy on the validation set and the common corruptions. For the training setup, we follow the settings of Sec. 4.1, but without JSD loss. Also, since we increase the size of the training set by (k + 1), we also divide the number of training epochs by the same factor, in order to keep the same overall number of gradient updates.</p><p>The performance on common corruptions is presented in <ref type="figure" target="#fig_4">Fig. 5</ref>. The first thing to notice is that, even for k = 1, the obtained robustness to common corruptions is already quite good. In fact, for IN-100 the accuracy (65%) is already better than AugMix (60.7% with JSD loss cf. Tab. 1). Regarding C-10, we observe that for k = 4 the actual difference with respect to the on-line augmentation is almost negligible (88.8% vs. 89.3%), especially considering the overhead of transforming the data at every epoch. Technically, this means that augmenting C-10 with 4 PRIME counterparts is enough for achieving good robustness to common corruptions. Finally, we also see in <ref type="figure" target="#fig_4">Fig. 5</ref> that the corruption accuracy on IN-100 presents a very slow improvement after k = 4. Comparing the accuracy at this point (67.2%) to the one obtained with on-line augmentation and without JSD (68.8% cf. Tab. 3) we observe a gap of 1.6%. Hence, given the cost of on-line augmentation on such large scale datasets, simply augmenting the training with 4 extra PRIME samples presents a good compromise for achieving competitive robustness. Nevertheless, the increase of 1.6% introduced by on-line augmentation is rather significant, hinting that generating transformed samples during training might be necessary for maximizing performance. In this regard, the lower computational complexity of PRIME allows it to easily achieve this +1.6% gain through on-line augmentation, since it only requires 1.27? additional training time compared to standard training, and only 1.06? compared to AugMix, but with much better performance. This can be a significant advantage with respect to complex methods, like DeepAugment, that cannot be even applied on-line (require heavy pretraining).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Common corruptions Towards evaluating the robustness of deep neural networks (DNNs) to natural distribution shifts, the authors in [21] proposed common corruptions benchmarks (CIFAR-10-C and ImageNet-C) constituting 15 realistic image distortions. Later studies [20] considered the example of blurring and demonstrated that performance improvements on these common corruptions do generalize to real-world images, which supports the use of common corruptions benchmarks. Recent work <ref type="bibr">[29]</ref> showed that current augmentation techniques undergo a performance degradation when evaluated on corruptions that are perceptually dissimilar from those in ImageNet-C. In addition to common corruptions, current literature studies other benchmarks e.g., adversarially filtered data [23], artistic renditions [20] and in-domain datasets <ref type="bibr">[36]</ref>. In Appendix J, we show that PRIME also improves robustness on these benchmarks. Improving corruption robustness Data augmentation has been the central pillar for improving the generalization of DNNs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">48,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr">47,</ref><ref type="bibr">26]</ref>. A notable augmentation scheme for endowing corruption robustness is AugMix <ref type="bibr">[22]</ref>, which employs a careful combination of stochastic augmentation operations and mixing. AugMix attains significant gains on CIFAR-10-C, but it does not perform as well on larger benchmarks like ImageNet-C. DeepAugment (DA) [20] addresses this issue and diversifies the space of augmentations by introducing distorted images computed by perturbing the weights of image-to-image networks. DA, combined with AugMix, achieves the current state-of-the-art on ImageNet-C. Other schemes include: (i) worst-case noise training <ref type="bibr">[37]</ref> or data augmentation through Fourier-based operations [41], (ii) inducing shape bias through stylized images [17], (iii) adversarial counterparts of DeepAugment <ref type="bibr" target="#b4">[5]</ref> and AugMix [44], (iv) pre-training and/or adversarial training [46,24], (v) constraining the total variation of convolutional layers <ref type="bibr">[38]</ref> or compressing the model <ref type="bibr" target="#b12">[13]</ref> and (vi) learning the image information in the phase rather than amplitude <ref type="bibr" target="#b5">[6]</ref> Besides, Vision Transformers <ref type="bibr" target="#b14">[15]</ref> have been shown to be more robust to common corruptions than standard CNNs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">31]</ref> when trained on big data. It would thus be interesting to study the effect of extra data alongside PRIME in future works. Finally, unsupervised domain adaptation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">39]</ref> using a few corrupted samples has also been shown to provide a considerable boost in corruption robustness. Nonetheless, domain adaptation is orthogonal to this work as it requires knowledge of the target distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Concluding remarks</head><p>We took a systematic approach to understand the notion of common corruptions and formulated a universal model that encompasses a wide variety of semanticpreserving image transformations. We then proposed a novel data augmentation scheme called PRIME, which instantiates our model of corruptions, to confer robustness against common corruptions. From a practical perspective, our method is principled yet efficient and can be conveniently incorporated into existing training procedures. Moreover, it yields a strong baseline on existing corruption benchmarks outperforming current standalone methods. Additionally, our thorough ablations demonstrate that diversity among basic augmentations (primitives) -which AugMix and other approaches lack -is essential, and that mixing plays a crucial role in the success of both prior methods and PRIME. In general, while complicated methods like DeepAugment perform well, it is difficult to understand, ablate and apply these online. Instead, we show that a simple model-based stance with a few guiding principles can be used to build a very effective augmentation scheme that can be easily understood, ablated and tuned. We believe that our insights and PRIME pave the way for building robust models in real-life scenarios. PRIME, for instance, provides a ready-to-use recipe for data-scarce domains such as medical imaging.</p><p>17. Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F.A., Brendel, W.:</p><p>Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In: International Conference on Learning Representations </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Maximum entropy transformations</head><p>To guarantee as much diversity as possible in our model of common corruptions, we follow the principle of maximum entropy to define our distributions of transformations <ref type="bibr" target="#b7">[8]</ref>. Note that using a set of augmentations that guarantees maximum entropy comes naturally when trying to optimize the sample complexity derived from certain information theoretic generalization bounds, both in the clean [45] and corrupted setting <ref type="bibr">[28]</ref>. Specifically, the principle of maximum entropy postulates favoring those distributions that are as unbiased as possible given the set of constraints that defines a family of distributions. In our case, these constraints are given in the form of an expected strength, i.e., ? 2 , desired smoothness, i.e., K, and/or some boundary conditions, e.g.,, the displacement field must be zero at the borders of an image. Let us make this formal. In particular, let I denote the space of all images x : R 2 ? R 3 , and let f : I ? I denote a random image transformation distributed according to the law ?. Further, let us define a set of constraints C ? F, which restrict the domain of applicability of f , i.e., f ? C, and where F denotes the space of functions I ? I. The principle of maximum entropy postulates using the distribution ? which has maximum entropy given the constraints:</p><formula xml:id="formula_9">maximize ? H(?) = F d?(f ) log(?(f )) (9) subject to f ? C ?f ? ?,</formula><p>where H(?) represents the entropy of the distribution ? <ref type="bibr" target="#b7">[8]</ref>. In its general form, solving Eq. (9) for any set of constraints C is intractable. However, leveraging results from statistical physics, we will see that for our domains of interest, Eq. (9) has a simple solution. In what follows we derive those distributions for each of our family of transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Spectral domain</head><p>As we introduced in Sec. 2, we propose to parameterize our family of spectral transformations using an FIR filter of size K ? ? K ? . That is, we are interested in finding a maximum entropy distribution over the space of spectral transformations with a finite spatial support. Nevertheless, on top of this smoothness constraint we are also interested in controlling the strength of the transformations. We define the strength of a distribution of random spectral transformations applied to an image x, as the expected L 2 norm of the difference between the clean and transformed images, i.e.,</p><formula xml:id="formula_10">E ? x ? ?(x) 2 2 = E ? ? * x 2 2 ,<label>(10)</label></formula><p>which using Young's convolution inequality is bounded as</p><formula xml:id="formula_11">E ? ? * x 2 2 ? x 2 1 E ? ? 2 2 .<label>(11)</label></formula><p>Indeed, we can see that the strength of a distribution of random smooth spectral transformations is governed by the expected norm of its filter. In the discrete domain, this can be simply computed as</p><formula xml:id="formula_12">E ? ? 2 2 = K? i=1 K? j=1 E ? ? 2 i,j .<label>(12)</label></formula><p>Considering this, we should then look for a maximum entropy distribution whose samples satisfy</p><formula xml:id="formula_13">C = ? ? R K??K? ? E ? ? 2 2 = K 2 ? ? 2 ? | ? ? ? ? .<label>(13)</label></formula><p>Now, note that this set is defined by an equality constraint involving a sum of K 2 ? quadratic random variables. In this sense, we know that the Equipartition Theorem <ref type="bibr" target="#b0">[1]</ref> applies and can be used to identify the distribution of maximum entropy. That is, the solution of Eq. (9) in the case that C is given by Eq. <ref type="formula" target="#formula_0">(13)</ref>, is equal to the distribution of FIR filters whose coefficients are iid with law N (0, ? 2 ? ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Spatial domain</head><p>The distribution of diffeomorphisms of maximum entropy with a fixed norm was derived by Petrini et al. in <ref type="bibr">[34]</ref>. The derivation is similar to the spectral domain, but with the additional constraint that the diffeomorphisms produce a null displacement at the borders of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Color domain</head><p>We can follow a very similar route to derive the distribution of maximum entropy among all color transformations, where, specifically, we constraint the transformations to yield ?(0) = 0 and ?(1) = 1 on every channel independently. Doing so, the derivation of the maximum entropy distribution can follow the same steps as in [34].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PRIME implementation details</head><p>In this section, we provide additional details regarding the implementation of PRIME described in Sec. 3. Since the parameters of the transformations are empirically selected, we first provide more visual examples for different values of smoothness K and strength ?. Then, we give the exact values of the parameters we use in our experiments supported by additional visual examples and we also describe the parameters we use for the mixing procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Additional transformed examples</head><p>We provide additional visual examples for each of the primitives of PRIME illustrating the effect of the following two factors: (i) smoothness controlled by parameter K, and (ii) strength of the transformation ? on the resulting transformed images created by the primitives <ref type="figure">. Figs. 6, 7</ref> and 8 demonstrate the resulting spectrum of images created by applying spectral, spatial and color transformations while varying the parameters K and ?. Notice how increasing the strength ? of each transformation drifts the augmented image farther away from its clean counterpart, yet produces plausible images when appropriately controlled.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Transformation parameters</head><p>We now provide the parameters of each transform that we selected and used in our experiments. In general, the values might vary for inputs of different dimensionality and resolution (i.e., CIFAR-10/100 vs ImageNet images).</p><p>Spectral transform Regarding the spectral transform of Eq. (3) we found out that, for the FIR filter ? , a size of K ? = 3 results into semantically preserving images for CIFAR-10/100 and ImageNet. For the latter, one can stretch the filter size to 5 ? 5 or even 7 ? 7, but then slight changes on the strength, ? ? , might destroy the image semantics. Eventually, given K ? = 3, we observed that ? ? = 4 is good enough for CIFAR-10/100 and ImageNet.</p><p>Spatial transform Concerning the spatial transform of Eq. (5), for the cut-off parameter K ? we followed the value regimes proposed by Petrini et al.</p><p>[34] and set K ? = 100 for CIFAR-10/100; K ? = 500 for ImageNet. Furthermore, for a given K ? , Petrini et al. also compute the appropriate bounds for the transformation strength, ? 2 ?min ? ? 2 ? ? ? 2 ?max , such that the resulting diffeomorphism remains bijective and the pixel displacement does not destroy the image. In fact, in their original implementation 5 , Petrini et al. directly sample ? ? ? U (? ?min , ? ?max ) instead of explicitly setting the strength. In our implementation, we also follow the same approach.</p><p>Color transform Regarding the color transform of Eq. (6) we found out that for CIFAR-10/100 a cut-off value of K ? = 10 and a strength of ? ? = 0.01 result into semantically preserving images for CIFAR-10/100; while for ImageNet, the corresponding values are K ? = 500 and ? ? = 0.05. As for the bandwidth (consecutive frequencies) ? we observed that a value of ? = 20 was memory sufficient for ImageNet, but for CIFAR-10/100, due to its lower dimensionality, we can afford all the frequencies to be used, e.g., ? = K ? .</p><p>Finally, as mentioned in Sec. 3, we randomly sample the strength of the transformations ? from a uniform distribution of given minimum and maximum values. Regarding the maximum, we always set it to be the one we selected through visual inspection, while the minimum is set to 0. <ref type="figure" target="#fig_8">Fig. 9</ref> displays additional augmented images created by applying each of the primitive transformations in our model using the aforementioned set of parameters on ImageNet. Our choice of parameters produces diverse image augmentations, while retaining the semantic content of the images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Parameters for mixing procedure</head><p>Regarding the mixing parameters of our experiments, we fix the total number of generated transformed images (width) to be n = 3. As for the composition of the transformations (depth), we follow a stochastic approach such that, on every iteration i ? {1, . . . , n}, onlym ? [1, m] compositions are performed, with m = 3. In fact, in Algorithm 1 we do not explicitly select randomly a newm for every i but we provide the identity operator Id instead. This guarantees that, in some cases, no transformation is performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Detailed experimental setup</head><p>We now provide all the experimental details for the performance evaluation of Sec. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional mixing examples</head><p>Continuing Sec. 5.2, we present additional examples in <ref type="figure">Fig. 10</ref> to demonstrate the significance of mixing in PRIME. We observe that the mixing procedure is capable of constructing augmented images that look perceptually similar to common corruptions. To illustrate this, we provide several examples in <ref type="figure">Fig. 10</ref> for PRIME (upper half) and AugMix (lower half) on CIFAR-10 and ImageNet-100. As shown in <ref type="figure">Figs. 10a and 10b</ref>, mixing spectral transformations with the clean images tends to create weather-like artefacts resembling frost and fog respectively. Carefully combining clean and spatially transformed images produces blurs ( <ref type="figure">Fig. 10c</ref>) and even elastic transform <ref type="figure">(Fig. 10e</ref>). Moreover, blending color augmentation with clean image produces shot noise as evident in <ref type="figure">Fig. 10d</ref>; Whereas spectral+color transformed image looks similar to snow corruption <ref type="figure">(Fig. 10f</ref>). All these observations explain the good performance of PRIME on the respective corruptions. Apart from the mixing in PRIME, the mixing in AugMix also plays a crucial role in its performance. In fact, a combination of translate and shear operations with the clean image create blur-like modifications that resemble defocus blur ( <ref type="figure">Fig. 10g</ref>) and motion blur <ref type="figure">(Fig. 10i</ref>). This answers why AugMix excels at blur corruptions and is even better than DeepAugment against blurs (cf. Tab. 7). In addition, on CIFAR-10, notice that mixing solarize and clean produces impulse noise-like modifications <ref type="figure">(Fig. 10j)</ref>  <ref type="figure">Fig. 10</ref>. The mixing procedure creates distorted images that look visually similar to the test-time corruptions. In each example (CIFAR-10/ImageNet-100), we show the clean image, the PRIME/AugMix augmented image and the corresponding common corruption that resembles the image produced by mixing. We also report the mixing combination used for recreating the corruption. ? stands for composition and + represents convex combination (mixing). (Top 3 rows): PRIME, and (Last 2 rows): AugMix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E SimCLR nearest neighbours</head><p>Regarding the minimum distances in the SimCLRv2 embedding space of Tab. 4, we also provide in <ref type="figure" target="#fig_9">Fig. 11</ref> some visual examples of the nearest neighbours of each method. In general, we observe that indeed smaller distance in the embedding space typically corresponds to closer visual similarity in the input space, with PRIME generating images that resemble more the corresponding common corruptions, compared to AugMix. Nevertheless, we also notice that for "Blurs" AugMix generates images that are more visually similar to the corruptions than PRIME, an observation that is on par with the lower performance of PRIME (without JSD) on blur corruptions (cf. Tab. 7) compared to AugMix.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Cosine distance statistics</head><p>Recall that in Tab. 4 we provide the average and the median of the minimum cosine distances computed in the SimCLRv2 embedding space. We now provide in Tab. 5 the values for different percentiles of these distances. We observe that the behaviour is consistent across different percentiles: PRIME (with or without mixing) is always producing feature representations that are more similar to the common corruptions, compared to any version of AugMix. Note also that for smaller percentiles (5%, 10%, 25%) it seems that PRIME without mixing reaches even lower values than PRIME. However, the difference with respect to PRIME can be considered as insignificant since it is in the order of 10 ?5 (note that all values in the table are in the order of 10 ?3 ); while a larger population of images (&gt; 1000) would potentially smooth out this difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Embedding space visualization</head><p>To qualitatively compare how diverse are the augmentations of PRIME with respect to other methods, we can follow the procedure in <ref type="bibr">[44]</ref>. We randomly select 3 images from ImageNet, each one belonging to a different class. For each image, we generate 100 transformed instances using AugMix and PRIME, while with DeepAugment we can only use the original images and the 2 transformed instances that are pre-generated with the EDSR and CAE image-to-image networks that DeepAugment uses. Then, we pass the transformed instances of each method through a ResNet-50 pre-trained on ImageNet and extract the features of its embedding space. On the features extracted for each method, we perform PCA and then visualize the projection of the features onto the first two principal components. We visualize the projected augmented space in <ref type="figure" target="#fig_0">Fig. 12</ref>, which demonstrates that PRIME generates more diverse (larger variance) features than AugMix and DeepAugment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Performance per corruption</head><p>Beyond the average corruption accuracy that we report in Tab. 1, we also provide here the performance of each method on the individual corruptions. The results on CIFAR-10/100 and ImageNet/ImageNet-100 are shown on Tab. 6 and Tab. 7 respectively. Compared to AugMix on CIFAR-10/100, the improvements from PRIME are mostly observed against Gaussian noise (+7.6%/12.3%), shot noise (+3.3%/7.0%), glass blur (+6.4%/11.0%) and JPEG compression (+1.3%/2.6%). These results show that PRIME can really push the performance against certain corruptions in CIFAR-10/100-C despite the fact that AugMix is already good on these datasets. However, AugMix turns out to be slightly better than PRIME against impulse noise, defocus blur and motion blur modifications; all of which have been shown to be resembled by AugMix created images (see <ref type="figure">Fig. 10</ref>). With ImageNet-100, PRIME enhances the diversity of augmented images, and leads to general improvements against all corruptions except certain blurs. On ImageNet, we observe that, in comparison to DeepAugment, the supremacy of PRIME is reflected on almost every corruption type, except some blurs and pixelate corruptions where DeepAugment is slightly better. When PRIME is used in conjunction with DeepAugment, compared to AugMix combined with DeepAugment, our method seems to lack behind only on blurs, while on the rest of the corruptions achieves higher robustness.  <ref type="table">Table 7</ref>. Per-corruption accuracy of different methods on IN-100 (ResNet-18) and IN (ResNet-50). ? indicates that JSD consistency loss is not used. * Models taken from <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Performance per severity level</head><p>We also want to investigate the robustness of each method on different severity levels of the corruptions. The results for CIFAR-10/100 and ImageNet/ImageNet-100 are presented in Tab. 8 and Tab. 9 respectively. With CIFAR-10/100, PRIME predominantly helps against corruptions with maximal severity and yields +3.9% and +7.1% gains on CIFAR-10 and CIFAR-100 respectively. Besides on ImageNet-100, PRIME again excels at corruptions with moderate to higher severity. This observations also holds when PRIME is employed in concert with DeepAugment. With ImageNet too this trend continues, and we observe that, compared to DeepAugment, PRIME improves significantly on corruptions of larger severity (+3.4% and +5.5% on severity levels 4 and 5 respectively). Also, this behaviour is consistent even when PRIME is combined with DeepAugment and is compared to DeepAugment+AugMix, where we see that again on levels 4 and 5 there is a significant improvement of +2.1% and +3.7% respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Performance on other corruptions</head><p>Finally, to examine the universality of PRIME, we evaluate the performance of our ImageNet-100 trained models against two other corrupted datasets: (i) ImageNet-100-C (IN-100-C) <ref type="bibr">[29]</ref>, and (ii) stylized ImageNet-100 (SIN-100) <ref type="bibr">[17]</ref>. While IN-100-C is composed of corruptions that are perceptually dissimilar to those in IN-100-C, stylized IN-100 only retains global shape information and discard local texture cues from IN-100 test images, via style transfer. Thus, it would be interesting test the performance of PRIME against these datasets since it would serve as a indicator for general corruption robustness of PRIME. More information about the corruption types contained in IN-100-C is available in the original paper <ref type="bibr">[29]</ref>. Tab. 10 enumerates the classification accuracy of different standalone approaches against IN-100-C on average, individual corruptions in IN-100-C and SIN-100. We can see that PRIME surpasses AugMix and DeepAugment by 4% and 1.2% respectively on IN-100-C. PRIME particularly helps against certain distortions such as blue noise sample (BSmpl), inverse sparkles and plasma noise. PRIME also works well against style-transferred images in SIN-100 and improves <ref type="table" target="#tab_0">Table 10</ref>. Classification accuracy of different methods on IN-100-C, IN-100-C and Stylized IN-100 (SIN-100) with ResNet-18.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>spectralFig. 2 .</head><label>2</label><figDesc>Images generated with the transformations of our common corruptions model. Despite the perceptibility of the distortion, the image semantics are preserved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>shows some visual examples for each kind of transformation, while additional visual examples along with the details of all the parameters can be found in Appendix B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>10 ? 3</head><label>3</label><figDesc>10 ?2 10 ?1 10 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Robustness vs. accuracy of a ResNet-18 (w/o JSD) on CIFAR-10 (left) and ImageNet-100 (right), when trained multiple times with PRIME. On each training instance, the transformation strength is scaled by ?. Note the different scale in axes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Accuracy of a ResNet-18 (w/o JSD) on CIFAR-10 (left) and ImageNet-100 (right) when augmenting the training sets with additional PRIME counterparts off-line. Dashed lines represent the accuracy achieved by training under the same setup, but generating the transformed samples during training (on-line augmentation). Validation accuracy is omitted because it is rather constant: around 93.4% for CIFAR-10 and around 87% for ImageNet-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>, R., Temme, C.R.M., Rauber, J., Sch?tt, H.H., Bethge, M., Wichmann, F.A.: Generalisation in humans and deep neural networks. In: Advances in Neural Information Processing Systems (2018) 19. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (2016) 20. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., Song, D., Steinhardt, J., Gilmer, J.: The many faces of robustness: A critical analysis of out-of-distribution generalization. In:IEEEConference on Computer Vision and Pattern Recognition (2021) 21. Hendrycks, D., Dietterich, T.: Benchmarking neural network robustness to common corruptions and perturbations. In: International Conference on Learning Representations (2019) 22. Hendrycks*, D., Mu*, N., Cubuk, E.D., Zoph, B., Gilmer, J., Lakshminarayanan, B.: Augmix: A simple method to improve robustness and uncertainty under data shift. In: International Conference on Learning Representations (2020) 23. Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., Song, D.: Natural adversarial examples. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2021) 24. Kireev, K., Andriushchenko, M., Flammarion, N.: On the effectiveness of adversarial training against common corruptions. arXiv preprint arXiv:2103.02325 (2021) 25. Krizhevsky, A.: Learning multiple layers of features from tiny images (2009) 26. Lopes, R.G., Yin, D., Poole, B., Gilmer, J., Cubuk, E.D.: Improving robustness without sacrificing accuracy with patch gaussian augmentation. arXiv preprint arXiv:1906.02611 (2019) 27. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning models resistant to adversarial attacks. In: International Conference on Learning Representations (Apr 2018) 28. Masiha, M.S., Gohari, A., Yassaee, M.H., Aref, M.R.: Learning under distribution mismatch and model misspecification. In: IEEE International Symposium on Information Theory, (ISIT) (2021) 29. Mintun, E., Kirillov, A., Xie, S.: On interaction between augmentations and corruptions in natural corruption robustness. arXiv preprint arXiv:2102.11273 (2021) 30. Moayeri, M., Feizi, S.: Sample efficient detection and classification of adversarial attacks via self-supervised embeddings. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021) 31. Morrison, K., Gilby, B., Lipchak, C., Mattioli, A., Kovashka, A.: Exploring corruption robustness: Inductive biases in vision transformers and mlp-mixers. arXiv preprint arXiv:2106.13122 (2021) 32. Nesterov, Y.E.: A method for solving the convex programming problem with convergence rate O(1/k 2 ). Dokl. Akad. Nauk SSSR (1983) 33. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.: Pytorch: An imperative style, high-performance deep learning library. In: Advances in Neural Information Processing Systems (2019) 34. Petrini, L., Favero, A., Geiger, M., Wyart, M.: Relative stability toward diffeomorphisms indicates performance in deep nets. In: Advances in Neural Information Processing Systems (2021) 35. Raghunathan, A., Xie, S.M., Yang, F., Duchi, J., Liang, P.: Understanding and mitigating the tradeoff between robustness and accuracy. In: Proceedings of the 37th International Conference on Machine Learning (Jul 2020) 36. Recht, B., Roelofs, R., Schmidt, L., Shankar, V.: Do ImageNet classifiers generalize to ImageNet? In: Proceedings of the 36th International Conference on Machine Learning (2019) 37. Rusak, E., Schott, L., Zimmermann, R.S., Bitterwolf, J., Bringmann, O., Bethge, M., Brendel, W.: A simple way to make neural networks robust against diverse image corruptions. In: Computer Vision -ECCV 2020 (2020) 38. Saikia, T., Schmid, C., Brox, T.: Improving robustness against common corruptions with frequency biased models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021) 39. Schneider, S., Rusak, E., Eck, L., Bringmann, O., Brendel, W., Bethge, M.: Improving robustness against common corruptions by covariate shift adaptation. In: Advances in Neural Information Processing Systems (2020) 40. Smith, L.N., Topin, N.: Super-convergence: Very fast training of residual networks using large learning rates. arXiv preprint arXiv:1708.07120 (2018) 41. Sun, J., Mehra, A., Kailkhura, B., Chen, P.Y., Hendrycks, D., Hamm, J., Mao, Z.M.: Certified adversarial defenses meet out-of-distribution corruptions: Benchmarking robustness and simple baselines. arXiv preprint arXiv:arXiv:2112.00659 (2021) 42. Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B., Schmidt, L.: Measuring robustness to natural distribution shifts in image classification. In: Advances in Neural Information Processing Systems (2020) 43. Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., Madry, A.: Robustness may be at odds with accuracy. In: International Conference on Learning Representations (May 2019) 44. Wang, H., Xiao, C., Kossaifi, J., Yu, Z., Anandkumar, A., Wang, Z.: Augmax: Adversarial composition of random augmentations for robust training. In: Advances in Neural Information Processing Systems (2021) 45. Xu, A., Raginsky, M.: Information-theoretic analysis of generalization capability of learning algorithms. In: Advances in Neural Information Processing Systems (2017) 46. Yi, M., Hou, L., Sun, J., Shang, L., Jiang, X., Liu, Q., Ma, Z.: Improved OOD generalization via adversarial training and pretraing. In: Proceedings of the 86th International Conference on Machine Learning (2021) 47. Yun, S., Han, D., Chun, S., Oh, S.J., Yoo, Y., Choe, J.: Cutmix: Regularization strategy to train strong classifiers with localizable features. In: 2019 IEEE/CVF International Conference on Computer Vision (2019) 48. Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk minimization. In: International Conference on Learning Representations (2018) 49. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>5 K = 7 Fig. 6 .Fig. 7 .</head><label>5767</label><figDesc>Example images (IN) generated with spectral transformations from our common corruptions model. In each row, we enlarge the transformation strength ?? from left to right. From top to bottom, we increase the spectral resolution of the filter K?. Example images (IN) generated with spatial transformations from our common corruptions model. In each row, we enlarge the transformation strength ?? from left to right. From top to bottom, we increase the cut frequency K? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Example images (IN) generated with color transformations from our common corruptions model. In each row, we enlarge the transformation strength ?? from left to right. From top to bottom, we increase K?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Example images (IN) generated with the transformations of our common corruptions model. Despite the perceptibility of the introduced distortion, the image semantics are preserved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Examples of nearest neighbours in SimCLRv2 embedding space. Columns: (first): the common corruption; (second): AugMix transformations (no mixing); (third): PRIME transformations (no mixing); (fourth): AugMix; (fifth): PRIME.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Projections of augmentations generated by different methods on the embedding space of a ResNet-50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Clean and corruption accuracy, and mean corruption error (mCE) for different methods with ResNet-18 on C-10, C-100, IN-100 and ResNet-50 on IN. mCE is the mean corruption error on common corruptions un-normalized for C-10 and C-100; normalized relative to standard model on IN-100 and IN. ? indicates that JSD consistency loss is not used.</figDesc><table><row><cell cols="3">Models taken from [9].</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Method</cell><cell cols="3">Clean Common Corruption Acc (?) Acc (?) mCE (?)</cell></row><row><cell></cell><cell>Standard</cell><cell>95.0</cell><cell>74.0</cell><cell>24.0</cell></row><row><cell>C-10</cell><cell>AugMix</cell><cell>95.2</cell><cell>88.6</cell><cell>11.4</cell></row><row><cell></cell><cell>PRIME</cell><cell>94.2</cell><cell>89.8</cell><cell>10.2</cell></row><row><cell></cell><cell>Standard</cell><cell>76.7</cell><cell>51.9</cell><cell>48.1</cell></row><row><cell>C-100</cell><cell>AugMix</cell><cell>78.2</cell><cell>64.9</cell><cell>35.1</cell></row><row><cell></cell><cell>PRIME</cell><cell>78.4</cell><cell>68.2</cell><cell>31.8</cell></row><row><cell></cell><cell>Standard</cell><cell>88.0</cell><cell>49.7</cell><cell>100.0</cell></row><row><cell></cell><cell>AugMix</cell><cell>88.7</cell><cell>60.7</cell><cell>79.1</cell></row><row><cell>IN-100</cell><cell>DA PRIME</cell><cell>86.3 85.9</cell><cell>67.7 71.6</cell><cell>68.1 61.0</cell></row><row><cell></cell><cell cols="2">DA+AugMix 86.5</cell><cell>73.1</cell><cell>57.3</cell></row><row><cell></cell><cell>DA+PRIME</cell><cell>84.9</cell><cell>74.9</cell><cell>54.6</cell></row><row><cell></cell><cell>Standard  *</cell><cell>76.1</cell><cell>38.1</cell><cell>76.7</cell></row><row><cell></cell><cell>AugMix  *</cell><cell>77.5</cell><cell>48.3</cell><cell>65.3</cell></row><row><cell></cell><cell>DA  *</cell><cell>76.7</cell><cell>52.6</cell><cell>60.4</cell></row><row><cell>IN</cell><cell>PRIME  ?</cell><cell>77.0</cell><cell>55.0</cell><cell>57.5</cell></row><row><cell></cell><cell cols="2">DA+AugMix 75.8</cell><cell>58.1</cell><cell>53.6</cell></row><row><cell></cell><cell cols="2">DA+PRIME  ? 75.5</cell><cell>59.9</cell><cell>51.3</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance of different methods in concert with domain adaptation on IN-100. Partial adaptation uses 8 samples; full adaptation uses 400 corrupted samples.</figDesc><table><row><cell>Network used: ResNet-18.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>IN-100-C acc. (?)</cell><cell>IN-100 (?)</cell></row><row><cell cols="2">Method w/o single partial full</cell><cell>single</cell></row><row><cell cols="2">Standard 49.7 53.8 62.0 63.9</cell><cell>88.1</cell></row><row><cell cols="2">AugMix 60.7 65.5 71.3 73.0</cell><cell>88.3</cell></row><row><cell>DA</cell><cell>67.7 70.2 72.7 74.6</cell><cell>86.3</cell></row><row><cell cols="2">PRIME 71.6 73.5 75.3 76.6</cell><cell>85.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Minimum cosine distances in the ResNet-50 SimCLRv2 embedding space between 100 augmented samples from 1000 ImageNet images, and their corresponding common corruptions.</figDesc><table><row><cell>Method</cell><cell cols="2">Min. cosine distance (?10 ?3 ) Avg. (?) Median (?)</cell></row><row><cell>None (clean)</cell><cell>25.38</cell><cell>6.44</cell></row><row><cell cols="2">AugMix (w/o mix) 20.57</cell><cell>3.56</cell></row><row><cell cols="2">PRIME (w/o mix) 10.61</cell><cell>1.88</cell></row><row><cell>AugMix</cell><cell>17.48</cell><cell>2.61</cell></row><row><cell>PRIME</cell><cell>7.71</cell><cell>1.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>, which justifies the improvements on noise attained by AugMix (refer Tab. 6).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">PRIME</cell><cell></cell><cell></cell></row><row><cell>clean</cell><cell>PRIME clean + spectral</cell><cell>ImageNet-100-C frost</cell><cell>clean</cell><cell cols="2">PRIME clean + spectral + spectral ImageNet-100-C fog</cell></row><row><cell cols="3">(a) clean+spectral</cell><cell cols="3">(b) clean+spectral</cell></row><row><cell></cell><cell>? frost</cell><cell></cell><cell></cell><cell cols="2">+spectral ? fog</cell></row><row><cell>clean</cell><cell>PRIME spatial + spatial</cell><cell>ImageNet-100-C glass_blur</cell><cell>clean</cell><cell>PRIME clean + color</cell><cell>ImageNet-100-C shot_noise</cell></row><row><cell cols="3">(c) spatial+spatial</cell><cell></cell><cell cols="2">(d) clean+color</cell></row><row><cell></cell><cell cols="2">? glass blur</cell><cell></cell><cell cols="2">? shot noise</cell></row><row><cell>clean</cell><cell>PRIME clean + spatial spatial</cell><cell>ImageNet-100-C elastic_transform</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(e) clean+spatial ? spatial</cell><cell cols="3">(f) spectral+color</cell></row><row><cell cols="3">? elastic transform</cell><cell></cell><cell>? snow</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">AugMix</cell><cell></cell><cell></cell></row><row><cell>clean</cell><cell>AugMix clean + trans + trans</cell><cell>ImageNet-100-C defocus_blur</cell><cell>clean</cell><cell>AugMix trans + shear shear</cell><cell>ImageNet-100-C elastic_transform</cell></row><row><cell cols="3">(g) clean+translate</cell><cell cols="3">(h) translate+shear ? shear</cell></row><row><cell cols="3">+translate ? defocus blur</cell><cell cols="3">? elastic transform</cell></row><row><cell>clean</cell><cell>AugMix shear + trans + trans</cell><cell>CIFAR-10-C motion_blur</cell><cell>clean</cell><cell>AugMix clean + solarize</cell><cell>CIFAR-10-C impulse_noise</cell></row><row><cell cols="3">(i) shear+translate+</cell><cell></cell><cell cols="2">(j) clean+solarize</cell></row><row><cell cols="3">translate ? motion blur</cell><cell></cell><cell cols="2">? impulse noise</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Percentiles of the minimum cosine distances in the ResNet-50 SimCLRv2 embedding space between 100 augmented samples from 1000 ImageNet images, and their corresponding common corruptions.</figDesc><table><row><cell>Method</cell><cell cols="2">Min. cosine distance (?10 ?3 ) (?) 5% 10% 25% 50% 75%</cell></row><row><cell>None (clean)</cell><cell>0.33 0.64 1.97 6.43</cell><cell>17.44</cell></row><row><cell cols="2">AugMix (w/o mix) 0.17 0.31 1.04 3.55</cell><cell>10.71</cell></row><row><cell cols="2">PRIME (w/o mix) 0.04 0.07 0.24 1.87</cell><cell>7.11</cell></row><row><cell>AugMix</cell><cell>0.11 0.21 0.69 2.61</cell><cell>8.37</cell></row><row><cell>PRIME</cell><cell>0.08 0.12 0.32 1.61</cell><cell>5.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Per-corruption accuracy of different methods on C-10/100 (ResNet-18). Shot Impulse Defoc. Glass Motion Zoom Snow Frost Fog Bright. Contr. Elastic Pixel. JPEG</figDesc><table><row><cell>Dataset Method Clean CC</cell><cell>Gauss.</cell><cell>Noise</cell><cell>Blur</cell><cell>Weather</cell><cell>Digital</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>Average accuracy for each corruption severity level of different methods on C-10 and C-100 (ResNet-18). Average accuracy for each corruption severity level of different methods on IN-100 (ResNet-18) and IN (ResNet-50). ? indicates that JSD consistency loss is not used. * Models taken from RobustBench<ref type="bibr" target="#b8">[9]</ref>.</figDesc><table><row><cell cols="3">Dataset Method Clean CC Avg.</cell><cell>1</cell><cell>2</cell><cell>Severity 3</cell><cell>4</cell><cell>5</cell></row><row><cell></cell><cell>Standard 95.0</cell><cell cols="5">74.0 87.4 81.7 75.7 68.3 56.7</cell></row><row><cell>C-10</cell><cell>AugMix 95.2</cell><cell cols="5">88.6 93.1 91.8 89.9 86.7 81.7</cell></row><row><cell></cell><cell>PRIME 94.2</cell><cell cols="5">89.8 92.8 91.6 90.4 88.6 85.6</cell></row><row><cell></cell><cell>Standard 76.7</cell><cell cols="5">51.9 66.7 59.4 52.8 45.0 35.4</cell></row><row><cell>C-100</cell><cell>AugMix 78.2</cell><cell cols="5">64.9 73.3 70.0 66.6 61.3 53.4</cell></row><row><cell></cell><cell>PRIME 78.4</cell><cell cols="5">68.2 74.0 71.6 69.2 65.6 60.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In practice, we will work with discrete images on a regular grid.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We provide the per-corruption performance of every method in Appendix H.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">A visualization of the augmented space using PCA can be found in Appendix G.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The official implementation of Petrini et al. diffeomorphisms can be found at https: //github.com/pcsl-epfl/diffeomorphism.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Alessandro Favero for the fruitful discussions and feedback. This work has been partially supported by the CHIST-ERA program under Swiss NSF Grant 20CH21 180444, and partially by Google via a Postdoctoral Fellowship and a GCP Research Credit Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Beale</surname></persName>
		</author>
		<title level="m">Statistical Mechanics</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Revisiting batch normalization for improving corruption robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Benz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karjauv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding robustness of Transformers for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Algorithms for nearrings of non-linear transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aichinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>N?bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mayr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Symbolic and Algebraic Computation. Association for Computing Machinery</title>
		<meeting>the International Symposium on Symbolic and Algebraic Computation. Association for Computing Machinery</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Calian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gyorgy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gowal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01086</idno>
		<title level="m">Defending against image corruptions through adversarial augmentations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Amplitude-phase recombination: Rethinking robustness of convolutional neural networks in frequency domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Wiley-Interscience</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robustbench: a standardized adversarial robustness benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sehwag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Debenedetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Flammarion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Man?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A winning hand: Compressing deep networks can improve out-of-distribution robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diffenderfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Bartoldson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaganti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kailkhura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding how image quality affects deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on Quality of Multimedia Experience (QoMEX)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Analysis of classifiers&apos; robustness to adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="481" to="508" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Shot Impulse Defoc. Glass Motion Zoom Snow Frost Fog Bright. Contr. Elastic Pixel</title>
		<imprint>
			<biblScope unit="page">100</biblScope>
		</imprint>
	</monogr>
	<note>Dataset Method Clean CC Noise Blur Weather Digital Gauss</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Method</forename><surname>Clean</surname></persName>
		</author>
		<idno>100-C IN-100-C IN-100-C SIN-100</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">BSmpl Brown Caustic Ckbd CSine ISpark Perlin Plasma SFreq Spark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Avg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Avg</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<title level="m">Table 11. Classification accuracy of different methods on IN-C, IN-C, ImageNet-R (IN-R) and Stylized IN (SIN) with ResNet-50. ? indicates that JSD consistency loss is not used. * Models taken from RobustBench</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">BSmpl Brown Caustic Ckbd CSine ISpark Perlin Plasma SFreq Spark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Avg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Avg</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Besides, the diversity of our method means that we can actually get a better performance by increasing the number of training epochs. With 1.5x training epochs, we observe about 1% accuracy refinement on each benchmark. We also perform a similar analysis with ImageNet trained models and evaluate their robustness on three other distribution shift benchmarks: (i)</title>
	</analytic>
	<monogr>
		<title level="m">1% over AugMix and 3.2% over DeepAugment</title>
		<imprint/>
	</monogr>
	<note>IN-C [29], (ii) SIN [17] as described previously and (iii) ImageNet-R (IN-R) [20</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">paintings, embroidery, etc.) of objects from the ImageNet dataset. The classification accuracy achieved by different methods on these datasets is listed in Tab. 11. On IN-C, PRIME outperforms AugMix and DeepAugment by 3.1% and 1.3% respectively. Besides, PRIME also obtains competitive results on IN-R and SIN datasets. Altogether, our empirical results indicate that the performance gains obtained by PRIME</title>
	</analytic>
	<monogr>
		<title level="m">ImageNet-R contains naturally occurring artistic renditions</title>
		<imprint/>
	</monogr>
	<note>indeed translate to other corrupted datasets</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
