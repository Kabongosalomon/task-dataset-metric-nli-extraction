<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CORDEL: A Contrastive Deep Learning Approach for Entity Linkage</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
							<email>zhengyang.wang@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bunyamin</forename><surname>Sisman</surname></persName>
							<email>bunyamis@amazon.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Xin</roleName><forename type="first">Hao</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luna</forename><surname>Dong</surname></persName>
							<email>lunadong@amazon.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Texas A&amp;M University College Station</orgName>
								<address>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CORDEL: A Contrastive Deep Learning Approach for Entity Linkage</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Entity linkage</term>
					<term>twin network</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Entity linkage (EL) is a critical problem in data cleaning and integration. In the past several decades, EL has typically been done by rule-based systems or traditional machine learning models with hand-curated features, both of which heavily depend on manual human inputs. With the ever-increasing growth of new data, deep learning (DL) based approaches have been proposed to alleviate the high cost of EL associated with the traditional models. Existing exploration of DL models for EL strictly follows the well-known twinnetwork architecture. However, we argue that the twin-network architecture is sub-optimal to EL, leading to inherent drawbacks of existing models. In order to address the drawbacks, we propose a novel and generic contrastive DL framework for EL. The proposed framework is able to capture both syntactic and semantic matching signals and pays attention to subtle but critical differences. Based on the framework, we develop a contrastive DL approach for EL, called CORDEL, with three powerful variants. We evaluate CORDEL with extensive experiments conducted on both public benchmark datasets and a real-world dataset. CORDEL outperforms previous state-of-the-art models by 5.2% on public benchmark datasets. Moreover, CORDEL yields a 2.4% improvement over the current best DL model on the real-world dataset, while reducing the number of training parameters by 97.6%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Entity linkage (EL), also known as entity matching, record linkage, entity resolution, and duplicate detection, refers to the task of determining whether two data records represent the same real-world entity. For example, in a product database, a black ink tank for printers produced by Canon can be represented as (Black ink tank, Canon) with attributes (Product title, Brand). However, there exist many other ways to build records for the same product, such as (Ink tank [black], Canon) and (Black ink tank, Canon R Ink). As a result, there might be many data records referring to the same real-world entity, needing to be cleaned and integrated.</p><p>EL has been a fundamental problem in data cleaning and integration in many domains such as e-commerce <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, and data warehouses <ref type="bibr" target="#b2">[3]</ref>. Because of its importance, it has been extensively studied for several decades <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b11">[12]</ref>. Models for EL have evolved with the development of machine learning <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b16">[17]</ref>, incorporating rule-based methods <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b20">[21]</ref> and crowd-sourcing <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>. However, because of the explosion in the volume and diversity of data, we are still far away from solving EL. Newly generated data may have different data distributions, requiring new models and thus a lot of human resources. For example, traditional machine learning models, such as support vector machines and random forests, usually require humans to hand-craft features for different data to maximize the model accuracies <ref type="bibr" target="#b24">[25]</ref>.</p><p>The success of DL approaches in various areas, such as natural language processing (NLP), computer vision, robotics, and database <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> in recent years have drawn the attention of the EL research community to a promising direction. Compared with traditional machine learning methods, DL is known to be capable of extracting task-specific features from raw data automatically through the learning process. In addition, the development of distributed representations enables DL models to process textual data directly <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b30">[31]</ref>. These properties of DL are highly desirable for EL frameworks.</p><p>Our work is not the first DL approach for EL. Existing DL methods for EL <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref> employ the twin-network architecture in <ref type="figure" target="#fig_0">Figure 1(a)</ref>, which is commonly used for other matching tasks in NLP in the literature. In NLP, the twin-network architecture is usually employed for semantic matching tasks such as question answering that require matching abstract text representations. However, semantic matching is not effective on many EL tasks. For example, in product EL tasks, the record pair (Black ink tank, Canon) and (Cyan ink tank, Canon), where the attributes are (Product title, Brand), is a non-match since they have different colors. However, the words representing different colors are semantically close to each other, making it difficult to distinguish this pair based on semantic matching. Another example is the record pair (Coca-Cola 12 fl oz 8 pack, Coca-Cola) and (Coca-Cola 12 fl oz 6 pack, Coca-Cola), where the only difference lies in the number of bottles in a pack. It is a non-match as well, even though words representing numbers have similar semantic meanings. In addition to these non-match cases, semantic matching could also fail on matches. For instance, the beer product record pair (Amber ale, Third Base Sports Bar &amp; Brewery) and (American red ale, Third Base Sports Bar &amp; Brewery) is a match. But the word 'American' in one record is not semantically similar to any word in the other record, which may confuse semantic matching models. Besides these examples, recent studies have also shown that deep neural networks work like low-pass filters and have the effect of smoothing out small differences <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Since the comparisons in the twin-network architecture is made after the records are projected onto the embedding space, small but crucial differences may be ignored, resulting in failures on EL tasks.</p><p>Because of these limitations of the twin-network architecture, existing DL models for EL do NOT show consistently improved performance over current non-DL machine learning models on various EL tasks. The fact that DL models may cause decreased performance in some cases hinders the use of these models for EL in practice.</p><p>In order to develop more effective and practical DL models for EL, we propose to jump out of the existing DL framework based on the twin-network architecture. Instead, we propose a new contrastive DL framework for EL, as shown in Figure 1(b) <ref type="bibr" target="#b0">1</ref> . In contrast to the twin-network architecture, our framework is able to capture both syntactic and semantic signals. More importantly, our framework avoids the smoothing effect of deep neural networks and pays attention to subtle but critical differences. As an instantiation of this contrastive DL framework, we build a powerful DL model called CORDEL (COntRastive Deep Entity Linkage). Our contributions can be summarized in three aspects:</p><p>? We propose a novel and generic contrastive DL framework for EL, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b). Our contrastive framework addresses the limitations of the twin-network architecture in <ref type="figure" target="#fig_0">Figure 1</ref>(a) by capturing both syntactic and semantic signals and paying attention to subtle but critical differences between entities. ? We propose a powerful DL model called CORDEL (COntRastive Deep Entity Linkage) as an instantiation of our proposed contrastive DL framework, as illustrated in <ref type="figure">Figure 2</ref>. Concretely, we develop three variants of CORDEL, named CORDEL-Sum, CORDEL-Attention, and CORDEL-Context Attention. ? We perform extensive experiments on both public benchmark datasets and a large real-world dataset. CORDEL is able to outperform previous state-of-the-art models by 5.2% on public benchmark datasets. CORDEL also yields a 2.4% improvement over the current best DL model on the real-world dataset, while reducing 97.6% training parameters. In addition, CORDEL shows great stability over different runs. These results indicate that CORDEL is a reliable, efficient, and effective DL approach for EL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we discuss the twin-network architecture and review previous DL models for EL.</p><p>The twin-network architecture in <ref type="figure" target="#fig_0">Figure 1</ref>(a) has been widely applied on matching tasks in natural language processing (NLP), such as paraphrase identification, question answering, automatic dialogue, and textual entailment <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b36">[37]</ref>. A notable property shared by these matching tasks is that they focus on semantic matching, i.e., the matching <ref type="bibr" target="#b0">1</ref> Our contrastive DL framework does not correspond to the contrastive learning in the fields of deep metric learning and self-supervised learning. The "contrastive" here refers to contrasting one input to the other in the raw string level, as explained in Sections III-B and III-C. prediction is mainly determined by the semantic relations between two textual inputs. Employing twin-networks with pre-trained distributed representations <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b30">[31]</ref> suits these tasks well. It is because distributed representations are able to model semantic meanings. For example, words with similar semantic meanings have distributed representations with small distances. On the other hand, there are other matching tasks in NLP that do not fall into this category, such as relevance matching <ref type="bibr" target="#b37">[38]</ref>, paper citation matching <ref type="bibr" target="#b38">[39]</ref>, etc. In these tasks, signals from syntactic matching are as important as those from semantic matching. In order to address these tasks, several DL matching models have been developed <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b39">[40]</ref>. Typically, these models perform local interaction among two inputs and construct a matching histogram or comparison matrix. Deep neural networks are then applied on the matching histogram or comparison matrix to make predictions. In the literature, most existing DL models for EL follow the twin-network architecture <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. DEEP-MATCHER <ref type="bibr" target="#b1">[2]</ref> proposed a general twin-network template of DL models for EL, with four different instantiations: SIF, RNN, Attention, and Hybrid. DEEPER <ref type="bibr" target="#b10">[11]</ref> shared high similarities with the SIF and RNN versions of DEEPMATCHER in terms of both the network architectures and performance. Seq2SeqMatcher <ref type="bibr" target="#b40">[41]</ref> augmented the twin-network architecture by proposed a sequence-to-sequence alignment layer, which shared certain similarities with the DEEPMATCHER-Attention. AutoEM <ref type="bibr" target="#b41">[42]</ref> explored the transfer learning settings while still employing twin-network based DL models.</p><p>We have pointed out that the twin-network architecture is not suitable to EL tasks in Section I. In addition, because of the differences between EL and other tasks, existing nontwin DL matching models in NLP <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b39">[40]</ref> could not be directly applied on EL tasks. In this work, we propose a novel and generic contrastive DL framework for EL, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b). We propose a simple yet effective instantiation of this framework, named CORDEL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section, we first formally define the problem of EL in Section III-A. Then we propose our contrastive DL framework for EL in Section III-B. As an instantiation of the framework, we introduce CORDEL, a novel DL model for EL in Section III-C. We provide different powerful variants of CORDEL in Section III-D. Finally, we analyze the advantages of CORDEL in Section III-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Definition</head><p>We focus on EL that refers to the matching task between two data records. In detail, data records are saved by following a certain schema. That is, given an ordered set of pre-defined attributes, data are stored by putting its values under corresponding attributes. For example, the product record (Black ink tank, Canon) is saved with pre-defined attributes (Product title, Brand).</p><p>Formally, given pre-defined attributes A 1 , A 2 , . . . , A m , a data record t can be represented as a tuple</p><formula xml:id="formula_0">(t[A 1 ], t[A 2 ], . . . , t[A m ]), where t[A i ], i = 1, 2, .</formula><p>. . , m refers to the value of the attribute A i in the record t. In an EL dataset, all the records should have the same schema, that is, the same set of attributes in the same order. The EL task is to determine whether a pair of records t 1 and t 2 , where t 1 = t 2 , refer to the same real-world entity. In particular, it is formulated as a binary classification problem:</p><formula xml:id="formula_1">y = F (t 1 , t 2 ) ? {0, 1},<label>(1)</label></formula><p>where F represents a model for EL that outputs a binary prediction y. In practice, it is common to let F first output a continuous number y ? [0, 1], and set a threshold to translate it into the binary classification result. The continuous output is called the matching score and can be interpreted as the likelihood of t 1 and t 2 being a match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Contrastive DL Framework</head><p>We first propose a novel and generic contrastive DL framework specially designed for EL, upon which we develop CORDEL. The framework is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>(b). We describe it component by component in this section.</p><p>Local interaction module (LIM): In order to allow syntactic signal to be captured, our contrastive DL framework avoids projecting inputs into the embedding space at the beginning. Instead, it first employs a LIM to enable the two input records to interact with each other in the raw string level. The LIM compares and contrasts the input records in terms of string tokens, where the tokens can be characters, words, and phrases. After the LIM, all the string tokens from two input records are re-grouped, where each group captures specific syntactic signals. As a result, the outputs of the LIM are simply several groups of string tokens. Our instantiation, CORDEL, explores a simple LIM that simply separates the different words from the shared words appearing in both records, as introduced in Section III-C. An illustration of our CORDEL described in Section III-C. It follows the proposed contrastive DL framework introduced in Section III-B. We provide different options for ? j , ? j , and ? in Section III-D, leading to three variants of CORDEL.</p><formula xml:id="formula_2">? Word Embeddings ? # ? # # &amp; # ( # &amp; [ # ] ( [ # ]</formula><p>Embedding: With syntactic signals captured by the LIM through grouping, distributed embeddings <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b30">[31]</ref> of string tokens allow semantic signals to be taken into consideration by the following deep neural network. Therefore, our framework has an embedding layer after the LIM, which transforms each string token into a numeric vector embedding through distributed representations. The outputs of the embedding layer are thus sequences of vector embeddings corresponding to groups of string tokens. Note that, as the syntactic signals are encoded by the grouping, they will not be lost through the embedding layer. In other words, both syntactic and semantic signals are captured in the outputs of the embedding layer.</p><p>Deep neural network: Finally, a deep neural network is applied on top of the embedding layer to process both syntactic and semantic signals and make the prediction. As the inputs are sequences of vector embeddings, the deep neural network can be decomposed into three parts: sequence processing, information aggregation, and classification. First, for each group of vector embeddings, a sequence processing module is employed to summarize the information into a fixed-size vector representation. Next, the information from different groups needs to be aggregated, and then serves as inputs to a classification module.</p><p>The proposed contrastive DL framework is the first DL framework for EL that considers both syntactic and semantic signals. In the next section, we propose a powerful DL model as an instantiation of this framework, called CORDEL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. An Instantiation-CORDEL</head><p>An illustration of the proposed CORDEL (COntRastive Deep Entity Linkage) is provided in <ref type="figure">Figure 2</ref>. Specifically, under our proposed contrastive DL framework for EL, we develop a simple yet effective LIM followed by a carefully designed deep neural network.</p><p>Local interaction module (LIM): The LIM of CORDEL is designed based on human intuition: given an input record pair, we tend to treat the differences between two records as signals for a non-match, and regard the common part as signals for a match. Therefore, our LIM simply separates the different words from the shared words appearing in both records. This results in re-clustering the tokens into three groups: two groups of unique words in either record, and one group of shared words. Specifically, the proposed LIM is achieved through simple set operations, as described below.</p><p>Formally, let t 1 and t 2 denote the input record pair, where</p><formula xml:id="formula_3">t i = (t i [A 1 ], t i [A 2 ], . . . , t i [A m ]), i = 1, 2, and each attribute value t i [A j ], i = 1, 2, j = 1, 2, . . . , m, is a sequence of words.</formula><p>Our LIM ? of CORDEL contrasts attribute-wise local tokens. For each attribute A j , j = 1, 2, . . . , m, the two sequences of words t 1 [A j ] and t 2 [A j ] are compared in terms of the exact matching between token sets. After ?, all the tokens in t 1 [A j ] and t 2 [A j ] are distributed into three groups:</p><formula xml:id="formula_4">(s[A j ], u 1 [A j ], u 2 [A j ]) = ?(t 1 [A j ], t 2 [A j ]),<label>(2)</label></formula><p>where s[A j ] contains shared words appearing in both</p><formula xml:id="formula_5">t 1 [A j ] and t 2 [A j ], and u i [A j ], i = 1, 2, includes the unique words that are only in t i [A j ].</formula><p>In other words, the comparison step ? can be written as</p><formula xml:id="formula_6">s[A j ] = t 1 [A j ] ? t 2 [A j ], u 1 [A j ] = t 1 [A j ] \ s[A j ], u 2 [A j ] = t 2 [A j ] \ s[A j ].<label>(3)</label></formula><p>Embedding: Accordingly, CORDEL employs pre-trained word embeddings to transform the outputs of ? into word embeddings. Without loss of clarity, the same notations</p><formula xml:id="formula_7">(s[A j ], u 1 [A j ], u 2 [A j ]</formula><p>) are used to denote the corresponding three sequences of word embeddings.</p><p>Deep neural network: We introduce the corresponding deep neural network in the order of sequence processing, information aggregation, and classification.</p><p>(1) Sequence processing: For each attribute A j , two sequence processing modules, ? j and ? j , are used to generate an attribute similarity representation vector sim[A j ] and an attribute difference representation vector dif</p><formula xml:id="formula_8">[A j ] from (s[A j ], u 1 [A j ], u 2 [A j ]), respectively: sim[A j ] = ? j (s[A j ]), (4) dif [A j ] = ? j (u 1 [A j ], u 2 [A j ]).<label>(5)</label></formula><p>Note that we use one sequence processing module ? j to process two groups u 1 [A j ] and u 2 [A j ] instead of two distinct ones. This is because both groups include different words, which can be viewed as one group as well. Here, the attribute similarity representation vector sim[A j ] encodes information from shared words under the attribute A j in both records, serving as evidence that supports the prediction of the input record pair as a match. On the contrary, the attribute difference representation vector dif [A j ] encodes information from different words under the attribute A j in either record, supporting the opposite prediction.</p><p>(2) Information aggregation: In order to aggregate information, CORDEL concatenates sim[A j ] and dif [A j ] as the attribute representation vector r[A j ]:</p><formula xml:id="formula_9">r[A j ] = Concat(sim[A j ], dif [A j ]).<label>(6)</label></formula><p>(3) Classification: Finally, a classification module ? takes all m attribute representation vectors as inputs and performs a binary classification task:</p><formula xml:id="formula_10">y = ?(r[A 1 ], r[A 2 ], . . . , r[A m ]) ? [0, 1],<label>(7)</label></formula><p>where y is the predicted matching score. A threshold can be set to translate the matching scores into binary classification results. The classification module ? has to merge m vectors first and makes the prediction. In DL, it is common to let ? output two numbers, use the Softmax function to normalize them, and treat one of them as the y in Eqn. (7) <ref type="bibr" target="#b24">[25]</ref>.</p><p>With the true label y * from the training dataset, CORDEL can be trained with the cross-entropy loss through backpropagation <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Variants of CORDEL</head><p>In this section, we provide variants of CORDEL by specifying ? j in Eqn. (4), ? j in Eqn. <ref type="bibr" target="#b4">(5)</ref>, and ? in Eqn. <ref type="bibr" target="#b6">(7)</ref>. In particular, ? j and ? j are each required to take in one and two variable-length sequence of word embeddings and produce a fixed-size vector, respectively. And ? has m fixed-size vectors as inputs and performs a two-way classification.</p><p>CORDEL-Sum: In order to demonstrate the effectiveness of our proposed CORDEL, we build CORDEL-Sum, an extremely simple variant of CORDEL.</p><p>CORDEL-Sum employs summation followed by a one-layer mutlilayer perceptron (MLP) for both ? j and ? j . Summation, although without any training parameters, is a powerful process in DL models for classification tasks <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b43">[44]</ref>. The one-layer MLP is used to perform dimension reduction, which avoids having an excessive number of parameters in the following classification module ?. Specifically, we have</p><formula xml:id="formula_11">sim[A j ] = ? j (s[A j ]) = ?(W ?j ? s?s[Aj ] s), dif [A j ] = ? j (u 1 [A j ], u 2 [A j ]) = ?(W ?j ? u?u1[Aj ]?u2[Aj ] u),</formula><p>where W ?j and W ?j represent corresponding one-layer MLPs, and ? refers to an activation function. The bias terms are omitted. In particular, ? j sums all the input word embeddings from both sequences of difference words. It is worth noting that the one-layer MLPs are independent for each attribute A j , leading to 2m one-layer MLPs in total.</p><p>Afterwards, ? of CORDEL-Sum is simply implemented as a concatenation of m input vectors followed by a two-layer MLP with two output units:</p><formula xml:id="formula_12">y = MLP(Concat(r[A 1 ], r[A 2 ], . . . , r[A m ])).<label>(8)</label></formula><p>CORDEL-Sum is extremely light-weight yet powerful. The training parameters only lie in 2m one-layer MLPs plus a twolayer MLP. As shown in Section IV, CORDEL-Sum achieves significantly improved performance over current non-DL and DL models. The success of CORDEL-Sum demonstrates the power of our proposed CORDEL.</p><p>Attention-based CORDEL: Despite the effectiveness of CORDEL-Sum, using summation to perform sequence processing may limit the performance in some cases, as summation gives equal importance to each word in the sequence. This contradicts with the intuition that words in s[A j ] and</p><formula xml:id="formula_13">(u 1 [A j ], u 2 [A j ]) should contribute differently to sim[A j ] and dif [A j ],</formula><p>respectively. Therefore, we explore attention-based modules for ? j , ? j , and ? to further enhance our CORDEL. The attention mechanism is able to perform a weighted summation over word embeddings, giving larger weights to more important words.</p><p>The attention mechanism has been widely used in DL models for various computer vision and NLP tasks <ref type="bibr" target="#b44">[45]</ref>- <ref type="bibr" target="#b50">[51]</ref>. In general, the attention mechanism has three parts of inputs: a query vector q ? R d1 , n key vectors that form a matrix K = [k 1 , k 2 , . . . , k n ] ? R d1?n , and n value vectors that form a matrix V = [v 1 , v 2 , . . . , v n ] ? R d2?n . Notably, the dimension of the query vector and key vectors are the same, and key vectors and value vectors have a one-to-one correspondence. The attention mechanism <ref type="bibr" target="#b45">[46]</ref> is defined as</p><formula xml:id="formula_14">o = V ? Softmax( K T ? q ? d 1 ) ? R d2 .<label>(9)</label></formula><p>In order to use the attention mechanism, we need to specify where the q, K, and V come. With different choices, we develop two attention-based variants of CORDEL, named CORDEL-Attention and CORDEL-Context Attention. They differ in ? j , while having the same ? j and ?.</p><p>We describe the shared ? j first. To simplify the notations, let s[A j ] = [x 1 , x 2 , . . . , x n ] ? R d?n denote the inputs to ? j . Note that n can be any number so that s[A j ] is a variable-length sequence of embeddings. The K and V in the attention mechanism are computed from the inputs s[A j ] through K = W k X and V = W v X, where W k ? R d1?d and W v ? R d2?d are training parameters. Meanwhile, the query vector q is simply randomly initialized and tuned during training <ref type="bibr" target="#b44">[45]</ref>.</p><p>In terms of ? j , both CORDEL-Attention and CORDEL-Context Attention follow a sub-twin architecture, that is, two attention mechanisms with shared training parameters are applied on u 1 [A j ] and u 2 [A j ], respectively. And the output of ? j is the summation of the outputs from the two attention mechanisms. Like the attention mechanism in in ? j , the attention mechanisms on u i [A j ] compute K and V from the inputs u i [A j ]. However, CORDEL-Attention and CORDEL-Context Attention have different choises on q. CORDEL-Attention employ the attention with trainable q as in ? j , while CORDEL-Context Attention uses the output of ? j as q, i.e., q = sim[A j ].</p><p>The motivation of CORDEL-Attention is straightforward. As the attention mechanism may be more powerful than summation in some cases, CORDEL-Attention uses attention mechanisms with trainable q to replace summa-tions in CORDEL-Sum. On the other hand, CORDEL-Context Attention uses sim[A j ] to guide the attention mechanisms that generate dif [A j ]. The motivation is that sim[A j ] may contain contextual information, and can be useful in determining the importance of words in u 1 [A j ] and u 2 [A j ].</p><p>For example, the model can figure out that the domain of the input records is music. Within this context, words indicating the versions of the music records, such as 'live' and 'remix', should be paid more attention to.</p><p>Both CORDEL-Attention and CORDEL-Context Attention exploit self-attention <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b47">[48]</ref> in ?. By having m query vectors, the attention mechanism is able to transform a sequence of embeddings into another sequence of embeddings with the same length <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b47">[48]</ref>. In particular,</p><formula xml:id="formula_15">let R = [r[A 1 ], r[A 2 ], . . . , r[A m ]], we have Q = W Q R, K = W k R, and V = W v R,</formula><p>where W q , W k , and W v are training parameters. Using self-attention to replace the concatenation in Eqn. <ref type="bibr" target="#b7">(8)</ref> allows explicit cross-attribute interaction, leading to improved performance in some cases, as shown in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Analysis of CORDEL</head><p>We analyze the CORDEL and demonstrate its advantages. In particular, we demonstrate how it appropriately addresses the problems of existing DL models for EL.</p><p>By taking the LIM ?, CORDEL takes syntactic signals from raw strings into consideration. Meanwhile, semantic signals are still captured through word embeddings. On one hand, ? helps CORDEL avoid mistakes caused by the fact that some semantically similar words are the key evidence for the prediction of a non-match. Taking the example of (Coca-Cola 12 fl oz 8 pack, Coca-Cola) and (Coca-Cola 12 fl oz 6 pack, Coca-Cola), the words '8' and '6' will be put into the groups of unique words in either record, and encoded by the attribute difference representation vector dif [A j ]. In the case that '8' and '6' have similar word embeddings as they are semantically close, CORDEL is still able to know that there is a numeric difference between the two input records, while the twin networks are not sensitive to such a difference. On the other hand, CORDEL is also effective in the case that semantically different but unimportant words make the model fail to identify a true match. As the final classifier takes both the attribute similarity representation vector sim[A j ] and the attribute difference representation vector dif [A j ] into consideration, CORDEL is able to determine whether the captured differences serve as important evidence for the prediction.</p><p>In addition, CORDEL is unaffected by the smoothing effect of deep neural networks. The differences are isolated from the common parts of the input record pair and processed separately. Therefore, no matter how small the differences are, CORDEL is capable of capturing them.</p><p>To summarize, unlike existing DL models for EL, CORDEL is able to identify subtle but critical differences between input records, which is a fundamental requirement for solving EL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL STUDIES</head><p>In this section, we conduct thorough experiments to evaluate our proposed CORDEL and show its superiority in the following aspects:</p><p>? On public benchmark datasets, CORDEL outperforms existing non-DL and DL models on all types of EL tasks.</p><p>In particular, CORDEL is the first DL approach with consistent and significant improvements over the non-DL approach on all three types of EL tasks. ? On a real-world dataset, CORDEL achieves better performance over existing DL models in terms of two practical evaluation metrics. In addition, CORDEL demonstrates significantly improved stability over independent training runs, which is highly desired in practice. ? CORDEL is a much more efficient DL approach in terms of required computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>We describe the models and configurations used in our experiments.</p><p>Baselines: We select non-DL and DL baselines for comparison.</p><p>? The non-DL baseline is Magellan <ref type="bibr" target="#b16">[17]</ref>, the state-of-theart machine learning based approach for EL. In particular, Magellan selects the best classifier from decision tree, random forest, Naive Bayes, support vector machine and logistic regression. The features used in Magellan are designed by experts. ? The DL baseline is DEEPMATCHER <ref type="bibr" target="#b1">[2]</ref>, which represents a wide range of twin-network based DL models for EL. DEEPMATCHER has four versions, named SIF, RNN, Attention, and Hybrid, with increasing complexity. DEEPER <ref type="bibr" target="#b10">[11]</ref> and Seq2SeqMatcher <ref type="bibr" target="#b40">[41]</ref> can be regarded as extensions of DEEPMATCHER. DEEPMATCHER has been made publicly available as a Python package. CORDEL: We evaluate CORDEL-Sum, CORDEL-Attention, and CORDEL-Context Attention in our experiments. The details are provided below.</p><p>Word Embeddings: For fair comparison, the distributed representations used to transform words into word embeddings are 300-dimensional pretrained FastText embeddings <ref type="bibr" target="#b30">[31]</ref>, which is the same as DEEPMATCHER <ref type="bibr" target="#b1">[2]</ref>. The embeddings are not fine-tuned during training.</p><p>Training: CORDEL is trained through the Adam optimizer <ref type="bibr" target="#b51">[52]</ref> with a learning rate of 0.0001. The training batch size is set to 64 for public datasets and 256 for the real-world dataset.</p><p>CORDEL-Sum: As described in Section III-D, the training parameters of CORDEL-Sum only lie in 2m one-layer MLPs plus a two-layer MLP, where m is the number of attributes in the dataset. The output dimension is set to 64 for the 2m one-layer MLPs. The dimension of the hidden layer in the two-layer MLP is set to 256.</p><p>CORDEL-Attention &amp; CORDEL-Context Attention: As introduced in Section III-D, we only need to specify the dimension of training parameters in the attention mechanism, i.e., d, d 1 and d 2 . In particular, d depends on the dimension of word embeddings so that is 300 as indicated above. In the attention mechanism with a trainable query vector q, we d 1 to 4, a Real-World Amazon-Wikipedia music ?0.4M ?0.2M 10 small number to prevent over-fitting. In the context-attention and self-attention modules, d 1 is set to 64. In all cases, d 2 is set to 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets</head><p>Experiments are performed on public benchmark datasets and a real-world dataset. Various evaluation metrics are used.</p><p>Public Benchmark Datasets: We conduct experiments on the public datasets provided by <ref type="bibr" target="#b1">[2]</ref>. These public datasets cover a wide range of EL tasks in different domains. In particular, they represent three types of EL tasks.</p><p>? Structured EL: In a structured EL dataset, the records in a pair have relatively clean and aligned attribute values. In addition, the number of tokens in an attribute value is usually limited. ? Textual EL: As indicated by the name, a textual EL dataset has long textual data as attribute values. ? Dirty EL: A dirty EL dataset differs from a structured EL dataset in the aspect that the attribute values may be mistakenly disposed. The value of one attribute could appear as part of the value of another attribute.</p><p>In total, there are 7 structured, 1 textual, and 4 dirty EL datasets. The statistics of these datasets are provided in <ref type="table" target="#tab_0">Table I</ref>. Following <ref type="bibr" target="#b1">[2]</ref>, we divide each dataset into training, validation, and evaluation splits with the ratio of 3:1:1.</p><p>In the experiments on these public datasets, we follow [2] to employ the F 1 score as the evaluation metric, which allows the direct comparison between our proposed CORDEL and baselines. Note that, according to Eqn. <ref type="bibr" target="#b6">(7)</ref>, the output of CORDEL is a matching score y ? [0, 1]. A threshold has to be set to transform the matching score into a binary classification result. As with <ref type="bibr" target="#b1">[2]</ref>, we set the threshold to 0.5 to compute F 1 .</p><p>It is easy and beneficial for research purpose to classify current public benchmark datasets for EL tasks <ref type="bibr" target="#b1">[2]</ref> according to such categorization. However, real-world EL datasets may be a mixture of the three types. Therefore, a general approach for EL that is able to achieve good performance consistently on any type of EL task is highly desired in practice.</p><p>Real-world Dataset: We collect a real-world EL dataset in the music domain. Specifically, music records are crawled and sampled from Amazon and Wikipedia <ref type="bibr" target="#b52">[53]</ref>. That is, in a record pair t 1 and t 2 from this dataset, t 1 is from Amazon and t 2 is from Wikipedia. We have 10 attributes describing basic information about the music track records. In order to obtain the training dataset, we sample 0.4 million record pairs involving 822,276 distinct entities and employ a noisy strong key to label them. Meanwhile, the testing dataset contains record pairs that are manually labelled by human annotators, ensuring that the evaluation is accurate.</p><p>We adopt more comprehensive and practical evaluation metrics for experiments on this real-world dataset: Area Under the Precision-Recall Curve (PRAUC) and Recall when Precision=95% (R@P=95%). The F 1 score evaluates the model when a chosen threshold. In contrast, PRAUC summarizes the model performance with all thresholds. In addition, as most EL datasets are imbalanced, PRAUC is known to be more suitable for evaluating binary classifiers on imbalanced datasets <ref type="bibr" target="#b53">[54]</ref>. R@P=95% is a practical evaluation metric for EL. Data integration typically has the requirement for high precision. That is because a low-precision approach for EL would result in wrongly merges records, causing unrecoverable data loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results on Public Datasets</head><p>We compare CORDEL with baselines on three types of public EL datasets separately. The results of baselines are provided by <ref type="bibr" target="#b1">[2]</ref>.</p><p>Structured EL: Results on the 7 structured EL datasets are reported in <ref type="table" target="#tab_0">Table II</ref>. All versions of CORDEL improve the performance by a large margin in terms of the average F 1 score. Notably, CORDEL-Sum achieves the state-of-theart performance on 5 out of 7 datasets. On DBLP-Scholar 1 , CORDEL-Sum is the second best model while the best model DEEPMATCHER-Hybrid has 32x more parameters, as shown in Section IV-D1. On Walmart-Amazon 1 , CORDEL-Sum outperforms all versions of the DL baseline. In addition, CORDEL-Attention achieves the best result on Walmart-Amazon 1 , being the only DL model that beats the non-DL baseline.</p><p>CORDEL-Sum yields a 5.2% improvement over the previous state-of-the-art model in terms of the average F 1 scores. While existing DL models can only achieve competitive performance with non-DL models, CORDEL is the first DL approach that demonstrates the advantages of DL on structured EL tasks.</p><p>Textual EL: <ref type="table" target="#tab_0">Table III</ref> shows the results on the textual EL dataset Abt-Buy. It is a valid concern that the local string comparison step ? breaks the long textual attribute values, such as sentences and paragraphs, which might harm the performance of CORDEL on textual EL tasks. However, experimental results indicate that our proposed CORDEL remains powerful. Moreover, CORDEL-Attention sets the new state-of-the-art record, increasing the best F 1 score by 3.3%.</p><p>Dirty EL: <ref type="table" target="#tab_0">Table IV</ref> provides the results on the 4 dirty EL datasets. The advantage of using DL models for dirty EL tasks is inherited by CORDEL. While only obtaining the best results on 1 out of 4 datasets by CORDEL-Context Attention, CORDEL achieves the best average F 1 score. Particularly, CORDEL-Context Attention improves the best average F 1 score by 2.6%. It indicates that CORDEL is more robust to different datasets.</p><p>To conclude, CORDEL is the first DL approach that yields consistently and significantly improved performance on various datasets for different types of EL tasks, serving as a general DL approach for EL. 1) Case Studies: We perform case studies to show why CORDEL achieves better performance. Specifically, we examine examples in the testing dataset, where CORDEL makes the correct prediction but DEEPMATCHER fails. <ref type="figure">Figure 3</ref> provides two representative examples from Walmart-Amazon 1 and BeerAdvo-RateBeer, respectively. Both of them are nonmatches, with subtle but critical differences. However, DEEP-MATCHER identifies them as matches, indicating its inability to capture those subtle but critical differences between input records. On the contrary, as discussed in Section III-E, CORDEL has an outstanding ability to handle such cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results on the Real-world Dataset</head><p>To further demonstrate the advantages of CORDEL over the DL baseline, we perform experiments on a real-world EL dataset, which casts more challenges compared to the public (a) Twin-Network (DEEPMATCHER-Hybrid) (b) CORDEL-Sum <ref type="figure">Fig. 4</ref>.</p><p>The precision-recall curves for DEEPMATCHER-Hybrid and CORDEL-Sum, with 10 independent runs for each of them. CORDEL-Sum is more stable with better performance especially for the high precision band. benchmark datasets. It is hard to classify a real-world dataset into one of the three types of EL task, since it is usually a mixture of them. In addition, a practical DL approach for EL needs to be stable, i.e., different training runs should lead to similar inference performance. This stability is crucial to make DL models reliable.</p><p>In order to evaluate the stability, we repeat each experiment for 10 times independently and report the mean and standard deviation over 10 runs. For the baseline DEEPMATCHER, we choose two versions, the simplest DEEPMATCHER-SIF and the most powerful DEEPMATCHER-Hybrid.</p><p>The comparisons between CORDEL and DEEPMATCHER are summarized in <ref type="table" target="#tab_3">Table V</ref>. CORDEL has better and more stable performance in terms of both PRAUC and R@P=95%. Formally, we conduct an unequal variance t-test on the PRAUC results between CORDEL-Attention and DEEPMATCHER-Hybrid. The p-value is 0.0069, indicating the improvement is statistically significant.</p><p>In order to show the superiority of CORDEL more directly, <ref type="figure">Figure 4</ref> plots the precision-recall curves for DEEPMATCHER-Hybrid and CORDEL-Sum, with 10 independent runs for each of them. The instability of DEEPMATCHER can be easily observed. In addition, it is worth noting that, particularly, CORDEL has a much better and stable performance in the high-precision area.</p><p>1) Efficiency Analysis: Another practical challenge in applying DL models on real-world EL tasks is the concern of efficiency. In particular, DL models tend to have a considerably large amount of training parameters, requiring large computational resources to train and deploy. We compare the number of training parameters between CORDEL and DEEPMATCHER in the last column of Table V. We can see that even the simplest DEEPMATCHER-SIF has more parameters than CORDEL, while CORDEL yields much better performance as shown in the experiments above. In addition, the existing state-of-the-art DL approach, DEEPMATCHER-Hybrid, has millions of training parameters, preventing it from being applied on large-scale datasets. On the contrary, CORDEL is a light-weight and efficient DL approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this work, we propose a novel contrastive DL approach for EL, called CORDEL. We point out the limitations of current twin-network DL models and motivate our work. We perform extensive experiments on both public benchmark datasets and a large real-world dataset for rigorous evaluations. The experimental results show the effectiveness of CORDEL with significant and consistent improvements in performance. Moreover, CORDEL is more efficient as a light-weight DL approach, and more reliable with stable performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Two types of DL architectures for matching tasks. (a) The twin architecture employed by existing DL models for EL. The problems of applying the twin-network architecture on EL are analyzed in Section I. (b) Our proposed contrastive DL framework for EL followed by CORDEL. Details are described in Sections III-B and III-C. The advantages of our framework are discussed in Section III-E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>For</head><label></label><figDesc>Fig. 2. An illustration of our CORDEL described in Section III-C. It follows the proposed contrastive DL framework introduced in Section III-B. We provide different options for ? j , ? j , and ? in Section III-D, leading to three variants of CORDEL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I STATISTICS</head><label>I</label><figDesc>OF PUBLIC BENCHMARK DATASETS PROVIDED BY [2] AND OUR REAL-WORLD MUSIC DATASET. THE RIGHTMOST THREE COLUMNS, i.e. #PAIRS, #MATCHES, #ATTRS, CORRESPOND TO THE NUMBER OF RECORD PAIRS, MATCHES, ATTRIBUTES IN THE DATASET, RESPECTIVELY.</figDesc><table><row><cell>Type</cell><cell>Dataset</cell><cell>Domain</cell><cell>#Pairs</cell><cell>#Matches</cell><cell>#Attrs</cell></row><row><cell></cell><cell>BeerAdvo-RateBeer</cell><cell>beer</cell><cell>450</cell><cell>68</cell><cell>4</cell></row><row><cell></cell><cell>iTunes-Amazon1</cell><cell>music</cell><cell>539</cell><cell>132</cell><cell>8</cell></row><row><cell></cell><cell>Fodors-Zagats</cell><cell>restaurant</cell><cell>946</cell><cell>110</cell><cell>6</cell></row><row><cell>Structured</cell><cell>DBLP-ACM1</cell><cell>citation</cell><cell>12,363</cell><cell>2,220</cell><cell>4</cell></row><row><cell></cell><cell>DBLP-Scholar1</cell><cell>citation</cell><cell>28,707</cell><cell>5,347</cell><cell>4</cell></row><row><cell></cell><cell>Amazon-Google</cell><cell>software</cell><cell>11,460</cell><cell>1,167</cell><cell>3</cell></row><row><cell></cell><cell>Walmart-Amazon1</cell><cell>electronics</cell><cell>10,242</cell><cell>962</cell><cell>5</cell></row><row><cell>Textual</cell><cell>Abt-Buy</cell><cell>product</cell><cell>9,575</cell><cell>1,028</cell><cell>3</cell></row><row><cell></cell><cell>iTunes-Amazon2</cell><cell>music</cell><cell>539</cell><cell>132</cell><cell>8</cell></row><row><cell>Dirty</cell><cell>DBLP-ACM2 DBLP-Scholar2</cell><cell>citation citation</cell><cell>12,363 28,707</cell><cell>2,220 5,347</cell><cell>4 4</cell></row><row><cell></cell><cell>Walmart-Amazon2</cell><cell>electronics</cell><cell>10,242</cell><cell>962</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISONS</head><label>II</label><figDesc>BETWEEN CORDEL AND BASELINES ON STRUCTURED EL DATASETS FROM [2] IN TERMS OF THE F 1 SCORE. "C ATTENTION" IS SHORT FOR "CONTEXT ATTENTION". THE BEST PERFORMANCE IS HIGHLIGHTED WITH BOLDFACE. IF CORDEL ACHIEVES THE BEST PERFORMANCE, WE MARK THE BEST RESULTS OBTAINED BY BASELINES WITH UNDERLINES, AND VICE VERSA. IN PARTICULAR, WHEN CORDEL SETS THE NEW STATE-OF-THE-ART RECORD, THE RELATIVE IMPROVEMENT RATE AGAINST THE PREVIOUS BEST PERFORMANCE IS COMPUTED. CORDEL AND BASELINES ON TEXTUAL EL DATASETS FROM [2] IN TERMS OF THE F 1 SCORE. CORDEL AND BASELINES ON DIRTY EL DATASETS FROM [2] IN TERMS OF THE F 1 SCORE.</figDesc><table><row><cell></cell><cell>Magellan [17]</cell><cell></cell><cell cols="2">DEEPMATCHER [2]</cell><cell></cell><cell></cell><cell>CORDEL (Ours)</cell><cell></cell></row><row><cell>Dataset</cell><cell></cell><cell>SIF</cell><cell>RNN</cell><cell>Attention</cell><cell>Hybrid</cell><cell>Sum</cell><cell>Attention</cell><cell>C Attention</cell></row><row><cell>BeerAdvo-RateBeer</cell><cell>78.8</cell><cell>58.1</cell><cell>72.2</cell><cell>64.0</cell><cell>72.7</cell><cell>88.9 ? 12.8%</cell><cell>85.7</cell><cell>86.7</cell></row><row><cell>iTunes-Amazon 1</cell><cell>91.2</cell><cell>81.4</cell><cell>88.5</cell><cell>80.8</cell><cell>88.0</cell><cell>100.0 ? 9.6%</cell><cell>96.3</cell><cell>94.5</cell></row><row><cell>Fodors-Zagats</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>82.1</cell><cell>100.0</cell><cell>100.0 ? 0.0%</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>DBLP-ACM 1</cell><cell>98.4</cell><cell>97.5</cell><cell>98.3</cell><cell>98.4</cell><cell>98.4</cell><cell>99.2 ? 0.8%</cell><cell>98.9</cell><cell>98.8</cell></row><row><cell>DBLP-Scholar 1</cell><cell>92.3</cell><cell>90.9</cell><cell>93.0</cell><cell>93.3</cell><cell>94.7</cell><cell>94.0</cell><cell>93.4</cell><cell>93.5</cell></row><row><cell>Amazon-Google</cell><cell>49.1</cell><cell>60.6</cell><cell>59.9</cell><cell>61.1</cell><cell>69.3</cell><cell>70.2 ? 1.3%</cell><cell>68.8</cell><cell>68.1</cell></row><row><cell>Walmart-Amazon 1</cell><cell>71.9</cell><cell>65.1</cell><cell>67.6</cell><cell>50.0</cell><cell>66.9</cell><cell>68.7</cell><cell>72.7 ? 1.1%</cell><cell>70.9</cell></row><row><cell>Average F 1</cell><cell>83.1</cell><cell>79.1</cell><cell>82.8</cell><cell>75.7</cell><cell>84.3</cell><cell>88.7 ? 5.2%</cell><cell>88.0</cell><cell>87.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE III</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">COMPARISONS BETWEEN Magellan [17]</cell><cell></cell><cell cols="2">DEEPMATCHER [2]</cell><cell></cell><cell></cell><cell>CORDEL (Ours)</cell><cell></cell></row><row><cell>Dataset</cell><cell></cell><cell>SIF</cell><cell>RNN</cell><cell>Attention</cell><cell>Hybrid</cell><cell>Sum</cell><cell>Attention</cell><cell>C Attention</cell></row><row><cell>Abt-Buy</cell><cell>43.6</cell><cell>35.1</cell><cell>39.4</cell><cell>56.8</cell><cell>62.8</cell><cell>58.2</cell><cell>64.9 ? 3.3%</cell><cell>61.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE IV</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">COMPARISONS BETWEEN Magellan [17]</cell><cell></cell><cell cols="2">DEEPMATCHER [2]</cell><cell></cell><cell></cell><cell>CORDEL (Ours)</cell><cell></cell></row><row><cell>Dataset</cell><cell></cell><cell>SIF</cell><cell>RNN</cell><cell>Attention</cell><cell>Hybrid</cell><cell>Sum</cell><cell>Attention</cell><cell>C Attention</cell></row><row><cell>iTunes-Amazon 2</cell><cell>46.8</cell><cell>66.7</cell><cell>79.4</cell><cell>63.6</cell><cell>74.5</cell><cell>82.1</cell><cell>78.0</cell><cell>82.4 ? 3.8%</cell></row><row><cell>DBLP-ACM 2</cell><cell>91.9</cell><cell>93.7</cell><cell>97.5</cell><cell>97.4</cell><cell>98.1</cell><cell>97.0</cell><cell>96.3</cell><cell>96.8</cell></row><row><cell>DBLP-Scholar 2</cell><cell>82.5</cell><cell>87.0</cell><cell>93.0</cell><cell>92.7</cell><cell>93.8</cell><cell>91.9</cell><cell>89.0</cell><cell>89.9</cell></row><row><cell>Walmart-Amazon 2</cell><cell>37.4</cell><cell>43.2</cell><cell>39.6</cell><cell>53.8</cell><cell>46.0</cell><cell>48.3</cell><cell>50.1</cell><cell>51.2</cell></row><row><cell>Average F 1</cell><cell>64.7</cell><cell>77.4</cell><cell>76.9</cell><cell>75.7</cell><cell>78.1</cell><cell>79.8</cell><cell>78.4</cell><cell>80.1 ? 2.6%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Fig. 3. Case studies on public benchmark datasets. The top example is from Walmart-Amazon 1 , the bottom example is from BeerAdvo-RateBeer. Both of them are non-matches, with subtle but critical differences. CORDEL makes the correct prediction in both cases, while DEEPMATCHER fails.</figDesc><table><row><cell></cell><cell>t1</cell><cell>t2</cell></row><row><cell>Title</cell><cell>canon cli-226 black ink tank</cell><cell>canon cli-226 cyan ink tank 4547b001</cell></row><row><cell>Category</cell><cell>printers</cell><cell>inkjet printer ink</cell></row><row><cell>Brand</cell><cell>canon</cell><cell>canon</cell></row><row><cell>Model Number</cell><cell>4546b001</cell><cell>4547b001</cell></row><row><cell>Price</cell><cell>13.97</cell><cell>11.99</cell></row><row><cell></cell><cell>t1</cell><cell>t2</cell></row><row><cell>Beer Name</cell><cell>Green Lakes Organic Ale</cell><cell>Deschutes Green Lakes Non-Organic Ale</cell></row><row><cell>Brew Factory Name</cell><cell>Deschutes Brewery</cell><cell>Deschutes Brewery</cell></row><row><cell>Style</cell><cell>American Amber / Red Ale</cell><cell>Amber Ale</cell></row><row><cell>ABV</cell><cell>5.20%</cell><cell>6.40%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V COMPARISONS</head><label>V</label><figDesc>BETWEEN CORDEL AND BASELINES ON A REAL-WORLD DATASET IN TERMS OF AREA UNDER THE PRECISION-RECALL CURVE (PRAUC), RECALL WHEN PRECISION=95% (R@P=95%), AND THE NUMBER OF TRAINING PARAMETERS IN TOTAL (#PARAMS). THE RELATIVE IMPROVEMENT RATES AGAINST THE PREVIOUS BEST MODEL, DEEPMATCHER-HYBRID, ARE COMPUTED.</figDesc><table><row><cell>Model</cell><cell>PRAUC</cell><cell>R@P=95%</cell><cell>#Params</cell></row><row><cell>DEEPMATCHER-SIF</cell><cell>88.1 ? 2.9</cell><cell>43.5 ? 17.0</cell><cell>728,802</cell></row><row><cell>DEEPMATCHER-Hybrid</cell><cell>90.5 ? 1.9</cell><cell>52.7 ? 25.1</cell><cell>22,151,812</cell></row><row><cell>CORDEL-Sum</cell><cell>91.6 ? 0.3</cell><cell>68.2 ? 2.4 ? 29.4%</cell><cell>713,730</cell></row><row><cell>CORDEL-Attention</cell><cell>92.7 ? 0.3 ? 2.4%</cell><cell>67.8 ? 1.3</cell><cell>522,850 ? 97.6%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Christos Faloutsos, Andrew Borthwick, Yifan Ethan Xu, and Jialong Han for valuable suggestions..</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Corleone: hands-off crowdsourcing for entity matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rampalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shavlik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM International Conference on Management of Data</title>
		<meeting>the 2014 ACM International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="601" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning for entity matching: A design space exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mudgal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rekatsinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arcaute</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Raghavendra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM International Conference on Management of Data</title>
		<meeting>the 2018 ACM International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data quality in data warehouses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Data Warehousing and Mining</title>
		<imprint>
			<publisher>IGI Global</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="550" to="555" />
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Record linkage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Public Health and the Nations Health</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1412" to="1416" />
			<date type="published" when="1946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A theory for record linkage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">P</forename><surname>Fellegi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Sunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">328</biblScope>
			<biblScope unit="page" from="1183" to="1210" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Duplicate record detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Verykios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An introduction to duplicate detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Herschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Data Management</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="87" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Data matching: concepts and techniques for record linkage, entity resolution, and duplicate detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Christen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Entity resolution: theory, practice &amp; open challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Machanavajjhala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="2018" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Privacy preserving record linkage with ppjoin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sehili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kolb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>BTW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of tuples for entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ebraheem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thirumuruganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ouzzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1454" to="1467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Linknbed: Multi-graph representation learning with entity linkage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="252" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to match and cluster large high-dimensional data sets for data integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 8th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="475" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interactive deduplication using active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhamidipaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 8th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="269" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive duplicate detection using learnable string similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 9th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Entity resolution with markov logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Data Mining</title>
		<meeting>the 6th International Conference on Data Mining</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="572" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Magellan: Toward building entity matching management systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ardalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Panahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naughton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1197" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Entity matching: How similar is similar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="622" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reasoning about record matching rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="407" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Synthesizing entity matching rules by examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Meduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Papotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-A</forename><surname>Quian?-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="189" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generating concise entity matching rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Meduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Papotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-A</forename><surname>Quian?-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM International Conference on Management of Data</title>
		<meeting>the 2017 ACM International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1635" to="1638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Crowder: crowdsourcing entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1483" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Data curation at scale: The data tamer system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">F</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Beskales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cherniack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Zdonik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cidr</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Crowdsourcing algorithms for entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vesdapunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bellare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1071" to="1082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Database meets deep learning: Challenges and opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="17" to="22" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep d-bar: Real-time electrical impedance tomography imaging with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2367" to="2377" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 22nd ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modeling interestingness with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning semantic representations using convolutional neural networks for web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on World Wide Web</title>
		<meeting>the 23rd International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="373" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Text matching as image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence. Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence. Association for the Advancement of Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A deep architecture for matching short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1367" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep sequence-to-sequence entity matching for heterogeneous entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="629" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Auto-em: End-to-end fuzzy entity-matching using pre-trained deep models and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2413" to="2424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Non-local U-Nets for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th AAAI Conference on Artificial Intelligence. Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<meeting>the 34th AAAI Conference on Artificial Intelligence. Association for the Advancement of Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00075</idno>
		<title level="m">icapsnets: Towards interpretable capsule networks for text classification</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Global pixel transformers for virtual staining of microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2256" to="2266" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Collective multi-type entity alignment between knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference. Association for Computing Machinery</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The precision-recall plot is more informative than the roc plot when evaluating binary classifiers on imbalanced datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rehmsmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
