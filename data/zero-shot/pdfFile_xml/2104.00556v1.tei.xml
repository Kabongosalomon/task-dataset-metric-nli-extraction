<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Two-View Structure-from-Motion Revisited</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<addrLine>3 NVIDIA</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolai</forename><surname>Smolyanskiy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Two-View Structure-from-Motion Revisited</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Two-view structure-from-motion (SfM) is the cornerstone of 3D reconstruction and visual SLAM. Existing deep learning-based approaches formulate the problem by either recovering absolute pose scales from two consecutive frames or predicting a depth map from a single image, both of which are ill-posed problems. In contrast, we propose to revisit the problem of deep two-view SfM by leveraging the well-posedness of the classic pipeline. Our method consists of 1) an optical flow estimation network that predicts dense correspondences between two frames; 2) a normalized pose estimation module that computes relative camera poses from the 2D optical flow correspondences, and 3) a scale-invariant depth estimation network that leverages epipolar geometry to reduce the search space, refine the dense correspondences, and estimate relative depth maps. Extensive experiments show that our method outperforms all state-of-the-art two-view SfM methods by a clear margin on KITTI depth, KITTI VO, MVS, Scenes11, and SUN3D datasets in both relative pose and depth estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Two-view structure-from-motion (SfM) is the problem of estimating the camera motion and scene geometry from two image frames of a monocular sequence. As the foundation of both 3D reconstruction and visual simultaneous localization and mapping (vSLAM), this important problem finds its way into a wide range of applications, including autonomous driving, augmented/virtual reality, and robotics.</p><p>Classic approaches to two-view SfM follow a standard pipeline of first matching features/edges between the two images, then inferring motion and geometry from those matches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52]</ref>. When imaging conditions are well-behaved (constant lighting, diffuse and rigid surfaces, and non-repeating visual texture), the match-* indicates equal contribution, listed in alphabetical order. Yiran is the corresponding author. Work was partially done when Yiran was an intern at NVIDIA, Redmond, WA.</p><p>ing process is well-posed. And, once the matches have been found, the motion and geometry can be recovered.</p><p>For decades, researchers who work in this area have generally required at least two views, and their methods have recovered only relative camera motion and relative scene geometry (that is, shape up to an unknown scale factor). Without a priori knowledge of scale or recognizable objects in the scene, it is impossible to recover scene geometry from a single view <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b24">25]</ref>. Similarly, it is impossible to infer absolute scale from two views of a scene <ref type="bibr" target="#b16">[17]</ref>.</p><p>With the rise of deep learning, a number of researchers have recently explored neural network-based solutions to two-view SfM. Most of these methods fall into one of two categories. In the first category, which we shall call Type I, the problem is treated as a joint optimization task of monocular depth and pose regression <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b31">32]</ref>. Two networks are used: one to estimate the up-to-scale depth from a single image, and the other one to predict the up-to-scale camera pose from two input images. Both networks act independently during inference. In the second category, denoted Type II, the scaled camera pose and the scaled depth are inferred from the image pair, and are iteratively refined via multi-view geometry <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. While the power of deep learning allows both Type I and Type II solutions to achieve compelling results, we note that their formulations attempt to solve one of the ill-posed <ref type="bibr" target="#b2">[3]</ref> problems mentioned above.</p><p>In this paper, we revisit the use of deep learning for twoview SfM. Our framework follows the classic SfM pipeline that features are matched between image frames to yield relative camera poses, from which relative depths are then estimated. By combining the strengths of deep learning within a classic pipeline, we are able to avoid ill-posedness, which allows our approach to achieve state-of-the-art results on several benchmarks.</p><p>A comparison between our approach and existing pipelines is shown in <ref type="figure">Fig. 1</ref>. Our method operates by first estimating dense matching points between two frames using a deep optical flow network <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b39">40]</ref>, from which a set of highly reliable matches are sampled in order to compute the relative camera pose via a GPU-accelerated classic five-point algorithm <ref type="bibr" target="#b23">[24]</ref> with RANSAC <ref type="bibr" target="#b12">[13]</ref>. Since these  <ref type="figure">Figure 1</ref>. Comparison between our method and previous deep monocular structure-from-motion methods. We formulate camera pose estimation as a 2D matching problem (optical flow) and depth prediction as a 1D matching problem along an epipolar line. In contrast, previous methods suffer from ill-posedness (either single-frame depth prediction, in the case of Type I, or scaled estimates, in the case of Type II).</p><p>relative camera poses have scale ambiguity, the estimated depth suffers from scale ambiguity as well. Therefore, in order to supervise the estimated scale-ambiguous depth with the (scaled) ground truth depth, we propose a scaleinvariant depth estimation network combined with scalespecific losses to estimate the final relative depth maps.</p><p>Since the search space of the depth estimation network is reduced to epipolar lines thanks to the camera poses, it yields higher accuracy than directly triangulating the optical flows with the estimated camera poses. We demonstrate the effectiveness of our framework by achieving state-of-the-art accuracy in both pose and depth estimation on KITTI depth, KITTI VO, MVS, Scenes11, and SUN3D datasets.</p><p>Our main contributions are summarized as: 1) We revisit the use of deep learning in SfM, and propose a new deep two-view SfM framework that avoids illposedness. Our framework combines the best of deep learning and classical geometry. 2) We propose a scale-invariant depth estimation module to handle the mismatched scales between ground truth depth and the estimated depth. 3) Our method outperforms all previous methods on various benchmarks for both relative pose estimation and depth estimation under the two-view SfM setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Two-view Geometry: Review and Analysis</head><p>The task of two-view SfM refers to estimating the relative camera poses and dense depth maps from two consecutive monocular frames. In classic geometric vision, it is well understood that the camera poses as well as the depth maps can be computed from image matching points alone without any other information <ref type="bibr" target="#b25">[26]</ref>. <ref type="bibr" target="#b0">1</ref> Given a set of image matching points in homogeneous coordinates, x i = x i y i 1 and x i = x i y i 1 with known camera intrinsic matrix K, the two-view SfM task is to find a camera rotation matrix R and a translation vector t as well as the corresponding 3D homogeneous point X i such that:</p><formula xml:id="formula_0">x i = K I | 0 X i x i = K R | t X i ?i.<label>(1)</label></formula><p>A classical method to solve this problem consists of three consecutive steps: 1) Computing the essential matrix E from the image matching points x i and x i ; 2) Extracting the relative camera pose R and t from the essential matrix E; 3) Triangulating the matching points x i and x i with the camera pose to get the 3D point X i .</p><p>All steps in this pipeline are well-posed problems. The essential matrix E can be solved with at least 5 matching points using the equation below:</p><formula xml:id="formula_1">x i K ? EK ?1 x i = 0 ?i.<label>(2)</label></formula><p>R and t can be computed from E using matrix decomposition such that E = SR, where S is a skew symmetric matrix and R is a rotation matrix. Since for any non-zero scaling factor ?, ?t ? R = ? t ? R = ?E provides a valid solution, there is a scale ambiguity for relative camera pose estimation. The 3D point X i can be computed by triangulation with a global scale ambiguity. The method above assumes the ideal case in which all image points are perfectly matched. To handle mismatched points in real scenarios, researchers have established a classical standard pipeline to estimate geometry information from two consecutive frames <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The Classic Standard Pipeline</head><p>With decades of development and refinement, the classic standard pipeline <ref type="bibr" target="#b16">[17]</ref> is widely used in many conventional state-of-the-art SfM and vSLAM systems <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b0">1]</ref>. Since almost all geometry information can be recovered from image matching points, the key is to recover a set of (sparse or dense) accurate matching points. To this end, the pipeline often starts with sparse (or semi-dense) distinct feature extraction and matching to get sparse matching points, as sparse matching is more accurate than dense matching. To further refine the matching results, the RANSAC scheme <ref type="bibr" target="#b12">[13]</ref> is used to filter the matching points that do not fit the majority motion. These outliers often include mismatches and dynamic objects in a scene. After retrieving the camera poses from the refined matching points, the depth of these points can be computed via triangulation. In some cases, if it is desired to estimate dense depth maps rather than the sparse 3D points, multi-view stereo matching algorithms can be used to recover the dense depth maps with the estimated camera poses.</p><p>The Achilles' heel of this pipeline is therefore the matching of points. Conventional matching algorithms often suffer from low accuracy on non-Lambertian, blurry, and textureless surfaces. However, this shortage can be largely alleviated by deep learning <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23]</ref>. With sufficient training data, such networks can learn to handle these scenarios. In our proposed approach, we leverage a deep optical flow network <ref type="bibr" target="#b39">[40]</ref> to compute these correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep Learning based Methods</head><p>As discussed earlier, two-view SfM requires to estimate both camera poses and dense depth maps. Existing deep learning based methods either formulate the problem as pose and monocular depth regression (Type I) or as pose regression and multi-view stereo matching (Type II). We analyze both types of methods below.</p><p>Type I methods consist of a monocular depth estimation network and a pose regression network. The two-view geometry constraints are used as self-supervisory signals to regularize both camera poses and depth maps <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b31">32]</ref>. As a result, most of these approaches are selfsupervised. Because single-view depth estimation is inherently ill-posed, as discussed earlier, these methods are fun-damentally limited by how well they can solve that challenging problem. They rely on the priors from the training data to predict depth only given a single image.</p><p>Moreover, since the two-view geometry constraints are only suitable for a stationary scene, SfMLearner <ref type="bibr" target="#b52">[53]</ref> simultaneously estimates an explainability mask to exclude the dynamic objects while GeoNet <ref type="bibr" target="#b45">[46]</ref> utilizes an optical flow module to mask out these outliers by comparing the rigid flow (computed by camera poses and depth maps) with the non-rigid flow (computed by the optical flow module). Other methods focus on implementing more robust loss functions, such as ICP loss <ref type="bibr" target="#b27">[28]</ref>, motion segmentation loss <ref type="bibr" target="#b31">[32]</ref>, or epipolar loss <ref type="bibr" target="#b4">[5]</ref>.</p><p>Type II methods require two image frames to estimate depth maps and camera poses at test time (unlike Type I methods, which estimate depth from a single frame). Most supervised deep methods fall into this category. As a pioneer of this type, DeMoN <ref type="bibr" target="#b38">[39]</ref> concatenates a pair of frames and uses multiple stacked encoder-decoder networks to regress camera poses and depth maps, implicitly utilizing multi-view geometry.</p><p>Similar strategies have been adapted by <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b35">36]</ref> through replacing generic layers between camera poses and depth maps with optimization layers that explicitly enforce multi-view geometry constraints. For example, BANet <ref type="bibr" target="#b34">[35]</ref> parameterizes dense depth maps with a set of depth bases <ref type="bibr" target="#b48">[49]</ref> and imposes bundle adjustment as a differentiable layer into the network architecture. Wang et al. <ref type="bibr" target="#b40">[41]</ref> use regressed camera poses to constrain the search space of optical flow, estimating dense depth maps via triangulation. DeepV2D <ref type="bibr" target="#b35">[36]</ref> separates the camera pose and depth estimation, iteratively updating them by minimizing geometric reprojection errors. Similarly, DeepSFM <ref type="bibr" target="#b41">[42]</ref> initiates its pose estimation from DeMoN <ref type="bibr" target="#b38">[39]</ref>, sampling nearby pose hypotheses to bundle adjust both poses and depth estimation. Nevertheless, with the ground truth depth as supervision, it requires the pose regression module to estimate camera poses with absolute scale, which is generally impossible from a pair or a sequence of monocular frames alone <ref type="bibr" target="#b16">[17]</ref>. To mitigate this ill-posed problem, they utilize dataset priors and semantic knowledge of the scene to estimate the absolute scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we propose a new deep two-view SfM framework that aims to address the Achilles' heel of the classical SfM pipeline (viz., matching) via deep learning. Our method is able to find better matching points and therefore more accurate poses and depth maps, especially for textureless and occluded areas. At the same time, it follows the wisdom of classic methods to avoid the ill-posed problems. By combining the best of both worlds, our approach is able to achieve state-of-the-art results, outperforming all previ-ous methods by a clear margin.</p><p>Following the classic standard pipeline <ref type="bibr" target="#b16">[17]</ref>, we formulate the two-frame structure-from-motion problem as a three-step process: 1) match corresponding points between the frames, 2) estimate the essential matrix and hence the relative camera pose, and 3) estimate dense depth maps up to an unknown scale factor. These steps, along with the loss function used for training, are described in more detail in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Optical Flow Estimation</head><p>As a fundamental problem in computer vision, optical flow estimation has been extensively studied for several decades <ref type="bibr" target="#b19">[20]</ref>. With the recent progress in deep learning, deep optical flow methods now dominate various benchmarks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4]</ref> and can handle large displacements as well as textureless, occluded, and non-Lambertian surfaces. In our framework, we utilize the state-of-the-art network, DICL-Flow <ref type="bibr" target="#b39">[40]</ref>, to generate dense matching points between two consecutive frames. This method uses a displacementinvariant matching cost learning strategy and a soft-argmin projection layer to ensure that the network learns dense matching points rather than image-flow regression. The network was trained on synthetic datasets (FlyingChairs <ref type="bibr" target="#b8">[9]</ref> and FlyingThings <ref type="bibr" target="#b28">[29]</ref>) to avoid data leakage, i.e., the network was not trained on any of the test datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Essential Matrix Estimation</head><p>The traditional approach to estimating camera pose between two image frames is to match sparse points, e.g., SIFT features <ref type="bibr" target="#b26">[27]</ref>. Then, given a set of matching points</p><p>x ? x and the camera intrinsic matrix K, the essential matrix <ref type="bibr" target="#b25">[26]</ref> E can be recovered from the five-point algorithm <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b23">24]</ref>. By decomposing the essential matrix as E = [t] ? R, the rotation matrix R and the translation vector t can be recovered up to a scale ambiguity. Conventionally, outliers in the matching points are filtered using robust fitting techniques such as RANSAC <ref type="bibr" target="#b12">[13]</ref>. RANSAC repeatedly estimates the essential matrix from randomly sampled minimal matching sets and selects the solution that is satisfied by the largest proportion of matching points under a certain criterion.</p><p>Unlike all previous deep learning-based methods that regress the camera poses from input images, we use matching points to compute the camera poses. The key question is this: How to robustly filter the noisy dense matches from optical flow in order to retain only the high quality matches? There are multiple ways to filter out unreliable matching points such as flow uncertainty, consistency check, or using a network to regress a mask. Empirically, we find that simply using SIFT keypoint locations (note that we do not use SIFT matching) to generate a mask works well in all datasets. The hypothesis is that optical flow is more ac-curate in rich textured areas. The optical flow matches at the locations within the mask are filtered by RANSAC with GPU acceleration, to avoid distraction by dynamic objects. After retrieving the essential matrix E, the camera pose (R, t) is recovered using matrix decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scale-Invariant Depth Estimation</head><p>Once we have recovered the up-to-scale relative camera pose, with the dense matching points from optical flow estimation, we could compute the dense depth map by performing triangulation. However, such an approach would not take advantage of the epipolar constraint. As a result, we perform the matching again by constraining the search space to epipolar lines computed from the relative camera poses. This process is similar to multi-view stereo (MVS) matching with one important difference: we do not have the absolute scale in inference. With the up-to-scale relative pose, if we were to directly supervise the depth estimation network with ground truth depth, there would be a mismatch between the scale of the camera motion and the scale of the depth map.</p><p>Previous approaches. To resolve this paradox, previous methods either use a scale-invariant loss <ref type="bibr" target="#b9">[10]</ref> or regress the absolute scale with a deep network <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>The scale-invariant loss SI is defined as:</p><formula xml:id="formula_2">SI = x log(d x ) ? log(d x ) + ?(d,d) 2 ,<label>(3)</label></formula><p>where d x andd x are the ground truth and estimated depth, respectively, at pixel x; and</p><formula xml:id="formula_3">?(d,d) = 1 N x log(d x ) ? log(d x )</formula><p>, where N is the number of pixels, measures the mean log difference between the two depth maps. While working for the direct depth regression pipelines, the scale-invariant loss introduces an ambiguity for network learning as the network could output depth maps with different scales for each sample. This loss may hinder the principle of plane-sweep, where depth maps with consistent scale across frames of the sequence are desired. Plane sweep <ref type="bibr" target="#b20">[21]</ref> is the process that enforces the epipolar constraint, which reduces the search space from 2D to 1D.</p><p>Plane-sweep powered networks require consistent scale during the training and testing process. For example, if we train a network with absolute scales and test it with a normalized scale, its performance will drop significantly (we provide an ablation study in Section 4.4). Since it is impossible to recover the absolute scale from two images, some previous methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref> use a network to regress a scale to mimic the absolute scale in inference. This strategy slightly alleviates the scale paradox at the cost of making the problem ill-posed again.</p><p>Scale-Invariant Matching. To solve this paradox and keep the problem well-posed, we propose a scale-invariant matching process to recover the up-to-scale dense depth map. Mathematically, given an image point x, we generate L matching candidates {x l } L l=1 :</p><formula xml:id="formula_4">x l ? K[R | t] (K ?1 x)d l 1 ,<label>(4)</label></formula><p>where d l = (L ? d min )/l, (l = 1, ..., L) is the depth hypothesis and d min is a fixed minimum depth.</p><p>In the standard plane-sweep setting, the sampling distribution of matching candidates varies depending on the scale factor ? = t 2 , as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. Instead, we normalize the translation vectors to t ? t/? such that t 2 = 1, since we do not know the absolute scale in our problem. Substituting the normalized translation t for t in Eq. (4), with fixed {d l } L l=1 , the distribution of matching candidates {x l } L l=1 are now invariant to scale. To make the estimated and ground truth depths compatible, according to Eq. (4), we need to scale the estimated depthd correspondingly to match the ground truth depth d:</p><formula xml:id="formula_5">d ? ? gtd ,<label>(5)</label></formula><p>where ? gt refers to the ground truth scale. This scale-invariant matching strategy plays a crucial role in our framework as it makes our network no longer suffer from the scale misalignment problem. Please note our competitors cannot benefit from scale-invariant matching because they usually avoid the scale misalignment problem by predicting absolute scales. A detailed discussion is provided in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>Our framework is trained in an end-to-end manner with the supervision of ground truth depth maps and ground truth scales. Given a predicted depthd and a ground truth depth d, we supervise the depth using the Huber loss:</p><formula xml:id="formula_6">L depth = x huber ? gtdx ? d x ,<label>(6)</label></formula><p>where huber (z) = 0.5z 2 if |z| &lt; 1, |z ? 0.5| otherwise. It should be noted that our predicted depth is up-to-scale and does not require ground truth scale at inference time.</p><p>If both ground truth camera pose (R, t) and ground truth depth d x are given, we can also update the optical flow network by computing the rigid flow u x ? x ? x for 2D point x:</p><formula xml:id="formula_7">x ? K[R | t] (K ?1 x)d x 1 .<label>(7)</label></formula><p>The rigid flow can work as a supervision signal, computing the 2 distance with the estimated optical flow? x :</p><formula xml:id="formula_8">L flow = x (? x ? u x ) 2 .<label>(8)</label></formula><p>The total loss function of our framework is then given by:</p><formula xml:id="formula_9">L total = L depth + ?L flow .<label>(9)</label></formula><p>We set ? = 1 to fine-tune the optical flow estimator, or ? = 0 to use the flow model pretrained on synthetic datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we provide quantitative and qualitative results of our framework on various datasets, showing comparison with state-of-the-art SfM methods. We also provide an extensive ablation study to justify our framework design. Due to the scale-ambiguity nature of the two-view SfM problem, we scale the results of ours and others using the same scaling strategy as in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35]</ref>. For all experiments, our optical flow estimator is <ref type="bibr" target="#b39">[40]</ref> while the architecture of depth estimator is based on <ref type="bibr" target="#b20">[21]</ref>. Implementation details (such as hyperparameters of the optimizer or network) are provided in the supplementary material. <ref type="bibr" target="#b15">[16]</ref> is primarily designed for monocular depth evaluation in autonomous driving scenarios, which does not take camera motions and dynamic objects into account. The Eigen split <ref type="bibr" target="#b9">[10]</ref>, which contains 697 single frames for testing, is a widely used split for evaluating monocular depth estimation. To adapt it for two-view SfM evaluation, we pair nearby frames. Also, since the Eigen split contains a number of frames with nearly static camera motion or many moving objects (which lead to ill-posed situations for two-view SfM), we filter out these frames to create an Eigen SfM split (256 frames) to evaluate SfM algorithms in well-conditioned scenarios. Specifically, we first pair each frame with its next frame then manually remove these pairs with small relative translations (less than 0.5 meters) or contain large dynamic objects 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets KITTI Depth</head><p>KITTI VO <ref type="bibr" target="#b15">[16]</ref> is primarily used for evaluating camera pose estimation. It contains ten sequences (more than 20k frames) with ground truth camera poses. According to the setting of <ref type="bibr" target="#b52">[53]</ref>, we test our pose estimation accuracy on all 2700 frames of the "09" and "10" sequences, using consecutive frames from the left camera. MVS, Scenes11, and SUN3D. MVS is collected from several outdoor datasets by <ref type="bibr" target="#b38">[39]</ref>. Different from KITTI which is built through video sequences with close scenes, MVS has outdoor scenes from various sources. Scenes11 <ref type="bibr" target="#b38">[39]</ref> is a synthetic dataset generated by random shapes and motions. It is therefore annotated with perfect depth and pose, though the images are not realistic. SUN3D <ref type="bibr" target="#b42">[43]</ref> provides indoor images with noisy depth and pose annotation. We use the SUN3D dataset post-processed by <ref type="bibr" target="#b38">[39]</ref>, which discards the samples with a high photoconsistency error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Depth Evaluation</head><p>We perform depth evaluation on KITTI Depth, MVS, Scenes11, and SUN3D datasets. KITTI Depth. We compare our framework with both types of deep SfM methods using seven commonly used depth metrics <ref type="bibr" target="#b9">[10]</ref>. We also leverage one disparity metric D1-all 3 as it measures the precision of the depth estimation. Since the Type I methods are self-supervised and they all perform single frame depth estimation in inference, we report the results of the state-of-the-art supervised single image depth estimation methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b44">45]</ref> as they can be viewed as the upper bounds of Type I methods.</p><p>Quantitative results are shown in <ref type="table" target="#tab_1">Table 1</ref>. Although only using a flow estimator trained on synthetic datasets, our method beats all previous methods with a clear margin on various metrics, e.g., 2.273 versus 2.727 in RMSE. Especially, our method largely outperforms DeepV2D although <ref type="bibr" target="#b2">3</ref> Percentage of stereo disparity outliers. We convert the estimated depth to disparities using the focal length and baseline provided by KITTI.</p><p>DeepV2D used ground truth camera pose and five-frame sequences for training. Note that there is a number of frames in the Eigen split that do not strictly satisfy the rigid SfM assumption such as stationary scene. When only keeping the frames that satisfy SfM assumptions, i.e., on the Eigen SfM split, our method achieves even better accuracy, with 3.1% vs 9.1% in D1-all. <ref type="figure" target="#fig_1">Fig. 3</ref> illustrates some qualitative results compared with the state-of-the-art supervised single image method <ref type="bibr" target="#b14">[15]</ref> and deep SfM method <ref type="bibr" target="#b35">[36]</ref>.</p><p>MVS, Scenes11, and SUN3D. We compare our framework to state-of-the-art Type II methods under two-view SfM setting using metrics by <ref type="bibr" target="#b38">[39]</ref>. We use the same strategy of iterative depth refinement as <ref type="bibr" target="#b41">[42]</ref> in inference for a fair comparison. As shown in <ref type="table" target="#tab_2">Table 2</ref>, our method achieves superior performance on all metrics among all three datasets comparing with the previous state-of-the-art Type II methods. <ref type="figure" target="#fig_2">Fig. 4</ref> provides some qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Camera Pose Estimation</head><p>We compare the camera pose estimation accuracy with Type I and Type II SfM methods on the KITTI VO, MVS, Scenes11, and SUN3D datasets.</p><p>KITTI VO. We measure the pose estimation accuracy on relative translational error t err and relative rotational error r err as in <ref type="bibr" target="#b53">[54]</ref>. For all results, we align the predicted trajectories to the ground truth via least square optimization <ref type="bibr" target="#b37">[38]</ref>. Our method achieves the best pose estimation accuracy with a clear margin compared with the Type I SfM methods <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b31">32]</ref>, and full-sequence visual odometry approach <ref type="bibr" target="#b53">[54]</ref>. In <ref type="figure">Fig. 5</ref> we visualize the full sequence odometry trajectories on 9 th and 10 th sequences. Our results are more aligned with the ground truth trajectories. It is worth noting that our model are only trained on sythetic datasets while the other methods are fine-tuned on the KITTI VO   dataset and take more frames to estimate the camera poses.</p><p>MVS, Scenes11 and SUN3D. The competitors use ground truth poses to train their pose estimation module on these three datasets, while we use the ground truth poses to fine-tune our optical flow model using Eq. <ref type="bibr" target="#b7">(8)</ref>. We also report the pose estimation accuracy in <ref type="table" target="#tab_2">Table 2</ref>, using the metrics of DeMoN <ref type="bibr" target="#b38">[39]</ref>. Our method beats the previous state-of-the-art on all three datasets with a clear margin, e.g., 60.8% better in translation estimation on MVS dataset and 31.5% better in rotation estimation on Scenes11 dataset. Moreover, we verify the effectiveness of rigid flow supervision, Eq (8), in <ref type="table" target="#tab_4">Table 4</ref>. With fine-tuning, the translation errors are largely suppressed, and the rotation errors are notably reduced. It is worth noting that our model that was trained on synthetic datasets has already achieved comparable performance with previous methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Framework Analysis and Justification</head><p>Estimating Camera Pose from Optical Flow. There are multiple ways to extract camera pose from optical flow. We consider two kinds of methods: deep regression and the classic five-point algorithm <ref type="bibr" target="#b23">[24]</ref> with RANSAC scheme. For deep regression methods, we build a PoseNet similar to the one used in <ref type="bibr" target="#b40">[41]</ref> with ResNet50 <ref type="bibr" target="#b18">[19]</ref> as the feature back- bone, using image pairs and optical flow as the input. For the five-point algorithm, we use flow matching pairs as the input. We also set a baseline by using SIFT matches. To filter out error matches and outliers, we compare different masking strategies, such as flow uncertainty maps (output of per-pixel softmax operation), learned confidence maps, and SIFT feature locations.</p><p>We evaluate these methods on the MVS dataset, see <ref type="table">Table 5</ref>. Deep regression methods have almost constant performance regardless of different inputs and masking strategies. The best option is to use flow matches with masks based on SIFT feature locations. <ref type="bibr" target="#b3">4</ref> Dealing With Misaligned Scales. It is impossible to perfectly recover absolute scales from two-view images. This scale-ambiguity problem will cause trouble if we would like to directly use ground truth depth for supervision or through the widely used scale-invariant loss <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b38">39]</ref>. We verify the effect of the proposed scale-invariant depth estimation module on the KITTI depth Eigen split. The baseline follows our pipeline but without scale-invariant depth module. It simply uses huber loss on the estimated depth and the ground depth regardless their scales, which forces the network to implicitly learn a scale. As shown in <ref type="table" target="#tab_6">Table 6</ref>, our scale-invariant depth module achieves a very similar accuracy to 'Oracle Pose', which is the upper bound of our method. On the other hand, the performance of the scaleinvariant loss is similar to the baseline method, which indicates that this loss cannot handle the scale problem.</p><p>Scale-invariant Matching on Other Frameworks. The scale-invariant matching is specifically designed for our pipeline to handle the scale ambiguity in depth estimation. Previous deep SfM methods like DeepV2D do not suffer from this problem as they force networks to regress camera poses with scales and then make the depth scaled. That means, these methods cannot benefit from the scaleinvariant matching. As a proof, we apply our scale-invariant matching to DeepV2D and test it on the KITTI Eigen dataset. The performance gain is minor: Abs Rel from 0.064 to 0.063 and RMSE from 2.946 to 2.938. Our superior performance benefits from the whole newly proposed deep SfM pipeline rather than a single component. Since all components are tightly coupled in our pipeline, replacing either of them will result in a severe performance drop. <ref type="table">Table 5</ref>. Estimating Camera Pose from Optical Flow. We compare different methods to estimate camera pose from optical flow on the MVS dataset. 'CNN' represents the pose regression network based on convolutional neural networks with ground truth pose supervision. '5-point' represents the five-point algorithm with RANSAC scheme. We also compare different flow masking strategies here.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have revisited the problem of deep neural network based two-view SfM. First, we argued that existing deep learning-based SfM approaches formulate depth estimation or pose estimation as ill-posed problems. Then we proposed a new deep two-view SfM framework that follows the classic well-posed SfM pipeline. Extensive experiments show that our proposed method outperforms all state-of-the-art methods in both pose and depth estimation with a clear margin. In the future, we plan to extend our framework to other SfM problems such as three-view SfM and multi-view SfM, where the loop consistency and temporal consistency could further constrain these already wellposed problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The Effect of Various Scale Factors during Plane Sweep. For a certain pixel, we visualize its six depth hypotheses with different colors in the target frame. As the scale factor ? changes, the sampling distribution varies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative Results on the KITTI Dataset. The yellow circles and boxes in the top row highlight tiny poles which are captured more accurately by our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative Examples on MVS, Scenes11, and SUN3D Datasets, where our method consistently achieves better results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Depth Evaluation on KITTI Depth Dataset. We compare our results to state-of-the-art single-frame depth estimation methods and deep SfM methods on the KITTI depth Eigen split. We evaluate all SfM methods under two-view SfM setting for a fair comparison. The "Eigen SfM" split (256 frames) excludes frames that are close to static or contain many dynamic objects in the Eigen split. The type S means supervised single frame depth estimation. Note that Type I methods are self-supervised methods. Bold indicates the best. Abs Rel Sq Rel RMSE RMSE log D1-all ? &lt; 1.25 ? &lt; 1.25 2 ? &lt; 1.25 3</figDesc><table><row><cell cols="2">Split Type</cell><cell>Method</cell><cell></cell><cell></cell><cell>lower is better</cell><cell></cell><cell></cell><cell></cell><cell>higher is better</cell><cell></cell></row><row><cell></cell><cell>S</cell><cell>DORN [15] VNL [45]</cell><cell>0.072 0.072</cell><cell>0.307 -</cell><cell>2.727 3.258</cell><cell>0.120 0.117</cell><cell>0.163 0.176</cell><cell>0.932 0.938</cell><cell>0.984 0.990</cell><cell>0.994 0.998</cell></row><row><cell></cell><cell></cell><cell>SfMLearner [53]</cell><cell>0.208</cell><cell>1.768</cell><cell>6.856</cell><cell>0.283</cell><cell>-</cell><cell>0.678</cell><cell>0.885</cell><cell>0.957</cell></row><row><cell></cell><cell></cell><cell>GeoNet [46]</cell><cell>0.155</cell><cell>1.296</cell><cell>5.857</cell><cell>0.233</cell><cell>-</cell><cell>0.793</cell><cell>0.931</cell><cell>0.973</cell></row><row><cell>Eigen</cell><cell>I II</cell><cell>CCNet [32] GLNet [5] BANet [35] DeepV2D [36]</cell><cell>0.140 0.099 0.083 0.064</cell><cell>1.070 0.796 -0.350</cell><cell>5.326 4.743 3.640 2.946</cell><cell>0.217 0.186 0.134 0.120</cell><cell>---0.142</cell><cell>0.826 0.884 -0.946</cell><cell>0.941 0.955 -0.982</cell><cell>0.975 0.979 -0.991</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell>0.055</cell><cell>0.224</cell><cell>2.273</cell><cell>0.091</cell><cell>0.107</cell><cell>0.956</cell><cell>0.984</cell><cell>0.993</cell></row><row><cell>Eigen SfM</cell><cell>S II</cell><cell>DORN [15] VNL [45] DeepV2D[36] Ours</cell><cell>0.067 0.065 0.050 0.034</cell><cell>0.295 0.297 0.212 0.103</cell><cell>2.929 3.172 2.483 1.919</cell><cell>0.108 0.106 0.089 0.057</cell><cell>0.130 0.168 0.091 0.031</cell><cell>0.949 0.945 0.973 0.989</cell><cell>0.988 0.989 0.992 0.998</cell><cell>0.995 0.997 0.997 0.999</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Depth and Pose Estimation Results on MVS, Scenes11, and SUN3D Datasets. Base-SIFT and Base-Matlab come from [39]. Sc-inv L1-rel Rot Tran L1-inv Sc-inv L1-rel Rot Tran L1-inv Sc-inv L1-rel Rot Tran Base-SIFT 0.056 0.309 0.361 21.180 60.516 0.051 0.900 1.027 6.179 56.650 0.029 0.290 0.286 7.702 41.825</figDesc><table><row><cell></cell><cell></cell><cell cols="3">MVS Dataset</cell><cell></cell><cell cols="3">Scenes11 Dataset</cell><cell></cell><cell cols="3">Sun3D Dataset</cell></row><row><cell>Method</cell><cell></cell><cell>Depth</cell><cell></cell><cell>Pose</cell><cell></cell><cell>Depth</cell><cell></cell><cell>Pose</cell><cell></cell><cell>Depth</cell><cell></cell><cell>Pose</cell></row><row><cell cols="2">L1-inv Base-Matlab -</cell><cell>-</cell><cell>-</cell><cell>10.843 32.736</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.917 14.639</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5.920 32.298</cell></row><row><cell>COLMAP [33]</cell><cell>-</cell><cell>-</cell><cell cols="2">0.384 7.961 23.469</cell><cell>-</cell><cell>-</cell><cell cols="2">0.625 4.834 10.682</cell><cell>-</cell><cell>-</cell><cell cols="2">0.623 4.235 15.956</cell></row><row><cell cols="13">DeMoN [39] 0.047 0.202 0.305 5.156 14.447 0.019 0.315 0.248 0.809 8.918 0.019 0.114 0.172 1.801 18.811</cell></row><row><cell>LS-Net [7]</cell><cell cols="12">0.051 0.221 0.311 4.653 11.221 0.010 0.410 0.210 4.653 8.210 0.015 0.189 0.650 1.521 14.347</cell></row><row><cell>BANet [35]</cell><cell cols="12">0.030 0.150 0.080 3.499 11.238 0.080 0.210 0.130 3.499 10.370 0.015 0.110 0.060 1.729 13.260</cell></row><row><cell cols="13">DeepSFM [42] 0.021 0.129 0.079 2.824 9.881 0.007 0.112 0.064 0.403 5.828 0.013 0.093 0.072 1.704 13.107</cell></row><row><cell>Ours</cell><cell cols="12">0.015 0.102 0.068 2.417 3.878 0.005 0.097 0.058 0.276 2.041 0.010 0.081 0.057 1.391 10.757</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Pose Estimation Accuracy on KITTI VO dataset. Bold indicates the best. For pose estimation, our method uses an optical flow model trained on synthetic data. The result of GANVO [2] is provided by its author. Visual Trajectory on the KITTI VO dataset. We compare our method against other deep learning based SfM methods on Seq.09 (Left) and Seq.10 (Right) of KITTI VO dataset.</figDesc><table><row><cell>Method</cell><cell cols="4">Seq. 09 terr(%) rerr( ? /100m) terr(%) rerr( ? /100m) Seq. 10</cell></row><row><cell cols="2">SfMLearner [53] 8.28</cell><cell>3.07</cell><cell>12.20</cell><cell>2.96</cell></row><row><cell>GANVO [2]</cell><cell>11.52</cell><cell>3.53</cell><cell>11.60</cell><cell>5.17</cell></row><row><cell>CCNet [32]</cell><cell>6.92</cell><cell>1.77</cell><cell>7.97</cell><cell>3.11</cell></row><row><cell>LTMVO [54]</cell><cell>3.49</cell><cell>1.03</cell><cell>5.81</cell><cell>1.82</cell></row><row><cell>Ours</cell><cell>1.70</cell><cell>0.48</cell><cell>1.49</cell><cell>0.55</cell></row><row><cell>Figure 5.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>The Effect of Optical Flow Fine-tuning. With the help of rigid flow supervision, our fine-tuned model achieves a much better camera pose result than the model trained on synthetic data. .637 10.984 0.587 6.617 1.67012.905   Our-finetune 2.417 3.878 0.276 2.041 1.391 10.757</figDesc><table><row><cell>Model</cell><cell>MVS Rot Tran</cell><cell>Scenes11 Rot Tran Rot SUN3D Tran</cell></row><row><cell cols="2">Our-synthetic 3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Dealing with Misaligned Scales. We compare different strategies to handle the misaligned scales between the estimated depth and ground truth depth on the KITTI Eigen split. 'Scale Inv Matching' indicates the scale invariant matching for plane sweeping, 'Scale Inv Loss' represents the scale invariant depth loss. The 'Oracle' means using the ground truth for both training and inference. Using ground truth pose for training achieves a worse result than the baseline, which verifies the scaling problem.</figDesc><table><row><cell>Strategy</cell><cell cols="4">Abs Rel Sq Rel RMSE RMSE log</cell></row><row><cell>Baseline</cell><cell>0.089</cell><cell>0.318</cell><cell>3.120</cell><cell>0.129</cell></row><row><cell>GT Pose Training</cell><cell>0.121</cell><cell>0.438</cell><cell>3.421</cell><cell>0.175</cell></row><row><cell>Scale Inv Loss</cell><cell>0.084</cell><cell>0.302</cell><cell>2.981</cell><cell>0.116</cell></row><row><cell>Scale Inv Matching</cell><cell>0.055</cell><cell>0.224</cell><cell>2.273</cell><cell>0.091</cell></row><row><cell>Oracle Scale</cell><cell>0.053</cell><cell>0.216</cell><cell>2.271</cell><cell>0.089</cell></row><row><cell>Oracle Pose</cell><cell>0.052</cell><cell>0.212</cell><cell>2.269</cell><cell>0.088</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Excluding degenerate cases.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We define a dynamic object which occupies more than 20% pixels of a scene as a large dynamic object.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that we use SIFT feature detection to obtain state-of-the-art results, whereas SIFT feature matching performs poorly and is not used.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements Yuchao Dai was supported in part by National Natural Science Foundation of China (61871325) and National Key Research and Development Program of China (2018AAA0102803). Hongdong Li was supported in part by ACRV (CE140100016), ARC-Discovery (DP 190102261), and ARC-LIEF (190100080) grants. We would like to thank Shihao Jiang, Dylan Campbell, Charles Loop for helpful discussions and Ke Chen for providing field test images from NVIDIA AV cars.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building rome in a day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="105" to="112" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ganvo: Unsupervised deep monocular visual odometry and depth estimation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Almalioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhamad</forename><surname>Risqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Saputra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro Pb De</forename><surname>Gusmao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ill-posed problems in early vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="869" to="889" />
			<date type="published" when="1988-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical neural architecture search for deep stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Ls-net: Learning to solve nonlinear least squares for monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Czarnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Davison</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02966</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Molton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Stasse</surname></persName>
		</author>
		<title level="m">MonoSLAM: Real-time single camera SLAM. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1052" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Direct sparse odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02565</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">LSD-SLAM: Largescale direct monocular SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sch?ps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SVO: Fast semi-direct monocular visual odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matia</forename><surname>Pizzoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Example based 3D reconstruction from single 2D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dpsnet: End-to-end deep plane sweep stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghoon</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae-Gon</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parallel tracking and mapping for small AR workspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMAR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arvo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04260</idno>
		<title level="m">Learning all-range volumetric correspondence for video deblurring</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Five-point motion estimation made easy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Pattern Recognition (ICPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="630" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Single-view object shape reconstruction using deep shape prior and silhouette</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A computer algorithm for reconstructing a scene from two projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Christopher Longuet-</forename><surname>Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">293</biblScope>
			<biblScope unit="issue">5828</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh IEEE international conference on computer vision</title>
		<meeting>the seventh IEEE international conference on computer vision</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ORB-SLAM: A versatile and accurate monocular SLAM system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose Maria Martinez</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">An efficient solution to the five-point relative pose problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nist?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="756" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Structurefrom-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ba-net: Dense bundle adjustment network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deepv2d: Video to depth with differentiable structure from motion. International Conference on Learning Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12039,2020.3</idno>
		<title level="m">Raft: Recurrent allpairs field transforms for optical flow</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Least-squares estimation of transformation parameters between two point patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Umeyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="376" to="380" />
			<date type="published" when="1991" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5038" to="5047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Displacement-invariant matching cost learning for accurate optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Flow-motion and depth network for monocular stereo and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deepsfm: Structure from motion via deep bundle adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingkui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sun3d: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1625" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pix2Vox: Context-aware 3D reconstruction from single and multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5684" to="5693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Selfsupervised learning for stereo matching with self-improving ability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00930</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Stereo computation for a single mixture image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors</editor>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="441" to="456" />
			<date type="published" when="2018" />
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Efficient depth completion using learned bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.01110,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised deep epipolar flow for stationary or dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12095" to="12104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Open-world stereo video matching with deep rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<idno>Septem- ber 2018. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Displacementinvariant cost computation for efficient stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Loop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonmin</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kamenev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00899</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning monocular visual odometry via self-supervised long-term modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc-Huy</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
