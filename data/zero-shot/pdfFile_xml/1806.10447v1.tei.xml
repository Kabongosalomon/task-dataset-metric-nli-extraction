<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LPRNet: License Plate Recognition via Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zherzdev Ex-Intel</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Gruzdev</surname></persName>
							<email>alexey.gruzdev@intel.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">IOTG Computer Vision Group</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Intel IOTG Computer Vision Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LPRNet: License Plate Recognition via Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes LPRNet -end-to-end method for Automatic License Plate Recognition without preliminary character segmentation. Our approach is inspired by recent breakthroughs in Deep Neural Networks, and works in real-time with recognition accuracy up to 95% for Chinese license plates: 3 ms/plate on nVIDIA R GeForce TM GTX 1080 and 1.3 ms/plate on Intel R Core TM i7-6700K CPU.</p><p>LPRNet consists of the lightweight Convolutional Neural Network, so it can be trained in end-to-end way. To the best of our knowledge, LPRNet is the first real-time License Plate Recognition system that does not use RNNs. As a result, the LPRNet algorithm may be used to create embedded solutions for LPR that feature high level accuracy even on challenging Chinese license plates.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatic License Plate Recognition is a challenging and important task which is used in traffic management, digital security surveillance, vehicle recognition, parking management of big cities. This task is a complex problem due to many factors which include but are not limited to: blurry images, poor lighting conditions, variability of license plates numbers (including special characters e.g. logograms for China, Japan), physical impact (deformations), weather conditions (see some examples in <ref type="figure" target="#fig_0">Fig. 1</ref>).</p><p>The robust Automatic License Plate Recognition system needs to cope with a variety of environments while maintaining a high level of accuracy, in other words this system should work well in natural conditions. This paper tackles the License Plate Recognition problem and introduces the LPRNet algorithm, which is designed to work without pre-segmentation and consequent recognition of characters. In the present paper, we do not consider License Plate Detection problem, however, for our particular case it can be done through LBP-cascade. <ref type="bibr">0</ref> This work was done when Sergey was an Intel employee LPRNet is based on Deep Convolutional Neural Network. Recent studies proved effectiveness and superiority of Convolutional Neural Networks in many Computer Vision tasks such as image classification, object detection and semantic segmentation. However, running most of them on embedded devices still remains a challenging problem.</p><p>LPRNet is a very efficient neural network, which takes only 0.34 GFLops to make a single forward pass. Also, our model is real-time on Intel Core i7-6700K SkyLake CPU with high accuracy on challenging Chinese License plates and can be trained end-to-end. Moreover, LPRNet can be partially ported on FPGA, which can free up CPU power for other parts of the pipeline. Our main contributions can be summarized as follows:</p><p>? LPRNet is a real-time framework for high-quality license plate recognition supporting template and character independent variable-length license plates, performing LPR without character pre-segmentation, trainable end-to-end from scratch for different national license plates.</p><p>? LPRNet is the first real-time approach that does not use Recurrent Neural Networks and is lightweight enough to run on variety of platforms, including embedded devices.</p><p>? Application of LPRNet to real traffic surveillance video shows that our approach is robust enough to handle difficult cases, such as perspective and cameradependent distortions, hard lighting conditions, change of viewpoint, etc.</p><p>The rest of the paper is organized as follows. Section 2 describes the related work. In sec. 3 we review the details of the LPRNet model. Sec. 4 reports the results on Chinese License Plates and includes an ablation study of our algorithm. We summarize and conclude our work in sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the earlier works on general LP recognition, such as <ref type="bibr" target="#b0">[1]</ref> the pipeline consist of character segmentation and char- ? Character classification typically utilizes one of the optical character recognition (OCR) methods -adopted for LP character set.</p><p>Since classification follows the character segmentation, end-to-end recognition quality depends heavily on the applied segmentation method. In order to solve the problem of character segmentation there were proposed endto-end Convolutional Neural Networks (CNNs) based solutions taking the whole LP image as input and producing the output character sequence.</p><p>The segmentation-free model in <ref type="bibr" target="#b1">[2]</ref> is based on variable length sequence decoding driven by connectionist temporal classification (CTC) loss <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. It uses hand-crafted features LBP built on a binarized image as CNN input to produce character classes probabilities. Applied to all input image positions via the sliding window approach it makes the input sequence for the bi-directional Long-Short Term Memory (LSTM) <ref type="bibr" target="#b4">[5]</ref> based decoder. Since the decoder output and target character sequence lengths are different, CTC loss is used for the pre-segmentation free end-to-end training.</p><p>The model in <ref type="bibr" target="#b5">[6]</ref> mostly follows the approach described in <ref type="bibr" target="#b1">[2]</ref> except that the sliding window method was replaced by CNN output spatial splitting to the RNN input sequence ("sliding window" over feature map instead of input).</p><p>In contrast <ref type="bibr" target="#b6">[7]</ref> uses the CNN-based model for the whole LP image to produce the global LP embedding which is decoded to a 11-character-length sequence via 11 fully connected model heads. Each of the heads is trained to classify the i-th target string character (which is assumed to be padded to the predefined fixed length), so the whole recognition can be done in a single feed-forward pass. It also utilizes the Spatial Transformer Network (STN) <ref type="bibr" target="#b7">[8]</ref> to reduce the effect of input image deformations.</p><p>The algorithm in <ref type="bibr" target="#b8">[9]</ref> makes an attempt to solve both license plate detection and license plate recognition problems by single Deep Neural Network.</p><p>Recent work <ref type="bibr" target="#b9">[10]</ref> tries to exploit synthetic data generation approach based on Generative Adversarial Networks <ref type="bibr" target="#b10">[11]</ref> for data generation procedure to obtain large representative license plates dataset.</p><p>In our approach, we avoided using hand-crafted features over a binarized image -instead we used raw RGB pixels as CNN input. The LSTM-based sequence decoder working on outputs of a sliding window CNN was replaced with a fully convolutional model which output is interpreted as character probabilities sequence for CTC loss training and greedy or prefix search string inference. For better performance the pre-decoder intermediate feature map was augmented by the global context embedding as described in <ref type="bibr" target="#b11">[12]</ref>. Also the backbone CNN model was reduced significantly using the low computation cost basic building block inspired by SqueezeNet Fire Blocks <ref type="bibr" target="#b12">[13]</ref> and Inception Blocks of <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. Batch Normalization <ref type="bibr" target="#b16">[17]</ref> and Dropout <ref type="bibr" target="#b17">[18]</ref> techniques were used for regularization.</p><p>LP image input size affects both the computational cost and the recognition quality <ref type="bibr" target="#b18">[19]</ref>, as a result there is a tradeoff between using high <ref type="bibr" target="#b5">[6]</ref> or moderate <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b1">2]</ref> resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">LPRNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Design architecture</head><p>In this section we describe our LPRNet network architecture design in detail.</p><p>In recent studies tend to use parts of the powerful classification networks such as VGG, ResNet or GoogLeNet as 'backbone' for their tasks by applying transfer learning techniques. However, this is not the best option for building fast and lightweight networks, so in our case we redesigned main 'backbone' network applying recently discovered architecture tricks.</p><p>The basic building block of our CNN model backbone ( <ref type="table">Table 2)</ref> was inspired by SqueezeNet Fire Blocks <ref type="bibr" target="#b12">[13]</ref> and Inception Blocks of <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. We also followed the research best practices and used Batch Normalization <ref type="bibr" target="#b16">[17]</ref> and ReLU activation after each convolution operation.</p><p>In a nutshell our design consists of:</p><p>? location network with Spatial Transformer Layer <ref type="bibr" target="#b7">[8]</ref> (optional)</p><p>? light-weight convolutional neural network (backbone)</p><p>? per-position character classification head</p><p>? character probabilities for further sequence decoding</p><p>? post-filtering procedure First, the input image is preprocessed by the Spatial Transformer Layer, as proposed in <ref type="bibr" target="#b7">[8]</ref>. This step is optional but allows to explore how one can transform the input image to have better characteristics for recognition. The original LocNet (see the The backbone network architecture is described in Table 3. The backbone takes a raw RGB image as input and calculates spatially distributed rich features. Wide convolution (with 1 ? 13 kernel) utilizes the local character context instead of using LSTM-based RNN. The backbone subnetwork output can be interpreted as a sequence of character probabilities whose length corresponds to the input image pixel width. Since the decoder output and the target character sequence lengths are of different length, we apply the method of CTC loss <ref type="bibr" target="#b19">[20]</ref> -for segmentation-free end-to-end training. CTC loss is a well-known approach for situations where input and output sequences are misaligned and have variable lengths. Moreover, CTC provides an efficient way to go from probabilities at each time step to the probability of an output sequence. More detailed explanation about CTC loss can be found in .  To further improve performance, the pre-decoder intermediate feature map was augmented with the global context embedding as in <ref type="bibr" target="#b11">[12]</ref>. It is computed via a fully-connected layer over backbone output, tiled to the desired size and concatenated with backbone output. In order to adjust the depth of feature map to the character class number additional 1 ? 1 convolution is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer</head><p>For the decoding procedure at the inference stage we considered 2 options: greedy search and beam search. While greedy search takes the maximum of class probabilities in each position, beam search maximizes the total probability of the output sequence <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>For post-filtering we use a task-oriented language model implemented as a set of the target country LP templates. Note that post-filtering is applied together with Beam Search. The post-filtering procedure gets top-N most probable sequences found by beam search and returns the first one that matches the set of predefined templates which depends on country LP regulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training details</head><p>All training experiments were done with the help of Ten-sorFlow <ref type="bibr" target="#b20">[21]</ref>. We train our model with 'Adam' optimizer using batch size of 32, initial learning rate 0.001 and gradient noise scale of 0.001. We drop the learning rate once after every 100k iterations by a factor of 10 and train our network for 250k iterations in total.</p><p>In our experiments we use data augmentation by random affine transformations, e.g. rotation, scaling and shift.</p><p>It is worth mentioning, that application of LocNet from the beginning of training leads to degradation of results, because LocNet cannot get reasonable gradients from a recognizer which is typically too weak for the first few iterations. So, in our experiments, we turn LocNet on only after 5k iterations.</p><p>All other hyper-parameters were chosen by crossvalidation over the target dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results of the Experiments</head><p>The LPRNet baseline network, from which we started our experiments with different architectures, was inspired by <ref type="bibr" target="#b1">[2]</ref>. It's mainly based on Inception blocks followed by a bidirectional LSTM (biLSTM) decoder and trained with CTC loss. We first performed some experiments aimed at replacing biLSTM with biGRU cells, but we did not observe any clear benefits of using biGRU over biLSTM.</p><p>Then, we focused on eliminating of the complicated biL-STM decoder, because most modern embedded devices still do not have sufficient compute and memory to efficiently execute biLSTM. Importantly, our LSTM is applied to a spatial sequence rather than to a temporal one. Thus all LSTM inputs are known upfront both at the training stage as well as at the inference stage. Therefore we believe that RNN can be replaced by spatial convolutions without a significant drop in accuracy. The RNN-less model with some backbone modifications is referenced as LPRNet basic and it was described in details in sec. 3.</p><p>To improve runtime performance we also modified LPR-Net basic by using 2 ? 2 strides for all pooling layers. This modification (the LPRNet reduced model) reduces the size of intermediate feature maps and total inference computational cost significantly (see GFLOPs column of the <ref type="table">Table  4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Chinese License Plates dataset</head><p>We tested our approach on a private dataset containing various images of Chinese license plates collected from different security and surveillance cameras. This dataset was first run through the LPB-based detector to get bounding boxes for each license plate. Then, all license plates were manually labeled. In total, the dataset contains 11696 cropped license plate images, which are split as 9:1 into training and validation subsets respectively.</p><p>Automatically cropped license plate images were used for training to make the network more robust to detection artifacts because in some cases plates are cropped with some background around edges, while in other cases they are cropped too close to edges with no background at all or event with some parts of the license plate missing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation study</head><p>It is of vital importance to conduct the ablation study to identify correlation between various enhancements and respective accuracy/performance improvements. This helps other researchers adopt ideas from the paper and reuse most promising architecture approaches. As one can see, the largest accuracy gain (36%) was achieved using the global context. The data augmentation techniques also help to improve accuracy significantly (by 28.6%). Without using data augmentation and the global context we could not train the model from scratch.</p><p>The STN-based alignment subnetwork provides noticeable improvement of 2.8-5.2%. Beam Search with postfiltering further improves recognition accuracy by 0.4-0.6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance analysis</head><p>The LPRNet reduced model was ported to various hardware platforms including CPU, GPU and FPGA. The results are presented in the  <ref type="table" target="#tab_5">Table 6</ref>. Performance analysis.</p><p>1 The LPRNet reduced model was used Here GPU is nVIDIA R GeForce TM 1080, CPU is Intel R Core TM i7-6700K SkyLake, FPGA is Intel R Arria TM 10 and IE is for Inference Engine from Intel R OpenVINO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In this work, we have shown that for License Plate Recognition one can utilize pretty small convolutional neural networks. LPRNet model was introduced, which can be used for challenging data, achieving up to 95% recognition accuracy. Architecture details, its motivation and the ablation study was conducted.</p><p>We showed that LPRNet can perform inference in realtime on a variety of hardware architectures including CPU, GPU and FPGA. We have no doubt that LPRNet could attain real-time performance even on more specialized embedded low-power devices.</p><p>The LPRNet can likely be compressed using modern pruning and quantization techniques, which would potentially help to reduce the computational complexity even further.</p><p>As a future direction of research, LPRNet work can be extended by merging CNN-based detection part into our algorithm, so that both detection and recognition tasks will be evaluated as a single network in order to outperform the LBP-based cascaded detector quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Example of LPRNet recognitions acter classification stages: ? Character segmentation typically uses different handcrafted algorithms, combining projections, connectivity and contour based image components. It takes a binary image or intermediate representation as input so character segmentation quality is highly affected by the input image noise, low resolution, blur or deformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 )Table 1 .</head><label>11</label><figDesc>architecture was used to estimate optimal transformation parameters. LocNet architecture</figDesc><table><row><cell>Layer Type</cell><cell>Parameters</cell><cell></cell></row><row><cell>Input</cell><cell cols="2">94x24 pixels RGB image</cell></row><row><cell>AvgPooling</cell><cell>#32 3x3 stride 2</cell><cell>-</cell></row><row><cell>Convolution</cell><cell cols="2">#32 5x5 stride 3 #32 5x5 stride 5</cell></row><row><cell>Concatenation</cell><cell>channel-wise</cell><cell></cell></row><row><cell>Dropout</cell><cell>0.5 ratio</cell><cell></cell></row><row><cell>FC</cell><cell cols="2">#32 with TanH activation</cell></row><row><cell>FC</cell><cell cols="2">#6 with scaled TanH activation</cell></row><row><cell>Layer Type</cell><cell cols="2">Parameters/Dimensions</cell></row><row><cell>Input</cell><cell cols="2">C in ? H ? W feature map</cell></row><row><cell>Convolution</cell><cell># C out /4 1x1 stride 1</cell><cell></cell></row><row><cell cols="3">Convolution # C out /4 3x1 strideh=1, padh=1</cell></row><row><cell cols="3">Convolution # C out /4 1x3 stridew=1, padw=1</cell></row><row><cell>Convolution</cell><cell># C out 1x1 stride 1</cell><cell></cell></row><row><cell>Output</cell><cell>C</cell><cell></cell></row></table><note>out ? H ? W feature map Table 2. Small Basic Block</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Back-bone Network Architecture</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 showsTable 4 .</head><label>44</label><figDesc>recognition accuracies achieved by different models. Results on Chinese License Plates.</figDesc><table><row><cell>Method</cell><cell cols="2">Recognition Accuracy, % GFLOPs</cell></row><row><cell>LPRNet baseline</cell><cell>94.1</cell><cell>0.71</cell></row><row><cell>LPRNet basic</cell><cell>95.0</cell><cell>0.34</cell></row><row><cell>LPRNet reduced</cell><cell>94.0</cell><cell>0.163</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 Table 5 .</head><label>55</label><figDesc>Effects of various tricks on LPRNet quality.</figDesc><table><row><cell>shows a sum-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table><row><cell>Target platform</cell><cell>1 LP processing time</cell></row><row><cell>GPU + cuDNN</cell><cell>3 ms</cell></row><row><cell>CPU (using Caffe [22])</cell><cell>11-15 ms</cell></row><row><cell>CPU + FPGA (using DLA [23])</cell><cell>4 ms 1</cell></row><row><cell>CPU (using IE from Intel OpenVINO [24])</cell><cell>1.3 ms</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We would like to thank Intel R IOTG Computer Vision (ICV) Optimization team for porting the model to the Intel Inference Engine of OpenVINO, as well as Intel R IOTG Computer Vision (ICV) OVX FPGA team for porting the model to the DLA. We also would like to thank Intel R PSG DLA and Intel R Computer Vision SDK teams for their tools and support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">License Plate Recognition From Still Images and Video Sequences: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N E</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Psoroulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Loumos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kayafas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="391" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Reading Car License Plates Using Deep Convolutional Neural Networks and LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.05610</idno>
		<idno>arXiv: 1601.05610</idno>
		<imprint>
			<date type="published" when="2016-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Supervised Sequence Labelling with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-02" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<pubPlace>Heidelberg; New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Segmentationfree Vehicle License Plate Recognition using ConvNet-RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Cheang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06439</idno>
		<idno>arXiv: 1701.06439. 2</idno>
		<imprint>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Automatic License Plate Recognition System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sasindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Indian Conference on Computer Vision, Graphics and Image Processing, ser. ICVGIP &apos;16</title>
		<meeting>the Tenth Indian Conference on Computer Vision, Graphics and Image Processing, ser. ICVGIP &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Spatial Transformer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02025</idno>
		<idno>arXiv: 1506.02025</idno>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Towards End-to-End Car License Plates Detection and Recognition with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adversarial Generation of Training Examples: Applications to Moving Vehicle License Plate Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<idno>arXiv: 1506.04579</idno>
		<title level="m">ParseNet: Looking Wider to See Better</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<idno>arXiv: 1602.07360</idno>
		<title level="m">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5mb model size</title>
		<imprint>
			<date type="published" when="2016-02" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<idno>arXiv: 1602.07261</idno>
		<imprint>
			<date type="published" when="2016-02" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<idno>arXiv: 1409.4842</idno>
		<title level="m">Going Deeper with Convolutions</title>
		<imprint>
			<date type="published" when="2014-09" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<idno>arXiv: 1512.00567</idno>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<idno>arXiv: 1502.03167</idno>
		<imprint>
			<date type="published" when="2015-02" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deciphering Severely Degraded License Plates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Sequence modeling with ctc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<ptr target="https://distill.pub/2017/ctc.3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<idno>arXiv: 1603.04467. 3</idno>
		<title level="m">Ten-sorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</title>
		<imprint>
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Aydonat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>O&amp;apos;connell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Capalija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Chiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03534</idno>
		<idno>arXiv: 1701.03534. 4</idno>
	</analytic>
	<monogr>
		<title level="j">An OpenCL(TM) Deep Learning Accelerator on Arria</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Intel OpenVINO Toolkit | Intel Software</title>
		<ptr target="https://software.intel.com/en-us/articles/OpenVINO-InferEngine4" />
		<imprint/>
	</monogr>
	<note>Online</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
