<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Restructuring Graph for Higher Homophily via Learnable Spectral Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouhneg</forename><surname>Li</surname></persName>
							<email>shouheng.li@anu.edu.au</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>POSTECH</roleName><forename type="first">Dongwoo</forename><forename type="middle">Kim</forename><surname>Gsai</surname></persName>
							<email>dongwookim@postech.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">South</forename><surname>Korea</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Wang</surname></persName>
							<email>qing.wang@anu.edu.au</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The Australian National University Data61</orgName>
								<address>
									<country>CSIRO Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Restructuring Graph for Higher Homophily via Learnable Spectral Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While a growing body of literature has been studying new Graph Neural Networks (GNNs) that work on both homophilic and heterophilic graphs, little work has been done on adapting classical GNNs to less-homophilic graphs. Although lacking the ability to work with less-homophilic graphs, classical GNNs still stand out in some properties such as efficiency, simplicity and explainability. We propose a novel graph restructuring method to maximize the benefit of prevalent GNNs with the homophilic assumption. Our contribution is threefold: a) learning the weight of pseudo-eigenvectors for an adaptive spectral clustering that aligns well with known node labels, b) proposing a new homophilic metric that measures how two nodes with the same label are likely to be connected, and c) reconstructing the adjacency matrix based on the result of adaptive spectral clustering to maximize the homophilic scores. The experimental results show that our graph restructuring method can significantly boost the performance of six classical GNNs by an average of 25% on less-homophilic graphs. The boosted performance is comparable to state-of-the-art methods.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) were originally inspired under the homophilic assumption -nodes of a graph with the same label are more likely to be connected than nodes with different labels. Recent studies reveal the limitations of this homophilic assumption when GNNs are applied on less-homophilic or heterophilic graphs <ref type="bibr" target="#b25">(Pei et al., 2020)</ref>. Ever since, a number of approaches have focused on developing deep learning architectures for heterophilic graphs <ref type="bibr" target="#b45">(Zhu et al., 2021b;</ref><ref type="bibr">Kim and Oh, 2021;</ref><ref type="bibr">Chien et al., 2021;</ref><ref type="bibr" target="#b5">Bo et al., 2021;</ref><ref type="bibr" target="#b23">Lim et al., 2021)</ref>. However, little work has been done on adapting classical GNNs to less-homophilic graphs. Although classical GNNs are lacking the ability to work with less-homophilic graphs, they still stand out in some properties such as efficiency <ref type="bibr" target="#b42">(Zeng et al., 2020)</ref>, simplicity <ref type="bibr" target="#b38">(Wu et al., 2019)</ref> and explainability <ref type="bibr" target="#b41">(Ying et al., 2019)</ref>.</p><p>Our work studies a graph restructuring method that reconstructs the adjacency matrix of a graph from scratch to uplift the benefit of prevalent homophilic GNNs. Our work is partially inspired by <ref type="bibr" target="#b20">Klicpera et al. (2019b)</ref>, we extend the restructuring approach to heterophilic graphs and improve the performance of GNNs which do not work well on heterophilic graphs. Our observation from spectral clustering of heterophilic graphs shows that the eigenvectors corresponding to the leading eigenvalues do not always align well with the node labels. Particularly, in a heterophilic graph, two adjacent nodes are unlikely to have the same label, which is in contradiction with the smoothness properties of leading eigenvectors. However, when we choose eigenvectors appropriately, the correlation between the similarity of spectral features and node labels increases. To generalize this observation, we propose a learnable spectral clustering algorithm that, a) divides the Laplacian spectrum into even slices called pseudo-eigenvalues, each slice corresponds to an embedding matrix called pseudo-eigenvector; b) learns the weights of pseudo-eigenvectors from existing node labels; c) restructures the graph according to node embedding distance to maximize homophily. To measure the homophilic level of graphs, we further introduce a new metric that addresses some limitations of the previous metrics. Our experimental results show that the performances of node-level prediction tasks with restructured graphs are greatly improved on classical GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>GNNs for heterophilic graphs. Early GNNs assume homophily implicitly. Such an inductive bias results in a degenerated performance on less-homophilic graphs <ref type="bibr" target="#b23">(Lim et al., 2021)</ref>. Recently, homophily is also shown to be an effective measure of a graph's robustness to both over-smoothing and adversarial attacks. In the over-smoothing setting, node representations converge to a stationary state of similar values ("over-smoothed") as a GNN goes deeper. A graph of low homophily is more prone to this issue as the stationary state is reached with fewer GNN layers <ref type="bibr" target="#b40">(Yan et al., 2021)</ref>. Similarly, homophilic graphs are more resilient to a graph injection attack than their heterophilic counterparts in the adversarial attack setting. Some techniques defend against such attacks by improving or retaining homophily under graph injection <ref type="bibr" target="#b44">(Zhu et al., 2021a;</ref><ref type="bibr" target="#b7">Chen et al., 2022)</ref>. <ref type="bibr" target="#b25">Pei et al. (2020)</ref> firstly draw attention to the limitation of GNN on less-homophilic graphs. Since then, various GNNs have been proposed to improve performance on these graphs. H 2 GCN <ref type="bibr" target="#b43">(Zhu et al., 2020)</ref> show that proper utilization of ego-embedding, higher-order neighbourhoods, and intermediate embeddings can improve results in heterophilic graphs. A recent scalable model, LINKX <ref type="bibr" target="#b23">(Lim et al., 2021)</ref>, shows separating feature and structural embedding improves performance. <ref type="bibr">Kim and Oh (2021)</ref> study this topic specifically for graph attention and finds improvements when an attention mechanism is chosen according to homophily and average degrees. <ref type="bibr">Chien et al. (2021)</ref> propose to use a generalized PageRank method that learns the weights of a polynomial filter and show that the model can adapt to both homophilic and heterophilic graphs. Similarly,  use learnable spectral filters for achieving an adaptive model on graphs of different homophilic levels. <ref type="bibr" target="#b45">Zhu et al. (2021b)</ref> recently propose to incorporate a learnable compatibility matrix into GNN-based methods to handle heterophily of graphs.</p><p>Learning spectral clustering. From the deep learning perspective, most previous studies about spectral clustering aim at clustering using learning approaches, or building relations between a supervised learning model and spectral clustering. <ref type="bibr" target="#b21">Law et al. (2017)</ref> and <ref type="bibr" target="#b1">Bach and Jordan (2003)</ref> reveal that minimizing the loss of node similarity matrix can be seen as learning the leading eigenvector representations used for spectral clustering. <ref type="bibr" target="#b3">Bianchi et al. (2020)</ref> train cluster assignment using a unsupervised MinCut objective and further use the learned clusters in a pooling layer to improve GNN performance. Instead of directly optimizing a similarity matrix, <ref type="bibr" target="#b34">Tian et al. (2014)</ref> adopt an unsupervised autoencoder design that uses representations learned in hidden layers to perform K-means. <ref type="bibr">Chowdhury and Needham (2021)</ref> show that Gromov-Wasserstein learning, an optimal transport-based clustering method, is connected with a two-way spectral clustering.</p><p>Graph restructuring and rewiring. GDC <ref type="bibr" target="#b20">(Klicpera et al., 2019b)</ref> is one of the first works proposed to rewire edges in a graph. It uses diffusion kernels, such as heat kernel and personalized PageRank, to redirect messages passing beyond existing edges. <ref type="bibr">Chamberlain et al. (2021)</ref> and <ref type="bibr">Eliasof et al. (2021)</ref> extend the diffusion kernels to different classes of partial differential equations. In a slightly different setting where graph structures are not readily available, some try to construct graphs from scratch instead of modifying the existing edges. <ref type="bibr" target="#b14">Fatemi et al. (2021)</ref> construct homophilic graphs via self-supervision on masked node features. <ref type="bibr" target="#b16">Kalofolias (2016)</ref> learns a smooth graph by minimizing tr(X T LX). These works belong to the "learning graphs from data" family <ref type="bibr" target="#b12">(Dong et al., 2019)</ref> which is relevant but different to our work because a) these methods infer graphs from data where no graph topology is readily available, while in our setting, the original graphs are the key ingredients; b) as shown by <ref type="bibr" target="#b14">Fatemi et al. (2021)</ref>, without the initial graph, the performance of these methods are not comparable to even a naive GCN; c) these methods are mostly used to solve graph generation problems instead of a node-or link-level task.</p><p>3 Preliminaries Spectral filtering. Let G = (V, E, A, X) be an undirected graph with N nodes, where V , E, and A ? {0, 1} N ?N are the node set, edge set, and adjacency matrix of G, respectively, and X ? R N ?F is the node feature matrix. The normalized Laplacian matrix of G is defined as L = I ? D ?1/2 AD ?1/2 , where I is the identity matrix with N diagonal entries and D ? R N ?N is the diagonal degree matrix of G. In spectral graph theory, the eigenvalues ? = diag(? 1 , ..., ? N ) and eigenvectors U of L = U ?U H are known as the graph's spectrum and spectral basis, respectively, where U H is the Hermitian transpose of U . The graph Fourier transform takes the form ofX = U H X and its inverse is X = UX.</p><p>It is known that the Laplacian spectrum and spectral basis carry important information on the connectivity of a graph <ref type="bibr" target="#b31">(Shuman et al., 2013)</ref>. Intuitively, lower frequencies correspond to global and smooth information on the graph, while higher frequencies correspond to local information, discontinuities and possible noise <ref type="bibr" target="#b31">(Shuman et al., 2013)</ref>. One can apply a spectral filter and use graph Fourier transform to manipulate signals on a graph in various ways, such as smoothing and denoising (Schaub and Segarra, 2018), abnormally detection <ref type="bibr" target="#b24">(Miller et al., 2011)</ref> and clustering <ref type="bibr" target="#b37">(Wai et al., 2018)</ref>. Spectral convolution on graphs is defined as the multiplication of a signal x with a filter g(?) in the Fourier domain, i.e.</p><formula xml:id="formula_0">g(L)x = g(U ?U H )x = U g(?)U H x = U g(?)x.</formula><p>(1)</p><p>Spectral Clustering (SC) as low-pass filtering. SC is a well-known method for clustering nodes in the spectral domain. A simplified SC algorithm is described in Appendix A.5. Spectral clustering can be interpreted as low-pass filtering on a one-hot node signal ? i ? R N , where the i-th element of ? i is 1, i.e. ? i (i) = 1, and 0 elsewhere, for each node i. Filtering ? i in the graph Fourier domain can be expressed as</p><formula xml:id="formula_1">f i = g ? L (?)U H ? i ,<label>(2)</label></formula><p>where g ? L is the low-pass filter that filter out components whose frequencies are greater than ? L as</p><formula xml:id="formula_2">g ? L (?) = 1 if ? ? ? L 0 otherwise.<label>(3)</label></formula><p>As pointed out by <ref type="bibr" target="#b35">Tremblay et al. (2016)</ref>; <ref type="bibr" target="#b26">Ramasamy and Madhow (2015)</ref>, f i ? f j 2 can be approximated by filtering random node features with g ? L . Consider a random node feature matrix R = [r 1 |r 2 |...|r ? ] ? R N ?? consisting of ? random features r i ? R N of V i.i.d sampled form the normal distribution with zero mean and 1/? variance. Let H ? L = U g ? L (?)U H . We can defin?</p><formula xml:id="formula_3">f i = (H ? L R) H ? i .</formula><p>(4) R is a random Gaussian matrix of zero mean and U is orthonormal. We apply the Johnson-Lindenstrauss lemma (Appendex A.3) to obtain the following error bounds. Proposition 3.1 <ref type="bibr" target="#b35">(Tremblay et al. (2016)</ref>). Let , ? &gt; 0 be given. If ? is larger than:</p><formula xml:id="formula_4">? 0 = 4 + 2? 2 /2 ? 3 /3 log N,<label>(5)</label></formula><p>then with probability at least 1 ? N ?? , we have:</p><formula xml:id="formula_5">?(i, j) ? [1, N ] 2 , (1 ? ) f i ? f j 2 ? f i ?f j 2 ? (1 + ) f i ? f j 2 .<label>(6)</label></formula><p>Hence, f i ?f j is a close estimation of the Euclidean distance f i ?f j . Note that the approximation error bounds in Equation 6 hold for any band-pass filter? e1,e2 :</p><formula xml:id="formula_6">g e1,e2 (?) = 1 if e 1 &lt; ? ? e 2 0 otherwise.<label>(7)</label></formula><p>where 0 ? e 1 ? e 2 ? 2 since the spectrum of a normalized Laplacian matrix has the range [0, 2]. This fact is especially useful when the band pass? e1,e2 can be expressed by a specific functional form such as polynomials, since H e1,e2 = U? e1,e2 (?)U H can be computed without eigen-decomposition.  <ref type="bibr">, 2020)</ref>. Colours represent node labels. Coordinates in 1a and 1c are computed using three-dimensional T-SNE. In 1b and 1d many nodes are overlapped so it appears to have fewer nodes than it actually has.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learnable Spectral Clustering</head><p>Now we propose a learnable method for spectral clustering which empowers us to restructure a graph to improve graph homophily while preserving the original graph structures as much as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Discrepancy between SC and labels</head><p>Spectral clustering aims to cluster nodes such that the edges between different clusters have low weights and the edges within a cluster have high weights. Such clusters are likely to align with the labels of nodes when a graph is homophilic, i.e., nodes with the same labels are likely to be connected. Whereas this may not hold in the case of less-homophilic graphs. <ref type="figure" target="#fig_0">Figure 1</ref> visualizes the nodes in the Wisconsin and Europe Airport datasets based on the eigenvectors corresponding to the five smallest eigenvalues. As shown in <ref type="figure" target="#fig_0">Figure 1a</ref> and 1c, the labels are not aligned with the node clusters. However, if we choose the eigenvectors carefully, nodes clusters can align better with their labels, as evidenced by <ref type="figure" target="#fig_0">Figure 1b</ref> and 1d, which are visualized using manually chosen eigenvectors. Below, we discuss how to develop a learnable SC method that aligns the clustering structure with node labels by learning the underlying frequency patterns. The spectral expressive power of this method is discussed separately in Appendix A.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning eigenvector coefficients</head><p>Let f Z i be the representation of a node i obtained from an arbitrary set Z ? [N ] of eigenvectors in place of the ones with the leading eigenvalues. We cast the problem of finding eigenvectors to align with the known node labels into a minimization problem of computing the distance between the representations of nodes i and j when their labels are the same. Let d(?) be a distance metric between two node representations. Then, our objective is formalized as</p><formula xml:id="formula_7">arg min Z i,j?V Y (d(f Z i , f Z j ), 1(y i , y j )),<label>(8)</label></formula><p>where V Y is a collection of nodes whose labels are available, 1 is an indicator function, y i is the label of node i, and is a loss function. The loss function penalizes the objective when two nodes with the same label are far from each other, as well as when two nodes with different labels are close. In this work, we use Euclidean distance and a contrastive loss function.</p><p>Solving the above objective requires iterating over all possible combinations of eigenvectors, which is infeasible in general. It also requires performing expensive eigendecomposition with O(N 3 ) time complexity. In addition, SC does not consider feature similarity between nodes. To address these challenges, we introduce two key ideas to generalize SC in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Eigendecomposition-free SC</head><p>As explained in Section 3, f i ? f j can be approximated by a filtering operation under the Johnson-Lindenstrauss lemma. The same holds true for the generalized case f Z i . However, the operation still requires expensive eigendecomposition as Equation 7 takes eigenvalues explicitly. To mitigate this issue, we propose to use a series of rectangular functions, each serving as a band-pass filter that "slices" the Laplacian spectrum into a finite set of equal-length and equal-magnitude ranges. Each filter takes the same form of Equation 7, but is relaxed on the continuous domain. This formulation comes with two major advantages. Firstly, rectangular functions can be efficiently approximated with polynomial rational functions, thus bypassing expensive eigendecomposition. Secondly, each g e1,e2 groups frequencies within the range (e 1 , e 2 ] to form a "coarsened" pseudo-eigenvalue. Because nearby eigenvalues capture similar structural information, the "coarsening" operation reduces the size of representations, while still largely preserving the representation power. Spectrum slicers. We approximate the band-pass rectangular filters in Equation 7 using a rational function</p><formula xml:id="formula_8">g s,a = 1 s 2m ? ? a 2 +? 2m + 1 s 2m ?1 (9)</formula><p>where s ? 2 is a parameter that controls the width of the passing window on the Laplacian spectrum, a ? [0, 2] is another parameter that controls the horizontal center of the function, and m is the approximation order. A larger m can reduce the approximate error but is more expensive to compute. <ref type="figure" target="#fig_1">Figure 2</ref> shows an example of these functions. With properly chosen s and a, the Laplacian spectrum can be evenly sliced into chunks of range (? i , ? i+1 ). Each chunk is a "coarsened eigenvalue" that umbrellas eigenvalues within the range. Substituting g(?) in Equation 1 with? s,a , the spectral filtering operation becomes</p><formula xml:id="formula_9">U? s,a (?)U H x = 1 s 2m L ? aI 2 +? 2m + I s 2m ?1 (10) where U ?U H = L.</formula><p>An important property of Equation 10 is that the matrix inversion can be computed via truncated Neumann series <ref type="bibr" target="#b39">(Wu et al., 2013)</ref>. This can bring the computation cost of O(N 3 ) down to O(pN 2 ).</p><formula xml:id="formula_10">Lemma 1. For all? &gt; 2s 2m s 2m ?1 ? 2, the inverse of T = L?aI 2+? 2m + I s 2m</formula><p>can be expressed by a Neumann series with guaranteed convergence. (A proof is given in Appendix A.4.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">SC with node features</head><p>Traditionally, SC does not use node features. However, independent from graph structure, node features can provide extra information that is valuable for a clustering task. This is especially true where node features are in disagreement with representations obtained from eigenvectors. Therefore, we incorporate node features into the SC filtering operation by concatenating it with the random signal before the filtering operation ? s,a =? s,a (L)(R X),</p><p>where is a column-wise concatenation operation. ? s,a has the shape of N ? (P + F ) and is sometimes referred as "dictionary" <ref type="bibr" target="#b33">(Thanou et al., 2014)</ref>. When using a series of g s,a , we have ? = (? s1,a1 ? s2,a2 ...). Let P be the dimension of embeddings and ? represent a learnable weight matrix or a feed-forward neural network. The concatenated dictionary is then fed into a learnable function to obtain a node embedding matrix H ? R N ?P as H = ?(?).</p><p>(13) Our objective in Equation 8 can then be realized as</p><formula xml:id="formula_13">L(?) = i,j?V Y k?N Y (i) yi=yj ||H i? ? H j? || 2 ? ||H i? ? H k? || 2 + +<label>(14)</label></formula><p>where N Y (k) is a set of negative samples whose labels are different from node i, i.e. y i = y k . The intuition is if the labels of nodes i and j are the same, then the distance between the two nodes needs to be less than the distance between i and k, to minimize the loss.</p><p>[a] + = max <ref type="bibr">(a, 0)</ref> and is a scalar offset between distances of intra-class and inter-class pairs. By minimizing the objective w.r.t the weight matrix ?, we can learn the weight of each band that aligns the best with the given labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Restructure graphs to maximize homophily</head><p>After training, we obtain a distance matrix D where D ij = H i ? ?H j ? . As suggested by <ref type="bibr" target="#b20">Klicpera et al. (2019b)</ref>, there are two ways to restructure a graph from the matrix. One way is to apply a threshold on D and entries below the threshold are kept as edges; another way is to draw edges between K nodes pairs of the smallest distance. As the D is learned with labels, the restructured graph is likely to have higher homophily than the original graph. In practice, for better control of sparsity, we draw edges between node pairs with a small distance. Specifically,? is the adjacency matrix whose entries are defined a?</p><formula xml:id="formula_14">A ij = 1, if (i, j) ? topK -1 (S) 0, otherwise,<label>(15)</label></formula><p>where topK -1 returns node pairs of the k smallest entries in D . In the implementation, we start with an empty adjacency matrix and iteratively add edges until the homophily score starts to drop. A simplified workflow is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. A detailed algorithm can be found in Appendix A.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Complexity analysis</head><p>The most expensive operation of our method is the matrix inversion in Equation 10 which has the time complexity of O(pN 2 ). A small p ? 4 is sufficient because the Neumann series is a geometric sum so exponential acceleration tricks can be applied. We use Equation 10 because it is closer to a rectangular function and better illustrates the spectrum slicing idea, but in practice, it can be replaced by other slicer functions that do not require matrix inversion, such as a quadratic function 1 ? (sL ? a) 2 , to further reduce cost. It is also worth noting that the matrix inversion and multiplication only need to be computed once and can be pre-computed offline. The training step can be mini-batched easily. We randomly sample 8 ? 64 negative samples per node so the cost of computing Equation 14 is low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Homophily Measure</head><p>Various methods have been proposed to measure the homophily of a graph <ref type="bibr" target="#b43">(Zhu et al., 2020;</ref><ref type="bibr" target="#b23">Lim et al., 2021)</ref>. The two most used are edge homophily h edge and node homophily h node : the former uses  <ref type="figure">Figure 4</ref>: Examples of graphs with different label-topology relationships and comparison of different homophily measures. The node colour represents the node labels. The red edges connect nodes of different labels, while the green edges connect nodes of the same labels. <ref type="figure">Figure 4a -Figure 4c</ref> shows homophilic graphs of different densities. h den gives a higher score when a graph is dense, while the other metrics give the same scores. <ref type="figure">Figure 4d</ref> and <ref type="figure">Figure 4e</ref> are two graphs that only consist of inter-class edges, but are of different densities. <ref type="figure">Figure 4f</ref> is a label-imbalanced graph. <ref type="figure">Figure 4g</ref> and <ref type="figure">Figure 4h</ref> are two regular graphs, where <ref type="figure">Figure 4g</ref> has an intra-class/inter-class edge ratio of 0.5, <ref type="figure">Figure 4h</ref> is an example of an Erdos-Reyi graph sampled with uniform edge probability. where N u is the neighbour set of node u, and y is the node label.</p><p>As pointed out by <ref type="bibr" target="#b23">Lim et al. (2021)</ref>, both edge and node homophily suffer from sensitivity to both label number and label imbalance. For instance, a balanced graph of C labels would induce homophily of 1 C for both measures. In addition, both metrics fail to handle cases of imbalanced labels, resulting in undesirably high homophily scores where the majority nodes are of the same label.</p><p>To mitigate these issues, <ref type="bibr" target="#b23">Lim et al. (2021)</ref> proposed a new metric h norm that takes into account the label-wise node and edge proportion:</p><formula xml:id="formula_15">h norm = 1 K ? 1 K?1 k=0 h k ? |Y k | N + , where h k = u?Y k |v ? N u : y u = y v = k| u?Y k |N u | ,<label>(16)</label></formula><p>K is the number of unique labels, Y k is the node set of label k, and h k is the label-wise homophily.</p><p>Nevertheless, h norm only captures relative edge proportions and ignores graph connectivity, resulting in high homophily scores for highly disconnected graphs. For example, <ref type="figure">Figure 4a</ref> has the same h norm as <ref type="figure">Figure 4b</ref> and 4c. The absence of edge density in the homophilic metric brings undesirable results in restructuring as the measurement always prefer a disconnected graph. Moreover, although h norm is lower-bounded by 0, it does not explicitly define the meaning of h norm = 0 but instead refers to such graphs as less-homophilic in general, resulting in further confusion when comparing less-homophilic graphs.</p><p>Given the limitations of existing metrics, we propose a density-aware homophily metric that measures the tendency of edges connecting nodes of the same label (intra-class) over nodes of different labels (inter-class). For a graph of K &gt; 1 labels, the following five propositions hold for our new metric:</p><p>1. A dense homophilic graph of a complete set of intra-class edges and zero inter-class edges has a score of 1. <ref type="figure">(Figure 4c</ref>) 2. A dense heterophilic graph of a complete set of inter-class edges and zero intra-class edges has a score of 0. <ref type="figure">(Figure 4e</ref>) 3. An Erdos-Renyi random graph G(n, p) of n nodes and the edge inclusion probability p has the score of ? 0.5, i.e. a graph that is uniformly random <ref type="figure">(Figure 4h</ref>). 4. A totally disconnected graph and a complete graph have the same score of 0.5. 5. For graphs with the same intra-class and inter-class edge ratios, the denser graph has a relatively higher score. <ref type="figure">(Figure 4a, 4b and 4c</ref>) <ref type="figure">(Figure 4d and 4e)</ref>, <ref type="figure">(Figure 4g and 4h)</ref> Proposition 1 and 2 define the limits of homophily and heterophily given a set of nodes and their labels. Proposition 3 and 4 define neutral graphs which are neither homophilic nor heterophilic. Proposition 3 states that a uniformly random graph, which has no label preference on edges, has homophily score of 0.5. Proposition 5 considers edge density: for graphs with the same tendencies of connecting inter-and intra-class nodes, the denser one has a higher absolute score value. The metric is defined as below.? den = min{d k ?d k } K?1 k=0 (17) where d k is the edge density of the subgraph formed by nodes from the label k, i.e. the intra-class edge density of k including self-loops, andd k is the maximum inter-class edge density of label k</p><formula xml:id="formula_16">d k = 2 |(u, v) ? E : k u = k v = k| |Y k |(|Y k | + 1) ,d k = max{d kj : j = 0, ..., K ? 1; j = k},<label>(18)</label></formula><p>where d kj is the inter-class edge density of label j and k, i.e. edge density of the subgraph formed by nodes from label k and j.</p><formula xml:id="formula_17">d kj = (u, v) ? E : k u = k, k v = j |Y k ||Y j | .<label>(19)</label></formula><p>Equation 17 has the range (?1, 1). To make it comparable with the other homophily metrics, we scale it to the range (0, 1) using</p><formula xml:id="formula_18">h den = 1 +? den 2 .<label>(20)</label></formula><p>Proposition 1, 2, 4 and 5 are easy to prove. We hereby give a brief description and proof for Proposition 3. Lemma 2. ?K &gt; 1, E[h den ] = 0.5 for the Erdos-Renyi random graph G(n, p). (A proof is given in Appendix A.4.) <ref type="figure">Figure 4</ref> shows some example graphs with four different metrics. Compared with h node and h edge , h den is not sensitive to the number of labels and label imbalance. Compared with h norm , h den is able to detect neutral graphs. h den gives scores in the range (0, 0.5) for graphs of low-homophily, allowing direct comparison between them. h den also considers edge density and therefore is more robust to disconnectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Empirical results</head><p>Datasets and models. We compare six classic GNNs, GCN (Kipf and Welling, 2017), SGC <ref type="bibr" target="#b38">(Wu et al., 2019)</ref>, ChevNet <ref type="bibr" target="#b11">(Defferrard et al., 2016)</ref>, <ref type="bibr">ARMANet (Bianchi et al., 2021)</ref>, GAT <ref type="bibr" target="#b36">(Velickovic et al., 2018)</ref>, and APPNP <ref type="bibr" target="#b19">(Klicpera et al., 2019a)</ref>, on their performance before and after graph restructuring. For comparison, we report the performance using an additional restructuring methods: GDC <ref type="bibr" target="#b20">(Klicpera et al., 2019b)</ref>. Three recent GNNs that target heterophilic graphs are also listed as baselines: GPRGNN <ref type="bibr">(Chien et al., 2021)</ref>, H 2 GCN <ref type="bibr" target="#b43">(Zhu et al., 2020)</ref> and Geom-GCN <ref type="bibr" target="#b25">(Pei et al., 2020)</ref>. We run experiments on six heterophilic graphs: TEXAS, CORNELL, WISCONSIN, ACTOR, <ref type="bibr">CHAMELEON and SQUIRREL (Rozemberczki et al., 2021;</ref><ref type="bibr" target="#b25">Pei et al., 2020)</ref>. Details of these datasets are given in Appendix A.1.</p><p>Experimental setup. Hyperparameters are tuned using grid search for all models on the unmodified and restructured graphs of each dataset. We record prediction accuracy on the test set averaged over 10 runs with different random initializations. We use the same split setting as <ref type="bibr" target="#b25">Pei et al. (2020)</ref>; <ref type="bibr" target="#b43">Zhu et al. (2020)</ref>. The results are averaged over all splits. We adopt early stopping and record the results from the epoch with highest validation accuracy. Homophily scores are computed using validation set only in the validation step. We report the averaged accuracy as well as the standard deviation. For the spectrum slicer in Equation 10, we use a set of 20 slicers with s = 40 and m = 4 so that the spectrum is sliced into 20 even range of 0.1. The graphs are restructured to have edges that give the highest h den before it starts to decrease. All experiments are run on a single NVIDIA RTX A6000 48GB GPU unless otherwise noted.</p><p>Node classification results. Node classification tasks predict labels of nodes based on graph structure and node features. We aim to improve the prediction accuracy of GNN models by restructuring edges via the learnable SC method, particularly for heterophilic graphs. The evaluation results are shown in <ref type="table" target="#tab_0">Table 1</ref>. On average, the performance of GNN models is improved by an average of 25%. We attribute this improvement to the increased homophily in the restructured graphs.</p><p>Performance vs. Homophily <ref type="figure" target="#fig_5">Figure 5b</ref> shows an ideal scenario where the "optimal edges" point corresponds to both the highest homophily and accuracy. <ref type="figure" target="#fig_5">Figure 5a</ref> shows a less-ideal scenario where the "optimal edges" point slightly lags behind best accuracy. In fact, in the latter case, the highest homophily scores of training, validation and test sets are reached at different edge numbers. The highest homophily on the training set is reached where the edge number is around 12,000, while it is 40,000 and 70,000 on the validation and test set. This suggests a discrepancy of frequency patterns in training, validation and test sets, as the clustering model is trained on training sets, it learns only the frequency patterns on the training set and generalizes sub-optimally on the unseen sets. This is also shown in <ref type="table" target="#tab_0">Table 1</ref> where the performance improvements achieved on SQUIRREL are greater than on CHAMELEON. We show this as a limitation of our model: when the frequency patterns on training and unseen sets are different, the learnable spectral clustering approach has limited ability to improve downstream model performance. We also noticed increased homophily on homophilic graphs does not always bring performance improvement, but we will leave this for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose an approach to enhance GNN performance on less-homophilic graphs by restructuring the graph to maximize its homophily. Our method is inspired and closely related to Spectral Clustering (SC). It extends SC beyond the leading eigenvalues and learns the frequencies that are best suited to cluster a graph. To achieve this, we use rectangular spectral filters expressed in the Neumann series to slice the graph spectrum into chunks that umbrellas small ranges of frequency. We also proposed a new homophily metric that is density-aware and is a better homophily indicator for the quality of graph restructuring. There are many promising extensions of this work, such as using it to guard against over-smoothing and adversarial attacks, by monitoring changes in homophily and adopting tactics to maintain it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Dataset Details</head><p>We use 6 datasets listed in <ref type="table" target="#tab_1">Table 2</ref>. TEXAS, WISCONSIN and CORNELL are graphs of web page links between universities, known as the CMU WebKB datasets. We use the pre-processed version in <ref type="bibr" target="#b25">Pei et al. (2020)</ref>, where nodes are classify into 5 categories of course, faculty, student, project and staff. SQUIRREL and CHAMELEON are graphs of web pages in Wikipedia, originally collected by <ref type="bibr" target="#b30">Sen et al. (2008)</ref>, then <ref type="bibr" target="#b25">Pei et al. (2020)</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Johnson-Lindenstrauss Theorem</head><p>Theorem A.1 (Johnson-Lindenstrauss Theorem <ref type="bibr" target="#b10">(Dasgupta and Gupta, 2003)</ref>). For any 0 &lt; &lt; 1 and any integer n, let k be a positive integer such that</p><formula xml:id="formula_19">k ? 4( 2 /2 ? e 3 /3) ?1 ln n<label>(21)</label></formula><p>Then for any set S of n points in R d , there is a map f : R d ?? R k such that for all u, v ? S,</p><formula xml:id="formula_20">(1 ? ) u ? v 2 ? f (u) ? f (v) 2 ? (1 + ) u ? v 2 .<label>(22)</label></formula><p>Furthermore, this map can be found in randomized polynomial time.</p><p>We refer the readers to <ref type="bibr" target="#b10">Dasgupta and Gupta (2003)</ref> for proof. In this work, we adopt a side result of their proof, which shows the projection f = R x is an instance of such mapping that satisfies Equation 22, where R is a matrix of random Gaussian variables with zero mean. <ref type="bibr" target="#b0">Achlioptas (2003)</ref> further extends the proof and shows that a random matrix drawn from [1, 0, ?1], or [1, ?1] also satisfies the theorem. We leave these projections for future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Proofs</head><p>We recall Lemma 1:</p><p>Lemma Proof. With a slight abuse of notation, we use ?( * ) = ? 1 , ? 2 , ... to denote eigenvalues of a matrix. The spectral radius of a matrix is the largest absolute value of its eigenvalues ?( * ) = max (|? ( * ) |). L is the normalized Laplacian matrix of G, therefore 0 ? ?(L) ? 2. According to eigenvalue properties, we have  In another word, ?(I ? T ) &lt; 1. Gelfand's formula shows that if ?(I ? T ) &lt; 1, then lim p?? (I ? T ) p = 0 and the inverse of T can be expressed by a Neumann series T ?1 = ? p=0 (I ? T ) p .</p><p>We recall Lemma 2: Lemma 2. ?K &gt; 1, E[h den ] = 0.5 for the Erdos-Renyi random graph G(n, p). (A proof is given in Appendix A.4.)</p><p>Proof. For each node label k of |Y k | nodes, the are at most |Y k |+1 2 = |Y k |(|Y k | + 1)/2 intra-class edges (including self-loops). For each pair of label (k, j), there are at most |Y k ||Y j | inter-class edges. On average G(n, p) has n+1 2 p edges, among which |Y k |+1 2 p = |Y k |(|Y k |+1)p 2 are intra-class for k, and |Y k ||Y j |p are inter-class for the class pair (k, j). Hence from Equation <ref type="formula" target="#formula_7">18</ref>   we minimize the square error between H ? R N ?1 in Equation 13 and the target pixel values, i.e.</p><formula xml:id="formula_21">L (?) = N i=1 (H i ? Y i ) 2</formula><p>where Y i is the target pixel value of node i. We train the models with 3000 iterations and stop early if the loss is not improving in 100 consecutive epochs.  <ref type="table" target="#tab_4">Table 3</ref> shows the square loss of our method along MLP and 2 GNNs. Our method consistently outperforms other models. Some output images are shown in <ref type="figure">Figure 8</ref>. As expected, MLP fails to learn the frequency pattern across all three categories. GCN, GAT and GIN are able to learn the low-pass pattern but failed in learning the band and high-frequency patterns. Although ChevNet shows comparable results in the high-pass task, it is achieved with 41,537 trainable parameters while our method only requires 2,050 parameters. Lastly, our method is the only one that can learn and accurately resemble the band-pass image, demonstrating a better flexibility in learning frequency patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Code</head><p>The code of our experiments is released under the MIT license at https://anonymous.4open. science/r/graph_restructure-1BBB. <ref type="figure">Figure 8</ref>: The low-pass, band-pass and high-pass images learned using MLP, GIN, ChevNet and our method. The images learned using our method better resemble the images shown in <ref type="figure" target="#fig_10">Figure 7</ref> across all three categories.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Node clusters using different eigenvector choices on Wisconsin<ref type="bibr" target="#b25">(Pei et al., 2020)</ref> and Europe Airpor<ref type="bibr" target="#b27">(Ribeiro et al., 2017;</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A set of slicers with s = 20 that divide the Laplacian spectrum range [0, 2] into equal-width pseudoeigenvalues.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Graph restructuring workflow using learnable spectral clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>the proportion of edges connecting nodes of the same label h edge = |(u,v)?E:yu=yv| |E| , while the later uses the proportion of a node's direct neighbours of the same label h node = 1 N u?V |v?Nu:yu=yv| |Nu|</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Homophily and accuracy of GCN as per edges numbers. The optimal number of edges are chosen based on h den on validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>classifies nodes into 5 classes according to their average traffic. ACTOR is a graph of actor co-occurrence in films based on Wikipedia, modified by<ref type="bibr" target="#b25">Pei et al. (2020)</ref> based on<ref type="bibr" target="#b32">Tang et al. (2009)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Node classification accuracy of GNNs with and without restructuring on heterophilic graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>and a ? [0, 2]. Because the power of the eigenvalues of a matrix is the eigenvalues of the matrix power, i.e. ?( * 2m ) = ?( * ) 2m , we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>we have E[d k ] = p , from Equation 19 we have E[d kj ] = p. Substitute d k and d kj in Equation 17 we have E[d den ] = 0 and E[d den ] = 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>The original image and the filtered output using low-pass, band-pass and high-pass filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Node classification accuracy. The ? results are obtained from<ref type="bibr" target="#b25">Pei et al. (2020)</ref>. The * results are obtained from<ref type="bibr" target="#b43">Zhu et al. (2020)</ref>. ? 2.6 (?10.2) 57.6 ? 4.1 (?1.6) 57.8 ? 4.1 (?6.3) GCN (ours) 36.2 ? 1.0 (+5.5) 66.9 ? 3.1 (+7.1) 55.7 ? 2.4 (+18.8) 83.1 ? 3.2 (+19.0) 79.2 ? 6.3 (+20.0) 78.4 ? 5.4 (+14.3)</figDesc><table><row><cell></cell><cell>ACTOR</cell><cell>CHAMELEON</cell><cell>SQUIRREL</cell><cell>WISCONSIN</cell><cell>CORNELL</cell><cell>TEXAS</cell></row><row><cell>GCN</cell><cell>30.7 ? 0.5</cell><cell>59.8 ? 2.6</cell><cell>36.9 ? 1.3</cell><cell>64.1 ? 6.3</cell><cell>59.2 ? 3.2</cell><cell>64.1 ? 4.9</cell></row><row><cell cols="5">GCN (GDC) 53.9 CHEV 35.0 ? 0.5 (+4.3) 62.2 ? 1.2 (+2.4) 45.3 ? 1.3 (+8.4) 34.5 ? 1.3 66.0 ? 2.3 39.6 ? 3.0 82.5 ? 2.8</cell><cell>76.5 ? 9.4</cell><cell>79.7 ? 5.0</cell></row><row><cell>CHEV(GDC)</cell><cell cols="2">35.0 ? 0.6 (+0.5) 63.0 ? 1.1 (?3.0)</cell><cell>48.2 ? 0.7 (+8.6)</cell><cell>83.5 ? 2.9 (+1.0)</cell><cell cols="2">81.1 ? 3.2 (+4.6) 79.2 ? 3.0 (?0.5)</cell></row><row><cell>CHEV(Ours)</cell><cell cols="2">36.0 ? 1.1 (+1.5) 66.8 ? 1.8 (+0.8)</cell><cell cols="2">55.0 ? 2.0 (+15.4) 84.3 ? 3.2 (+1.8)</cell><cell>80.8 ? 4.1 (+4.3)</cell><cell>80.0 ? 4.8 (+0.3)</cell></row><row><cell>ARMA</cell><cell>34.9 ? 0.8</cell><cell>62.1 ? 3.6</cell><cell>47.8 ? 3.5</cell><cell>78.4 ? 4.6</cell><cell>74.9 ? 2.9</cell><cell>82.2 ? 5.1</cell></row><row><cell>ARMA (GDC)</cell><cell cols="2">35.9 ? 0.5 (+1.0) 60.2 ? 0.6 (?1.9)</cell><cell>47.8 ? 0.8 (+0.0)</cell><cell>79.8 ? 2.6 (+1.4)</cell><cell cols="2">78.4 ? 4.1 (+3.5) 78.4 ? 3.2 (?3.8)</cell></row><row><cell>ARMA (ours)</cell><cell cols="2">35.2 ? 0.7 (+0.3) 68.4 ? 2.3 (+6.3)</cell><cell>55.6 ? 1.7 (+7.8)</cell><cell>84.5 ? 0.3 (+6.1)</cell><cell>81.1 ? 6.1 (+6.2)</cell><cell>81.1 ? 4.2 (?1.1)</cell></row><row><cell>GAT</cell><cell>25.9 ? 1.8</cell><cell>54.7 ? 2.0</cell><cell>30.6 ? 2.1</cell><cell>62.0 ? 5.2</cell><cell>58.9 ? 3.3</cell><cell>60.0 ? 5.7</cell></row><row><cell>GAT (GDC)</cell><cell cols="2">35.0 ? 0.6 (+9.1) 63.8 ? 1.2 (+9.1)</cell><cell cols="4">48.6 ? 2.1 (+18.0) 51.4 ? 4.5 (?10.6) 58.9 ? 2.2 (+0.0) 77.1 ? 8.3 (+17.1)</cell></row><row><cell>GAT (ours)</cell><cell cols="6">35.6 ? 0.7 (+9.7) 66.5 ? 2.6 (+11.8) 56.3 ? 2.2 (+25.7) 84.3 ? 3.7 (+22.3) 81.9 ? 5.4 (+23.0) 79.8 ? 4.3 (+19.8)</cell></row><row><cell>SGC</cell><cell>28.7 ? 1.2</cell><cell>33.7 ? 3.5</cell><cell>46.9 ? 1.7</cell><cell>51.8 ? 5.9</cell><cell>58.1 ? 4.6</cell><cell>58.9 ? 6.1</cell></row><row><cell>SGC (GDC)</cell><cell cols="3">34.3 ? 0.6 (+5.6) 60.6 ? 1.5 (+26.9) 51.4 ? 1.6 (+4.5)</cell><cell>53.7 ? 5.1 (+1.9)</cell><cell cols="2">56.2 ? 3.8 (?1.9) 60.3 ? 6.3 (+1.4)</cell></row><row><cell>SGC (ours)</cell><cell cols="3">34.9 ? 0.7 (+6.2) 67.1 ? 2.9 (+33.4) 52.3 ? 2.3 (+5.4)</cell><cell cols="3">77.8 ? 4.7 (+26.0) 73.5 ? 4.3 (+15.4) 74.4 ? 6.0 (+15.5)</cell></row><row><cell>APPNP</cell><cell>35.0 ? 1.4</cell><cell>45.3 ? 1.6</cell><cell>31.0 ? 1.6</cell><cell>81.2 ? 2.5</cell><cell>70.3 ? 9.3</cell><cell>79.5 ? 4.6</cell></row><row><cell>APPNP (GDC)</cell><cell cols="2">35.7 ? 0.5 (+0.7) 52.3 ? 1.4 (+7.0)</cell><cell>40.5 ? 0.8 (+9.5)</cell><cell>80.2 ? 2.4 (?1.0)</cell><cell cols="2">77.8 ? 3.5 (+7.5) 76.2 ? 4.6 (?3.3)</cell></row><row><cell>APPNP (ours)</cell><cell cols="4">35.9 ? 1.1 (+0.9) 66.7 ? 2.7 (+21.4) 55.9 ? 2.9 (+24.9) 84.3 ? 4.2 (+3.1)</cell><cell cols="2">81.6 ? 5.4 (+11.3) 80.3 ? 4.8 (+0.8)</cell></row><row><cell>GPRGNN</cell><cell>33.4 ? 1.4</cell><cell>64.4 ? 1.6</cell><cell>41.9 ? 2.2</cell><cell>85.5 ? 5.0</cell><cell>79.5 ? 7.0</cell><cell>84.6 ? 4.0</cell></row><row><cell cols="3">GPRGNN (GDC) 34.4 ? 1.0 (+1.0) 61.9 ? 1.7 (?2.5)</cell><cell>39.2 ? 1.5 (?1.7)</cell><cell>85.1 ? 5.0 (?0.4)</cell><cell>82.4 ? 4.7 (+2.9)</cell><cell>80.8 ? 4.9 (?3.8)</cell></row><row><cell>GPRGNN (ours)</cell><cell cols="2">34.1 ? 1.1 (+0.7) 65.5 ? 2.2 (+1.1)</cell><cell>47.1 ? 2.4 (+5.2)</cell><cell>85.1 ? 4.1 (?0.4)</cell><cell cols="2">80.3 ? 6.3 (+0.8) 84.3 ? 5.1 (?0.3)</cell></row><row><cell>Geom-GCN  ? H2GCN  *</cell><cell>31.6 35.9 ? 1.0</cell><cell>60.9 59.4 ? 2.0</cell><cell>38.1 37.9 ? 2.0</cell><cell>64.1 86.7 ? 4.7</cell><cell>60.8 82.2 ? 6.0</cell><cell>67.6 84.9 ? 6.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Dataset detailsFigure 6shows the node classification accuracy fromTable 1. The performance of classical GNNs before and after restructuring are plotted as histograms. Performance of three GNNs that target less-homophilic datasets, Geom-GCN, H 2 GCN and GPRGNN, are plotted as horizontal lines.</figDesc><table><row><cell></cell><cell></cell><cell>Dataset</cell><cell cols="2">Classes Nodes</cell><cell>Edges</cell><cell>Features</cell></row><row><cell></cell><cell></cell><cell>CHAMELEON</cell><cell>5</cell><cell>2,277</cell><cell>36,101</cell><cell>2,325</cell></row><row><cell></cell><cell></cell><cell>SQUIRREL</cell><cell>5</cell><cell cols="2">5,201 217,073</cell><cell>2,089</cell></row><row><cell></cell><cell></cell><cell>ACTOR</cell><cell>5</cell><cell>7,600</cell><cell>26,752</cell><cell>931</cell></row><row><cell></cell><cell></cell><cell>TEXAS</cell><cell>5</cell><cell>183</cell><cell>325</cell><cell>1,703</cell></row><row><cell></cell><cell></cell><cell>CORNELL</cell><cell>5</cell><cell>183</cell><cell>298</cell><cell>1,703</cell></row><row><cell></cell><cell></cell><cell>WISCONSIN</cell><cell>5</cell><cell>251</cell><cell>515</cell><cell>1,703</cell></row><row><cell cols="3">A.2 Experiment Results</cell><cell></cell><cell></cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy (%)</cell><cell>40 50 60</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30</cell><cell>GCN GAT APPNP SGC CHEV ARMA</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1. For all? &gt; 2s 2m s 2m ?1 ? 2, the inverse of T = L?aI</figDesc><table><row><cell>2+?</cell><cell>2m</cell><cell>+ I s 2m can be expressed by a</cell></row><row><cell cols="3">Neumann series with guaranteed convergence. (A proof is given in Appendix A.4.)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Sum of squared errors. All</figDesc><table><row><cell>Task</cell><cell>MLP GCN</cell><cell>GIN</cell><cell cols="3">GAT ChevNet Ours</cell></row><row><cell>Low-pass</cell><cell>43.42 5.79</cell><cell>1.44</cell><cell>2.30</cell><cell>0.17</cell><cell>0.07</cell></row><row><cell cols="4">Band-pass 71.81 74.31 46.80 74.04</cell><cell>27.70</cell><cell>2.94</cell></row><row><cell cols="4">High-pass 19.95 24.74 17.80 24.57</cell><cell>2.16</cell><cell>1.36</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Graph Restructuring Algorithm</head><p>The restructuring algorithm is illustrated in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Graph Restructuring Algorithm</head><p>Input: Graph G, homophily metric h, number of random sample P , spectrum band length s, edge step size n Output: Restructured adjacency matrix?</p><p>compute the new homophily score end while return A + A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Spectral Expressive Power</head><p>In this section, we analyze the ability of the learnable spectral clustering to learn specific frequency patterns. Being able to adjust to different frequency patterns, to an extent, demonstrates the express power of a model in the spectral domain. As pointed out by <ref type="bibr">Balcilar et al. (2021)</ref>, the majority of GNNs are limited to only low-pass filters and thus have limited expressive power, while only a few are able to capture high-pass and band-pass patterns.</p><p>To evaluate this, we adopt the experimental setup of <ref type="bibr">Balcilar et al. (2021)</ref> using filtered images. A real 100x100 image is filtered by three pre-defined low-pass, band-pass and high-pass filters: ? 1 (?) = exp ?100? 2 , ? 2 (?) = exp ?1000(? ? 0.5) 2 and ? 3 (?) = 1 ? exp ?10? 2 , where ? = ? 2 1 + ? 2 2 and ? 1 and ? 2 are the normalized frequencies in each direction of an image. The original image and the three filtered versions are shown in <ref type="figure">Figure 7</ref>. The task is framed as a node regression problem, where</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Database-friendly random projections: Johnson-Lindenstrauss with binary coins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Achlioptas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="671" to="687" />
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analyzing the expressive power of graph neural networks in a spectral perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammet</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Renton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>H?roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Ga?z?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral clustering with graph neural networks for graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="874" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Graph neural networks with convolutional arma filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alippi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond low-frequency information in graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">GRAND: graph neural diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Rowbottom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">I</forename><surname>Gorinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML</title>
		<meeting>the 38th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Understanding and improving graph injection attack by promoting unnoticeability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaili</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
		<idno>abs/2202.08057</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generalized spectral clustering via gromov-wasserstein learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samir</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Needham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 24th International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>AISTATS</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An elementary proof of a theorem of johnson and lindenstrauss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Random Struct. Algorithms</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="65" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning graphs from data: A signal representation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorina</forename><surname>Thanou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="44" to="63" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PDE-GCN: Novel architectures for graph neural networks motivated by partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Eliasof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Treister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SLAPS: self-supervision improves structure learning for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahare</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed Mehran</forename><surname>Kazemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gralsp: Graph neural networks with local structural patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How to learn a graph from smooth signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><surname>Kalofolias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 19th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>AISTATS</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How to find your friendly neighborhood: Graph attention design with Self-Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongkwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations (ICLR</title>
		<meeting>the 5th International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 7th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13333" to="13345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep spectral clustering learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning, ICML. PMLR</title>
		<meeting>the 34th International Conference on Machine Learning, ICML. PMLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond Low-Pass filters: Adaptive feature propagation on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases. Research Track -European Conference, ECML PKDD</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Sijia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishnavi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omkar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>NeurIPS</publisher>
			<date type="published" when="2021-05" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Matched filtering for subgraph detection in dynamic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Beard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Bliss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Statistical Signal Processing Workshop (SSP)</title>
		<imprint>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Compressive spectral embedding: sidestepping the SVD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Ramasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Upamanyu</forename><surname>Madhow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Struc2vec: Learning node representations from structural identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">R</forename><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Flow smoothing and denoising: Graph signal processing in the edge-space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Schaub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Segarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="735" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Social influence analysis in large-scale networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning parametric dictionaries for signals on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorina</forename><surname>Thanou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="3849" to="3862" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning deep representations for graph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1293" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Accelerated spectral clustering using graph filtering of random signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Borgnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 6th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Community detection from low-rank excitations of a graph filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Ozdaglar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Scaglione</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jadbabaie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4044" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML)</title>
		<meeting>the 36th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Approximate matrix inversion for high-throughput data detection in the large-scale MIMO uplink</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Joseph R Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<date type="published" when="2013-05" />
			<biblScope unit="page" from="2155" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Two sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06462</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">GNNExplainer: Generating explanations for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">GraphSAINT: Graph sampling based inductive learning method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><forename type="middle">K</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>NeurIPS</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Improving robustness of graph neural networks with heterophily-inspired designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">T</forename><surname>Schaub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno>abs/2106.07767</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph neural networks with heterophily</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Anup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nedim</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nesreen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Spectral Clustering A simplified Spectral Clustering (SC) algorithm involves the following four steps: 1. Perform eigendecomposition for the Laplacian matrix to obtain eigenvalues</title>
		<imprint/>
	</monogr>
	<note>sorted in ascending order</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>? R L</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Perform K-means with a distance measurement, such as the Euclidean distance ||f i ? f j || or dot product similarity f T i f j , to partition the nodes into K clusters</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<title level="m">Low-pass: MLP (b) Low-pass: GIN (c) Low-pass: ChevNet (d) Low-pass: ours (e) Band-pass: MLP (f) Band-pass: GIN (g) Band-pass: ChevNet (h) Band-pass: ours (i) High-pass: MLP (j) High-pass: GIN (k) High-pass: ChevNet (l) High-pass: ours</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
