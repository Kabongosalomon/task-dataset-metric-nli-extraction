<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CelebV-HQ: A Large-Scale Video Facial Attributes Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Jiang</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CelebV-HQ: A Large-Scale Video Facial Attributes Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a) Appearance (c) Emotion (b) Action Fig. 1: Overview of CelebV-HQ. CelebV-HQ contains 35, 666 videos, including 15, 653 identities. Each video was manually labeled with 83 facial attributes, covering appearance, action, and emotion attributes.</p><p>Abstract. Large-scale datasets have played indispensable roles in the recent success of face generation/editing and significantly facilitated the advances of emerging research fields. However, the academic community still lacks a video dataset with diverse facial attribute annotations, which is crucial for the research on face-related videos. In this work, we propose a large-scale, high-quality, and diverse video dataset with rich facial attribute annotations, named the High-Quality Celebrity Video Dataset (CelebV-HQ). CelebV-HQ contains 35, 666 video clips with the resolution of 512 ? 512 at least, involving 15, 653 identities. All clips * Equal contribution. Corresponding author (wuwenyan0503@gmail.com). arXiv:2207.12393v1 [cs.CV] 25 Jul 2022 2 H. Zhu et al.</p><p>are labeled manually with 83 facial attributes, covering appearance, action, and emotion. We conduct a comprehensive analysis in terms of age, ethnicity, brightness stability, motion smoothness, head pose diversity, and data quality to demonstrate the diversity and temporal coherence of CelebV-HQ. Besides, its versatility and potential are validated on two representative tasks, i.e., unconditional video generation and video facial attribute editing. Furthermore, we envision the future potential of CelebV-HQ, as well as the new opportunities and challenges it would bring to related research directions. Data, code, and models are publicly available 4 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(a) Appearance (c) Emotion (b) Action Abstract. Large-scale datasets have played indispensable roles in the recent success of face generation/editing and significantly facilitated the advances of emerging research fields. However, the academic community still lacks a video dataset with diverse facial attribute annotations, which is crucial for the research on face-related videos. In this work, we propose a large-scale, high-quality, and diverse video dataset with rich facial attribute annotations, named the High-Quality Celebrity Video Dataset (CelebV-HQ). CelebV-HQ contains 35, 666 video clips with the resolution of 512 ? 512 at least, involving 15, 653 identities. All clips are labeled manually with 83 facial attributes, covering appearance, action, and emotion. We conduct a comprehensive analysis in terms of age, ethnicity, brightness stability, motion smoothness, head pose diversity, and data quality to demonstrate the diversity and temporal coherence of CelebV-HQ. Besides, its versatility and potential are validated on two representative tasks, i.e., unconditional video generation and video facial attribute editing. Furthermore, we envision the future potential of CelebV-HQ, as well as the new opportunities and challenges it would bring to related research directions. Data, code, and models are publicly available 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The rapid development of Generative Adversarial Networks (GANs) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b61">64,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b41">43]</ref> has demonstrably promoted advances in face generation and editing. This progress relies heavily on the contribution of large-scale datasets, e.g., CelebA <ref type="bibr" target="#b51">[53]</ref>, CelebA-HQ <ref type="bibr" target="#b40">[42]</ref> and FFHQ <ref type="bibr" target="#b42">[44]</ref>. These datasets, with high-quality facial images, have facilitated the development of a series of face generation and editing tasks, such as unconditional face generation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b61">64,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b72">75,</ref><ref type="bibr" target="#b91">94,</ref><ref type="bibr" target="#b70">73,</ref><ref type="bibr" target="#b37">39]</ref>, facial attribute editing <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b67">70,</ref><ref type="bibr" target="#b73">76,</ref><ref type="bibr" target="#b85">88]</ref> and neural rendering <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b58">61,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b58">61,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11]</ref>. However, most of these efforts are based on static image modality. In industry, with the booming development of mobile internet <ref type="bibr" target="#b14">[15]</ref> and mobile phone <ref type="bibr" target="#b88">[91]</ref>, video modality data begins to take a bigger and bigger share in customers' daily shootings <ref type="bibr">[36,</ref><ref type="bibr">56]</ref>. A well-suited dataset, which is capable of supporting the face generation and editing tasks in video modality, is eagerly asked.</p><p>Recent works <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b5">6]</ref> have shown that the scale and quality are essential factors for a facial dataset in image modality. A more sufficient utilization of largescale datasets would improve model generalization <ref type="bibr" target="#b64">[67]</ref>, while the quality of the dataset largely determines the limit of the generative models <ref type="bibr" target="#b61">[64,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b72">75,</ref><ref type="bibr" target="#b91">94,</ref><ref type="bibr" target="#b70">73]</ref>. In addition, facial attribute provides effective information to help researchers go more deeply into the face-related topics <ref type="bibr" target="#b51">[53,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b67">70]</ref>. However, the current public facial datasets consist of either static images with attribute labels <ref type="bibr" target="#b51">[53,</ref><ref type="bibr" target="#b40">42]</ref> or videos with insufficient scale <ref type="bibr" target="#b79">[82]</ref> and quality <ref type="bibr" target="#b56">[59,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Constructing a large-scale and high-quality face video dataset with diverse facial attribute's annotations is still an open question, given the challenges brought by the nature of video data. 1) Scale. The collected videos need to meet several requirements, such as temporal consistency, high-resolution and full-head. The strict standards together with the limited sources, make the expansion of dataset's scale both time and labor consuming. 2) Quality. The quality is not only reflected in the high fidelity and resolution, but also in the diverse and natural distribution of data samples. It asks for a well-designed data-filtering process to ensure all of the requirements of fidelity, resolution and data distribution. 3) Attribute Annotation. The coverage of the facial attribute set need to be sufficient to describe a human face thoroughly, both in the time-invariant and time-variant perspective. Also, the annotation process need to be accurate and highly efficient.</p><p>In order to tackle the challenges discussed above, we carefully device a procedure for dataset construction. First, to ensure the scale of the collected video, we build a large and diverse set of Internet queries that are built from the Wikipedia dataset <ref type="bibr" target="#b0">[1]</ref>, covering 11 languages, with 8376 entities, and 3717 actions. The designed queries cover a rich set of scenarios and thus successfully enable a huge raw data pool with millions of clips. Then, to filter out high-quality data from the raw data pool, we introduce an automatic pre-processing pipeline. In this pipeline, we leverage face detection and alignment tools to ensure the high fidelity and resolution. Also, with the huge raw data pool, after the pre-processing, we are still able to get a dataset with real-world distributions with large diversity. Finally, we propose a facial attributes set with extensive coverage, including appearance, action and emotion. To ensure the accuracy and efficiency of the annotation, we design a systematic attributes annotation process, including annotator training, automatic judgment and quality check steps.</p><p>To this end, we successfully conduct the High-Quality Celebrity Video (CelebV-HQ) Dataset, a large-scale, diverse, and high-quality video facial dataset with abundant attributes' annotations. CelebV-HQ contains 35, 666 in-the-wild video clips with the resolution of 512 ? 512 at least, involving 15, 653 person identities and 83 manually labeled facial attributes. Our careful labeling comprises a comprehensive set of face-related attributes, including 40 appearance attributes, 35 action attributes, and 8 emotion attributes. Samples on CelebV-HQ are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>After constructing the CelebV-HQ, we perform a comprehensive analysis of data distribution to demonstrate its statistical superiority to both image and video datasets. First, compared to image datasets with attribute annotations <ref type="bibr" target="#b51">[53,</ref><ref type="bibr" target="#b40">42]</ref>, CelebV-HQ has much higher resolution (2?) than CelebA <ref type="bibr" target="#b51">[53]</ref> and comparable scale to high-quality dataset <ref type="bibr" target="#b40">[42]</ref>. Also, by comparing CelebV-HQ with CelebA-HQ <ref type="bibr" target="#b40">[42]</ref> in the time-invariant aspects, we demonstrate that CelebV-HQ has a reasonable distribution on appearance and facial geometry. Furthermore, we compare CelebV-HQ with a representative video face dataset VoxCeleb2 <ref type="bibr" target="#b13">[14]</ref> in the time-variant aspects, such as temporal data quality, brightness variation, head pose distribution, and motion smoothness, which verify that CelebV-HQ has superior video quality.</p><p>Besides, to demonstrate the effectiveness and potential of CelebV-HQ, we evaluate representative baselines in two typical tasks: unconditional video generation and video facial attribute editing. For the task of unconditional video generation, we train state-of-the-art unconditional video GANs <ref type="bibr" target="#b72">[75,</ref><ref type="bibr" target="#b91">94]</ref> on CelebV-HQ fullset and its subsets that divided by different actions. When trained on different subsets of CelebV-HQ, the corresponding actions can be successfully generated. Further, we explore the video facial attribute editing task using temporal constrained image-to-image baselines <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b12">13]</ref>. Thanks to the rich sequential information included in CelebV-HQ dataset, we show that a simple modification of current image-based methods can bring remarkable improvement in the temporal consistency of generated videos. The experiments conducted above empirically demonstrate the effectiveness of our proposed CelebV-HQ dataset.</p><p>In addition to face video generation/editing tasks we evaluated in this work, CelebV-HQ could potentially benefit the academic community in many other fields. For example, neural rendering (e.g., novel view synthesis <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b58">61]</ref> and 3d face generation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b58">61,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11]</ref>) and face analysis (e.g., attribute recognition <ref type="bibr" target="#b94">[97,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b39">41]</ref>, action recognition <ref type="bibr" target="#b78">[81,</ref><ref type="bibr" target="#b35">37]</ref>, emotion recognition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b45">47]</ref> and forgery detection <ref type="bibr" target="#b48">[50,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b101">104]</ref>). We finally provide several empirical insights during constructing CelebV-HQ dataset and make an exhaustive discussion of the potential of CelebV-HQ in research community.</p><p>In summary, our contributions are threefold: 1) We contribute the first largescale face video dataset, named CelebV-HQ, with high-quality video data and diverse manually annotated attributes. Corresponding to CelebA-HQ <ref type="bibr" target="#b40">[42]</ref>, CelebV-HQ fills in the blank on video modality and facilitates future research. 2) We perform a comprehensive statistical analysis in terms of attributes diversity and temporal statistics to show the superiority of CelebV-HQ. 3) We conduct extensive experiments on typical video generation/editing tasks, which demonstrates the effectiveness and potential of CelebV-HQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Video Face Generation and Editing</head><p>Recent advances in face video generation typically focused on unconditional video generation <ref type="bibr" target="#b77">[80,</ref><ref type="bibr" target="#b63">66,</ref><ref type="bibr" target="#b74">77,</ref><ref type="bibr" target="#b91">94,</ref><ref type="bibr" target="#b72">75,</ref><ref type="bibr" target="#b70">73]</ref> and conditional face video generation <ref type="bibr" target="#b84">[87,</ref><ref type="bibr" target="#b68">71,</ref><ref type="bibr" target="#b92">95,</ref><ref type="bibr" target="#b80">83,</ref><ref type="bibr" target="#b95">98,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b99">102,</ref><ref type="bibr" target="#b96">99,</ref><ref type="bibr" target="#b36">38]</ref>. Conventional unconditional video face generation <ref type="bibr" target="#b77">[80,</ref><ref type="bibr" target="#b63">66,</ref><ref type="bibr" target="#b74">77,</ref><ref type="bibr" target="#b91">94]</ref> are based on GANs <ref type="bibr" target="#b23">[24]</ref>. These models usually decompose the latent code into content and motion codes to control the corresponding signals. Some recent efforts <ref type="bibr" target="#b72">[75,</ref><ref type="bibr" target="#b70">73]</ref> aimed to extend high-quality pre-trained image generators to a video version to exploit the rich prior information. Conditional face video generation mainly including face reenactment <ref type="bibr" target="#b84">[87,</ref><ref type="bibr" target="#b68">71,</ref><ref type="bibr" target="#b92">95,</ref><ref type="bibr" target="#b80">83]</ref> and talking face generation <ref type="bibr" target="#b95">[98,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b99">102,</ref><ref type="bibr" target="#b96">99,</ref><ref type="bibr" target="#b36">38]</ref>. The motivation of these tasks is to use visual and audio modalities to guide the motion of a face video.</p><p>Face video editing is another emerging field <ref type="bibr" target="#b89">[92,</ref><ref type="bibr" target="#b75">78]</ref>. The common characteristic of these works is to edit face attributes on the StyleGAN <ref type="bibr" target="#b43">[45]</ref> latent space. Nevertheless, due to the lack of large-scale high-quality video datasets, these video-based editing efforts are still trained on images, by exploiting the rich information of a pre-trained image model <ref type="bibr" target="#b43">[45]</ref>. This leads to the main problem they have to solve to improve temporal consistency. Therefore, with such a face video dataset proposed, there is much room for improvement in current video face generation and editing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Face Datasets</head><p>Face datasets can be divided into two categories: image datasets and video datasets. First, many face image datasets are initially proposed for face recogni- tion, like LFW <ref type="bibr" target="#b32">[33]</ref> and CelebFaces <ref type="bibr" target="#b71">[74]</ref> which largely promote the development of related fields. To analyze facial attributes, datasets like CelebA and LFWA <ref type="bibr" target="#b51">[53]</ref> have been proposed. Both of them have 40 facial attribute annotations and have advanced the research field to a finer level of granularity. CelebA-HQ <ref type="bibr" target="#b40">[42]</ref> improves 30k images in CelebA to 1024?1024 resolution. CelebAMask-HQ <ref type="bibr" target="#b44">[46]</ref> further labels 19 classes of segmentation masks. CelebA-Dialog <ref type="bibr" target="#b38">[40]</ref> labels captions describing the attributes. In addition to the above image datasets, many video datasets have also been released. CelebV <ref type="bibr" target="#b84">[87]</ref> was proposed for face reenactment, and it contains five long videos collected from the Internet. Audiovisual datasets such as VoxCeleb <ref type="bibr" target="#b56">[59]</ref> and VoxCeleb2 <ref type="bibr" target="#b13">[14]</ref> were originally released for speaker recognition, and further stimulated the development of audiovisual speaker separation and talking face generation domains. MEAD <ref type="bibr" target="#b79">[82]</ref> is the largest emotional video dataset, which includes 60 actors recorded from seven view directions. However, all of these datasets either contains only images with attribute annotations or are unlabeled videos with insufficient diversity. The rapidly growing demand for video facial attribute editing cannot be met. A video version of the dataset like CelebA-HQ <ref type="bibr" target="#b40">[42]</ref> is urgently needed to be proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CelebV-HQ Construction</head><p>The dataset lies the foundation for model training, and its quality greatly affects the downstream tasks. The principle of building CelebV-HQ is to reflect realworld distribution with large-scale, high-quality, and diverse video clips. Hence, we design a rigorous and efficient pipeline to construct CelebV-HQ dataset, including Data Collection, Data Pre-processing, and Data Annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>The data collection process consists of the following steps. We start by creating various queries in order to retrieve human videos that are diverse in content and rich in attributes. The queries are designed to include keywords of different categories such as celebrity names, movie trailers, street interviews and vlogs, all in different languages. Then, we use these queries to collect the raw videos from the Internet. During the collection process, we have several constraints to filter the unsatisfactory videos. For each query, we only collect the first 30 results to reduce duplicate human IDs, and the raw videos are required to have a resolution greater than 1080p with a normal bitrate. Consequently, we obtain a raw data pool with millions of video clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Pre-processing</head><p>In order to sample high-quality face video clips from the raw data pool, we develop an automatic video pre-processing pipeline. The pipeline is illustrated in Appendix A. First, we detect 98 facial landmarks <ref type="bibr" target="#b83">[86]</ref> from the video, and use these to extract the bounding boxes. Faces smaller than 450 pixels are filtered out. Then, we check whether the adjacent frames belong to the same person based on the motion <ref type="bibr" target="#b3">[4]</ref> and identity <ref type="bibr" target="#b15">[16]</ref>. If not, we will split the video into different clips. Next, given a sequence of bounding boxes, we calculate their minimum bounding rectangle. To reduce data loss, we expand the bounding rectangle smaller than 512 2 to this size, and use the bounding box to crop the original video. Finally, only clips longer than 3 seconds are kept. We choose 512 2 as the normalized resolution due to the following reasons. 1) The face regions of web videos usually do not reach the resolution of 1024 2 or higher, and it is difficult to obtain super high-resolution videos and ensure their diversity. Before the rescaling, the percentage of video resolution: 0.6% for 450 2 ?512 2 , 76.6% for 512 2 ?1024 2 , and 22.7% for 1024 2 +. 2) We need to make sure that all the videos are at the same resolution when training models. We choose 512 2 to ensure that all the clips are not upsampled significantly, which would affect the video quality.</p><p>Also, to meet different usage scenario, a tool is provided on our project page that offers options to keep the original resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Annotation</head><p>Data annotation is a core part of CelebV-HQ, and the annotation accuracy is vital. We first describe how we select the attributes to be annotated, then present the standard protocol of manual annotation. Attribute Selection. We decouple a face video into three factors, i.e., appearance, action, and emotion. Appearance describes the facial attributes that do not change along with the video sequence, such as hair color and gender. Action describes facial attributes that are related with video sequence, such as laugh and talk. Emotion describes the high-level mental status of human, such as neutral and happy. These three categories serve as important feature dimensions to characterize face video clips.</p><p>For appearance attributes, we derive most of the classes from CelebA <ref type="bibr" target="#b51">[53]</ref>. However, we find that three common attributes (i.e., "long hair", "sunglasses" and "wearing a mask") in real-world videos are not defined in CelebA <ref type="bibr" target="#b51">[53]</ref>. We add these three attributes to the appearance attributes as well. Meanwhile, some action-related attributes, such as "smiling" and "mouth slightly open", have been removed. This process yields 40 appearance attributes in total. For action attributes, inspired by Kinetics-700 <ref type="bibr" target="#b6">[7]</ref>, we select the face-related actions from its classes and add other facial actions from Internet tags to ensure that the final 35 attributes could cover common facial actions. For emotion attributes, we follow the 8 emotions designed in RAVESS <ref type="bibr" target="#b52">[54]</ref>, including neutral, anger, contempt, disgust, fear, happiness, sadness, and surprise. Note that the appearance and action attributes are all multi-label as the classes are not mutually exclusive, while emotion attributes are designed to be single-label. We append the complete list of all the attributes to the Appendix D. Attribute Annotation. To ensure the accuracy of the annotations, our entire annotation process includes the training of annotators, annotation, and quality control. Before the labeling begins, training courses are provided to help annotators understand each attribute and to have the same criteria for judging each attribute. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we first set up a Multi-label Annotation Table for each video, the table contains all labels that need to be labeled. Each video clip is independently annotated by 5 trained annotators. We design an Automatic Judgment strategy, when there is a discrepancy in the annotations of an attribute, we select the annotation that has been agreed the most. If the annotation is only marginally agreed (3 vs 2), the sample will be re-labeled. Finally, we additionally take a Quality Check process, in which the annotated data is further inspected by a professional quality inspector with research experiments in Computer Vision. If the annotated data does not meet the standard, it will also be re-labeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Statistics</head><p>In this section, we present the statistics of CelebV-HQ to demonstrate its statistical superiority. Then, we make comparison of CelebV-HQ with two most related and representative image and video datasets (i.e., CelebA-HQ <ref type="bibr" target="#b40">[42]</ref> and VoxCeleb2 <ref type="bibr" target="#b13">[14]</ref>) respectively, in which we verify that the proposed CelebV-HQ has a natural distribution and better quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Analysis of CelebV-HQ</head><p>CelebV-HQ consists of 35, 666 video clips of 3 to 20 seconds each, involving 15, 653 identities, with a total video duration of about 65 hours. For the facial attributes, CelebV-HQ not only has a wide distribution of time-invariant attributes (i.e., appearance), but also has rich samples of time-variant attributes (i.e., action and emotion).</p><p>As shown in <ref type="table">Table 1</ref>, compared to the image datasets that contain facial attribute annotations <ref type="bibr" target="#b51">[53,</ref><ref type="bibr" target="#b40">42]</ref>, the resolution of CelebV-HQ is more than twice that <ref type="table">Table 1</ref>: Face datasets comparison. The symbol "#" indicates the number. The abbreviations "Id.", "Reso.", "Dura.", "App.", "Act.", "Emo.", "Env.", and "Fmt." stand for Identity, Resolution, Duration, Appearance, Action, Emotion, Environment, and Format, respectively. The " * " denotes the estimated resolution.</p><p>Meta of CelebA <ref type="bibr" target="#b40">[42]</ref>, and has a comparable scale to the high-quality dataset, CelebA-HQ <ref type="bibr" target="#b51">[53]</ref>. More importantly, CelebV-HQ, as a video dataset, contains not only appearance attribute annotations, but also action and emotion attribute annotations, which make it contains richer information than image datasets. Other than the diverse annotations, compared to the recent in-the-wild video datasets (CelebV <ref type="bibr" target="#b84">[87]</ref>, FaceForensics <ref type="bibr" target="#b62">[65]</ref>, VoxCeleb <ref type="bibr" target="#b56">[59]</ref> and VoxCeleb2 <ref type="bibr" target="#b13">[14]</ref>), CelebV-HQ has a much higher resolution. Specifically, VoxCeleb2 <ref type="bibr" target="#b13">[14]</ref> and MEAD <ref type="bibr" target="#b79">[82]</ref>, as two representative face video datasets, are the largest audiovisual video face datasets under in-the-wild and lab-controlled environments respectively. Althgough the data volume of VoxCeleb2 <ref type="bibr" target="#b13">[14]</ref> and MEAD <ref type="bibr" target="#b79">[82]</ref> is relatively large, the videos on these two datasets are homogeneous and in limited distributions. The videos on VoxCeleb2 are mainly talking face, while MEAD was collected in a constrained laboratory environment. In contrast, CelebV-HQ is collected in real-world scenarios with a diverse corpus, making it more natural and rich in the distribution of attributes.</p><p>We start our analysis of CelebV-HQ with the attribute distribution. 1) CelebV-HQ contains a total of 40 appearance attributes, as shown in <ref type="figure" target="#fig_3">Fig. 3 (a)</ref>, of which 10 attributes account for more than 20% each, while there are more than 10 attributes accounting for about 10% each. Meanwhile, the overall attribute distribution has a long tail, with 10 attributes accounting for less than 3% each. We compare the hair colors separately, as they are mutually exclusive. From <ref type="figure" target="#fig_3">Fig. 3 (b)</ref>, the distribution in hair color is even, and there are no significant deviations. 2) There are diverse action attributes in CelebV-HQ as shown in <ref type="figure" target="#fig_3">Fig. 3 (c)</ref>. The common actions, such as "talk", "smile", and "head wagging", account for over 20% each. About 20 uncommon actions, such as "yawn", "cough" and "sneeze", account for less than 1% each. This result is in line with our expectation that these uncommon attributes remain open challenges for the academic community.</p><p>3) The proportion of emotion attributes also varies as shown in <ref type="figure" target="#fig_3">Fig. 3 (d)</ref>, with "neutral" accounting for the largest proportion, followed by  "happiness" and "sadness" emotions. Unlike the data collected in the laboratory, we do not strictly control the proportion of each attribute, so the overall distribution is more in line with the natural distribution. Overall, the CelebV-HQ is a real-world dataset with diverse facial attributes in a natural distribution, bringing new opportunities and challenges to the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Image Dataset</head><p>Due to CelebV-HQ can be considered as a video version of CelebA-HQ <ref type="bibr" target="#b40">[42]</ref> which is a commonly used dataset and its facial attributes annotation is successful in many works <ref type="bibr" target="#b66">[69,</ref><ref type="bibr" target="#b73">76,</ref><ref type="bibr" target="#b90">93,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b12">13]</ref>. We argue that a face video dataset that has similar distribution with CelebA-HQ would also effective.</p><p>To show a reasonable distribution of CelebV-HQ, we compare the proposed CelebV-HQ with CelebA-HQ <ref type="bibr" target="#b40">[42]</ref> in face attribute aspects such as age, ethnicity, and face shape. These factors reflect the basic face information in terms of facial appearance and geometry. Since ethnicity and age attributes are not explicitly labeled, we estimate them for both datasets using an off-the-shelf facial attribute analysis framework <ref type="bibr" target="#b65">[68]</ref>. Age distribution. We evaluate whether the dataset is biased towards certain age groups. From <ref type="figure" target="#fig_4">Fig. 4 (a)</ref>, we can see that the age in CelebA-HQ is mainly distributed below 35 years old, while the age distribution of CelebV-HQ is smoother.</p><p>Ethnic Distribution. The ethnic distribution roughly reflects the data distribution in terms of geography. As shown in <ref type="figure" target="#fig_4">Fig. 4 (b)</ref>, CelebV-HQ achieves a distribution close to CelebA-HQ <ref type="bibr" target="#b40">[42]</ref>, and has a more even distribution in Latino Hispanic, Asian, Middle-eastern, and African. As shown in <ref type="figure" target="#fig_4">Fig. 4 (c</ref>  Face Shape Ratio. In addition, the distribution of face shape ratio indicates the diversity of the dataset in terms of face types. Therefore, a simple analysis of face shape is proposed, where we calculate the ratio of a face using key points <ref type="bibr" target="#b83">[86]</ref> as shown in <ref type="figure" target="#fig_6">Fig. 5 (a)</ref>. The distance from the left and right of the cheeks to the nose is recorded as the width, and the distance from the highest point of the cheeks to the chin is recorded as the height. The width-to-height ratio is used as the definition of the face shape ratio. As shown in <ref type="figure" target="#fig_6">Fig. 5 (b)</ref>, CelebV-HQ has a more uniform distribution, which indicates that the samples in it have diverse face types.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Video Dataset</head><p>As stated before, VoxCeleb2 <ref type="bibr" target="#b13">[14]</ref> is one of the largest in-the-wild face video datasets, and it contains massive face videos, that have contributed to the development of many fields <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b100">103,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b95">98,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b99">102,</ref><ref type="bibr" target="#b96">99,</ref><ref type="bibr" target="#b36">38]</ref>. However, CelebV-HQ not only contains speech-based videos, we believe that if it is more diverse and of higher quality than the videos in VoxCeleb2 <ref type="bibr" target="#b13">[14]</ref>, this can be used to further improve the performance of the related models.</p><p>To demonstrate the superiority of CelebV-HQ and its ability to better support relevant studies, we compare with VoxCeleb2 <ref type="bibr" target="#b13">[14]</ref> in terms of the data quality and temporal smoothness. For temporal smoothness, we conduct a comprehensive evaluation in brightness, head pose, and motion movement. Data Quality. We use BRISQUE <ref type="bibr" target="#b54">[57]</ref> as a static quality evaluation metric, which is a non-reference evaluation algorithm. For each video clip, we average the BRISQUE <ref type="bibr" target="#b54">[57]</ref> value between frames. For the comparison of video quality distributions, we apply the VSFA <ref type="bibr" target="#b46">[48]</ref> measurement, a non-reference evaluation method that scores content dependency and temporal memory effects. The distributions are shown in <ref type="figure" target="#fig_7">Fig. 6</ref>, and CelebV-HQ offers higher quality than VoxCeleb2 <ref type="bibr" target="#b13">[14]</ref> at both image and video levels. Brightness Variation. We also compare video brightness variance distribution with VoxCeleb2 <ref type="bibr" target="#b13">[14]</ref>. We first obtain the brightness of each frame and compute the standard deviation in the temporal dimension. The brightness is calculated by averaging the pixels and then converting them to "perceived brightness" <ref type="bibr" target="#b4">[5]</ref>. The lower variance of brightness indicates more similar luminance within the video clip, i.e., better brightness uniformity. <ref type="figure" target="#fig_7">Fig. 6 (c)</ref> shows that CelebV-HQ contains more low variance videos, that demonstrates the brightness change during videos in CelebV-HQ is more stable in the temporal dimension. As shown in <ref type="figure" target="#fig_7">Fig. 6 (c)</ref>, CelebV-HQ contains videos of diverse brightness conditions, to further facilitate the usage of CelebV-HQ, we categorized the videos in terms of brightness, so that users can flexibly choose which samples to use. Head Pose Distribution. The head pose distribution is compared in two aspects: the average head pose of a video and the range of head pose movement. These two are used to show the diversity of head poses across the dataset and within the videos, respectively. As stated before, we leverage <ref type="bibr" target="#b83">[86]</ref> to detect the head pose in the yaw direction. As shown in <ref type="figure" target="#fig_8">Fig. 7 (a)</ref>, CelebV-HQ is more diverse and smoother than VoxCeleb2 <ref type="bibr" target="#b13">[14]</ref> in the overall distribution. From <ref type="figure" target="#fig_8">Fig. 7 (b)</ref>, we see that we have about 75% of the data with movements less than 30 ? , which means that most of the data are stable, while there are still 25% of movements between 30 ? and 100 ? , indicating the overall distribution is diverse. The illustration of average head pose and movement range is shown in <ref type="figure" target="#fig_8">Fig. 7 (c)</ref>. Action Unit Analysis. Facial Action Units (AUs) are the basic actions of a muscle or muscle group, and we use <ref type="bibr" target="#b19">[20]</ref> to detect AUs. The dataset is analyzed in both muscle movement richness and naturalness. <ref type="figure" target="#fig_9">Fig. 8 (a)</ref> shows that CelebV-HQ is more uniformly distributed over different AU values that represents action strength. The main reason is that videos in VoxCeleb2 <ref type="bibr" target="#b13">[14]</ref> are mainly talking videos, while CelebV-HQ consists of more types of facial actions. Meanwhile, the smoothness is measured by log dimensionless jerk <ref type="bibr" target="#b1">[2]</ref>. As shown in <ref type="figure" target="#fig_9">Fig. 8 (b)</ref>, CelebV-HQ is smoother than VoxCeleb2 <ref type="bibr" target="#b13">[14]</ref>, as we highlight with the "Mean value line". More AU results are presented in Appendix B.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In this section, we describe our experimental setups and the implementation details of baseline methods. We report the results on state-of-the-art baselines in two typical video generation/editing tasks, i.e., unconditional video generation and video facial attribute editing. The comprehensive evaluation shows the effectiveness and potential of our proposed CelebV-HQ dataset.  <ref type="figure">Fig. 9</ref>: Qualitative results of unconditional video generation. We present "Full set" and "Subset" settings of MocoGAN-HD <ref type="bibr" target="#b72">[75]</ref>, DIGAN <ref type="bibr" target="#b91">[94]</ref>, and StyleGAN-V <ref type="bibr" target="#b70">[73]</ref> respectively. CelebV-HQ is readily applicable to these unconditional video GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Unconditional Video Generation</head><p>Settings. We employ four unconditional video generation methods, i.e., VideoGPT <ref type="bibr" target="#b87">[90]</ref>, MoCoGAN-HD <ref type="bibr" target="#b72">[75]</ref>, DIGAN <ref type="bibr" target="#b91">[94]</ref>, and StyleGAN-V <ref type="bibr" target="#b70">[73]</ref>. We chose these methods based on their performance and code availability. Furthermore, since CelebV-HQ contains action annotations, the models are evaluated under two settings, i.e., the full set of data and the subsets split by different action attributes, e.g., smile. We strictly followed the original authors' setting. The videos generated by MoCoGAN-HD <ref type="bibr" target="#b72">[75]</ref> and StyleGAN-V <ref type="bibr" target="#b70">[73]</ref> are 256?256 resolution, and the one generated by VideoGPT <ref type="bibr" target="#b87">[90]</ref> and DIGAN <ref type="bibr" target="#b91">[94]</ref> are 128?128 resolution. To evaluate the model performance, we leverage FVD <ref type="bibr" target="#b76">[79]</ref> and FID <ref type="bibr" target="#b28">[29]</ref> to access video quality and image quality, respectively. Please refer to Appendix C.1 for more details. <ref type="table">Table 2</ref>: Quantitative results of unconditional video generation. We evaluate VideoGPT <ref type="bibr" target="#b87">[90]</ref>, MoCoGAN-HD <ref type="bibr" target="#b72">[75]</ref>, DIGAN <ref type="bibr" target="#b91">[94]</ref>, and StyleGAN-V [73] on different datasets and report the FVD and FID scores. "?" means a lower value is better.</p><p>FaceForensics <ref type="bibr" target="#b62">[65]</ref> Vox <ref type="bibr" target="#b56">[59]</ref> MEAD <ref type="bibr" target="#b79">[82]</ref> CelebV-HQ FVD (?) FID (?) FVD (?) FID (?) FVD (?) FID (?) FVD (?) FID (?) VideoGPT <ref type="bibr" target="#b87">[90]</ref> 185. <ref type="bibr" target="#b87">90</ref>   <ref type="table">Table 3</ref>: Quantitative results of video facial attribute editing. We evaluate two video facial editing baselines. The "Video" version achieves lower FVD scores and comparable FID performance than "Original". "?" means a lower value is better. Results. As shown in <ref type="figure">Fig. 9</ref>, we first focus on temporal consistency. MocoGAN-HD <ref type="bibr" target="#b72">[75]</ref>, DIGAN <ref type="bibr" target="#b91">[94]</ref>, and StyleGAN-V <ref type="bibr" target="#b70">[73]</ref> can generate consistent videos trained on CelebV-HQ. Besides, all methods can successfully produce the desire actions when trained on different subsets of CelebV-HQ with specific attributes. Satisfactory results achieved on the these state-of-the-art methods, demonstrating the effectiveness of CelebV-HQ.</p><p>Benchmark. We construct a benchmark of unconditional video generation task, for four currently prevalent models (VideoGPT <ref type="bibr" target="#b87">[90]</ref>, MoCoGAN-HD <ref type="bibr" target="#b72">[75]</ref>, DI-GAN <ref type="bibr" target="#b91">[94]</ref>, and StyleGAN-V [73]) on 4 face video datasets (FaceForensics <ref type="bibr" target="#b62">[65]</ref>, Vox <ref type="bibr" target="#b56">[59]</ref>, MEAD <ref type="bibr" target="#b79">[82]</ref> and CelebV-HQ). The benchmark is presented in <ref type="table">Table 2</ref>. Firstly, it can be observed that the ranking achieved by CelebV-HQ is similar to other prevalent datasets within different methods, which indicates the effectiveness of CelebV-HQ. In addition, the current video generation models <ref type="bibr" target="#b87">[90,</ref><ref type="bibr" target="#b72">75,</ref><ref type="bibr" target="#b91">94,</ref><ref type="bibr" target="#b70">73]</ref> obtained good FVD/FID metrics compared to the Vox <ref type="bibr" target="#b56">[59]</ref> dataset with similar data size. This illustrates that CelebV-HQ further exploits the potential of the current work, allowing it to generate more diverse and higher quality results. However, as a challenging real-world dataset, CelebV-HQ still has a considerable room for community to make improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Video Facial Attribute Editing</head><p>Settings. We employ two representative facial editing baselines, i.e., StarGAN-v2 <ref type="bibr" target="#b12">[13]</ref> and MUNIT <ref type="bibr" target="#b33">[34]</ref>, to explore the potential of CelebV-HQ on video facial attribute editing task. The canonical StarGAN-v2 <ref type="bibr" target="#b12">[13]</ref> and MUNIT <ref type="bibr" target="#b33">[34]</ref> are desiged for static image data. To evaluate the effectiveness of the proposed video dataset, we modify these models by simply adding a vanilla temporal constraint, i.e., estimating the optical flows for i-th frame and (i + t)-th frame in different domains by LiteFlowNet <ref type="bibr" target="#b34">[35]</ref> and enforcing L2 Loss between the flows. Other losses the original authors proposed remain unchanged. To demonstrate the prac- tical value of our dataset, we select a common used appearance attribute, i.e., "Gender", for different baselines.</p><p>Results. The baseline methods achieve good results when editing the Gender attribute. The main difference lies in the temporal consistency. In <ref type="figure" target="#fig_0">Fig. 10</ref>, we observe that the results generated by the original image models are sometimes unstable in the hair area. For instance, StarGAN-v2 suffers hair shape inconsistencies, and MUNIT produces jittering color block defects in hair generation. As reported in <ref type="table">Table 3</ref>, the "Video" version outperform the "Original" one with respect to the FVD metric in all cases (highlighted in blue). Furthermore, the "Video" version obtains comparable FID results as "Original". These results indicate that a simple modification using the temporal cues in video can bring performance enhancement, which demonstrate the effectiveness of the proposed dataset in video facial editing tasks. Please refer to the Appendix C.2 for more attributes results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Empirical Insights</head><p>Some empirical insights are drawn during the construction of CelebV-HQ and the baseline benchmarking. 1) We observe a trend in the growing demand for video facial editing due to the prevalence of short videos, e.g., TikTok [56] and Snapchat <ref type="bibr">[36]</ref>. However, as we stated before, current applications are mainly based on static images <ref type="bibr" target="#b53">[55,</ref><ref type="bibr">36]</ref>. Therefore, the research on transforming face editing from images to videos would be an emerging direction in the future.</p><p>2) An effective video alignment strategy is important for coherent video generation. In most image generation studies, faces are usually aligned by key points, which may be jittery if applied directly in video generation <ref type="bibr" target="#b92">[95]</ref>. On the other hand, the generation quality might degrade if faces in the video are not aligned. This suggests a new method that can simultaneously retain temporal information and align the face may improve the temporal consistency of generated videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Future Work</head><p>Finally, we envision the research areas that may benefit from CelebV-HQ. Video Generation/Editing. CelebV-HQ provides the possibility of improving Video Generation/Editing, such as unconditional face generation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b61">64,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b72">75,</ref><ref type="bibr" target="#b91">94,</ref><ref type="bibr" target="#b70">73]</ref>, text-to-video generation <ref type="bibr" target="#b49">[51,</ref><ref type="bibr" target="#b82">85,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b2">3]</ref>, video facial attributes editing <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b67">70,</ref><ref type="bibr" target="#b73">76,</ref><ref type="bibr" target="#b85">88]</ref>, face reenactment <ref type="bibr" target="#b84">[87,</ref><ref type="bibr" target="#b68">71,</ref><ref type="bibr" target="#b92">95,</ref><ref type="bibr" target="#b80">83]</ref>, and face swapping <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b98">101,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b57">60,</ref><ref type="bibr" target="#b86">89]</ref>. These tasks rely heavily on the scale and quality of the dataset for generalization. Moreover, some video generation methods still leverage the frame-level quality while neglecting the temporal information <ref type="bibr" target="#b69">[72,</ref><ref type="bibr" target="#b81">84,</ref><ref type="bibr" target="#b97">100]</ref>. Nevertheless, temporal modeling is essential for generating smooth and realistic videos, which deserves further investigation. Since CelebV-HQ also contains rich annotations of facial attributes, this would allow researcher to go deeper when using these information, e.g., synthesize text description with templates and learning disentanglement of facial attributes. Neural Rendering. CelebV-HQ has great potential for applications in Neural Rendering. Current tasks, such as novel view synthesis <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b58">61,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b50">52]</ref> and 3d generation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b58">61,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11]</ref>, are trained on in-the-wild image datasets <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b42">44]</ref> which lacks facial dynamics to provide natural geometries. CelebV-HQ, as a high-quality and large-scale video dataset, provides natural facial dynamics and diverse 3D geometries. These features on video modality could not only be further exploited to improve the quality of current models, but also stimulate the emerging of several budding topics, such as Dynamic NeRF <ref type="bibr" target="#b60">[63]</ref> and Animatable NeRF <ref type="bibr" target="#b59">[62]</ref>. Face Analysis. Face Analysis tasks, such as Attribute Recognition <ref type="bibr" target="#b94">[97,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b39">41]</ref>, Action Recognition <ref type="bibr" target="#b78">[81,</ref><ref type="bibr" target="#b35">37]</ref>, Emotion Recognition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b45">47]</ref>, Forgery Detection <ref type="bibr" target="#b48">[50,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b101">104]</ref>, and Multi-modal Recognition <ref type="bibr" target="#b93">[96,</ref><ref type="bibr" target="#b55">58]</ref>. These tasks usually require the dataset to have diverse attribute coverage and natural distribution.</p><p>CelebV-HQ not only meets these requirements, but also could helps to transfer previous image tasks, such as attribute recognition, to the video version by learning spatio-temporal representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose a large-scale, high-quality, and diverse video dataset with rich facial attributes, called CelebV-HQ. CelebV-HQ contains 35, 666 video clips involving 15, 653 identities, accompanied by 40 appearance attributes, 35 action attributes, and 8 emotion attributes. Through extensive statistical analysis of the dataset, we show the rich diversity of CelebV-HQ in terms of age, ethnicity, brightness, motion smoothness, pose diversity, data quality, etc. The effectiveness and future potential of CelebV-HQ are also demonstrated via the unconditional video generation and video facial attribute editing tasks. Finally, we provide an outlook on the future prospects of CelebV-HQ, which we believe can bring new opportunities and challenges to the academic community. In the future, we are going to maintain a continued evolution of CelebV-HQ, including the scale, quality and annotations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Statistic Comparisons</head><p>B.1 Comparison of appearance attribute statistics with CelebA-HQ As shown in <ref type="figure" target="#fig_1">Fig. A2</ref>, CelebV-HQ has a similar distribution to CelebA-HQ, and the distribution of most appearance attributes is close to that of CelebA-HQ. This indicates that there is no significant deviation in the distribution of CelebV-HQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Comparison of clip duration statistics with Vox</head><p>As reported in <ref type="figure" target="#fig_3">Fig. A3</ref>, the clip time distribution is shorter compared to Vox <ref type="bibr" target="#b56">[59]</ref> for ensuring video consistency and annotation accuracy. Also, the videos in CelebV-HQ are all less than 20s, this is because we truncate all the videos at 20s to avoid the attributes changing in the long video.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Additional AUs Distribution</head><p>We provide additional action units (AUs) distributions as shown in <ref type="figure" target="#fig_4">Fig.A4</ref>. <ref type="figure" target="#fig_4">Fig. A4</ref> (c) and (f) show the locations represented by the different AUs. In <ref type="figure" target="#fig_4">Fig. A4 (a)</ref> and (d), we can see that the action of CelebV-HQ is smoother than VoxCeleb2 <ref type="bibr" target="#b13">[14]</ref>. Meanwhile, <ref type="figure" target="#fig_4">Fig. A4 (b)</ref> and (e) suggest that CelebV-HQ is more evenly distributed at different AU values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 FVD/FID Setting Details</head><p>We leverage FID 5 <ref type="bibr" target="#b28">[29]</ref> and FVD 6 <ref type="bibr" target="#b76">[79]</ref> to assess the image and video quality of the video generation and editing models. As both metrics are sensitive to the amount of data in the test set, we first select 2048 videos randomly as our  <ref type="table">Table A1</ref>: Quantitative results of video facial attribute editing. We evaluate two video facial editing baselines. The "Video" version achieves lower FVD scores and comparable FID performance than "Original". "?" means a lower value is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StarGAN-v2 (Brown Hair) MUNIT (Eyeglasses) Metrics</head><p>Original test set. All videos in the test set are used as the "real" part in the metric experiments. For the unconditional generation, we also randomly generate 2048 videos as the "fake" part. For the editing of video facial attribution, we generate corresponding fake results for each real video, yielding 2048 fake videos as well.</p><p>To provide enough images for FID testing, we sample 4 frames from each video. In total, we have 8192 images for the real data and fake data respectively. For the FVD, we use all the real and generated videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Additional Video Facial Attribute Editing Results</head><p>To demonstrate the practical value of our dataset for facial attributes editing in low-level appearance attribute. We additional select "Brown Hair" attributes for StarGAN-v2 <ref type="bibr" target="#b12">[13]</ref>, as well as "Eyeglasses" attributes for MUNIT <ref type="bibr" target="#b33">[34]</ref>. The additional results are reported in <ref type="table">Table A1</ref> and <ref type="figure" target="#fig_6">Fig. A5</ref>. By simply adding a temporal regularization term, we improve the results of StarGAN-v2 <ref type="bibr" target="#b12">[13]</ref> and  MUNIT <ref type="bibr" target="#b33">[34]</ref> in terms of realism and coherence. Note that the temporal regularization is enabled by CelebV-HQ which contains rich annotations and facial dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Experiment on labeled Vox</head><p>We labeled Vox dataset <ref type="bibr" target="#b56">[59]</ref> using an open-source algorithm 7 . As reported in <ref type="table" target="#tab_4">Table A2</ref>, models trained on CelebV-HQ yields better performance. Experiment verified algorithmically labeling existing dataset is not suitable substitutes for CelebV-HQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Complete Attributes List</head><p>The complete list of all the attributes is reported in <ref type="table" target="#tab_5">Table A3</ref>. In (a), we edit the attribute brown hair with StarGAN-v2 <ref type="bibr" target="#b12">[13]</ref>. In (b), we edit the attribute eyeglasses with MUNIT <ref type="bibr" target="#b33">[34]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Overview of CelebV-HQ. CelebV-HQ contains 35, 666 videos, including 15, 653 identities. Each video was manually labeled with 83 facial attributes, covering appearance, action, and emotion attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The pipeline of data annotation. Each video clip is annotated by 5 annotators. When there is a large discrepancy in the annotations or the labeling results do not meet the standard of professional quality inspectors, these samples will be re-labeled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>The distributions of each attribute. CelebV-HQ has a diverse distribution on each attribute category. (Please zoom in for details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Distributions of age and ethnicity compared with CelebA-HQ<ref type="bibr" target="#b40">[42]</ref>. (a) and (b) show that CelebV-HQ has a similar distribution compared to CelebA-HQ<ref type="bibr" target="#b40">[42]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>), we show a random sample of each ethnic group in the CelebV-HQ. ? = Max( ./. ? 3 . , ./. ? 56. ) ? = Max( :/. ? &lt; . , 6= . ? :/. )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Distributions of head pose and face shape ratio compared with CelebA-HQ<ref type="bibr" target="#b40">[42]</ref>. CelebV-HQ contains more diverse head pose and face shape ratio distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Distributions of image and video quality, and brightness variance. (a) Image quality and (b) video quality are measured by BRISQUE<ref type="bibr" target="#b54">[57]</ref> and VSFA<ref type="bibr" target="#b46">[48]</ref>, the higher score, the better quality. (c) The video brightness is measured by<ref type="bibr" target="#b4">[5]</ref>, the low variance reflects the more stable in brightness aspect.(d) Samples at different brightness, with brightness values in the upper right corner. Average pose:~? ; Movement range: ~?A verage pose:~? ; Movement range: ~?( c) Examples of average pose and movement range (b) Distribution of movement range (a) Distribution of average pose</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Distributions of average head pose and movement range. There is a wide range of head movement in CelebV-HQ, including both stable videos (less than 20 ? of movement) and videos with significant movement (from 75 ? to 100 ? ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :</head><label>8</label><figDesc>Distribution and smoothness of action units. We evaluate the distribution (a) and smoothness (b) of action units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 :</head><label>10</label><figDesc>Qualitative results of video facial attribute editing. Results of "Original" tend to have a jittering in the hair area, while results of "Video" are more stable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. A1 :</head><label>A1</label><figDesc>Pipeline of data pre-process (a) We start from the bounding box detection for each frame. (b) A tracking framework<ref type="bibr" target="#b3">[4]</ref> is introduced to track different identities. (c) Given bounding box sequences (dotted orange boxes), we calculate their minimum bounding rectangles (blue box). If bounding rectangles smaller than 512?512, we expand it to this size (red box). (d) Finally, the videos are cropped using the bounding rectangles (blue/red boxes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>PercentFig. A2 :</head><label>A2</label><figDesc>Comparison of appearance attribute statistics with CelebA-HQ<ref type="bibr" target="#b40">[42]</ref>. Please zoom in for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. A3 :</head><label>A3</label><figDesc>Comparison of clip duration statistics with Vox [59].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. A4 :</head><label>A4</label><figDesc>(a) Action unit smoothness (AU10) (b) Action unit distribution (AU10) (c) Example of AU10 (d) Action unit smoothness (AU6) (e) Action unit distribution (AU6) (f) Example of AU6 Distributions of different AUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. A5 :</head><label>A5</label><figDesc>Qualitative results of video facial attribute editing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>) 323.71 244.58 295.74 232.63 204.12 158.87 FID (?) 77.26 64.82 89.07 69.68 30.65 31.23</figDesc><table><row><cell>Video Reference Label Reference Label</cell><cell>Original</cell><cell>Video</cell></row><row><cell>FVD (?</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table A2 :</head><label>A2</label><figDesc>Quantitative results of video facial attribute editing. Metrics Vox-labeled CelebV-HQ (Ours) Vox-labeled CelebV-HQ (Ours) Reference Label Reference Label Reference Label Reference Label FVD (?) 568.79 629.09 262.01 189.04 542.88 500.77 295.74 232.63 FID (?) 104.00 85.14 82.99 55.73 99.57 131.18 89.07 69.68</figDesc><table><row><cell>StarGAN-v2 (Gender)</cell><cell>StarGAN-v2 (Brown hair)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table A3 :</head><label>A3</label><figDesc>Complete attribute list. CelebV-HQ contains 83 annotations, including 40 appearance attributes, 35 action attributes, and 8 emotion attributes.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">(a) Appearance Attribute</cell><cell></cell><cell></cell><cell></cell></row><row><cell>blurry</cell><cell>male</cell><cell>young</cell><cell>chubby</cell><cell cols="2">pale skin rosy cheeks</cell><cell>oval face</cell><cell>receding hairline</cell></row><row><cell>bald</cell><cell>bangs</cell><cell>black hair</cell><cell>blond hair</cell><cell cols="2">gray hair brown hair</cell><cell>straight hair</cell><cell>wavy hair</cell></row><row><cell>long hair</cell><cell>arched eyebrows</cell><cell>bushy eyebrows</cell><cell cols="5">bags under eyes eyeglasses sunglasses narrow eyes big nose</cell></row><row><cell>pointy nose</cell><cell>high cheekbones</cell><cell>big lips</cell><cell cols="2">double chin no beard</cell><cell>5 o clock shadow</cell><cell>goatee</cell><cell>sideburns</cell></row><row><cell>mustache</cell><cell>heavy makeup</cell><cell>wearing earrings</cell><cell>wearing hat</cell><cell>wearing lipstick</cell><cell>wearing necklace</cell><cell>wearing necktie</cell><cell>wearing mask</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Action Attributes</cell><cell></cell><cell></cell><cell></cell></row><row><cell>blow</cell><cell>chew</cell><cell>close eyes</cell><cell>cough</cell><cell>cry</cell><cell>drink</cell><cell>eat</cell><cell>frown</cell></row><row><cell>gaze</cell><cell>glare</cell><cell>head wagging</cell><cell>kiss</cell><cell cols="4">laugh listen to music look around make a face</cell></row><row><cell>nod</cell><cell>play instrument</cell><cell>read</cell><cell>shake head</cell><cell>shout</cell><cell>sign</cell><cell>sing</cell><cell>sleep</cell></row><row><cell>smile</cell><cell>smoke</cell><cell>sneeze</cell><cell>sneer</cell><cell>sniff</cell><cell>talk</cell><cell>turn</cell><cell>weep</cell></row><row><cell>whisper</cell><cell>wink</cell><cell>yawn</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">(c) Emotion Attributes</cell><cell></cell><cell></cell><cell></cell></row><row><cell>neutral</cell><cell>anger</cell><cell>contempt</cell><cell>disgust</cell><cell>fear</cell><cell>happy</cell><cell>sadness</cell><cell>surprise</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Project page: https://celebv-hq.github.io/ Code and models: https://github.com/CelebV-HQ/CelebV-HQ</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/mseitzer/pytorch-fid 6 https://github.com/sihyun-yu/digan/tree/master/src/metrics</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/ewrfcas/face attribute classification pytorch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work is partly supported by Shanghai AI Laboratory and SenseTime Research. It is also supported by NTU NAP, MOE AcRF Tier 1 (2021-T1-001-088), and under the RIE2020 Industry Alignment Fund -Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Data Pre-processing</head><p>We provide a illustration of data pre-processing as shown in <ref type="figure">Fig. A1</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<idno>Ac- cessed: 2022-05-22. 3</idno>
		<ptr target="https://dumps.wikimedia.org/backup-index.html" />
		<title level="m">Wikimedia downloads</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A robust and sensitive metric for quantifying movement smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivakumar</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Melendez-Calderon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Burdet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TBE</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Text2live: Text-driven layered image and video editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Bar-Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dolev</forename><surname>Ofri-Amar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafail</forename><surname>Fridman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoni</forename><surname>Katen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02491</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Brightness calculation in digital image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bezryadin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Bourov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ilinih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TDPF</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A short note on the kinetics-700 human action dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arxiv:1907.06987</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><forename type="middle">Z</forename><surname>Eric R Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koki</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxiao</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Shalini De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallo</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<pubPlace>Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis</pubPlace>
		</imprint>
	</monogr>
	<note>et al. Efficient geometry-aware 3d generative adversarial networks. In CVPR, 2022. 2, 4</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Eric R Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kellnhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical crossmodal talking face generation with dynamic pixel-wise loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sem2nerf: Converting single-view semantic masks to neural radiance fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuedong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanxia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtan</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno>arxiv:2204.11798</idno>
		<title level="m">Generalizable neural performer: Learning robust radiance fields for human novel view synthesis</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stargan v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Voxceleb2: Deep speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Internet of things in industries: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li Da</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shancang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TII</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2233" to="2243" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Niannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A deep cascade network for unaligned face attribute classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human emotion recognition: Review of sensors and methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrius</forename><surname>Dzedzickis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Art?ras Kaklauskas, and Vytautas Bucinskas</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Looking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinatan</forename><surname>Hassidim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rubinstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Facial action unit intensity estimation via semantic correspondence learning with dynamic graph convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingruo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacqueline</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic neural radiance fields for monocular 4d facial avatar reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Gafni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Information bottleneck disentanglement for identity swapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gege</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyou</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualvoice: Audio-visual speech separation with cross-modal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stylenerf: A stylebased 3d aware generator for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ad-nerf: Audio driven neural radiance fields for talking head synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stavros Petridis, and Maja Pantic. Lips don&apos;t lie: A generalisable and robust approach to face forgery detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Haliassos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Vougioukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Heterogeneous face attribute estimation: A deep multi-task learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2597" to="2609" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><forename type="middle">A</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Cogvideo: Large-scale pretraining for text-to-video generation via transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.15868</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Headnerf: A real-time nerf-based parametric head model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">LiteFlowNet3: Resolving Correspondence Ambiguity for More Accurate Optical Flow Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<ptr target="https://www.snapchat.com/,2022.2" />
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Snap Inc. Snapchat</publisher>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vision-based human action recognition: An overview and real world challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imen</forename><surname>Jegham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anouar</forename><surname>Ben Khalifa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ihsen</forename><surname>Alouani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Ali</forename><surname>Mahjoub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forensic Science International: Digital Investigation</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Audio-driven emotional video portraits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deceive D: adaptive pseudo augmentation for GAN training with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Talkto-edit: Fine-grained facial editing via dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimmo</forename><surname>Karkkainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungseock</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2021</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Maskgan: Towards diverse and interactive facial image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Cheng-Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Context-aware emotion recognition networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Quality assessment of in-the-wild videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Faceshifter: Towards high fidelity and occlusion aware face swapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<idno>arxiv:1912.13457</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Face x-ray for more general face forgery detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Video generation from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Expressive talking head generation with granular audio-visual control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">A</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">196391</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">TikTok Pte. Ltd. Tiktok</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faceapp Technology Ltd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faceapp</surname></persName>
		</author>
		<ptr target="https://www.tiktok.com" />
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Anush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi-modal domain adaptation for finegrained action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Voxceleb: a large-scale speaker identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fsgan: Subject agnostic face swapping and reenactment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosi</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Stylesdf: High-resolution 3d-consistent image and geometry generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong Joon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Animatable neural radiance fields for modeling dynamic human bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangzhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">D-nerf: Neural radiance fields for dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enric</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno>arxiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Faceforensics: A large-scale video dataset for forgery detection in human faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>R?ssler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<idno>arxiv:1803.09179</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Temporal generative adversarial nets with singular value clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Adversarially robust generalization requires more data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Hyperextended lightface: A facial attribute analysis framework</title>
	</analytic>
	<monogr>
		<title level="m">ICEET, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning residual images for face attribute manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rujie</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Interfacegan: Interpreting the disentangled face representation learned by gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">First order motion model for image animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Lathuili?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Motion representations for articulated animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Skorokhodov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A good image generator is what you need for high-resolution video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Or Patashnik, and Daniel Cohen-Or. Designing an encoder for stylegan image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Tov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Nitzan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Mocogan: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rotem</forename><surname>Tzaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rinon</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bermano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen-Or</surname></persName>
		</author>
		<idno>arxiv:2201.08361, 2022. 4</idno>
		<title level="m">Stitch it in time: Gan-based facial editing of real videos</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Towards accurate generative models of video: A new metric &amp; challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
		<idno>arxiv:1812.01717</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Mead: A large-scale audio-visual dataset for emotional talking-face generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linsen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoqian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">One-shot free-view neural talking-head synthesis for video conferencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">One-shot free-view neural talking-head synthesis for video conferencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">N\&quot; uwa: Visual synthesis pre-training for neural visual world creation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12417</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yici</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Reenactgan: Learning to reenact faces via boundary transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Transeditor: Transformer-based dual-space GAN for highly controllable facial editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqin</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2022</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Mobilefaceswap: A lightweight framework for video face swapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<idno>arxiv:2201.03808</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Videogpt: Video generation using vq-vae and transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<idno>arxiv:2104.10157</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Analysis of the reasons and development of short video application-taking tik tok as an example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICISS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">A latent transformer for disentangled face editing in images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alasdair</forename><surname>Newson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Gousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Hellier</surname></persName>
		</author>
		<idno>ICCV, 2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A latent transformer for disentangled face editing in images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alasdair</forename><surname>Newson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Gousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Hellier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Generating videos with dynamics-aware implicit generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Aliaksandra Shysheya, and Victor Lempitsky. Fast bi-layer neural synthesis of one-shot realistic head avatars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Ivakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review. Information Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Nichele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Face attribute prediction using off-the-shelf cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICB</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Talking face generation by adversarially disentangled audio-visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Pose-controllable talking face generation by implicitly modularized audio-visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Pose-controllable talking face generation by implicitly modularized audio-visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Aot: Appearance optimal transport based identity swapping for forgery detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyou</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Aihua Zheng, and Ran He. Arbitrary talking face generation via attentional audio-visual coherence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Ai-Hua Zheng, and Ran He. Deep audio-visual learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man-Di</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJAC</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Face forgery detection by 3d decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
