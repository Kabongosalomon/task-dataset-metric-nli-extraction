<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">N-GCN: Multi-scale Graph Convolution for Semi-supervised Node Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
							<email>amol.kapoor@columbia.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
							<email>bperozzi@acm.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
							<email>joonseok@google.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Google Research New</orgName>
								<address>
									<settlement>York City</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">N-GCN: Multi-scale Graph Convolution for Semi-supervised Node Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Neural networks</term>
					<term>? Informa- tion systems ? Social networks</term>
					<term>KEYWORDS Graph, Convolution, Spectral, Semi-Supervised Learning, Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolutional Networks (GCNs) have shown significant improvements in semi-supervised learning on graph-structured data. Concurrently, unsupervised learning of graph embeddings has benefited from the information contained in random walks. In this paper, we propose a model: Network of GCNs (N-GCN), which marries these two lines of work. At its core, N-GCN trains multiple instances of GCNs over node pairs discovered at different distances in random walks, and learns a combination of the instance outputs which optimizes the classification objective. Our experiments show that our proposed N-GCN model improves state-of-the-art baselines on all of the challenging node classification tasks we consider: Cora, Citeseer, Pubmed, and PPI. In addition, our proposed method has other desirable properties, including generalization to recently proposed semi-supervised learning methods such as GraphSAGE, allowing us to propose N-SAGE, and resilience to adversarial input perturbations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Semi-supervised learning on graphs is important in many realworld applications, where the goal is to recover labels for all nodes given only a fraction of labeled ones. Some applications include social networks, where one wishes to predict user interests, or in health care, where one wishes to predict whether a patient should be screened for cancer. In many such cases, collecting node labels can be prohibitive. However, edges between nodes can be easier to obtain, either using an explicit graph (e.g. social network) or * Work was done while Amol was an intern at Google Research. He is returning to Google Research, full time, after completing his degree from Columbia University. implicitly by calculating pairwise similarities [e.g. using a patientpatient similarity kernel, <ref type="bibr" target="#b18">19]</ref>.</p><p>Convolutional Neural Networks <ref type="bibr" target="#b15">[16]</ref> learn location-invariant hierarchical filters, enabling significant improvements on Computer Vision tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23]</ref>. This success has motivated researchers <ref type="bibr" target="#b7">[8]</ref> to extend convolutions from spatial (i.e. regular lattice) domains to graph-structured (i.e. irregular) domains, yielding a class of algorithms known as Graph Convolutional Networks (GCNs).</p><p>Formally, we are interested in semi-supervised learning where we are given a graph G = (V, E) with N = |V | nodes; adjacency matrix A; and matrix X ? R N ?F of node features. Labels for only a subset of nodes V L ? V are observed. In general, |V L | ? |V |. Our goal is to recover labels for all unlabeled nodes V U = V ?V L , using the feature matrix X , the known labels for nodes in V L , and the graph G. In this setting, one treats the graph as the "unsupervised" and labels of V L as the "supervised" portions of the data.</p><p>Depicted in <ref type="figure" target="#fig_1">Figure 1</ref>, our model for semi-supervised node classification builds on the GCN module proposed by Kipf and Welling <ref type="bibr" target="#b13">[14]</ref>, which operates on the normalized adjacency matrix?, as in GCN(?), where? = D ? 1 2 AD ? 1 2 , and D is diagonal matrix of node degrees. Our proposed extension of GCNs is inspired by the recent advancements in random walk based graph embeddings [e.g. <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22]</ref>. We make a Network of GCN modules (N-GCN), feeding each module a different power of?, as in {GCN(? 0 ), GCN(? 1 ), GCN(? 2 ), . . . }. The k-th power contains statistics from the k-th step of a random walk on the graph. Therefore, our N-GCN model is able to combine information from various step-sizes (i.e. graph scales). We then combine the output of all GCN modules into a classification sub-network, and we jointly train all GCN modules and the classification sub-network on the upstream objective for semi-supervised node classification. Weights of the classification sub-network give us insight on how the N-GCN model works. For instance, in the presence of input perturbations, we observe that the classification sub-network weights shift towards GCN modules utilizing higher powers of the adjacency matrix, effectively widening the "receptive field" of the (spectral) convolutional filters. We achieve state-ofthe-art on several semi-supervised graph learning tasks, showing that explicit random walks enhance the representational power of vanilla GCN's.</p><p>The rest of this paper is organized as follows. Section 2 reviews background work that provides the foundation for this paper. In Section 3, we describe our proposed method, followed by experimental evaluation in Section 4. We compare our work with recent closely-related methods in Section 5. Finally, we conclude with our contributions and future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND 2.1 Semi-Supervised Node Classification</head><p>Traditional label propagation algorithms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref> learn a model that transforms node features into node labels and uses the graph to add a regularizer term:</p><formula xml:id="formula_0">L label.propagation = L classification (f (X ), V L ) + ? f (X ) T ?f (X ), (1)</formula><p>The first term L classification trains the model f : R N ?d 0 ? R N ?C to to predict the known labels V L . The second term is the graphbased regularizer, ensuring that connected nodes have a similar model output, with ? being the graph Laplacian and ? ? R is the regularization coefficient hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Convolutional Networks</head><p>Graph Convolution <ref type="bibr" target="#b7">[8]</ref> generalizes convolution from Euclidean domains to graph-structured data. Convolving a "filter" over a signal on graph nodes can be calculated by transforming both the filter and the signal to the Fourier domain, multiplying them, and then transforming the result back into the discrete domain. The signal transform is achieved by multiplying with the eigenvectors of the graph Laplacian. The transformation requires a quadratic eigendecomposition of the symmetric Laplacian; however, the low-rank approximation of the eigendecomposition can be calculated using truncated Chebyshev polynomials <ref type="bibr" target="#b11">[12]</ref>. For instance, <ref type="bibr" target="#b13">[14]</ref> calculates a rank-1 approximation of the decomposition. They propose a multilayer Graph Convolutional Networks (GCNs) for semi-supervised graph learning. Every layer computes the transformation:</p><formula xml:id="formula_1">H (l +1) = ? ? H (l ) W (l ) ,<label>(2)</label></formula><p>where H (l ) ? R N ?d l is the input activation matrix to the l-th hidden layer with row H (l )</p><p>i containing a d l -dimensional feature vector for vertex i ? V, and W (l ) ? R d l ?d l +1 is the layer's trainable weights. The first hidden layer H (0) is set to the input features X . A softmax on the last layer is used to classify labels. All layers use the same "normalized adjacency"?, obtained by the "renormalization trick" utilized by <ref type="bibr" target="#b13">[14]</ref>, as? = D ? 1 2 AD ? 1 2 . 1 <ref type="figure">Eq.</ref> (2) is a first order approximation of convolving filter W (l ) over signal H (l ) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>. The left-multiplication with? averages node features with their direct neighbors; this signal is then passed through a non-linearity function ? (?) (e.g, ReLU(z) = max(0, z)). Successive layers effectively diffuse signals from nodes to neighbors.</p><p>Two-layer GCN model can be defined in terms of vertex features X and normalized adjacency? as:</p><formula xml:id="formula_2">GCN 2-layer (?, X ; ? ) = softmax ? ? (?XW (0) )W (1) ,<label>(3)</label></formula><p>where the GCN parameters ? = W (0) ,W <ref type="bibr" target="#b0">(1)</ref> are trained to minimize the cross-entropy error over labeled examples. The output of the GCN model is a matrix R N ?C , where N is the number of nodes 1 with added self-connections added as A ii = 1, similar to <ref type="bibr" target="#b13">[14]</ref> and C is the number of labels. Each row contains the label scores for one node, assuming there are C classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Embeddings</head><p>Node Embedding methods represent graph nodes in a continuous vector space. They learn a dictionary Z ? R N ?d , with one d-dimensional embedding per node. Traditional methods use the adjacency matrix to learn embeddings. For example, Eigenmaps <ref type="bibr" target="#b4">[5]</ref> performs the following constrained optimization:</p><formula xml:id="formula_3">i, j ||A i j (Z i ? Z J )|| s.t. Z T DZ = I ,<label>(4)</label></formula><p>where I is identity vector. Skipgram models on text corpora <ref type="bibr" target="#b19">[20]</ref> inspired modern graph embedding methods, which simulate random walks to learn node embeddings <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>. Each random walk generates a sequence of nodes. Sequences are converted to textual paragraphs, and are passed to a word2vec-style embedding learning algorithm <ref type="bibr" target="#b19">[20]</ref>. As shown in <ref type="bibr" target="#b1">[2]</ref>, this learning-by-simulation is equivalent, in expectation, to the decomposition of a random walk co-occurrence statistics matrix D. The expectation on D can be written as:</p><formula xml:id="formula_4">E[D] ? E q?Q (T ) q = E q?Q D ?1 A q ,<label>(5)</label></formula><p>where T = D ?1 A is the row-normalized transition matrix (a.k.a right-stochastic adjacency matrix), and Q is a "context distribution" that is determined by random walk hyperparameters, such as the length of the random walk. The expectation therefore weights the importance of one node on another as a function of how wellconnected they are, and the distance between them. The main difference between traditional node embedding methods and random walk methods is the optimization criteria: the former minimizes a loss on representing the adjacency matrix A (see Eq. 4), while the latter minimizes a loss on representing random walk co-occurrence statistics D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR METHOD 3.1 Motivation</head><p>Graph Convolutional Networks and random walk graph embeddings are individually powerful. <ref type="bibr" target="#b13">[14]</ref> uses GCNs for semi-supervised node classification. Instead of following traditional methods that use the graph for regularization (e.g. Eq. 4), <ref type="bibr" target="#b13">[14]</ref> use the adjacency matrix for training and inference, effectively diffusing information across edges at all GCN layers (see Eq. 3). Separately, recent work has showed that random walk statistics can be very powerful for learning an unsupervised representation of nodes that can preserve the structure of the graph <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22]</ref>. Under special conditions, it is possible for the GCN model to learn random walks. In particular, consider a two-layer GCN defined in Eq. 3 with the assumption that first-layer activation is identity as ? (z) = z, and weight W (0) is an identity matrix (either explicitly set or learned to satisfy the upstream objective). Under these two identity conditions, the model reduces to:  <ref type="figure" target="#fig_1">Figure 1</ref>: Left: Model architecture, where? is the normalized normalized adjacency matrix, I is the identity matrix, X is node features matrix, and ? is matrix-matrix multiply operator. We calculate K powers of the?, feeding each power into r GCNs, along with X . The output of all K ? r GCNs can be concatenated along the column dimension, then fed into fully-connected layers, outputting C channels per node, where C is size of label space. We calculate cross entropy error, between rows prediction N ? C with known labels, and use them to update parameters of classification sub-network and all GCNs. Right: pre-relu activations after the first fully-connected layer of a 2-layer classification sub-network. Activations are PCA-ed to 50 dimensions then visualized using t-SNE.</p><formula xml:id="formula_5">GCN 2-layer-special (?, X ) = softmax ?? XW (1) = softmax ? 2 XW (1) I X? GCN ? GCN ? ? GCN ? ? . . . . . . . . . GCN GCN GCN GCN GCN GCN GCN ? ? GCN GCN . . . N C0 C1 C2 CK?1 r concatenate N K ?1 k =0 r Ck fully-connected N C (a) N-GCN Architecture (b) t-SNE</formula><p>where? 2 can be expanded as:</p><formula xml:id="formula_6">A 2 = D ? 1 2 AD ? 1 2 D ? 1 2 AD ? 1 2 = D ? 1 2 A D ?1 A D ? 1 2 = D ? 1 2 AT D ? 1 2<label>(6)</label></formula><p>By multiplying the adjacency A with the transition matrix T before, the GCN 2-layer-special is effectively doing a one-step random walk i.e. diffusing signals from nodes to neighbors, without non-linearities, then applying a non-linear Graph Conv layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Explicit Random Walks</head><p>The special conditions described above are not true in practice. Although stacking hidden GCN layers allows information to flow through graph edges, this flow is indirect as the information goes through feature reduction (matrix multiplication) and a non-linearity (activation function ? (?)). Therefore, the vanilla GCN cannot directly learn high powers of?, and could struggle with modeling information across distant nodes. We hypothesize that making the GCN directly operate on random walk statistics will allow the network to better utilize information across distant nodes, in the same way that node embedding methods (e.g. DeepWalk, <ref type="bibr" target="#b21">[22]</ref>) operating on D are superior to traditional embedding methods operating on the adjacency matrix (e.g. Eigenmaps, <ref type="bibr" target="#b4">[5]</ref>). Therefore, in addition to feeding only? to the GCN model as proposed by <ref type="bibr" target="#b13">[14]</ref> (see Eq. 3), we propose to feed a K-degree polynomial of? to K instantiations of GCN. Generalizing Eq. (6) to arbitrary power k gives:</p><formula xml:id="formula_7">A k = D ? 1 2 AT k ?1 D ? 1 2 .<label>(7)</label></formula><p>We also define? 0 to be the identity matrix. Similar to <ref type="bibr" target="#b13">[14]</ref>, we add self-connections and convert directed graphs to undirected ones, making? and hence? k symmetric matrices. The eigendecomposition of symmetric matrices is real. Therefore, the low-rank approximation of the eigendecomposition <ref type="bibr" target="#b11">[12]</ref> is still valid, and a one layer of <ref type="bibr" target="#b13">[14]</ref> utilizing? k should still approximate multiplication in the Fourier domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network of GCNs</head><formula xml:id="formula_8">Consider K instantiations of {GCN(? 0 , X ), GCN(? 1 , X ), . . . , GCN(? K ?1 , X )}.</formula><p>Each GCN outputs a matrix R N ?C k , where the v-th row describes a latent representation of that particular GCN for node v ? V, and C k is the latent dimensionality. Though C k can be different for each GCN, we set all C k to be the same for simplicity. We then combine the output of all K GCN and feed them into a classification sub-network, allowing us to jointly train all GCNs and the classification sub-network via backpropagation. This should allow the classification sub-network to choose features from the various GCNs, effectively allowing the overall model to learn a combination of features using the raw (normalized) adjacency, different steps of random walks (i.e. graph scales), and the input features X (as they are multiplied by identity? 0 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Fully-Connected Classification</head><p>Network. From a deep learning prospective, it is intuitive to represent the classification network as a fully-connected layer. We can concatenate the output of the K GCNs along the column dimension, i.e. concatenating all</p><formula xml:id="formula_9">GCN(X ,? k ), each ? R N ?C k into matrix ? R N ?C K where C K = k C k .</formula><p>We add a fully-connected layer f fc :</p><formula xml:id="formula_10">R N ?C K ? R N ?C ,</formula><p>with trainable parameter matrix W fc ? R C K ?C , written as:</p><formula xml:id="formula_11">N-GCN fc (?, A;W fc , ? ) = softmax (8) GCN(? 0 , X ; ? (0) ) GCN(? 1 , X ; ? (1) ) . . . W fc .</formula><p>The classifier parameters W fc are jointly trained with GCN parameters ? = {? (0) , ? <ref type="bibr" target="#b0">(1)</ref> , . . . }. We use subscript fc on N-GCN to indicate the classification network is a fully-connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Attention Classification Network.</head><p>We also propose a classification network based on "softmax attention", which learns a convex combination of the GCN instantiations. Our attention model (N-GCN a ) is parametrized by vector m ? R K , one scalar for each GCN. It can be written as:</p><formula xml:id="formula_12">N-GCN a (?, X ; m, ? ) = k m k GCN(? k , X ; ? (k) )<label>(9)</label></formula><p>where m is output of a softmax: m = softmax( m). This softmax attention is similar to "Mixture of Experts" model, especially if we set the number of output channels for all GCNs equal to the number of classes, as in C 0 = C 1 = ? ? ? = C. This allows us to add cross entropy loss terms on all GCN outputs in addition to the loss applied at the output NGCN, forcing all GCN's to be independently useful. It is possible to set the m ? R K parameter vector "by hand" using the validation split, especially for reasonable K such as K ? 6. One possible choice might be setting m 0 to some small value and remaining m 1 , . . . , m K ?1 to the harmonic series 1 k ; another choice may be linear decay K ?k K ?1 . These are respectively similar to the context distributions of GloVe <ref type="bibr" target="#b20">[21]</ref> and word2vec <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>. We note that if on average a node's information is captured by its direct or nearby neighbors, then the output of GCNs consuming lower powers of? should be weighted highly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>We minimize the cross entropy between our model output and the known training labels Y as:</p><formula xml:id="formula_13">min diag(V L ) Y ? log N-GCN(X ,?) ,<label>(10)</label></formula><p>where ? is Hadamard product, and diag(V L ) denotes a diagonal matrix, with entry at (i, i) set to 1 if i ? V L and 0 otherwise. In addition, we can apply intermediate supervision for the NGCN a to attempt make all GCN become independently useful, yielding minimization objective:</p><formula xml:id="formula_14">min m,? diag(V L ) Y ? log N-GCN a (?, X ; m, ? ) + k Y ? log GCN(? k , X ; ? (k ) ) .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">GCN Replication Factor r</head><p>To simplify notation, our N-GCN derivations (e.g. Eq. 8) assume that there is one GCN per? power. However, our implementation feeds every? to r GCN modules, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Generalization to other Graph Models</head><p>In addition to vanilla GCNs [e.g. <ref type="bibr" target="#b13">14]</ref>, our derivation also applies to other graph models including GraphSAGE <ref type="bibr">[SAGE,</ref><ref type="bibr" target="#b10">11]</ref>. Algorithm 1 shows a generalization that allows us to make a network of arbitrary graph models (e.g. GCN, SAGE, or others). Algorithms 2 and 3, respectively, show pseudo-code for the vanilla GCN <ref type="bibr" target="#b13">[14]</ref> and GraphSAGE 2 <ref type="bibr" target="#b10">[11]</ref>. Finally, Algorithm 4 defines our full Network of GCN model (N-GCN) by plugging Algorithm 2 into Algorithm 1. Similarly, Algorithm 5 defines our N-SAGE model by plugging Algorithm 3 in Algorithm 1. We can recover the original algorithms GCN <ref type="bibr" target="#b13">[14]</ref> and SAGE <ref type="bibr" target="#b10">[11]</ref>, respectively, by using Algorithms 4 (N-GCN) and 5 (N-SAGE) with r = 1, K = 1, identity ClassifierFn, and modifying line 2 in Algorithm 1 to P ??. Moreover, we can recover original DCNN <ref type="bibr" target="#b2">[3]</ref> by calling Algorithm 4 with L = 1, r = 1, modifying line 3 t? A ? D ?1 A, and keeping K &gt; 1 as their proposed model operates on the power series of the transition matrix i.e. unmodified random walks, like ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Datasets</head><p>We experiment on three citation graph datasets: Pubmed, Citeseer, Cora, and a biological graph: Protein-Protein Interactions (PPI). We choose the aforementioned datasets because they are available online and are used by our baselines. The citation datasets are prepared by <ref type="bibr" target="#b24">[25]</ref>, and the PPI dataset is prepared by <ref type="bibr" target="#b10">[11]</ref>. <ref type="table" target="#tab_0">Table 1</ref> summarizes dataset statistics.</p><p>Each node in the citation datasets represents an article published in the corresponding journal. An edge between two nodes represents a citation from one article to another, and a label represents the subject of the article. Each dataset contains a binary Bag-of-Words (BoW) feature vector for each node. The BoW are extracted from the article abstract. Therefore, the task is to predict the subject of articles, given the BoW of their abstract and the citations to other (possibly labeled) articles. Following <ref type="bibr" target="#b24">[25]</ref> and <ref type="bibr" target="#b13">[14]</ref>, we use 20 nodes per class for training, 500 (overall) nodes for validation, and 1000 nodes for evaluation. We note that the validation set is larger than training |V L | for these datasets! The PPI graph, as processed and described by <ref type="bibr" target="#b10">[11]</ref>, consists of 24 disjoint subgraphs, each corresponding to a different human tissue. 20 of those subgraphs are used for training, 2 for validation, and 2 for testing, as partitioned by <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head><p>For the citation datasets, we copy baseline numbers from <ref type="bibr" target="#b13">[14]</ref>. These include label propagation (LP, <ref type="bibr" target="#b25">[26]</ref>); semi-supervised embedding (SemiEmb, <ref type="bibr" target="#b23">[24]</ref>); manifold regularization (ManiReg, <ref type="bibr" target="#b6">[7]</ref>); skip-gram graph embeddings [DeepWalk 22]; Iterative Classification Algorithm [ICA, 18]; Planetoid <ref type="bibr" target="#b24">[25]</ref>; vanilla GCN <ref type="bibr" target="#b13">[14]</ref>. For PPI, we copy baseline numbers from <ref type="bibr" target="#b10">[11]</ref>, which include GraphSAGE with LSTM aggregation (SAGE-LSTM) and GraphSAGE with pooling aggregation (SAGE). Further, for all datasets, we use our implementation to run baselines DCNN <ref type="bibr" target="#b2">[3]</ref>, GCN <ref type="bibr" target="#b13">[14]</ref>, and SAGE [with pooling for k = 1 to K do <ref type="bibr">5:</ref> for i = 1 to r do <ref type="bibr">6:</ref> GraphModels.append(GraphModelFn(P, X , L)) 7: P ??P <ref type="bibr">8:</ref> return ClassifierFn(GraphModels) Algorithm 2 GCN Model <ref type="bibr" target="#b13">[14]</ref> Require:? is a normalization of A 1: function GcnModel(?, X , L)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2:</head><p>Z ? X 3:</p><formula xml:id="formula_15">for i = 1 to L do 4: Z ? ? (?ZW (i) ) 5: return Z Algorithm 3 SAGE Model [11]</formula><p>Require:? is a normalization of A 1: function SageModel(?, X , L)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2:</head><p>Z ? X 3:</p><formula xml:id="formula_16">for i = 1 to L do 4: Z ? ? ( Z?Z W (i) ) 5: Z ? L2NormalizeRows(Z ) 6: return Z Algorithm 4 N-GCN 1: function Ngcn(A, X , L = 2) 2: D ? diag(A1) ? Sum rows 3:? ? D ?1/2 AD ?1/2 4:</formula><p>return Network(GcnModel,?, X , L)</p><formula xml:id="formula_17">Algorithm 5 N-SAGE 1: function Nsage(A, X ) 2: D ? diag(A1) ? Sum rows 3:? ? D ?1 A 4:</formula><p>return Network(SageModel,?, X , 2) aggregation, 11], as these baselines can be recovered as special cases of our algorithm, as explained in Section 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation</head><p>We use TensorFlow <ref type="bibr" target="#b0">[1]</ref> to implement our methods, which we use to also measure the performance of baselines GCN, SAGE, and DCNN. For our methods and baselines, all GCN and SAGE modules that we train are 2 layers 3 , where the first outputs 16 dimensions per node and the second outputs the number of classes (dataset-dependent). DCNN baseline has one layer and outputs 16 dimensions per node, and its channels (one per transition matrix power) are concatenated into a fully-connected layer that outputs the number of classes. We use 50% dropout and L2 regularization of 10 ?5 for all of the aforementioned models. <ref type="table">Table 2</ref> shows node classification accuracy results. We run 20 different random initializations for every model (baselines and ours), train using Adam optimizer <ref type="bibr" target="#b3">[4]</ref> with learning rate of 0.01 for 600 steps, capturing the model parameters at peak validation accuracy to avoid overfitting. For our models, we sweep our hyperparameters r , K, and choice of classification sub-network ? {fc, a}. For baselines and our models, we choose the model with the highest 3 except as clearly indicated in <ref type="table">Table 4</ref> accuracy on validation set, and use it to record metrics on the test set in <ref type="table">Table 2</ref>. <ref type="table">Table 2</ref> shows that N-GCN outperforms GCN <ref type="bibr" target="#b13">[14]</ref> and N-SAGE improves on SAGE for all datasets, showing that unmodified random walks indeed help in semi-supervised node classification. Finally, our proposed models acheive state-of-the-art on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Node Classification Accuracy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Sensitivity Analysis</head><p>We analyze the impact of random walk length K and replication factor r on classification accuracy in <ref type="figure" target="#fig_3">Figure 2</ref>. In general, model performance improves when increasing K and r . We note utilizing random walks by setting K &gt; 1 improves model accuracy due to the additional information, not due to increased model capacity: Contrast K = 1, r &gt; 1 (i.e. mixture of GCNs, no random walks) with K &gt; 1, r = 1 (i.e. N-GCN on random walks) -in both scenarios, the model has more capacity, but the latter shows better performance. The same holds for SAGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Tolerance to feature noise</head><p>We test our method under feature noise perturbations by removing node features at random. This is practical, as article authors might forget to include relevant terms in the article abstract, and more generally not all nodes will have the same amount of detailed information. <ref type="figure" target="#fig_4">Figure 3</ref> shows that when features are removed, methods utilizing unmodified random walks: N-GCN, N-SAGE, and DCNN,  <ref type="table">Table 2</ref>: Node classification performance (% accuracy for the first three, citation datasets, and f1 micro-averaged for multiclass PPI), using data splits of <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref> and <ref type="bibr" target="#b10">[11]</ref>. We report the test accuracy corresponding to the run with the highest validation accuracy. Results in rows (a) through (g) are copied from <ref type="bibr" target="#b13">[14]</ref>, rows (h) and (i) from <ref type="bibr" target="#b10">[11]</ref>, and (j) through (l) are generated using our code since we can recover other algorithms as explained in Section 3.6. Rows (m) and (n) are our models. Entries with "-" indicate that authors from whom we copied results did not run on those datasets. Nonetheless, we run all datasets using our implementation of the most-competitive baselines.</p><p>Nodes per class  . We report mean and standard deviations on 10 runs. We use a different random seed for every run (i.e. selecting different labeled nodes), but the same 10 random seeds across models. Convolution-based methods (e.g. SAGE) work well with few training examples, but unmodified random walk methods (e.g. DCNN) work well with more training data. Our methods combine convolution and random walks, making them work well in both conditions. outperform convolutional methods including GCN and SAGE. Moreover, the performance gap widens as we remove more features. This suggests that our methods can somewhat recover removed features by directly pulling-in features from nearby and distant neighbors. We visualize in <ref type="figure">Figure 4</ref> the attention weights as a function of % features removed. With little feature removal, there is some weight on? 0 , and the attention weights for? 1 ,? 2 , . . . follow some decay function. Maliciously dropping features causes our model to shift its attention weights towards higher powers of?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Random Walk Steps Versus GCN Depth</head><p>K-step random walk will allow every node to accumulate information from its neighbors, up to distance K. Similarly, a K-layer GCN <ref type="bibr" target="#b13">[14]</ref> will do the same. The difference between the two was  , but features removed at random, averaging 10 runs. We use a different random seed for every run (i.e. removing different features per node), but the same 10 random seeds across models.</p><p>mathematically explained in Section 3.1. To summarize: the former averages node feature vectors according to the random walk covisit statistics, whereas the latter creates non-linearities and matrix multiplies at every step. So far, we displayed experiments where our models (N-GCN and N-SAGE) were able to use information from distant nodes (e.g. K = 5), but for all GCN and SAGE modules, we used 2 GCN layer for baselines and our models. Even though the authors of GCN <ref type="bibr" target="#b13">[14]</ref> and SAGE <ref type="bibr" target="#b10">[11]</ref> suggest using two GCN layers, according by holdout validation, for a fair comparison with our models, we run experiments utilizing deeper GCN and SAGE are models so that its "receptive field" is comparable to ours. <ref type="table">Table 4</ref> shows test accuracies when training deeper GCN and SAGE models, using our implementation. We notice that, unlike our method which benefits from a wider "receptive field", there is no direct correspondence between depth and improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>The field of graph learning algorithms is quickly evolving. We review work most similar to ours.</p><p>Defferrard et al <ref type="bibr" target="#b8">[9]</ref> define graph convolutions as a K-degree polynomial of the Laplacian, where the polynomial coefficients are learned. In their setup, the K-th degree Laplacian is a sparse square matrix where entry at (i, j) will be zero if nodes i and j are   <ref type="table">Table 4</ref>: Performance of deeper GCN and SAGE models, both using our implementation. Deeper GCN (or SAGE) does not consistently improve classification accuracy, suggesting that N-GCN and N-SAGE are more performant and are easier to train. They use shallower convolution models that operate on multiple scales of the graph. more than K hops apart. Their sparsity analysis also applies here. A minor difference is the adjacency normalization. We use? whereas they use the Laplacian defined as I ??. Raising? to power K will produce a square matrix with entry (i, j) being the probability of random walker ending at node i after K steps from node j. The major difference is the order of random walk versus non-linearity.</p><p>In particular, their model calculates learns a linear combination of K-degree polynomial and pass through classifier function ?, as in ?( k q k A k ), while our (e.g. N-GCN) model calculates k q k ?( A k ),</p><p>where A is? in our model and I ?? in theirs, and our ? can be a GCN module. In fact, <ref type="bibr" target="#b8">[9]</ref> is also similar to work by <ref type="bibr" target="#b1">[2]</ref>, as they both learn polynomial coefficients to some normalized adjacency matrix.</p><p>Atwood and Towsley <ref type="bibr" target="#b2">[3]</ref> propose DCNN, which calculates powers of the transition matrix and keeps each power in a separate channel until the classification sub-network at the end. Their model is therefore similar to our work in that it also falls under k q k ?( A k ). However, where their model multiplies features with each power A k once, our model makes use of GCN's <ref type="bibr" target="#b13">[14]</ref> that multiply by A k at every GCN layer (see Eq. 2). Thus, DCNN model <ref type="bibr" target="#b2">[3]</ref> is a special case of ours, when GCN module contains only one layer, as explained in Section 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we propose a meta-model that can run arbitrary Graph Convolution models, such as GCN <ref type="bibr" target="#b13">[14]</ref> and SAGE <ref type="bibr" target="#b10">[11]</ref>, on the output of random walks. Traditional Graph Convolution models operate on the normalized adjacency matrix. We make multiple instantiations of such models, feeding each instantiation a power of the adjacency matrix, and then concatenating the output of all instances into a classification sub-network. Each instantiation is therefore operating on different scale of the graph. Our model, Network of GCNs (and similarly, Network of SAGE), is end-to-end trainable, and is able to directly learn information across near or distant neighbors. We inspect the distribution of parameter weights in our classification sub-network, which reveal to us that our model is effectively able to circumvent adversarial perturbations on the input by shifting weights towards model instances consuming higher powers of the adjacency matrix. For future work, we plan to extend our methods to a stochastic implementation and tackle other (larger) graph datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>visualization of fully-connected (fc) hidden layer of NGCN when trained over Cora graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>General Implementation: Network of Graph ModelsRequire:? is a normalization of A 1: function Network(GraphModelFn,?, X , L, r = 4, K = 6, ClassifierFn=FcLayer)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>implementation) 63.0 ? 1.0 72.3 ? 0.4 79.2 ? 0.2 82.6 ? 0.3 GCN (our implementation) 64.6 ? 0.3 70.0 ? 3.7 79.1 ? 0.3 81.8 ? 0.3 SAGE (our implementation) 69.0 ? 1.4 72.0 ? 1.3 77.2 ? 0.5 80.7 ? 0.7 N-GCN a (ours) 65.1 ? 0.7 71.2 ? 1.1 79.7 ? 0.3 83.0 ? 0.4 N-GCN fc (ours) 65.0 ? 2.1 71.7 ? 0.7 79.7 ? 0.4 82.9 ? 0.3 N-SAGE a (ours) 66.9 ? 0.4 73.4 ? 0.7 79.0 ? 0.3 82.5 ? 0.2 N-SAGE fc (ours) 70.7 ? 0.4 74.1 ? 0.8 78.5 ? 1.0 81.8 ? 0.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Sensitivity Analysis. Model performance when varying random walk steps K and replication factor r . Best viewed with zoom. Overall, model performance increases with larger values of K and r . In addition, having random walk steps (larger K) boosts performance more than increasing model capacity (larger r ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Classification accuracy for the Cora dataset with 20 labeled nodes per class (|V | = 20 ? C)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>5 Figure 4 :</head><label>54</label><figDesc>Attention weights (m) for N-GCN a when trained with feature removal perturbation on the Cora dataset. Removing features shifts the attention weights to the right, suggesting the model is relying more on long range dependencies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset used for experiments. For citation datasets, 20 training nodes per class are observed, with |V L | = 20 ? C</figDesc><table><row><cell>Dataset</cell><cell>Type</cell><cell cols="2">Nodes Edges</cell><cell>Classes</cell><cell cols="3">Features Labeled nodes</cell></row><row><cell></cell><cell></cell><cell>|V |</cell><cell>|E |</cell><cell>C</cell><cell></cell><cell>F</cell><cell>|V L |</cell></row><row><cell cols="2">Citeseer citaction</cell><cell>3,327</cell><cell>4,732</cell><cell cols="2">6 (single class)</cell><cell>3,703</cell><cell>120</cell></row><row><cell>Cora</cell><cell>citaction</cell><cell>2,708</cell><cell>5,429</cell><cell cols="2">7 (single class)</cell><cell>1,433</cell><cell>140</cell></row><row><cell cols="4">Pubmed citaction 19,717 44,338</cell><cell cols="2">3 (single class)</cell><cell>500</cell><cell>60</cell></row><row><cell>PPI</cell><cell cols="5">biological 56,944 818,716 121 (multi-class)</cell><cell>50</cell><cell>44,906</cell></row><row><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell cols="4">Citeseer Cora Pubmed PPI</cell></row><row><cell></cell><cell cols="2">(a) ManiReg [7]</cell><cell></cell><cell>60.1</cell><cell>59.5</cell><cell>70.7</cell><cell>-</cell></row><row><cell></cell><cell cols="2">(b) SemiEmb [24]</cell><cell></cell><cell>59.6</cell><cell>59.0</cell><cell>71.1</cell><cell>-</cell></row><row><cell></cell><cell>(c) LP [26]</cell><cell></cell><cell></cell><cell>45.3</cell><cell>68.0</cell><cell>63.0</cell><cell>-</cell></row><row><cell></cell><cell cols="2">(d) DeepWalk [22]</cell><cell></cell><cell>43.2</cell><cell>67.2</cell><cell>65.3</cell><cell>-</cell></row><row><cell></cell><cell>(e) ICA [18]</cell><cell></cell><cell></cell><cell>69.1</cell><cell>75.1</cell><cell>73.9</cell><cell>-</cell></row><row><cell></cell><cell cols="2">(f) Planetoid [25]</cell><cell></cell><cell>64.7</cell><cell>75.7</cell><cell>77.2</cell><cell>-</cell></row><row><cell></cell><cell cols="2">(g) GCN [14]</cell><cell></cell><cell>70.3</cell><cell>81.5</cell><cell>79.0</cell><cell>-</cell></row><row><cell></cell><cell cols="2">(h) SAGE-LSTM [11]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>61.2</cell></row><row><cell></cell><cell cols="2">(i) SAGE [11]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>60.0</cell></row><row><cell></cell><cell cols="3">(j) DCNN (our implementation)</cell><cell>71.1</cell><cell>81.3</cell><cell>79.3</cell><cell>44.0</cell></row><row><cell></cell><cell cols="3">(k) GCN (our implementation)</cell><cell>71.2</cell><cell>81.0</cell><cell>78.8</cell><cell>46.2</cell></row><row><cell></cell><cell cols="3">(l) SAGE (our implementation)</cell><cell>63.5</cell><cell>77.4</cell><cell>77.6</cell><cell>59.8</cell></row><row><cell cols="3">(m) N-GCN (ours)</cell><cell></cell><cell>72.2</cell><cell>83.0</cell><cell>79.5</cell><cell>46.8</cell></row><row><cell></cell><cell cols="2">(n) N-SAGE (ours)</cell><cell></cell><cell>71.0</cell><cell>81.8</cell><cell>79.4</cell><cell>65.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Node classification accuracy (in %) for our largest dataset (Pubmed) as we vary size of training data</figDesc><table><row><cell>|V | C ? {5, 10, 20, 100}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Dataset Model 64 ? C 64 ? 64 ? C 64 ? 64 ? 64 ? C</figDesc><table><row><cell cols="2">Citeseer GCN</cell><cell>0.699</cell><cell>0.632</cell><cell>0.659</cell></row><row><cell cols="3">Citeseer SAGE 0.668</cell><cell>0.660</cell><cell>0.674</cell></row><row><cell>Cora</cell><cell>GCN</cell><cell>0.803</cell><cell>0.800</cell><cell>0.780</cell></row><row><cell>Cora</cell><cell cols="2">SAGE 0.761</cell><cell>0.763</cell><cell>0.757</cell></row><row><cell cols="2">Pubmed GCN</cell><cell>0.762</cell><cell>0.771</cell><cell>0.781</cell></row><row><cell cols="3">Pubmed SAGE 0.770</cell><cell>0.776</cell><cell>0.775</cell></row><row><cell>PPI</cell><cell>GCN</cell><cell>0.460</cell><cell>0.461</cell><cell>0.466</cell></row><row><cell>PPI</cell><cell cols="2">SAGE 0.658</cell><cell>0.672</cell><cell>0.650</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our implementation assumes mean-pool aggregation by<ref type="bibr" target="#b10">[11]</ref>, which performs on-par to their top performer max-pool aggregation. In addition, our Algorithm 3 lists a full-batch implementation whereas<ref type="bibr" target="#b10">[11]</ref> offer a mini-batch implementation.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
	</analytic>
	<monogr>
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</title>
		<editor>Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi?gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng</editor>
		<meeting><address><addrLine>Dan Man?, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Watch Your Step: Learning Graph Embeddings Through Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Alemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In arxiv</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diffusion-Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Laplacian Eigenmaps for Dimensionality Reduction and Data Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computation</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of machine learning research (JMLR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of machine learning research (JMLR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral Networks and Locally Connected Networks on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving Distributional Similarity with Lessons Learned from Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Data Analytics for Optimal Detection of Metastatic Prostate Cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Selin</forename><surname>Merdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><forename type="middle">L</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">T</forename><surname>Denton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DeepWalk: Online Learning of Social Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deeplearning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Revisiting Semi-Supervised Learning with Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
