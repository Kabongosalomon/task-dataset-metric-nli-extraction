<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Triangulation as a Form of Self-Supervision for 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumava</forename><forename type="middle">Kumar</forename><surname>Roy</surname></persName>
							<email>soumava.roy@epfl.chleonardo.citraro@epfl.chsina.honari@epfl.chpascal.fua@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Citraro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On Triangulation as a Form of Self-Supervision for 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised approaches to 3D pose estimation from single images are remarkably effective when labeled data is abundant. However, as the acquisition of ground-truth 3D labels is labor intensive and time consuming, recent attention has shifted towards semi-and weakly-supervised learning. Generating an effective form of supervision with little annotations still poses major challenge in crowded scenes. In this paper we propose to impose multi-view geometrical constraints by means of a weighted differentiable triangulation and use it as a form of self-supervision when no labels are available. We therefore train a 2D pose estimator in such a way that its predictions correspond to the re-projection of the triangulated 3D pose and train an auxiliary network on them to produce the final 3D poses. We complement the triangulation with a weighting mechanism that alleviates the impact of noisy predictions caused by self-occlusion or occlusion from other subjects. We demonstrate the effectiveness of our semisupervised approach on Human3.6M and MPI-INF-3DHP datasets, as well as on a new multi-view multi-person dataset that features occlusion. * equal contribution</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Supervised approaches in capturing human 3D pose are now remarkably effective, provided that enough annotated training data is available <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>. However, there are many scenarios that involve unusual activities for which not enough annotated data can be obtained. Semi-supervised or unsupervised methods are then required <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45]</ref>. A promising subset of those rely on constraints imposed by multi-view geometry: If a scene is observed by several cameras, there is a single a priori unknown 3D pose whose projection is correct in all views <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38]</ref>. This is used to provide a supervisory signal with limited need for manual annotations. There are many venues, such as a sports arena, that are equipped with multiple cameras, which can be easily used for this purpose.</p><p>However, some of these semi-supervised approaches are sensitive to occlusions and noisy predictions <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b37">38]</ref>. Others rely on a non-differentiable implementation of triangulation, which precludes making it a full-fledged component of the learning pipeline <ref type="bibr" target="#b15">[16]</ref>, while some others require full labels to apply a learnable triangulation <ref type="bibr" target="#b12">[13]</ref>. In this paper, we propose an approach that overcomes these limitations. To this end, we start from the observation that there are now many off-the-shelf pre-trained 2D pose estimation models that do not necessarily perform well in new environments but can be fine-tuned to do so. Our approach therefore starts with one of these that we run on all multi-view images whose results can be triangulated. The model is then trained so that the prediction in all views correspond to the re-projection of the pseudo 3D pose target. In particular, we give to the 2D prediction of each view a weight, that is derived from the level of agreement of that view with respect to all other views. In doing so, we apply a principled triangulation weighting mechanism to obtain pseudo 3D targets that can discard erroneous views, hence helping the model bootstrap in semi-supervised setups. Our approach can be used in any multi-view context, without restriction on camera placement. At inference time, our retrained network can be used on single-view images and have their output lifted to 3D by an auxiliary network.</p><p>We demonstrate the effectiveness of our approach on the traditional Human3.6M <ref type="bibr" target="#b9">[10]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b25">[26]</ref> datasets. To highlight its ability to handle more crowded scenes, we test the robustness of our approach on a new multi-person dataset of an amateur basketball match.</p><p>In short, our contributions are:</p><p>a self-supervised multi-view consistency loss based on differentiable triangulation that takes the 2D network output as its input, without requiring annotations; a weighting strategy that mitigates the effect of occlusion and noisy predictions; a new multi-view multi-person datasets of an amateur basketball match featuring occlusion and difficult light conditions.</p><p>We will make the dataset publicly available along with our code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>With the advent of deep learning, direct regression of 2D and 3D poses from images has become the dominant approach for human pose estimation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. Recent variations on this theme use volumetric representations <ref type="bibr" target="#b12">[13]</ref>, lift 2D poses to 3D <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref>, use graph convolutional network (GCN) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b52">53]</ref> or generate and refine pseudo labels from video sequences <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref>. Given enough annotated training data, these methods are now robust and accurate. Thus, in recent years, the attention has shifted from learning 3D human poses in fully supervised setups to semi-or weaklysupervised ones <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>, with a view to drastically reduce the required amount of annotated images. In many venues such as sport arenas, there are multiple cameras designed to provide different viewpoints, often for broadcasting purposes. Hence, a valid approach is to exploit the constraints that multi-view geometry provides for training purposes, while at inference time, the trained network operates on single views. For example, in <ref type="bibr" target="#b15">[16]</ref>, a single-view 3D pose estimator is trained using pseudo labels generated by triangulating the predictions of a pre-trained 2D pose estimator in multiple images. However, as the 2D pose estimator is fixed, pseudo label errors caused by noisy 2D detections are never corrected.</p><p>In <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38]</ref>, a regressor is trained to produce 3D poses from single-views, while guaranteeing consistency of the predictions across multiple views. A problem with these approaches is that the predictions can be consistent but wrong, which makes it necessary to use some annotated 3D or 2D data. The method of <ref type="bibr" target="#b36">[37]</ref> relies on a similar setup but reduces the required amount of annotated data by using a multi-view unsupervised pretraining strategy that learns a low-dimensional latent representation from which the 3D poses can be inferred using a simple regressor. However, due to the low-dimensionality of the learned latent space, the structural information from the image is lost, which makes it less accurate in practice. In <ref type="bibr" target="#b16">[17]</ref>, an encoder-decoder based self-supervised training algorithm is used to disentangle the appearance and the pose features of the same subject extracted from two different images. It also uses a part-based 2D puppet model as an additional form of prior knowledge regarding the 2D human skeleton structure and appearance to guide the learning process. This results in a very complex pipeline that requires a great deal of additional data during training without delivering superior performance as we demonstrate in the result section.</p><p>Triangulation is at the heart of our approach and is also used extensively in the fullysupervised method of <ref type="bibr" target="#b12">[13]</ref>, which uses 3D labels to refine the triangulation process, and of <ref type="bibr" target="#b35">[36]</ref> where 2D poses are fused in a latent space. While <ref type="bibr" target="#b35">[36]</ref> uses a standard triangulation that does not weigh different views, <ref type="bibr" target="#b12">[13]</ref> learns a weight for each view, which is designed for a fully-supervised setup in order to have an effective weighting scheme. In contrast, ours is derived from the agreement of 2D views; therefore, it is better suited for semi-and weakly-supervised setups because the weights are computed from multi-view consistency rather than learned. In the result section, we show that the triangulation of <ref type="bibr" target="#b12">[13]</ref> applied to our semi-supervised framework results in less accurate poses when the amount of labeled data is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our goal is to train a deep neural network to estimate 3D human poses from single images using as little labeled data as possible. To this end, we acquire multiple views of our subjects and train a 2D pose estimator to detect joint locations consistently across views. This detection network is complemented by a lifting network that infers 3D poses from 2D ones. During training, we check for consistency across views of the unlabeled images by triangulating the 2D detections, then we re-project the results into the images and use the triangulated 3D points to train the lifting network.</p><p>Both networks are trained in a semi-supervised way on our target dataset, that is, we assume that the ground-truth 2D and 3D poses are known only for a small subset of the entire dataset. To train the detection network, in addition to a small set of 2D labels, we use a robust differentiable triangulation layer that takes as input the predicted 2D poses for the unlabeled images in individual views and produces the 3D estimates. These are used as pseudo labels to check for consistency of the 2D pose estimates and to train both the detection and the lifting networks. The complete pipeline is depicted by <ref type="figure">Figure 1</ref> and is end-to-end trainable. Making the triangulation process robust to mis-detections in some of the views is a key component of our approach because some joints are not visible in some views, especially in crowded scenes. <ref type="figure">Fig. 1</ref>: Network Architecture. It comprises a detection network that outputs 2D joint locations in individual views and a lifting network that predicts root relative distances to the camera which are then turned to 3D poses using the 2D ones and the intrinsic camera parameters. They are trained jointly using a small number of images with corresponding 3D annotations and a much larger set of images acquired using multiple cameras. During training, the prediction networks feed their 2D detections into a differentiable triangulation layer that estimates 3D joint locations. These are re-projected into the images to minimize the discrepancy between the projections and the original detections, along with a supervised loss that operates on the frames for which annotations are available. The estimated 3D joint locations are also used to train the lifting network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><formula xml:id="formula_0">Let L = {[I l,c ] N C c=1 , p l 3D } N L l=1</formula><p>be a sequence of labeled RGB images I l,c ? R h?w?3 taken using N C different cameras, and p l 3D be the ground-truth 3D pose, where c denotes the index of cameras and l is the index of the labeled images. Similarly, let</p><formula xml:id="formula_1">U = {[I u,c ] N C v=1 } N U u=1</formula><p>, where u denotes the unlabeled, be a larger sequence of multiview images without associated 3D pose labels. We take p l 3D to be the entire set of body joints {X l j } N J j=1 , where X l j ? R 3 denotes the 3D coordinates of joint j for the l sample. Similarly, let p l 2D be {x l,c j } N J j=1 , where x l,c j ? R 2 is the projection of X l j in view c. In the remainder of this section, we drop the l, u, and c notations when there is no ambiguity.</p><p>Let f ? and g ? denote the detection and lifting network with weights ? and ? respectively. The detection network f ? takes I c as input and returns a 2D pose estimate in that viewp c 2D = {x c j } N J j=1 . The lifting network g ? takes as input such a 2D pose predictions and returns root relative distances to the camera which are then turned to a 3D pose estimatep 3D using the known intrinsic camera parameters K.</p><p>Finally, let ? be a function that takes as input the known camera parameters and a set of image points, one per view, and triangulates to a location in the worldX j = ?({x c j } N C c=1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training the Detection and Lifting Networks</head><p>We train the detection network f ? and the lifting network g ? by minimizing two loss functions that we define below, which we describe in more details in the following subsections.</p><p>Detection Loss L f (?). We take it to be the sum of a supervised term computed on L and an unsupervised one computed on U. We write it as</p><formula xml:id="formula_2">L f (?) = L sup (?; L) + L tri (?; U) , L sup (?; L) = N L l=1 N J j=1 N C c=1 x l,c j ? x l,c j 2 2 ,<label>(1)</label></formula><formula xml:id="formula_3">L tri (?; U) = N U u=1 N J j=1 N C c=1 x u,c j ?x u,c j 2 2 ,</formula><p>wherex u,c j denotes the projection ofX u j obtained by triangulating thex u,c j predicted by the network. L sup is the supervised MSE loss between the predicted 2D poses and the ground-truth ones. L tri is the self-supervised loss that is at its minimum when the detected 2D poses are consistent with the projections of the triangulated poses. L tri pools information from the multiple views to provide a supervisory signal to our model without using any labels. All the image locations within the losses are normalized in the range [?1, 1] with respect to their subject's bounding box.</p><p>As shown in <ref type="figure">Figure 1</ref>, L tri provides two sets of gradients during back-propagation. One from the re-projected triangulated pose directly to the detection model and one that flows into the triangulation. Blocking either one of these results in a different behavior that makes this self-supervised loss less stable in practice. We provide more details and experiments to highlight this in the supplementary.</p><p>Lifting Loss L g (?; f ? ). Once f ? is trained, we use its predictions on U and the triangulation operation ? to create a new set with pseudo 3D poses?</p><formula xml:id="formula_4">= {[I u,c ] N C c=1 ,p u 3D } N U u=1</formula><p>. We consider the lifting loss to be</p><formula xml:id="formula_5">L g (?; f ? ) = L 3D semi?sup (?; f ? , L ?? )<label>(2)</label></formula><p>with respect to ?. L 3D semi?sup is the MSE loss between the predicted 3D poses and ground-truth or triangulated ones. The parameters of the 2D pose estimator f ? are fixed and provide the 2D poses as input to g ? . ? denotes the concatenation operation on L and?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-Supervision by Triangulation</head><p>Computing L tri (?; U) of Eq. 1 requires triangulating each joint j in the set of 2D locations {x c j } N C c=1 . In theory, the most reliable way to triangulate is to perform non-linear minimization <ref type="bibr" target="#b7">[8]</ref>. However, because this computation is part of our deep learning, it must be differentiable, which is not easy to achieve for this kind of minimization. We therefore rely on the Direct Linear Transform (DLT) <ref type="bibr" target="#b0">[1]</ref>, which involves transforming the projection equations to a system of linear ones whose solution can be found using Singular Value Decomposition <ref type="bibr" target="#b6">[7]</ref>. This is less accurate but easily differentiable.</p><p>In practice, occlusions and mis-detections pose a much more significant challenge than any loss of accuracy due to the use of DLT. It is inevitable that some joints are hidden in some views and their estimated 2D locationsx u,c j are erroneous, which results in inaccurate triangulations. To prevent this, we introduce a weighing strategy that reduces the influence of mis-detections in such problematic views. We apply these weights in DLT and re-write the unsupervised loss of Eq. 1 as</p><formula xml:id="formula_6">L tri (?; U) = N U u=1 N J j=1 N C c=1 w u,c j ?x u,c j ?x u,c j ? 2 2 ,<label>(3)</label></formula><p>where w u,c j denotes the reliability of the estimate of the location of joint j in view c. To estimate these weights we use the purely geometrical approach based on robust statistics described below.</p><p>Robust Localization. To robustly estimate the 3D location of a joint j, we first compute candidate 3D locations using all pairs of 2D detections (x c j ,x c ? j ) in pairs of views. Let</p><formula xml:id="formula_7">T j = {? c,c ? j = ?(x c j , x c ? j )|(c, c ? ) ? N C 2 } ,<label>(4)</label></formula><formula xml:id="formula_8">X j = GeoMed(T j )</formula><p>where T j is a detection cluster andX j is the center of the cluster. ? denotes the triangulation, N C 2 denotes the set of all pairs of views, and GeoMed refers to calculating the geometric median of T j . Given T j andX j , we take the weights to be</p><formula xml:id="formula_9">w c j = Median(W c j ) (5) W c j = {w c,c ? j = exp(? ?X j ?? c,c ? j ? 2 2 ? 2 )|c ? ? C j \{c}}</formula><p>where W c j is a set of weights for joint j th from view c to all other views. These encode the distance of each candidate location in T j to its center and are bounded in the range [0, 1] using a Gaussian function. ? is a parameter of the model that is set according to the amount of noise present in the candidate joints. Intuitively, when a joint estimate in T j is an outlier, its weights should be closer to zero, otherwise they should be close to one. In all our experiments we set it to 10 millimeters.</p><p>Joint Selection. The above formulation relies onX j , obtained in Eq. 4, being reliable, which is usually true when more than 50% of the 2D detections are correct. However, this is not always the case. To handle this, at each training iteration, we assign a score to each T j and momentarily discards the joints whose score is below a certain threshold. To compute this score, we use the normalized Within-Cluster-Sum of Squared Errors (WSS) as follows:</p><formula xml:id="formula_10">W SS j = 1 |T j | ? c,c ? j ?Tj ?? c,c ? j ?X j ? 2 2 .<label>(6)</label></formula><p>WSS is a measure of homogeneity defined within an agglomeration often used in clustering methods. Intuitively, when the image points are erroneous, the joint estimates composing the cluster T j are distant from one another resulting in a high W SS j . Therefore, if W SS j is greater than a manually defined value during training, we momentarily discard the joint. As the training progresses, the model improves and allows the previously discarded joints to contribute again towards the training of the model. In all our experiments we set this threshold to 20 millimeters that we found empirically.</p><p>In summary, the weights computed using our purely geometric approach of Eq. 5 allow the triangulation to output a more robust estimate of the true location of the joint. That is the center of the clusterX j . Instead, Eq. 6 allows to discard the joints that are considered unreliable, thus reducing their negative impact on the model. In Section 4.2, we demonstrate how our approach produces more accurate triangulations when less than 50% of the views are affected by the noise, while obtaining equal results to a standard triangulation approach otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>Lifting 2D Poses to 3D. The second stage of our pipeline "lifts" the predictions of our finetuned 2D pose estimator to 3D poses in the camera coordinate system. We adopt a commonly used pose representation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42]</ref> </p><formula xml:id="formula_11">namely 2.5D p 2.5D = {(u j , v j , d root +d rel j )} N J j=1</formula><p>where u j and v j are the components of joint j th in the undistorted image space, d root is a scalar representing the depth of the root joint with respect to the camera and d rel j is the relative depth of each joint to the root. The advantage of using this representation lies in the fact that the 2D pose {(u j , v j )} N J j=1 is spatially coincident with the image, which allows to fully exploit the characteristics of convolutional neural networks. In addition, the 2D pose estimator can be further improved using additional in-the-wild 2D pose datasets.</p><p>To obtain the 3D poses, we first train a multi-layer neural network as in <ref type="bibr" target="#b24">[25]</ref> to output root relative depths d rel j = g ? (f ? (I)). Then, we recover the 3D pose in the camera or in the world coordinate system using the inverse of the projection equations</p><formula xml:id="formula_12">(d root + d rel j ) ? ? u j v j 1 ? ? = K ? ? X C j Y C j (d root + d rel j ) ? ? = K ? ? R ? ? X j Y j Z j ? ? + t ? ?<label>(7)</label></formula><p>where X C and Y C are the first two components of a joint in camera space,</p><formula xml:id="formula_13">(X j , Y j , Z j )</formula><p>is the joint in the world coordinate and K ? R 3?3 , R ? R 3?3 , and t ? R 3?1 are the intrinsic matrix, rotation matrix, and translation vector of the camera respectively. The above formulation assumes that the depth of the root joint d root is known. In earlier works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref>, the position of the root is taken to be the ground-truth one or not used at all simply because the dataset itself is provided in camera coordinate. We follow the same protocol and use the ground-truth depth of the root, however, an approximation can be obtain analytically as shown in <ref type="bibr" target="#b10">[11]</ref>. A simple multi-layer neural network can effectively learn the distribution of relative depths from the joint coordinate of the 2D poses, however, the image can still provide useful information to disambiguate certain cases. For this reason, we use an additional convolutional neural network h ? , parameterized by ?, that takes an image I as input and outputs a small feature vector. We then concatenate this feature vector with the joint coordinates of a 2D pose and feed it into the multi-layer neural network to output the relative depth d rel j = g ? (f ? (I) ? h ? (I)). This additional network contributes to distinguish classes of poses such as when subjects are standing, sitting and lying that can be more easily captured from the images themselves. In our experiments we consider h ? to be a ResNet50 <ref type="bibr" target="#b8">[9]</ref> pretrained on ImageNet <ref type="bibr" target="#b40">[41]</ref> and replace its last linear layer with an MLP that shrinks the size of the output feature from 2048 to 16.</p><p>Detection and Lifting models. We use Alphapose <ref type="bibr" target="#b5">[6]</ref> trained on Crowdpose dataset <ref type="bibr" target="#b19">[20]</ref> as our 2D pose estimator f ? where we replace the original non-differentiable Non-Maximum Suppression (NMS) by a two-dimensional Soft-argmax as in <ref type="bibr" target="#b41">[42]</ref>. For the lifting network g ? we used a multi-layer perceptron network with 2 hidden layers with a hidden size of 2048. The first two layers are composed of a linear layer followed by ReLU activation and 10% dropout while the last layer is a linear layer. The lifting network takes as input the feature vector obtained by concatenating the joint coordinates of a 2D pose and the 16 dimensional feature obtained from h ? . h ? takes the cropped image resized to 256?256 as the input. The 2D pose is defined in the undistorted image space and is normalized in the range [?1, 1] with respect to the bounding box.</p><p>Training Procedure. We resize the input crops to 320 ? 256 pixels by keeping the original aspect ratio and apply random color jittering for regularization. In all our experiments we pretrained the 2D detection network first on the small set of labeled set and then we finetuned it on both labeled and unlabeled sets with our losses. During training, we used mini-batches of size 24 samples where 8 samples come from the labeled set and the others from the unlabeled one. We use Adam optimizer <ref type="bibr" target="#b14">[15]</ref> with constant learning rate of 1e ? 4 for the pre-training and 1e ? 5 for the fine-tuning. We do not use weight decay regularization. All the 3D losses were computed on poses expressed in meters and the image coordinates of the 2D losses are all normalized in the range [?1, 1] with respect to the bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We primarily report our results in the semi-supervised learning setup, where we have 3D annotations only for a subset of the training images. We use the projection of these 3D annotations to train f ? and they are directly used to supervise the training of g ? . For the unannotated samples, we use the 3D poses triangulated from 2D estimations and their projections to train both g ? and f ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>We validate our proposed framework on two large 3D pose estimation datasets and a new multi-view dataset featuring an amateur basketball match to study the influence of occlusions in a crowded scene. We will release it publicly. Human3.6M <ref type="bibr" target="#b9">[10]</ref>. It is the most widely used indoor dataset for single and multi-view 3D pose estimation. It consists of 3.6 million images captured form 4 calibrated cameras. As in most published papers, we use subjects S1, S5, S6, S7, S8 for training and S9, S11 for testing. In the semi-supervised setup we follow two separate protocols:</p><p>1. Subject S1 is the only source of supervision, while other training subjects are treated as unlabeled, as done in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38</ref>]. 2. Only 10% of uniformly sampled training samples are considered annotated and the rest as unlabeled <ref type="bibr" target="#b16">[17]</ref>.</p><p>MPI-INF-3DHP <ref type="bibr" target="#b25">[26]</ref>. This dataset contains both constrained indoor and some complex outdoor images for single person 3D pose estimation. It features 8 subjects performing 8 different actions, which are recorded using 14 different cameras, thereby covering a wide range of diverse 3D poses. We follow the standard protocol and use the 5 chest-height cameras only. In the semi-supervised setup, as for the Human3.6M dataset, we exploit annotations for subject S1 of the training set. The sampling rate for the unlabeled set is set to 5 for both the above mentioned datasets.</p><p>SportCenter. We filmed an amateur basketball match using 8 fixed and calibrated cameras. 2 of the 8 cameras are mounted on the roof of the court, which makes them useful for location tracking but less useful for pose estimation. The images feature a variable number of subjects ranging from 10 to 13. They are either running, walking, or standing still. The players are often occluded either by others or by various objects, such as the metal frames of the nets. There are also substantial light variations that makes it even more challenging. We computed the players' trajectories for the whole sequence and manually annotated a subset with 2D poses. Thereafter, we obtained the 3D poses by triangulating the manually annotated 2D detections. The dataset comprises 315, 000 images out of which 560 are provided with 3, 740 2D poses and 700 3D poses. We use two subjects for testing and the remaining subjects are used for training. In total, we have 140 annotated 3D poses for the test phase, while the remaining 560 annotated 3D poses are used for training in the semi-supervised setup.</p><p>Metrics. We report the Mean Per-Joint Position Error (MPJPE), the normalized NM-PJPE, and the procrustes aligned PMPJPE in millimeters. The best score is always shown in bold.</p><p>Calibration and Data Association. Our self-supervised loss function assumes the intrinsic and extrinsic camera parameters to be known. In practice, they can be obtained using Bundle Adjustment <ref type="bibr" target="#b43">[44]</ref>, which is a well established technique that is implemented by numerous software packages. In other words, this requirement is much less onerous than having to manually annotate 3D ground-truth poses. For our experiments, we used the camera parameters provided in the Human3.6M <ref type="bibr" target="#b9">[10]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b25">[26]</ref> datasets and performed Bundle Adjustment to obtain the camera parameters for the SportCenter dataset. We also assume that the ground-location of each person to be given. This lets us compute the bounding boxes for each person by assuming an average person height and projecting a cylinder of that height in each view. For Hu-man3.6M and MPI-INF-3DHP we use the ground-truth locations provided in each of the datasets respectively, while, for the SportCenter dataset, we computed the trajectories using <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b47">48]</ref> and manually fixed the error in the highly crowded frames, which represent about 20% of the total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Evaluation</head><p>We report results for our semi-supervised learning setup in <ref type="table" target="#tab_0">Tables 1, 2 and 3</ref> for Hu-man3.6M <ref type="bibr" target="#b9">[10]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b25">[26]</ref> and SportCenter datasets respectively. Considering only 10% of data as labeled for Human3.6M dataset in <ref type="table" target="#tab_0">Table 1</ref>, our approach outperforms by a significant margin Kundu et al. <ref type="bibr" target="#b16">[17]</ref>. Similarly, we perform better than baselines when considering all samples of S1 as labeled (Only S1). The second best method is Kundu et al. <ref type="bibr" target="#b16">[17]</ref> but note that they rely on additional datasets such as in-the-wild YouTube videos and the MADS <ref type="bibr" target="#b49">[50]</ref> dataset, in addition to a part-based puppet to instill prior human skeletal knowledge in their overall learning framework, which we do not. In other words, we were able to eliminate the need of such supplementary components by designing an effective geometrical reasoning based on a weighting strategy that mitigates the impact of noisy detections. <ref type="table" target="#tab_1">Table 2</ref> shows a comparative performance on the MPI-INF-3DHP dataset. Again, we outperform the competing methods without having to use any additional training datasets.</p><p>In <ref type="figure">Figure 2</ref>, we amplify on these results by showing NMJPE and PMPJPE performances as a function of the percentage of annotated training data we use. As observed we outperform the other methods in the semi-supervised setups with small number of annotations, in particular less than 100% of S1. In the fully-supervised setup, there are other methods such as <ref type="bibr" target="#b33">[34]</ref> that perform better. This can be attributed to using temporal consistency in videos whereas we operate on single frames. Hence, we can safely assume that using temporal consistency would also boost our numbers and this is something we will investigate in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Triangulation Components Impact</head><p>We evaluate the effectiveness of our approach and variants of it on the SportCenter dataset, which contains occlusions. To this end, we compare our triangulation approach (Ours) against three variants, namely (a) Ours non-differentiable, where we remove the gradient flowing through the triangulation process, (b) Ours w/o weights where we disable the weighting mechanism such that each view is given equal importance in the triangulation process and (c) Ours non-differentiable w/o weights which is a combination of the above two variants. In addition, we compare our approach to Ours+Iskakov where we use the weighting strategy proposed in <ref type="bibr" target="#b12">[13]</ref>. This entails training a neural network module to assign to each views weights when triangulating. In contrast, our approach to weighting relies purely on geometry, which is beneficial when only little labeled data is available.</p><p>We use the same architecture for all these experiments to showcase the effect of the different weighting schemes. We report the multi-view (MV) and the single-view (SV) results in <ref type="table" target="#tab_2">Table 3</ref>. Our approach performs better than Ours w/o weights because of its ability to produce reliable 3D poses from multiple 2D views even in the presence of occlusions. This is true both in the multi-view and single-view case. Using a differentiable triangulation also demonstrates to provide better supervision. Moreover, our approach also outperforms Ours+Iskakov on this task, presumably because the latter suffers from the lack of adequate amount of labeled data and the noisy 2D detections caused due the occlusions. Hence it fails to yield the optimal weights for triangulation purposes. <ref type="figure">Figure 5</ref>, 6 and 7 provide a qualitative comparison between our approach and a variant of ours with disabled weighting mechanism. It can be noted how our weighting strategy produces more precise 3D poses, which in turn provides better supervision for occluded samples. Some failure cases where the lifting network g ? fails to generate plausible 3D poses is shown in <ref type="figure">Figure 8</ref>, which we aim to mitigate in the future by using a discriminator network on the predicted 3D poses by g ? in addition to using a form of temporal consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Triangulation Robustness Analysis</head><p>We analyze how the estimated 3D object locations, that are the results of applying triangulation, are affected by 2D localization error when comparing our triangulation approach with the standard one. To do so, we create a simulation using six views with real camera parameters. We first generate N three-dimensional object locations and then obtain their corresponding ground-truth projections in each one of the views using the camera parameters. We then run experiments in which we randomly select a subset of cameras and gradually increase the amount of 2D noise applied to the projected points in these views. The noise that we apply to the 2D points is uniformly sampled on a circle of radius r around the ground-truth 2D locations, where r is the level of noise. For our triangulation approach with W SS of Eq. 6, when W SS is bigger then a threshold ? = 20 millimeters we set all the weights to a small but equal weight. This is equivalent to using standard triangulation. In <ref type="figure" target="#fig_0">Figure 3</ref> we report the MPJPE score between the triangulated results and the ground-truth object locations, where noise is applied to different number of cameras, for our approach (w/ weights w/ WSS) and variants with . Triangulation error with respect to an increasing level of noise expressed in pixels and applied to a varying number of views. Our triangulation approach (w/ weights w/ WSS) performs better than standard triangulation (wo/ weights) when less then 50% of the cameras are affected by the noise and on par otherwise. Without WSS (w/ weights wo/ WSS) instead, produces less accurate locations when most of the image points are erroneous. <ref type="table">Table 4</ref>: Comparative study on the impact of unlabeled data for different levels of supervision L.</p><p>We report the MPJPE results in millimeters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L</head><p>1% S1 5% S1 10% S1 50% S1 S1 S1, S5 S1, S5, S6 disabled weights (wo/ weights) and disabled W SS (w/ weights wo/ WSS). Note that our triangulation produces more accurate results than the standard triangulation when less than 50% of the views are affected by the noise and on par otherwise. This is because the weights allow to reduce the impact of the noisy predictions while W SS discards the joint when more than 50% of the views are unreliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stability of L tri</head><p>As discussed in Section ? 3.2.1, the self-supervised loss term L tri minimizes the difference between the predicted image pointsx and the projection of the triangulated onex using the Mean Squared Error (MSE). Since both are function of the network f ? , there are two gradients that affect the network weights ?. To better understand the contribution of each gradient we decompose L tri in two terms as shown in Eq. 8. In the first term, we considerx to be ground-truth data, that means that there is no gradient flowing in it. In the second term instead we considerx as to be the ground-truth. Note that,x andx are dependent on each other and can change from one iteration to another.</p><formula xml:id="formula_14">L tri (x,x) = ?L(x,x GT ) + (1 ? ?)L(x GT ,x)<label>(8)</label></formula><p>When ? = 0, the gradient flows only through the differentiable triangulation vi? x. When ? = 1 it flows only through the predictionsx, with the triangulated points considered as the ground-truth. Since we want to exploit triangulation, setting ? = 0 seems a reasonable choice, however, we demonstrate with a simulation <ref type="figure" target="#fig_1">(Figure 4</ref>) that it can be counterproductive.</p><p>To do so, we run an experiment in a controlled environment by isolating the selfsupervised loss L tri from the rest of the pipeline; neural networks are not used here either. We first generate N three-dimensional object locations and project them in each one of the views (3 in our case) to obtain ground-truth image locations x using camera parameters. Then, to simulate the error of the 2D pose estimator, we inject a moderate amount (1 pixel) of Gaussian noise to all the image points and a higher amount (10 pixels) to one view only. Finally, we optimize the position of the noisy image location? x using L tri where at each iteration we computex using the standard differentiable triangulation. The problem can be expressed asx * = arg minx L tri (x,x). <ref type="figure" target="#fig_1">Figure 4</ref> depicts the loss values and the MPJPE between the triangulated points and the ground-truth ones for different values of ?. It can be noted that when ? = 0 the MPJPE can reach its lowest value but, as the training continues, it deviates quickly. This is due to having the model aiming to put all the joint locations at the center of the image, which is a target that can minimize the triangulation loss without minimizing the MPJPE error. This means that minimizing the self-supervised loss does not correspond to minimizing the MPJPE. On the other hand, when ? = 1 the error on the noisy image key-points is hardly reduced but is rather propagated to the other views leading to the degradation of the learnt model. When both terms in Eq. 8 are active, the image points that are precise deviate less and the MPJPE curve is more stable. Note that with additional supervision from labeled data all the MPJPE curves in <ref type="figure" target="#fig_1">Figure 4</ref> would improve over time. In our experiment we found that using ? = 0.5 is a good compromise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploiting the Unlabeled Data</head><p>Here we study the impact of the unlabeled set of images while training the lifting network. <ref type="table">Table 4</ref> compares the results of our semi-supervised approach (Ours) and that of a standard supervised approach (Baseline) as we progressively increase the amount of available 3D supervision on the Human3.6M <ref type="bibr" target="#b9">[10]</ref> dataset for learning the lifting network g ? in the single view setup. We first train the 2D pose estimator f ? and then freeze its parameters and use it for both Ours and Baseline. In Baseline, we train g ? directly on the labeled set of images L for each setup using the following loss function:</p><formula xml:id="formula_15">L g (?; f ? ) = L 3D (?; f ? , L) ,<label>(9)</label></formula><p>where L 3D is the MSE loss between the predicted and the ground truth 3D poses on L. In Ours, in addition to the labeled set we also train the lifting network using the estimated triangulated 3D poses on the unlabeled data as in Eq. 2 (of the main text). As expected, the performance of both networks improve as the amount of labeled data increases. In addition, Ours produces consistently better results than the Baseline. This proves the effectiveness of the pseudo labels generated using our proposed weighted triangulation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Second Spectrum Dataset</head><p>Here we evaluate and compare our proposed method against the Standard DLT on a basketball dataset provided by Second Spectrum. The dataset consists of 15 players and 3 referees playing a game of basketball at the National Basketball Association (NBA). The videos are captured using 8 calibrated cameras, with 4 placed in each half of the court. We have considered 2 (out of 15) players and 1 referee (out of 3) to be the test set, while the remaining 13 players and 2 referees are considered as the train set. Each subject has its annotated 2D and 3D ground truth poses. In the semi-supervised learning setup, we have considered 2, 160 annotated images as the labeled set L and the unlabeled set U consists of 19, 865 images. The results for multiview and single view experiments are shown in <ref type="table" target="#tab_4">Table 5</ref>. As seen, we consistently outperform the Ours w/o weights across all the three evaluation metrics for both the multiview and the single view experiments; thereby clearly demonstrating the importance and effectiveness of our proposed weighting strategy in handling noisy 2D detections and producing reliable triangulated 3D poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Mobile Camera</head><p>In this section, we evaluate and compare our proposed methods against (a) Ours w/o weights and (b) Ours+Iskakov [13] on a video sequence of SportCenter dataset captured on a single mobile camera. The images are captured using a calibrated Iphone6. Like before, we evaluate the learnt models of <ref type="table" target="#tab_2">Table 3</ref> on two test subjects, thereby resulting in a test set of 38 images. The results using the predictions of the learnt 2D pose estimator model f ? (i.e.x) and the 2D ground truth annotations (i.e. x) are shown in <ref type="table" target="#tab_5">Table 6</ref>. As expected, there exists a significant performance gap(? 60 mm) between usingx and x as the input to the lifting network g ? . We believe this gap in performance can be bridged provided sufficient amount of data obtained using a mobile camera is available to improve the performance of f ? . Having said that, our proposed method still remains the best performing method against the other two baseline methods which clearly shows the effectiveness of our weighting strategy to generate suitable triangulated 3D poses. <ref type="figure">Figure 9</ref>, 10, 11 and 12 show qualitative results for Human3.6M <ref type="bibr" target="#b9">[10]</ref>, MPI-INF-3DHP <ref type="bibr" target="#b25">[26]</ref>, and SportCenter datasets. For the SportCenter dataset we also show the 3D pose result of triangulating 2D predictions from multiple views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we have presented an effective way of using triangulation as a form of self-supervision for single view 3D pose estimation. Our approach imposes geometrical constraints on multi-view images to train models with little annotations in a semisupervised learning setup. Making the pipeline end-to-end differentiable was key to this. We have demonstrated how the proposed robust triangulation can reliably generate pseudo labels even in crowded scenes and how to use them to supervise a single view 3D pose estimator. The experimental results, especially in the SportCenter dataset, clearly shows its advantages in learning a precise triangulated 3D pose over other triangulation methods. In future, we plan to further investigate the impact of integrating temporal or spatial constraints into our learning framework. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Triangulation Error (lower is better)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Balancing Ltri. Ltri, described in Section ? 3.2.1, measures the difference between the detected and re-projected 2D point estimates after triangulation. It provides two gradients flows that we control with parameter ?. MPJPE, on the other hand, shows the difference between the triangulated and ground-truth 3D points. We show the effect on the Ltri loss and MPJPE for different values of ? in a controlled test setup. When ? = 0 the model can potentially reach the best MPJPE but then deviates quickly as the training continues. On the other hand, when ? = 1 the model cannot eliminate the noise. A trade-off between the two makes the model stable while reduces the impact of noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :Fig. 6 :Fig. 7 :Fig. 8 :Fig. 9 :Fig. 10 :Fig. 11 :Fig. 12 :</head><label>56789101112</label><figDesc>Qualitative results on the SportCenter dataset. From left to right, multi-view triangulated pose with (a) our approach and (b) Standard DLT (without weighting mechanism). Single view predicted results of (c) our approach and (d) without weighting. It can be noted that our weighting strategy produces more robust 3D poses which provide better supervision on occluded samples. Qualitative results on the SportCenter dataset. From left to right, multi-view triangulated pose with (a) our approach and (b) Standard DLT (without weighting mechanism). Single view predicted results of (c) our approach and (d) without weighting. Qualitative results on the SportCenter dataset. From left to right, multi-view triangulated pose with (a) our approach and (b) Standard DLT (without weighting mechanism). Single view predicted results of (c) our approach and (d) without weighting. Failure cases where the lifting network g ? fail to generate plausible 3D poses on theSportCenter dataset. From left to right, multi-view triangulated pose with (a) our approach and (b) Standard DLT (without weighting mechanism). Single view predicted results of (c) our approach and (d) without weighting. Qualitative results for the test set of Human3.6M. The superimposed pose in the top image (a) corresponds to the projection of the 3D prediction in this view from our single view model trained on 10% of labeled data. This projected pose is equivalent to the prediction of the 2D pose estimator. The bottom image (b) depicts the same 3D pose but from another viewpoint. This allow us to visualise the depth information, which would be not visible otherwise. Qualitative results for the test set of MPI-INF-3DHP. We show the single view 3D predictions of our model trained under the semi-supervised setting. Qualitative results on the test set of the SportCenter dataset. From top to bottom, superimposed 3D poses results of (a) multi-view triangulation, (b) our single-view lifting approach, and (c) same 3D pose as b) but projected into another view to visualize the depth information. Qualitative results on the unlabeled (training) set of the SportCenter dataset. From top to bottom, superimposed 3D poses results of (a) multi-view triangulation, (b) our single-view lifting approach, and (c) same 3D pose as b) but projected into another view to visualize the depth information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results Human3.6M (Semi-supervised) 10% of All Data Method MPJPE ? NMPJPE ? PMPJPE ?</figDesc><table><row><cell>Kundu et.al. [17]</cell><cell>-</cell><cell>-</cell><cell>50.8</cell></row><row><cell>Ours</cell><cell>56.9</cell><cell>56.6</cell><cell>45.4</cell></row><row><cell></cell><cell>Only S1</cell><cell></cell><cell></cell></row><row><cell>Rhodin et al. [37]</cell><cell>131.7</cell><cell>122.6</cell><cell>98.2</cell></row><row><cell cols="2">Pavlako et al. [30] 110.7</cell><cell>97.6</cell><cell>74.5</cell></row><row><cell>Li et al. [23]</cell><cell>88.8</cell><cell>80.1</cell><cell>66.5</cell></row><row><cell>Rhodin et al. [38]</cell><cell>-</cell><cell>80.1</cell><cell>65.1</cell></row><row><cell>Kocabas et al. [16]</cell><cell>-</cell><cell>67.0</cell><cell>60.2</cell></row><row><cell>Pavllo et al. [34]</cell><cell>64.7</cell><cell>61.8</cell><cell>-</cell></row><row><cell>Iqbal et al. [12]</cell><cell>62.8</cell><cell>59.6</cell><cell>51.4</cell></row><row><cell>Kundu et al. [17]</cell><cell>-</cell><cell>-</cell><cell>52</cell></row><row><cell>Ours</cell><cell>60.8</cell><cell>60.4</cell><cell>48.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results MPI-INF-3DHP (Semi-supervised)</figDesc><table><row><cell>Method</cell><cell cols="3">MPJPE ? NMPJPE ? PMPJPE ?</cell></row><row><cell></cell><cell>Only S1</cell><cell></cell><cell></cell></row><row><cell>Rhodin et al. [38]</cell><cell>-</cell><cell>121.8</cell><cell>-</cell></row><row><cell>Kocabas et al. [16]</cell><cell>-</cell><cell>119.9</cell><cell>-</cell></row><row><cell>Iqbal et al. [12]</cell><cell>113.8</cell><cell>102.2</cell><cell>-</cell></row><row><cell>Ours</cell><cell>102.2</cell><cell>99.6</cell><cell>93.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Quantitative results on SportCenter (semi-supervised) using different triangulation approaches.</figDesc><table><row><cell cols="2">Method</cell><cell></cell><cell cols="2">Multiview</cell><cell></cell><cell></cell><cell>Single View</cell></row><row><cell></cell><cell></cell><cell cols="6">MPJPE NMPJPE PMPJPE MPJPE NMPJPE PMPJPE</cell></row><row><cell cols="3">Ours non-differentiable w/o weights 109.7</cell><cell cols="2">107.7</cell><cell>97.6</cell><cell>142.9</cell><cell>140.4</cell><cell>108.6</cell></row><row><cell cols="2">Ours non-differentiable</cell><cell>83.0</cell><cell cols="2">79.3</cell><cell>70.2</cell><cell>111.4</cell><cell>107.7</cell><cell>82.5</cell></row><row><cell cols="2">Ours w/o weights</cell><cell>80.5</cell><cell cols="2">78.4</cell><cell>66.7</cell><cell>118.5</cell><cell>116.5</cell><cell>95.4</cell></row><row><cell cols="2">Ours+Iskakov [13]</cell><cell>88.3</cell><cell cols="2">83.6</cell><cell>70.9</cell><cell>121.1</cell><cell>119.5</cell><cell>99</cell></row><row><cell cols="2">Ours</cell><cell>66.9</cell><cell cols="2">65.5</cell><cell>55.4</cell><cell>104.4</cell><cell>102.1</cell><cell>81.1</cell></row><row><cell>NMPJPE [mm]</cell><cell>70 80 90 100 110</cell><cell>Resnet50 Pavllo et al. Ours + Iskakov et al. Ours</cell><cell>PMPJPE [mm]</cell><cell>60 70 80 90</cell><cell></cell><cell></cell><cell>Resnet50 Kundu et al. Ours + Iskakov et al.</cell></row><row><cell></cell><cell>50 60</cell><cell></cell><cell></cell><cell>50</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">5 % S 1 1 0 % S 1 5 0 % S 1 S 1 S 1 , 5 S 1 , 5 , 6 A ll</cell><cell></cell><cell cols="4">5 % S 1 1 0 % S 1 5 0 % S 1 S 1 S 1 , 5 S 1 , 5 , 6 A ll</cell></row></table><note>Ours Fig. 2: Comparison with Pavllo et al. [34], Kundu et al. [17] and our semi-supervised variant with the triangulation of Iskakov [13] on (left) NMPJPE and (right) PMPJPE metrics on Human3.6m with different amount of supervision.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Quantitative results on the Second Spectrum dataset (semi-supervised) using different triangulation approaches.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Multiview</cell><cell></cell><cell></cell><cell>Single View</cell><cell></cell></row><row><cell></cell><cell cols="6">MPJPE NMPJPE PMPJPE MPJPE NMPJPE PMPJPE</cell></row><row><cell cols="2">Ours w/o weights 61.5</cell><cell>60.3</cell><cell>52.3</cell><cell>95.7</cell><cell>93.9</cell><cell>75.8</cell></row><row><cell>Ours</cell><cell>54.3</cell><cell>52.9</cell><cell>46.2</cell><cell>94.7</cell><cell>92.8</cell><cell>74.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Quantitative reuslts on SportCenter dataset captured on a mobile camera (Single View).</figDesc><table><row><cell></cell><cell></cell><cell>Usingx</cell><cell></cell><cell></cell><cell>Using x</cell><cell></cell></row><row><cell>Method</cell><cell cols="6">MPJPE NMPJPE PMPJPE MPJPE NMPJPE PMPJPE</cell></row><row><cell>Ours w/o weights</cell><cell>199</cell><cell>197.3</cell><cell>140</cell><cell>138.3</cell><cell>138.4</cell><cell>88.1</cell></row><row><cell cols="2">Ours + Iskakov [13] 197.7</cell><cell>193.7</cell><cell>130.1</cell><cell>131.8</cell><cell>132.5</cell><cell>82.8</cell></row><row><cell>Ours</cell><cell>188.9</cell><cell>186.2</cell><cell>130.5</cell><cell>129.1</cell><cell>126.8</cell><cell>85.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Direct Linear Transformation from Comparator Coordinates into Object Space Coordinates in Close-Range Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Abdel-Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASP/UI Symposium on Close-Range Photogrammetry</title>
		<imprint>
			<date type="published" when="1971" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting Spatial-Temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised 3D Pose Estimation with Geometric Selfsupervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adversarial Posenet: A Structure-Aware Convolutional Network for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimizing Network Structure for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">RMPE: Regional Multi-Person Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Loan</surname></persName>
		</author>
		<title level="m">Matrix Computations (Johns Hopkins Studies in Mathematical Sciences)</title>
		<imprint>
			<publisher>The Johns Hopkins University Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reconstruction from Projections Using Grassmann Tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hand Pose Estimation via Latent 2.5 D Heatmap Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weakly-Supervised 3D Human Pose Learning via Multi-View Images in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (2020) 2, 3, 7</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learnable Triangulation of Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning 3D Human Dynamics from Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning of 3D Human Pose Using Multi-View Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-Supervised 3D Human Pose Estimation via Part Guided Novel Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Followme: Efficient Online Min-Cost Flow Tracking with Bounded Memory and Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High Performance Visual Tracking with Siamese Region Proposal Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visualizing the Loss Landscape of Neural Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation from Monocular Images with Deep Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maximum-Margin Structured Learning with Deep Networks for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On Boosting Single-Frame 3D Human Pose Estimation via Monocular Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Comprehensive Study of Weight Sharing in Graph Networks for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Simple Yet Effective Baseline for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Monocular 3D Human Pose Estimation in the Wild Using Improved CNN Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Vnect: Real-Time 3D Human Pose Estimation with a Single RGB Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiview-Consistent Semi-Supervised Learning for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked Hourglass Networks for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Texturepose: Supervising Human Mesh Estimation with Texture Consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ordinal Depth Supervision for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Coarse-To-Fine Volumetric Prediction for Single-Image 3D Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Harvesting Multiple Views for Marker-Less 3D Human Pose Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kostas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation in Video with Temporal Convolutions and Semi-Supervised Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep Multitask Architecture for Integrated 2D and 3D Human Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lightweight Multi-View 3D Pose Estimation through Camera-Disentangled Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Remelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised Geometry-Aware Representation for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning Monocular 3D Human Pose Estimation from Multi-View Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Spoerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lcr-Net: Localization-Classification-Regression for Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Lcr-Net++: Multi-Person 2D and 3D Pose Detection in Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Imagenet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Integral Human Pose Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Structured Prediction of 3D Human Pose with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bundle Adjustment -A Modern Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mclauchlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision Algorithms: Theory and Practice</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<title level="m">Adversarial Inverse Graphics Networks: Learning 2D-To-3D Lifting and Image-To-Image Translation from Unpaired Supervision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>International Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">RepNet: Weakly Supervised Training of an Adversarial Reprojection Network for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Distill Knowledge from NRFSM for Weakly Supervised 3D Pose Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">muSSP: Efficient Min-Cost Flow Algorithm for Multi-Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Srnet: Improving Generalization in 3D Human Pose Estimation with a Split-And-Recombine Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Martial arts, dancing and sports dataset: A challenging stereo and multi-view dataset for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Towards 3D Human Pose Estimation in the Wild: A Weakly-Supervised Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep Kinematic Pose Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Modulated Graph Convolutional Network for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
