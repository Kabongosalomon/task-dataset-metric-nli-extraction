<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TURL: Table Understanding through Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021">2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pvldb Reference Format: Xiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyssa</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Lees</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University Columbus</orgName>
								<address>
									<region>Ohio</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<settlement>Columbus</settlement>
									<region>Ohio</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Google Research</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Google Research</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Google Research</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TURL: Table Understanding through Representation Learning</title>
					</analytic>
					<monogr>
						<title level="j" type="main">TURL: Table Understanding through Representation Learning. PVLDB</title>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">3</biblScope>
							<biblScope unit="page" from="307" to="319"/>
							<date type="published" when="2021">2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.14778/3430915.3430921</idno>
					<note>PVLDB Artifact Availability: The source code, data, and/or other artifacts have been made available at https://github.com/sunlab-osu/TURL.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relational tables on the Web store a vast amount of knowledge.</p><p>Owing to the wealth of such tables, there has been tremendous progress on a variety of tasks in the area of table understanding. However, existing work generally relies on heavily-engineered taskspecific features and model architectures. In this paper, we present TURL, a novel framework that introduces the pre-training/finetuning paradigm to relational Web tables. During pre-training, our framework learns deep contextualized representations on relational tables in an unsupervised manner. Its universal model design with pre-trained representations can be applied to a wide range of tasks with minimal task-specific fine-tuning.</p><p>Specifically, we propose a structure-aware Transformer encoder to model the row-column structure of relational tables, and present a new Masked Entity Recovery (MER) objective for pre-training to capture the semantics and knowledge in large-scale unlabeled data. We systematically evaluate TURL with a benchmark consisting of 6 different tasks for table understanding (e.g., relation extraction, cell filling). We show that TURL generalizes well to all tasks and substantially outperforms existing methods in almost all instances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Relational tables are in abundance on the Web and store a large amount of knowledge, often with key entities in one column and attributes in the others. Over the past decade, various large-scale collections of such tables have been aggregated <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b26">25]</ref>. For example, Cafarella et al. <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7]</ref> reported 154M relational tables out of Owing to the wealth and utility of these datasets, various tasks such as table interpretation <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b49">48]</ref>, table augmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b47">46]</ref>, etc., have made tremendous progress in the past few years.</p><p>However, previous work such as <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b47">46]</ref> often rely on heavilyengineered task-specific methods such as simple statistical/language features or straightforward string matching. These techniques suffer from several disadvantages. First, simple features only capture shallow patterns and often fail to handle the flexible schema and varied expressions in Web tables. Second, task-specific features and model architectures require effort to design and do not generalize well across tasks.</p><p>Recently, the pre-training/fine-tuning paradigm has achieved notable success on unstructured text data. Advanced language models such as BERT <ref type="bibr" target="#b16">[15]</ref> can be pre-trained on large-scale unsupervised text and subsequently fine-tuned on downstream tasks using task-specific supervision. In contrast, little effort has been extended to the study of such paradigms on relational tables. Our work fills this research gap.</p><p>Promising results in some table based tasks were achieved by the representation learning model of <ref type="bibr" target="#b14">[13]</ref>. This work serializes a table into a sequence of words and entities (similar to text data) and learns embedding vectors for words and entities using Word2Vec <ref type="bibr" target="#b29">[28]</ref>. However, <ref type="bibr" target="#b14">[13]</ref> cannot generate contextualized representations, i.e., it does not consider varied use of words/entities in different contexts and only produces a single fixed embedding vector for word/entity. In addition, shallow neural models like Word2Vec have relatively limited learning capabilities, which hinder the capture of complex semantic knowledge contained in relational tables.</p><p>We propose TURL, a novel framework for learning deep contextualized representations on relational tables via pre-training in an unsupervised manner and task-specific fine-tuning.</p><p>There are two main challenges in the development of TURL: (1) Relational table encoding. Existing neural network encoders are designed for linearized sequence input and are a good fit with unstructured texts. However, data in relational tables is organized in a semi-structured format. Moreover, a relational table contains multiple components including the table caption, headers and cell values. The challenge is to develop a means of modeling the rowand-column structure as well as integrating the heterogeneous information from different components of the table. (2) Factual knowledge modeling. Pre-trained language models like BERT <ref type="bibr" target="#b16">[15]</ref> and ELMo <ref type="bibr" target="#b32">[31]</ref> focus on modeling the syntactic and semantic characteristics of word use in natural sentences. However, relational tables contain a vast amount of factual knowledge about entities, which cannot be captured by existing language models directly. Effectively modelling such knowledge in TURL is a second challenge.</p><p>To address the first challenge, we encode information from different table components into separate input embeddings and fuse them together. We then employ a structure-aware Transformer <ref type="bibr" target="#b39">[38]</ref> encoder with masked self-attention. The conventional Transformer model is a bi-directional encoder, so each element (i.e., token/entity) can attend to all other elements in the sequence. We explicitly model the row-and-column structure by restraining each element to only aggregate information from other structurally related elements. To achieve this, we build a visibility matrix based on the table structure and use it as an additional mask for the self-attention layer.</p><p>For the second challenge, we first learn embeddings for each entity during pre-training. We then model the relation between entities in the same row or column with the assistance of the visibility matrix. Finally, we propose a Masked Entity Recovery (MER) pre-training objective. The technique randomly masks out entities in a table with the objective of recovering the masked items based on other entities and the table context (e.g., caption/header). This encourages the model to learn factual knowledge from the tables and encode it into entity embeddings. In addition, we utilize the entity mention by keeping it as additional information for a certain percentage of masked entities. This helps our model build connections between words and entities . We also adopt the Masked Language Model (MLM) objective from BERT, which aims to model the complex characteristics of word use in table metadata.</p><p>We pre-train our model on 570K relational tables from Wikipedia to generate contextualized representations for tokens and entities in the relational tables. We then fine-tune our model for specific downstream tasks using task-specific labeled data. A distinguishing feature of TURL is its universal architecture across different tasks -only minimal modification is needed to cope with each downstream task. To facilitate research in this direction, we compiled a benchmark that consists of 6 diverse tasks, including entity linking, column type annotation, relation extraction, row population, cell filling and schema augmentation. We created new datasets in addition to including results for existing datasets when publicly available. Experimental results show that TURL substantially outperforms existing task-specific and shallow Word2Vec based methods.</p><p>Our contributions are summarized as follows:</p><p>? To the best of our knowledge, TURL is the first framework that introduces the pre-training/fine-tuning paradigm to relational Web tables. The pre-trained representations along with the universal model design save tremendous effort on engineering task-specific features and architectures. ? We propose a structure-aware Transformer encoder to model the structure information in relational tables. We also present a novel Masked Entity Recovery (MER) pre-training objective to learn the semantics as well as the factual knowledge about entities in relational tables. ? To facilitate research in this direction, we present a benchmark that consists of 6 different tasks for table interpretation and augmentation. We show that TURL generalizes well to various tasks and substantially outperforms existing models. Our source code, benchmark, as well as pre-trained models will be available online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY</head><p>We now present our data model and give a formal task definition:</p><p>In this work, we focus on relational Web tables and are most interested in the factual knowledge about entities. Each table ? T is associated with the following: <ref type="bibr" target="#b0">(1)</ref>  , where e is the specific entity linked to the cell and m is the entity mention (i.e., the text string).</p><p>( , , ) is also known as table metadata and is the actual table content. Notations used in the data model are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Explicitly, we study the unsupervised representation learning on relational Web tables, which is defined as follows.</p><p>Definition 2.1. Given a relational Web table corpus, our representation learning task aims to learn in an unsupervised manner a task-agnostic contextualized vector representation for each token in all table captions 's and headers 's and for each entity (i.e., all entity cells 's and topic entities 's).</p><p>Representation Learning. The pre-training/fine-tuning paradigm has drawn tremendous attention in recent years. Extensive effort has been devoted to the development of unsupervised representation learning methods for both unstructured text and structured knowledge bases, which in turn can be utilized for a wide variety of downstream tasks via fine-tuning.</p><p>Earlier work, including Word2Vec <ref type="bibr" target="#b29">[28]</ref> and GloVe <ref type="bibr" target="#b31">[30]</ref>, pre-train distributed representations for words on large collections of documents. The resulting representations are widely used as input embeddings and offer significant improvements over randomly initialized parameters. However, pre-trained word embeddings suffer from word polysemy: they cannot model varied word use across linguistic contexts. This complexity motivated the development of contextualized word representations <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b44">43]</ref> . Instead of learning fixed embeddings per word, these works construct language models that learn the joint probabilities of sentences. Such pre-trained language models have had huge success and yield state-of-the-art results on various NLP tasks <ref type="bibr" target="#b40">[39]</ref>.</p><p>Similarly, unsupervised representation learning has also been adopted in the space of structured data like knowledge bases (KB) and databases. Entities and relations in KB have been embedded into continuous vector spaces that still preserve the inherent structure of the KB <ref type="bibr" target="#b38">[37]</ref>. These entity and relation embeddings are utilized by a variety of tasks, such as KB completion <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b41">40]</ref>, relation extraction <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b42">41]</ref>, entity resolution <ref type="bibr" target="#b19">[18]</ref>, etc. Similarly, <ref type="bibr" target="#b18">[17]</ref> learned embeddings for heterogeneous data in databases and used it for data integration tasks.</p><p>More recently, there has been a corpus of work incorporating knowledge information into pre-trained language models <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b50">49]</ref>. ERNIE <ref type="bibr" target="#b50">[49]</ref> injects knowledge base information into a pre-trained BERT model by utilizing pre-trained KB embeddings and a denoising entity autoencoder objective. The experimental results demonstrate that knowledge information is extremely helpful for tasks such as entity linking, entity typing and relation extraction.</p><p>Despite the success of representation learning on text and KB, few works have thoroughly explored contextualized representation learning on relational Web tables. Pre-trained language models are directly adopted in <ref type="bibr" target="#b27">[26]</ref> for entity matching. Two recent papers from the NLP community <ref type="bibr" target="#b21">[20,</ref><ref type="bibr" target="#b45">44]</ref> study pre-training on Web tables to assist in semantic parsing or question answering tasks on tables. In this work, we introduce TURL, a new methodology for learning deep contextualized representations for relational Web tables that preserve both semantic and knowledge information. In addition, we conduct comprehensive experiments on a much wider range of table-related tasks. <ref type="table">Table Interpretation</ref>. The Web stores large amounts of knowledge in relational tables. Table interpretation aims to uncover the semantic attributes of the data contained in relational tables, and transform this information into machine understandable knowledge. This task is usually accomplished with help from existing knowledge bases. In turn, the extracted knowledge can be used for KB construction and population.</p><p>There are three main tasks for table interpretation: entity linking, column type annotation and relation extraction <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b48">47]</ref>. Entity linking is the task of detecting and disambiguating specific entities mentioned in a table. Since relational tables are centered around entities, entity linking is a key step for table interpretation, and a fundamental component to many table-related tasks <ref type="bibr" target="#b48">[47]</ref>. <ref type="bibr" target="#b4">[4]</ref> employed a graphical model, and used a collective classification technique to optimize a global coherence score for a set of entities in a table. <ref type="bibr" target="#b36">[35]</ref> presented the T2K framework, which is an iterative matching approach that combines both schema and entity matching. More recently, <ref type="bibr" target="#b17">[16]</ref> introduced a hybrid method that combines both entity lookup and entity embeddings, which resulted in superior performance on various benchmarks.</p><p>Column type annotation and relation extraction both work with table columns. The former aims to annotate columns with KB types while the latter intends to use KB predicates to interpret relations between column pairs. Prior work has generally coupled these two tasks with entity linking <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b49">48]</ref>. After linking cells to entities, the types and relations associated with the entities in KB can then be used to annotate columns. In recent work, column annotation without entity linking has been explored <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b22">21]</ref>. These works modify text classification models to fit relational tables and have shown promising results. Moreover, relation extraction on web tables has also been studied for KB augmentation <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b37">36]</ref>. <ref type="table">Table Augmentation</ref>. Tables are a popular data format to organize and present relational information. Users often have to manually compose tables when gathering information. It is desirable to offer some intelligent assistance to the user, which motivates the study of table augmentation <ref type="bibr" target="#b46">[45]</ref>. Table augmentation refers to the task of expanding a seed query table with additional data. Specifically, for relational tables this can be divided into three sub-tasks: row population for retrieving entities for the subject column <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b46">45]</ref>, cell filling that fills the cell values for given subject entities <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b47">46]</ref> and schema augmentation that recommends headers to complete the table schema <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b46">45]</ref>. For row population tasks, <ref type="bibr" target="#b12">[12]</ref> searches for complement tables that are semantically related to seed entities and the top ranked tables are used for population. <ref type="bibr" target="#b46">[45]</ref> further incorporates knowledge base information with a table corpus, and develops a generative probabilistic model to rank candidate entities with entity similarity features. For cell filling, <ref type="bibr" target="#b43">[42]</ref> uses the query table to search for matching tables, and extracts attribute values from those tables. More recently, <ref type="bibr" target="#b47">[46]</ref> proposed the CellAu-toComplete framework that makes use of a large table corpus and a knowledge base as data sources, and incorporates preprocessing, candidate value finding, and value ranking components. In terms of schema augmentation, <ref type="bibr" target="#b6">[6]</ref> tackles this problem by utilizing an attribute correlation statistics database (ACSDb) collected from a table corpus. <ref type="bibr" target="#b46">[45]</ref> utilizes a similar approach to the row population techniques and ranks candidate headers with sets of features.</p><p>Existing benchmarks. Several benchmarks have been proposed for table interpretation: (1) T2Dv2 <ref type="bibr" target="#b26">[25]</ref> proposed in 2016 contains 779 tables from various websites. It contains 546 relational tables, with 25119 entity annotations, 237 table-to-class annotations and 618 attribute-to-property annotations. (2) Limaye et al. <ref type="bibr" target="#b28">[27]</ref> proposed a benchmark in 2010 which contains 296 tables from Wikipedia. It was used in <ref type="bibr" target="#b17">[16]</ref> for entity linking, and was also used in <ref type="bibr" target="#b11">[11]</ref> for column type annotation. (3) Efthymiou et al. <ref type="bibr" target="#b17">[16]</ref> created a benchmark (referred to as "WikiGS" in our experiments) that includes 485,096 tables from Wikipedia. WikiGS was originally used for entity linking with 4,453,329 entity matches. <ref type="bibr" target="#b11">[11]</ref> further annotated a subset of it containing 620 entity columns with 31 DBpedia types and used it for column type annotation. (4) The recent SemTab 2019 <ref type="bibr" target="#b24">[23]</ref> challenge also aims at benchmarking systems that match tabular data to KBs, including three tasks, i.e., assigning a semantic type to an column, matching a cell to an entity, and assigning a property to the relationship between two columns. It used sampled tables from T2Dv2 <ref type="bibr" target="#b26">[25]</ref> and WikiGS <ref type="bibr" target="#b17">[16]</ref> in the first two rounds, and automatically generated tables in later rounds.</p><p>In contrast to table interpretation, few benchmarks have been released for table augmentation. Zhang et al. <ref type="bibr" target="#b46">[45]</ref> studied row population and schema augmentation with 2000 randomly sampled Wikipedia tables in total for validation and testing. <ref type="bibr" target="#b47">[46]</ref> curated a test collection with 200 columns containing 1000 cells from Wikipedia tables for evaluating cell filling.</p><p>Although these benchmarks have been used in various recent studies, they still suffer from a few shortcomings: (1) They are typically small sets of sampled tables with limited annotations. (2) SemTab 2019 contains a large number of instances; however, most of them are automatically generated and lack metadata/context of the Web tables. In this work, we compile a larger benchmark covering both table interpretation and table augmentation tasks. We also use some of these existing datasets for more comprehensive evaluation. By leveraging large-scale relational tables on Wikipedia and a curated KB, we ensure both the size and quality of our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>In this section, we introduce our TURL framework for unsupervised representation learning on relational tables. TURL is first trained on an unlabeled relational Web table corpus with pre-training objectives carefully designed to learn word semantics as well as relational knowledge between entities. The model architecture is general and can be applied to a wide range of downstream tasks with minimal modifications. Moreover, the pre-training process alleviates the need for large-scale labeled data for each downstream task. <ref type="figure" target="#fig_1">Figure 2</ref> presents an overview of TURL which consists of three modules: (1) an embedding layer to convert different components of an input table into input embeddings, (2) N stacked structure-aware Transformer <ref type="bibr" target="#b39">[38]</ref> encoders to capture the textual information and relational knowledge, and (3) a final projection layer for pre-training objectives. <ref type="figure" target="#fig_2">Figure 3</ref> shows an input-output example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Embedding Layer</head><p>Given a table =( , , , ), we first linearize the input into a sequence of tokens and entity cells by concatenating the table metadata and scanning the table content row by row. The embedding layer then converts each token in and and each entity in and into an embedding representation. Input token representation. For each token , its vector representation is obtained as follows:</p><formula xml:id="formula_0">x t = w + t + p.<label>(1)</label></formula><p>Here w is the word embedding vector, t is called the type embedding vector and aims to differentiate whether token is in the table caption or a header, and p is the position embedding vector that provides relative position information for a token within the caption or a header. Input entity representation. For each entity cell = ( e , m ) ( same for topic entity ), we fuse the information from the linked entity e and entity mention m together, and use an additional type embedding vector t e to differentiate three types of entity cells (i.e., subject/object/topic entities). Specifically, we calculate the input entity representation x e as:</p><p>x e = LINEAR([e e ; e m ]) + t e ;</p><p>(2)</p><formula xml:id="formula_1">e m = MEAN(w 1 , w 2 , . . . , w , . . .).<label>(3)</label></formula><p>Here e e is the entity embedding learned during pre-training. To represent entity mention m , we use its average word embedding w 's. LINEAR is a linear layer to fuse e e and e m .</p><p>A sequence of token and entity representations (x t 's and x e 's) are then fed into the next module of TURL, a structure-aware Transformer encoder, which will produce contextualized representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Structure-aware Transformer Encoder</head><p>We choose Transformer <ref type="bibr" target="#b39">[38]</ref> as our base encoder block, since it has been widely used in pre-trained language models <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b34">33]</ref> and achieves superior performance on various natural language processing tasks <ref type="bibr" target="#b40">[39]</ref>. Due to space constraints, we only briefly introduce the conventional Transformer encoder and refer readers to <ref type="bibr" target="#b39">[38]</ref> for more details. Finally, we present a detailed explanation on our proposed visibility matrix for modeling table structure.</p><p>Each Transformer block is composed of a multi-head self-attention layer followed by a point-wise, fully connected layer <ref type="bibr" target="#b39">[38]</ref>. Specifically, we calculate the multi-head attention as follows:</p><formula xml:id="formula_2">MultiHead(h) = [head 1 ; ...; head i ; ...; head k ] ; head i = Attention h , h , h ; Attention( , , ) = Softmax ? .<label>(4)</label></formula><p>Here h ? R ? model is the hidden state output from the previous Transformer layer or the input embedding layer and is the input sequence length. 1 ? is the scaling factor.</p><formula xml:id="formula_3">? R model ? , ? R model ? , ? R model ? and ? R k ? intermediate are parameter matrices.</formula><p>For each head, we have = model /k, where k is the number of attention heads. ? R ? is the visibility matrix which we detail next.   should not be related to <ref type="bibr">[Pratidwandi]</ref>. Similarly, [Hindi] is a "language" and its representation has little to do with the header "Film". We propose a visibility matrix to model such structure information in a relational table. <ref type="figure" target="#fig_3">Figure 4</ref> shows an example of .</p><p>Our visibility matrix acts as an attention mask so that each token (or entity) can only aggregate information from other structurally related tokens/entities during the self-attention calculation. is a symmetric binary matrix with , = 1 if and only if element is visible to element . The element here can be a token in the caption or a header, or an entity in a table cell. Specifically, we define as follows:</p><p>? If element is the topic entity or a token in table caption, ? , , = 1.  <ref type="figure" target="#fig_4">Figure 5</ref> as an example. "National Film ... recipients ..." are tokens from the caption and [National Film Award for Best Direction] is the topic entity, and hence they can aggregate information from all other elements. "Year" is a token from a column header, and it can attend to all elements except for entity cells not belonging to that column. [Satyajit] is an entity from a table cell, so it can only attend to the caption, topic entity, entity cells in the same row/column as well as the header of that column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Pre-training Objective</head><p>In order to pre-train our model on an unlabeled table corpus, we adopt the Masked Language Model (MLM) objective from BERT to learn representations for tokens in table metadata and propose a Masked Entity Recovery (MER) objective to learn entity cell representations. Masked Language Model. We adopt the same Masked Language Model objective as BERT, which trains the model to capture the lexical, semantic and contextual information described by table metadata. Given an input token sequence including table caption and table headers, we simply mask some percentage of the tokens at random, and then predict these masked tokens. We adopt the same percentage settings as BERT. The pre-training data processor selects 20% of the token positions at random (note, we use a slightly larger ratio compared with 15% in <ref type="bibr" target="#b16">[15]</ref> as we want to make the pre-training more challenging). For a selected position, (1) 80% of the time we replace it with a special [MASK] token, (2) 10% of the time we replace it with another random token, and (3) 10% of the time we keep it unchanged.  <ref type="figure" target="#fig_2">Figure 3</ref> shows an example of the above random process in MLM, where (1) "film", "award" and "recipient" are chosen randomly, and (2) the input word embedding of "film" is further chosen randomly to be replaced with the embedding of [MASK], (3) the input word embedding of "recipient" to be replaced with the embedding of a random word "milk", and (4) the input word embedding of "award" to remain the same.</p><p>Given a token position selected for MLM, which has a contexutalized representation h t output by our encoder, the probability of predicting its original token ? W is then calculated as: In addition, our proposed masking mechanism takes advantage of entity mentions. Specifically, as shown in Eqn. 2, the input entity representation has two parts: the entity embedding e e and the entity mention representation e m . For some percentage of masked entity cells, we only mask e e , and as such the model receives additional entity mention information to help form predictions. This assists the model in building a connection between entity embeddings and entity mentions, and helps downstream tasks where only cell texts are available.</p><formula xml:id="formula_4">( ) = exp LINEAR(h t ) ? w ?W exp LINEAR(h t ) ? w<label>(5)</label></formula><p>Specifically, we propose the following masking mechanism for MER: The pre-training data processor chooses 60% of entity cells at random. Here we adopt a higher masking ratio for MER compared with MLM, because oftentimes in downstream tasks, none or few entities are given. For one chosen entity cell, (1) 10% of the time we keep both e m and e e unchanged (2) 63% (i.e., 70% of the left 90%) of the time we mask both e m and e e (3) 27% (i.e., 30% of the left 90%) of the time we keep e m unchanged, and mask e e (among which we replace e e with embedding of a random entity to inject noise in 10% of the time). Similar to BERT, in both MLM and MER we keep a certain portion of the selected positions unchanged so that the model can generate good representations for non-masked tokens/entity cells. Trained with random tokens/entities replacing the original ones, the model is robust and utilizes contextual information to make predictions rather than simply copying the input representation. Given an entity cell selected for MER with a contexutalized representation h e output by our encoder, the probability of predicting entity ? E is then calculated as follows.</p><formula xml:id="formula_5">( ) = exp (LINEAR(h e ) ? e e ) ?E exp LINEAR(h e ) ? e e<label>(6)</label></formula><p>In reality, considering the entity vocabulary E is quite large, we only use the above equation to rank entities from a given candidate set.</p><p>For efficient training, we construct the candidate set with (1) entities in the current table, (2) entities that have co-occurred with those in the current table, and (3) randomly sampled negative entities. We use a cross-entropy loss function for both MLM and MER objectives and the final pre-training loss is given as follows:</p><formula xml:id="formula_6">= ?? log ( ( )) + ?? log ( ( )),<label>(7)</label></formula><p>where the sums are over all tokens and entity cells selected in MLM and MER respectively.</p><p>Pre-training details. In this work, we denote the number of Transformer blocks as N, the hidden dimension of input embeddings and all Transformer block outputs as model , the hidden dimension of the fully connected layer in a Transformer block as intermediate , and the number of self-attention heads as k. We take advantage of a pre-trained TinyBERT <ref type="bibr" target="#b23">[22]</ref> model, which is a knowledge distilled version of BERT with a smaller size, and set the hyperparameters as follows: N = 4, model = 312, intermediate = 1200, k = 12. We initialize our structure-aware Transformer encoder parameters, word embeddings and position embeddings with TinyBERT <ref type="bibr" target="#b23">[22]</ref>. Entity embeddings are initialized using averaged word embeddings in entity names, and type embeddings are randomly initialized. We use the Adam <ref type="bibr" target="#b25">[24]</ref> optimizer with a linearly decreasing learning rate. The initial learning rate is 1e-4 chosen from [1e-3, 5e-4, 1e-4, 1e-5] based on our validation set. We pre-trained the model for 80 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DATASET CONSTRUCTION FOR PRE-TRAINING</head><p>We construct a dataset for unsupervised representation learning based on the WikiTable corpus <ref type="bibr" target="#b4">[4]</ref>, which originally contains around 1.65M tables extracted from Wikipedia pages. The corpus contains a large amount of factual knowledge on various topics ranging from sport events (e.g., Olympics) to artistic works (e.g., TV series). The following sections introduce our data construction process as well as characteristics of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Pre-processing and Partitioning</head><p>Pre-processing. The corresponding Wikipedia page of a table often provides much contextual information, such as page title and section title that can aid in the understanding of a table topic. We concatenate page title, section title and table caption to obtain a comprehensive description.</p><p>In addition, each table in the corpus contains one or more header rows and several rows of table content. For tables with more than one header row, we concatenate headers in the same column to obtain one header for each column. For each cell, we obtain hyperlinks to Wikipedia pages in it and use them to normalize different entity mentions corresponding to the same entity. We treat each Wikipedia page as an individual entity and do not use additional tools to perform entity linking with an external KB. For cells containing multiple hyperlinks, we only keep the first link. We also discard rows that have merged columns in a table. Identify relational tables. We first locate all columns that contain at least one linked cell after pre-processing. We further filter out noisy columns with empty or illegal headers (e.g., note, comment, reference, digit numbers, etc.). The columns left are entity-centric and are referred to as entity columns. We then identify relational tables by finding tables that have a subject column. A simple heuristic is employed for subject column detection: the subject column must be located in the first two columns of the table and contain unique entities which we treat as subject entities. We further filter out tables containing less than three entities or more than twenty columns. With this process, we obtain 670,171 relational tables. Data partitioning. From the above 670,171 tables, we select a high quality subset for evaluation: From tables that have (1) more than four linked entities in the subject column, (2) at least three entity columns including the subject column, and (3) more than half of the cells in entity columns are linked, we randomly select 10000 to form a held-out set. We further randomly partition this set into validation/test sets via a rough 1:1 ratio for model evaluation. All relational tables not in the evaluation set are used for pre-training. In sum, we have 570171 / 5036 / 4964 tables respectively for pretraining/validation/test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dataset Statistics in Pre-training</head><p>Fine-grained statistics of our datasets are summarized in <ref type="table" target="#tab_4">Table 2</ref>. We can see that most tables in our pre-training dataset have moderate size, with median of 8 rows, 2 entity columns and 9 entities per table. We build a token vocabulary using the BERT-based tokenizer <ref type="bibr" target="#b16">[15]</ref> (with 30,522 tokens in total). For the entity vocabulary, we construct it based on the training table corpus and obtain 926,135 entities after removing those that appear only once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>To systematically evaluate our pre-trained framework as well as facilitate research, we compile a table understanding benchmark consisting of 6 widely studied tasks covering table interpretation (e.g., entity linking, column type annotation, relation extraction) and table augmentation (e.g., row population, cell filling, schema augmentation). We include existing datasets for entity linking. However, due to the lack of large-scale open-sourced datasets, we create <ref type="table">Table 3</ref>: An overview of our benchmark tasks and strategies to finetune TURL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>Finetune Strategy <ref type="table">Table Interpretation   Table Augmentation</ref> new datasets for other tasks based on our held-out set of relational tables and an existing KB. Next we introduce the definition, baselines, dataset and results for each task. Our pre-trained framework is general and can be fine-tuned for all the independent tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">General Setup across All Tasks</head><p>We use the pre-training tables to create the training set for each task, and always build data for evaluation using the held-out validation/test tables. This way we ensure that there is no overlapping tables in training and validation/test. For fine-tuning, we initialize the parameters with a pre-trained model, and further train all parameters with a task-specific objective. To demonstrate the efficiency of pre-training, we only fine-tune our model for 10 epochs unless otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Entity Linking</head><p>Entity linking is a fundamental task in table interpretation, which is defined as: Definition 6.1. Given a table and a knowledge base KB, entity linking aims to link each potential mention in cells of to its referent entity ? KB.</p><p>Entity linking is usually addressed in two steps: a candidate generation module first proposes a set of potential entities, and an entity disambiguation module then ranks and selects the entity that best matches the surface form and is most consistent with the table context. Following existing work <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b36">35]</ref>, we focus on entity disambiguation and use an existing Wikidata Lookup service for candidate generation.</p><p>Baselines. We compare against the most recent methods for table entity linking T2K <ref type="bibr" target="#b36">[35]</ref>, Hybrid II <ref type="bibr" target="#b17">[16]</ref>, as well as the off-the-shelf Wikidata Lookup service. T2K uses an iterative matching approach that combines both schema and entity matching. Hybrid II <ref type="bibr" target="#b17">[16]</ref> combines a lookup method with an entity embedding method. For Wikidata Lookup, we simply use the top-1 returned result as the prediction.</p><p>Fine-tuning TURL. Entity disambiguation is essentially matching a table cell with candidate entities. We treat each cell as a potential entity, and input its cell text (entity mention e m in Eqn. 2) as well as table metadata to our Transformer encoder and obtain a contextualized representation h e for each cell. To represent each candidate entity, we utilize the name and description as well as type information from a KB. The intuition is that when the candidate generation module proposes multiple entity candidates with similar names, we will utilize the description and type information to find the candidate that is most consistent with the table context. Specifically, for a KB entity , given its name and description (both are a sequence of words) and types , we get its representation e kb as follows:</p><formula xml:id="formula_7">e kb = [MEAN ? (w) , MEAN ? (w) , MEAN ? (t)].<label>(8)</label></formula><p>Here, w is the embedding for word , which is shared with the embedding layer of pre-trained model. t is the embedding for entity type to be learned during this fine-tuning phase. We then calculate a matching score between e kb and h e similarly as Eqn. 6. We do not use the entity embeddings pre-trained by our model here, as the goal is to link mentions to entities in a target KB, not necessarily those appear in our pre-training table corpus. The model is fine-tuned with a cross-entropy loss.</p><p>Task-specific Datasets. We use three datasets to compare different entity linking models: (1) We adopt the Wikipedia gold standards (WikiGS) dataset from <ref type="bibr" target="#b17">[16]</ref>, which contains 4,453,329 entity mentions extracted from 485,096 Wikipedia tables and links them to DBpedia <ref type="bibr" target="#b3">[3]</ref>. <ref type="formula">(2)</ref> Since tables in WikiGS also come from Wikipedia, some of the tables may have already been seen during pre-training, despite their entity linking information is mainly used to pre-train entity embeddings (which are not used here). For a better comparison, we also create our own test set from the held-out test tables mentioned in Section 5.1, which contains 297,018 entity mentions from 4,964 tables. <ref type="formula" target="#formula_1">(3)</ref> To test our model on Web tables (i.e., those from websites other than Wikipedia), we also include the T2D dataset <ref type="bibr" target="#b26">[25]</ref> which contains 26,124 entity mentions from 233 Web tables. <ref type="bibr" target="#b0">1</ref> We use names and descriptions returned by Wikidata Lookup, and entity types from DBpedia. The training set for fine-tuning TURL is based on our pre-training corpus, but with tables in the above WikiGS removed. We also remove duplicate entity mentions and mentions where Wikidata Lookup fails to return the ground truth entity in candidates, and finally obtain 1,264,217 entity mentions in 192,728 tables to fine-tune our model for the entity linking task.</p><p>Results. We set the maximum candidate size for Wikidata Lookup at 50 and also include the result of a Wikidata Lookup (Oracle), which considers an entity linking instance as correct if the groundtruth entity is in the candidate set. Due to lack of open-sourced <ref type="bibr" target="#b0">1</ref> We use the data released by <ref type="bibr" target="#b17">[16]</ref> (https://doi.org/10.6084/m9.figshare.5229847). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>WikiGS Our Test Set T2D F1 P R F1 P R F1 P R T2K <ref type="bibr" target="#b36">[35]</ref> 34 70 22 ---82 90 76 Hybrid II <ref type="bibr" target="#b17">[16]</ref> 64 <ref type="formula" target="#formula_5">69</ref>  implementations, we directly use the results of T2K and Hybrid II in <ref type="bibr" target="#b17">[16]</ref>. We use F1, precision (P) and recall (R) measures for evaluation. False positive is the number of mentions where the model links to wrong entities, not including the cases where the model makes no prediction (e.g., Wikidata Lookup returns empty candidate set). As shown in <ref type="table" target="#tab_5">Table 4</ref>, our model gets the best F1 score and substantially improves precision on WikiGS and our own test set. The disambiguation accuracy on WikiGS is 89.62% (predict the correct entity if it is in the candidate set). A more advanced candidate generation module can help achieve better results in the future. We also conduct an ablation study on our model by removing the description or type information of a candidate entity from Eqn. 8. From <ref type="table" target="#tab_5">Table 4</ref>, we can see that entity description is very important for disambiguation, while entity type information only results in a minor improvement. This is perhaps due to the incompleteness of DBpedia, where a lot of entities have no types assigned or have missing types.</p><p>On the T2D dataset, all models perform much better than on the two Wikipedia datasets, mainly because of its smaller size and limited types of entities. The Wikidata Lookup baseline achieved high performance, and re-ranking using our model does not further improve. However, we adopt simple reweighting 2 to take into account the original result returned by Wikidata Lookup, which brings the F1 score to 0.82. This demonstrates the potential of using features such as entity popularity (used in Wikidata Lookup) and ensembling strong base models. Additionally, we conduct an error analysis on T2D comparing our model (TURL + fine-tuning + reweighting) with Wikidata Lookup. From <ref type="table" target="#tab_7">Table 5</ref>, we can see that while in many cases, our model can infer the correct entity type based on the context and re-rank the candidate list accordingly, it makes mistakes when there are entities in the KB that look very similar to the mentions. To summarize, <ref type="table" target="#tab_5">Table 4</ref> and 5 show that there is room for further improvement of our model on entity linking, which we leave as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Column Type Annotation</head><p>We define the task of column type annotation as follow: Definition 6.2. Given a table and a set of semantic types L, column type annotation refers to the task of annotating a column in with ? L so that all entities in the column have type . Note that a column can have multiple types. Column type annotation is a crucial task for table understanding and is a fundamental step for many downstream tasks like data integration and knowledge discovery. Earlier work <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b49">48]</ref> on column type annotation often coupled the task with entity linking. First entities in a column are linked to a KB and then majority voting is employed on the types of the linked entities. More recently, <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b22">21]</ref> have studied column type annotation based on cell texts only. Here we adopt a similar setting, i.e., use the available information in a given table directly for column type annotation without performing entity linking first.</p><p>Baselines. We compare our results with the state-of-the-art model Sherlock <ref type="bibr" target="#b22">[21]</ref> for column type annotation. Sherlock uses 1588 features describing statistical properties, character distributions, word embeddings, and paragraph vectors of the cell values in a column. It was originally designed to predict a single type for a given column. We change its final layer to |L| Sigmoid activation functions, each with a binary cross-entropy loss, to fit our multilabel setting. We also evaluate our model using two datasets in <ref type="bibr" target="#b11">[11]</ref>, and include the HNN + P2Vec model as baseline. HNN + P2Vec employs a hybrid neural network to extract cell, row and column features, and combines it with property features retrieved from KB.</p><p>Fine-tuning TURL. To predict the type(s) for a column, we first extract the contextualized representation of the column h as follows:</p><formula xml:id="formula_8">h = [MEAN h t , . . . ; MEAN h e , . . . ].<label>(9)</label></formula><p>Here h t 's are representations of tokens in the column header, h e 's are representations of entity cells in the column. The probability of predicting type is then given as,</p><formula xml:id="formula_9">( ) = Sigmoid (h + ) .<label>(10)</label></formula><p>Same as with the baselines, we optimize the binary cross-entropy loss, is the ground truth label for type</p><formula xml:id="formula_10">= ?? log ( ( )) + (1 ? ) log (1 ? ( ))<label>(11)</label></formula><p>Task-specific Datasets. We refer to Freebase <ref type="bibr" target="#b20">[19]</ref> to obtain semantic types L because of its richness, diversity, and scale. We only keep those columns in our relational table corpus that have at least three linked entities to Freebase, and for each column, we use the common types of its entities as annotations. We further filter out types with less than 100 training instances and keep only the most representative types. In the end, we get a total number of 255 types, 628,254 columns from 397,098 tables for training, 13,025 (13,391) columns from 4,764 (4,844) tables for test (validation). We also test our model on two existing small-scale datasets, T2D-Te and Efthymiou (a subset of WikiGS annotated with types) from <ref type="bibr" target="#b11">[11]</ref>   <ref type="table">Table 7</ref>: Accuracy on T2D-Te and Efthymiou, where scores for HNN + P2Vec are copied from <ref type="bibr" target="#b11">[11]</ref> (trained with 70% of T2D and Efthymiou respectively and tested on the rest). We directly apply our models by type mapping without retraining.</p><p>Method T2D-Te Efthymiou HNN + P2Vec (entity mention + KB) <ref type="bibr" target="#b11">[11]</ref> 0.966 0.865 TURL + fine-tuning (only entity mention) 0.888 0.745 + table metadata 0.860 0.904 <ref type="table">Table 8</ref>: Accuracy on T2D-Te and Efthymiou. Here all models use T2D-Tr (70% of T2D) as training set, following the setting in <ref type="bibr" target="#b11">[11]</ref>.</p><p>Method T2D-Te Efthymiou HNN + P2Vec (entity mention + KB) <ref type="bibr" target="#b11">[11]</ref> 0.966 0.650 TURL + fine-tuning (only entity mention) 0.940 0.516 + table metadata 0.962 0.746 and conduct two auxiliary experiments: (1) We first directly test our trained models and see how they generalize to existing datasets. We manually map 24 out of the 37 types used in <ref type="bibr" target="#b11">[11]</ref> to our types, which results in 107 (of the original 133) columns in T2D-Te and 416 (of the original 614) columns in Efthymiou.</p><p>(2) We follow the setting in <ref type="bibr" target="#b11">[11]</ref> and use 70% of T2D as training data, which contains 250 columns. <ref type="bibr" target="#b3">3</ref> Results. For the main results on our test set, we use the validation set for early stopping in training the Sherlock model, which takes over 100 epochs. We evaluate model performance using micro F1, Precision (P) and Recall (R) measures. Results are shown in <ref type="table" target="#tab_8">Table  6</ref>. Our model substantially outperforms the baseline, even when using the same input information (only entity mention vs Sherlock). Adding table metadata information and entity embedding learned during pre-training further boost the performance to 94.75 under F1. In addition, our model achieves such performance using only 10 epochs for fine-tuning, which demonstrates the efficiency of the pre-training/fine-tuning paradigm. More detailed results for several types are shown in <ref type="table" target="#tab_9">Table 9</ref>, where we observe that all methods work well for coarse-grained types like person. However, fine-grained   <ref type="table">Table  7</ref> and 8. The scores shown are accuracy, i.e., the ratio of correctly labeled columns, given each column is annotated with one ground truth label. For HNN + P2Vec, the scores are directly copied from the original paper <ref type="bibr" target="#b11">[11]</ref>. Note that in <ref type="table">Table 7</ref>, the numbers from our models are not directly comparable with HNN + P2Vec, due to mapping the types in the original datasets to ours as mentioned earlier. However, taking HNN + P2Vec trained on in-domain data as reference, we can see that without retraining, our models still obtain high accuracy on both Web table corpus (T2D-Te) and Wikipedia table corpus <ref type="bibr">(Efthymiou)</ref>. We also notice that adding table metadata slightly decreases the performance on T2D while increasing that on Efthymiou, which is possibly due to the distributional differences between Wikipedia tables and general web tables. From <ref type="table">Table 8</ref> we can see that when trained on the same T2D-Tr split, our model with both entity mention and table metadata still outperforms or is on par with the baseline. However, when using only entity mention, our model does not perform as well as the baseline, especially when generalizing to Efthymiou. This is because: (1) Our model is pretrained with both table metadata and entity embedding. Removing both creates a big mismatch between pretraining and fine-tuning.</p><p>(2) With only 250 training instances, it is easy for deep models to overfit. The better performance of models leveraging table metadata under both settings demonstrates the usefulness of context for table understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Relation Extraction</head><p>Relation extraction is the task of mapping column pairs in a table to relations in a KB. A formal definition is given as follows. Definition 6.3. Given a table and a set of relations R in KB. For a subject-object column pair in , we aim to annotate it with ? R so that holds between all entity pairs in the columns. Most existing work <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b49">48]</ref> assumes that all relations between entities are known in KB and relations between columns can be easily inferred based on entity linking results. However, such methods rely on entity linking performance and suffer from KB incompleteness. Here we aim to conduct relation extraction without explicitly linking table cells to entities. This is important as it allows the extraction of new knowledge from Web tables for tasks like knowledge base population.</p><p>Baselines. We compare our model with a state-of-the-art text based relation extraction model <ref type="bibr" target="#b50">[49]</ref> which utilizes a pretrained BERT model to encode the table information. For text based relation extraction, the task is to predict the relation between two entity mentions in a sentence. Here we adapt the setting by treating the concatenated table metadata as a sentence, and the headers of the two columns as entity mentions. Although our setting is different from the entity linking based relation extraction systems in <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b49">48]</ref>, here we implement a similar system using our entity linking model described in Section 6.2, and obtain relation annotations based on majority voting of linked entity pairs, i.e., predict a relation if it holds between a minimum portion of linked entity pairs in KB (i.e., the minimum agreement ratio is larger than a threshold).</p><p>Fine-tuning TURL. We use similar model architecture as column type annotation as follows.</p><formula xml:id="formula_11">( ) = Sigmoid ([h ; h ? ] + ) .<label>(12)</label></formula><p>Here h , h ? are aggregated representation for the two columns obtained same as Eqn. 9. We use binary cross-entropy loss for optimization. Task-specific Datasets. We prepare datasets for relation extraction in a similar way as the previous column type annotation task, based on our pre-training table partitions. Specifically, we obtain relations R from Freebase. For each table in our corpus, we pair its subject column with each of its object columns, and annotate the column pair with relations shared by more than half of the entity pairs in the columns. We only keep relations that have more than 100 training instances. Finally, we obtain a total number of 121 relations, 62,954 column pairs from 52,943 tables for training, and 2072 (2,175) column pairs from 1467 <ref type="bibr" target="#b0">(1,</ref><ref type="bibr">560)</ref> tables for test (validation). Results. We fine-tune the BERT-based model for 25 epochs. We use micro F1, Precision (P) and Recall (R) measures for evaluation. Results are summarized in <ref type="table" target="#tab_0">Table 10</ref>.</p><p>From <ref type="table" target="#tab_0">Table 10</ref> we can see that: (1) Both the BERT-based baseline and our model achieve good performance, with F1 scores larger than 0.9. (2) Our model outperforms the BERT-based baseline under all settings, even when using the same information (i.e., only table metadata vs BERT-based). Moreover, we plot the mean average precision (MAP) curve on our validation set during training in <ref type="figure" target="#fig_7">Figure  6</ref>. As one can see, our model converges much faster in comparison to the BERT-based baseline, demonstrating that our model learns a better initialization through pre-training.</p><p>As mentioned earlier, we also experiment with an entity linking based system. Results are summarized in <ref type="table" target="#tab_0">Table 11</ref>. We can see that it can achieve high precision, but suffers from low recall: The upperbound of recall is only 79.85%, achieved at an agreement ratio of 0 (i.e., taking all relations that exist between the linked entity pairs as positive). As seen from <ref type="table" target="#tab_0">Table 10</ref> and 11, our model also substantially outperforms the system based on a strong entity linker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Row Population</head><p>Row population is the task of augmenting a given table with more rows or row elements. For relational tables, existing work has tackled this problem by retrieving entities to fill the subject column <ref type="bibr" target="#b46">[45,</ref><ref type="bibr" target="#b48">47]</ref>. A formal definition of the task is given below. Definition 6.4. Given a partial table , and an optional set of seed subject entities, row population aims to retrieve more entities to fill the subject column.</p><p>Baselines. We adopt models from <ref type="bibr" target="#b46">[45]</ref> and <ref type="bibr" target="#b14">[13]</ref> as baselines. <ref type="bibr" target="#b46">[45]</ref> uses a generative probabilistic model which ranks candidate entities considering both table metadata and entity co-occurrence statistics. <ref type="bibr" target="#b14">[13]</ref> further improves upon <ref type="bibr" target="#b46">[45]</ref> by utilizing entity embeddings trained on the table corpus to estimate entity similarity. We use the same candidate generation module from <ref type="bibr" target="#b46">[45]</ref> for all methods, which formulates a search query using either the table caption or seed entities and then retrieves tables via the BM25 retrieval algorithm. Subject entities in those retrieved tables will be candidates for row population.</p><p>Fine-tuning TURL. We adopt the same candidate generation module used by baselines. We then append the [MASK] token to the input, and use the hidden representation h e of [MASK] to rank these candidates as shown in <ref type="table">Table 3</ref>. We fine-tune our model with   multi-label soft margin loss as shown below:</p><formula xml:id="formula_12">( ) = Sigmoid LINEAR(h e ) ? e e , = ?? ?E log ( ( )) + (1 ? ) log (1 ? ( )) .<label>(13)</label></formula><p>Here E is the candidate entity set, and is the ground truth label of whether is a subject entity of the table.</p><p>Task-specific Datasets. Tables in our pre-training set with more than 3 subject entities are used for fine-tuning TURL and developing baseline models, while tables in our held-out set with more than 5 subject entities are used for evaluation. In total, we obtain 432,660 tables for fine-tuning with 10 subject entities on average, and 4,132 (4,205) tables for test (validation) with 16 (15) subject entities on average.</p><p>Results. The experiments are conducted under two settings: without any seed entity and with one seed entity. For experiments without the seed entity, we only use table caption for candidate generation. For entity ranking in EntiTables <ref type="bibr" target="#b46">[45]</ref>, we use the combination of caption and label likelihood when there is no seed entity, and only use entity similarity when seed entities are available. This strategy works best on our validation set. As shown in <ref type="table" target="#tab_0">Table 12</ref>, our method outperforms all baselines. In particular, previous methods rely on entity similarity and are not applicable or have poor results when there is no seed entity available. Our method achieves a decent performance even without any seed entity, which demonstrates the effectiveness of TURL for generating contextualized representations based on both table metadata and content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Cell Filling</head><p>We examine the utility of our model in filling other table cells, assuming the subject column is given. This is similar to the setting in <ref type="bibr" target="#b43">[42,</ref><ref type="bibr" target="#b47">46]</ref>, which we formally define as follows.</p><p>Definition 6.5. Given a partial table with the subject column filled and an object column header, cell filling aims to predict the object entity for each subject entity. Baselines. We adopt <ref type="bibr" target="#b47">[46]</ref> as our base model. It has two main components, candidate value finding and value ranking. The same candidate value finding module is used for all methods: Given a subject entity and object header ? for the to-be-filled cells, we find all entities that appear in the same row with in our pre-training table corpus , and only keep entities whose source header ? ? is related to ?. Here we use the formula from <ref type="bibr" target="#b47">[46]</ref> to measure the relevance of two headers (? ? |?),</p><formula xml:id="formula_13">(? ? |?) = (? ? , ?) ? ?? (? ?? , ?) .<label>(14)</label></formula><p>Here (? ? , ?) is the number of table pairs in the table corpus that contain the same entity for a given subject entity in columns ? ? and ?. The intuition is that if two tables contain the same object entity for a given subject entity in columns with headings ? and ? , then ? and ? might refer to the same attribute. For value ranking, the key is to match the given header ? with the source header ? ? , we can then get the probability of the candidate entity belongs to the cell ( |?) as follows:</p><formula xml:id="formula_14">( |?) = MAX sim(? ? , ?) .<label>(15)</label></formula><p>Here ? ? 's are the source headers associated with the candidate entity in the pre-training table corpus. sim(? ? , ?) is the similarity between ? ? and ?. We develop three baseline methods for sim(? ? , ?):</p><p>(1) Exact: predict the entity with exact matched header, (2) H2H: use the (? ? |?) described above. <ref type="formula" target="#formula_1">(3)</ref> H2V: similar to <ref type="bibr" target="#b14">[13]</ref>, we train header embeddings with Word2Vec on the table corpus. We then measure the similarity between headers using cosine similarity.</p><p>Fine-tuning TURL. Since cell filling is very similar to the MER pre-training task, we do not fine-tune the model, and directly use [MASK] to select from candidate entities same as MER (Eqn. 6).</p><p>Task-specific Datasets. To evaluate different methods on this task, we use the held-out test tables in our pre-training phase and extract from them those subject-object column pairs that have at least three valid entity pairs. Finally we obtain 9,075 column pairs for evaluation.</p><p>Results. For candidate value finding, using all entities appearing in the same row with a given subject entity achieves a recall of 62.51% with 165 candidates on average. After filtering with (? ? |?) &gt; 0, the recall drops slightly to 61.45% and the average number of candidates reduces to 86. For value ranking, we only consider those test instances with the target object entity in the candidate set and evaluate them under Precision@K (or, P@K). Results are summarized in <ref type="table" target="#tab_0">Table 13</ref>, from which we show: (1) Simple Exact match achieves decent performance, and using H2H or H2V only sightly improves the results. (2) Even though our model directly ranks the candidate entities without explicitly using their source table information, it outperforms other methods. This indicates that our model already encodes the factual knowledge in tables into entity embeddings through pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Schema Augmentation</head><p>Aside from completing the <ref type="table">table content, another direction of table  augmentation focuses on augmenting the table schema,</ref> i.e., discovering new column headers to extend a table with more columns <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b14">13,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b46">45]</ref>. Following <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b46">45]</ref>, we formally define the task below.</p><p>Definition 6.6. Given a partial table , which has a caption and zero or a few seed headers, and a header vocabulary H , schema augmentation aims to recommend a ranked list of headers ? ? H to add to .</p><p>Baselines. We adopt the method in <ref type="bibr" target="#b46">[45]</ref> which searches our pretraining table corpus for related tables, and use headers in those related tables for augmentation. More specifically, we encode the given table caption as a tf-idf vector and then use the K-nearest neighbors algorithm (kNN) <ref type="bibr" target="#b2">[2]</ref> with cosine similarity to find the top-10 most related tables. We rank headers from those tables by aggregating the cosine similarities for tables they belong to. When seed headers are available, we re-weight the tables by the overlap of their schemas with seed headers same as <ref type="bibr" target="#b46">[45]</ref>.</p><p>Fine-tuning TURL. We concatenate the table caption, seed headers and a [MASK] token as input to our model. The output for [MASK] is then used to predict the headers in a given header vocabulary H . We fine-tune our model use binary cross-entropy loss.</p><p>Task-specific Datasets. We collect H from the pre-training table corpus. We normalize the headers using simple rules, only keep those that appear in at least 10 different tables, and finally obtain 5652 unique headers, with 316,858 training tables and 4,646 (4,708) test (validation) tables.  Results. We fine-tune our model for 50 epochs for this task, based on the performance on the validation set. We use mean average precision (MAP) for evaluation.</p><p>From <ref type="table" target="#tab_0">Table 14</ref>, we observe that both kNN baseline and our model achieve good performance. Our model works better when no seed header is available, but does not perform as well when there is one seed header. We then conduct a further analysis in <ref type="table" target="#tab_0">Table 15</ref> using a few examples: One major reason why kNN works well is that there exist tables in the pre-training table corpus that are very similar to the query table and have almost the same table schema. On the other hand, our model oftentimes suggests plausible, semantically related headers, but misses the ground-truth headers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8">Ablation Study</head><p>In this section, we examine the effects of two important designs in TURL: the visibility matrix and MER with different mask ratios. During the pre-training phase, at each training step, we evaluate TURL on the validation set for object entity prediction. We choose this task because it is similar to the cell filling downstream task and it is convenient to conduct during pre-training (e.g., ground-truth is readily available and no need to modify the model architecture, etc.).</p><p>Given a table in our validation set, we predict each object entity by first masking the entity cell (both e e and e m ) and obtaining a contextualized representation of the [MASK] (which attends to the table caption, corresponding header, as well as other entities in the same row/column before the current cell position) and then applying Eqn. 6. We compare the top-1 predicted entity with the ground truth and show the accuracy (ACC) on average. Results are summarized in <ref type="figure" target="#fig_9">Figure 7</ref>. <ref type="figure" target="#fig_9">Figure 7a</ref> clearly demonstrates the advantage of our visibility matrix design. Without the visibility matrix (an element can attend to every other element during pre-training), it is hard for the model to capture the most relevant information (e.g., relations between entities) in the table for prediction. From <ref type="figure" target="#fig_9">Figure 7b</ref>, we observe that at a mask ratio of 0.8, the objective entity prediction performance drops in comparison with other lower ratios. This is because this task requires the model to not only understand the table metadata, but also learn the relation between entities. A high mask ratio forces the model to put more emphasis on the table metadata, while a lower mask ratio encourages the model to leverage the relation between entities. Meanwhile, a very low mask ratio such as 0.2 also hurts the pre-training performance, because only a small portion of entity cells are actually used for training in each iteration. A low mask ratio also creates a mismatch between pre-training and finetuning, since for many downstream tasks, only few seed entities are given. Considering both aspects as well as that the results are not sensitive w.r.t. this parameter, we set the MER mask ratio at 0.6 in pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>This paper presents a novel pre-training/fine-tuning framework (TURL) for relational table understanding. It consists of a structureaware Transformer encoder to model the row-column structure as well as a new Masked Entity Recovery objective to capture the semantics and knowledge in relational Web tables during pretraining. On our compiled benchmark, we show that TURL can be applied to a wide range of tasks with minimal fine-tuning and achieves superior performance in most scenarios. Interesting future work includes: (1) Focusing on other types of knowledge such as numerical attributes in relational Web tables, in addition to entity relations. (2) Incorporating the rich information contained in an external KB into pre-training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of a relational table from Wikipedia. a total of 14.1 billion tables in 2008. More recently, Bhagavatula et al. [4] extracted 1.6M high-quality relational tables from Wikipedia.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our TURL framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the model input-output. The input table is first transformed into a sequence of tokens and entity cells, and processed for structure-aware Transformer encoder as described in Section 4.4. We then get contextualized representations for the table and use them for pre-training. Here [15th] (which means 15th National Film Awards), [Satyajit], ... are linked entity cells.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Graphical illustration of visibility matrix (symmetric).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Graphical illustration of masked self-attention by our visibility matrix. Each token/entity in a table can only attend to its directly connected neighbors (shown as edges here).Visibility matrix. To interpret relational tables and extract the knowledge embedded in them, it is important to model row-column structure. For example, inFigure 1,[Satyajit] and [Chiriyakhana] are related because they are in the same row, which implies that [Satyajit] directs [Chiriyakhana]. In contrast, [Satyajit]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Example 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Example 4 . 3 .</head><label>43</label><figDesc>Take Figure 3 as an example. [15th], [Satyajit], [17th] and [Mrinal] are first chosen for MER. Then, (1) the input mention representation and entity embedding of [Satyajit] remain the same. (2) The input mention representation and entity embedding of [15th] are both replaced with the embedding of [MASK] (3) The input entity embedding of [Mrinal] is replaced with embedding of [MASK], while the input entity embedding of [17th] is replaced with the embedding of a random entity [10th]. In both cases, the input mention representation are unchanged.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of fine-tuning our model and BERT for relation extraction: Our model converges much faster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a) Effect of visibility matrix.(b) Effect of different MER mask ratios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Ablation study results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of notations for our table data. Description A relational table = ( , , , ) Table caption (a sequence of tokens) Table schema = {? 0 , ..., ? , ..., ? } ? A column header (a sequence of tokens) Columns in table that contains entities The topic entity of the table = ( e ,</figDesc><table><row><cell>Symbol</cell></row></table><note>m ) An entity cell = ( e , m )</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table caption</head><label>caption</label><figDesc>, which is a short text description summarizing what the table is about. When the page title or section title of a table is available, we concatenate these with the table caption. (2) Table headers , which define the table schema; (3) Topic entity , which describes what the table is about and is usually extracted from the table caption or page title; (4) Table cells containing entities. Each entity cell ? contains a specific object with a unique identifier. For each cell, we define the entity as = ( e , m )</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table caption and topic entity are visible to all components of the table.? If element is a token or an entity in the table and element is a token or an entity in the same row or the same column, Entities and text content in the same row or the same column are visible to each other.</figDesc><table /><note>, = 1.Example 4.1. Use</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Masked Entity Recovery. In addition to MLM, we propose a novel Masked Entity Recovery (MER) objective to help the model capture the factual knowledge embedded in the table content as well as the associations between table metadata and table content. Essentially, we mask a certain percentage of input entity cells and then recover the linked entity based on surrounding entity cells and table metadata. This requires the model to be able to infer the relation between entities from table metadata and encode the knowledge in entity embeddings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics (per table) in pre-training.</figDesc><table><row><cell></cell><cell>split</cell><cell>min</cell><cell cols="2">mean median</cell><cell>max</cell></row><row><cell></cell><cell>train</cell><cell>1</cell><cell>13</cell><cell>8</cell><cell>4670</cell></row><row><cell># row</cell><cell>dev</cell><cell>5</cell><cell>20</cell><cell>12</cell><cell>667</cell></row><row><cell></cell><cell>test</cell><cell>5</cell><cell>21</cell><cell>12</cell><cell>3143</cell></row><row><cell></cell><cell>train</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>20</cell></row><row><cell># ent. columns</cell><cell>dev</cell><cell>3</cell><cell>4</cell><cell>3</cell><cell>15</cell></row><row><cell></cell><cell>test</cell><cell>3</cell><cell>4</cell><cell>3</cell><cell>15</cell></row><row><cell></cell><cell>train</cell><cell>3</cell><cell>19</cell><cell>9</cell><cell>3911</cell></row><row><cell># ent.</cell><cell>dev</cell><cell>8</cell><cell>57</cell><cell>34</cell><cell>2132</cell></row><row><cell></cell><cell>test</cell><cell>8</cell><cell>60</cell><cell>34</cell><cell>9215</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Model evaluation on entity linking task. All three datasets are evaluated with the same TURL + fine-tuning model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Further analysis for entity linking on T2D corpus.</figDesc><table><row><cell>Mention</cell><cell>Page title</cell><cell>Header</cell><cell>Wikidata Lookup result</cell><cell cols="2">TURL + fine-tuning + reweighting result Improve</cell></row><row><cell>philip</cell><cell>List of saints</cell><cell>Saint</cell><cell>Philip, male given name</cell><cell>Philip the Apostle, Christian saint and apostle</cell><cell>Yes</cell></row><row><cell>haycock</cell><cell>All 214 Wainwright fells from the pictorial guides -Wainwright Walks</cell><cell>Fell Name</cell><cell>Haycock, family name</cell><cell>Haycock, mountain in United Kingdom</cell><cell>Yes</cell></row><row><cell>don't you forget about me</cell><cell>Empty Ochestra Band -Karaoke</cell><cell>Name</cell><cell>Don't You Forget About Me, episode of Supernatural (S11 E12)</cell><cell>Don't You (Forget About Me), by Keith Forsey and Steve Schiff original song written and composed</cell><cell>Yes</cell></row><row><cell>bank of nova scotia</cell><cell>The Global 2000 -Forbes.com</cell><cell>Company</cell><cell>Scotiabank, Canadian bank based in Toronto</cell><cell>Bank of Nova Scotia, bank building in Calgary</cell><cell>No</cell></row><row><cell>purple finch</cell><cell cols="3">The Sea Ranch Association List of Birds Common Name Haemorhous purpureus, species of bird</cell><cell>Purple Finch, print in the National Gallery of Art</cell><cell>No</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Model evaluation on column type annotation task.</figDesc><table><row><cell>Method</cell><cell>F1</cell><cell>P</cell><cell>R</cell></row><row><cell>Sherlock (only entity mention) [21]</cell><cell>78.47</cell><cell>88.40</cell><cell>70.55</cell></row><row><cell cols="2">TURL + fine-tuning (only entity mention) 88.86</cell><cell>90.54</cell><cell>87.23</cell></row><row><cell>TURL + fine-tuning</cell><cell cols="3">94.75 94.95 94.56</cell></row><row><cell>w/o table metadata</cell><cell>93.77</cell><cell>94.80</cell><cell>92.76</cell></row><row><cell>w/o learned embedding</cell><cell>92.69</cell><cell>92.75</cell><cell>92.63</cell></row><row><cell>only table metadata</cell><cell>90.24</cell><cell>89.91</cell><cell>90.58</cell></row><row><cell>only learned embedding</cell><cell>93.33</cell><cell>94.72</cell><cell>91.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Further analysis on column type annotation: Model performance for 5 selected types. Results are F1 on validation set.</figDesc><table><row><cell>Method</cell><cell>person</cell><cell>pro_athlete</cell><cell>actor</cell><cell>location</cell><cell>citytown</cell></row><row><cell>Sherlock</cell><cell>96.85</cell><cell>74.39</cell><cell>29.07</cell><cell>91.22</cell><cell>55.72</cell></row><row><cell>TURL + fine-tuning</cell><cell>99.71</cell><cell>91.14</cell><cell>74.85</cell><cell>99.32</cell><cell>79.72</cell></row><row><cell>only entity mention</cell><cell>98.44</cell><cell>87.11</cell><cell>58.86</cell><cell>96.59</cell><cell>60.13</cell></row><row><cell>w/o table metadata</cell><cell>99.63</cell><cell>90.38</cell><cell>74.46</cell><cell>99.01</cell><cell>77.37</cell></row><row><cell>w/o learned embedding</cell><cell>99.38</cell><cell>90.56</cell><cell>71.39</cell><cell>98.91</cell><cell>75.55</cell></row><row><cell>only table metadata</cell><cell>98.26</cell><cell>88.80</cell><cell>70.86</cell><cell>98.11</cell><cell>72.54</cell></row><row><cell>only learned embedding</cell><cell>98.72</cell><cell>91.06</cell><cell>73.62</cell><cell>97.78</cell><cell>75.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Model evaluation on relation extraction task.</figDesc><table><row><cell>Method</cell><cell>F1</cell><cell>P</cell><cell>R</cell></row><row><cell>BERT-based</cell><cell>90.94</cell><cell>91.18</cell><cell>90.69</cell></row><row><cell cols="2">TURL + fine-tuning (only table metadata) 92.13</cell><cell>91.17</cell><cell>93.12</cell></row><row><cell>TURL + fine-tuning</cell><cell cols="3">94.91 94.57 95.25</cell></row><row><cell>w/o table metadata</cell><cell>93.85</cell><cell>93.78</cell><cell>93.91</cell></row><row><cell>w/o learned embedding</cell><cell>93.35</cell><cell>92.90</cell><cell>93.80</cell></row></table><note>types like actor and pro_athlete are much more difficult to predict. Specifically, it is hard for a model to predict such types for a column only based on entity mentions in cells. On the other hand, using table metadata works much better than using entity mentions (e.g., 70.86 vs 58.86 for actor). This indicates the importance of table context information for predicting fine-grained column types. Results of the auxiliary experiments are summarized in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Relation extraction results of an entity linking based system, under different agreement ratio thresholds.</figDesc><table><row><cell>Min Ag. Ratio</cell><cell>F1</cell><cell>P</cell><cell>R</cell></row><row><cell>0</cell><cell cols="3">68.73 60.33 79.85</cell></row><row><cell>0.4</cell><cell cols="3">82.10 94.65 72.50</cell></row><row><cell>0.5</cell><cell cols="3">77.68 98.33 64.20</cell></row><row><cell>0.7</cell><cell cols="3">63.10 99.37 46.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Model evaluation on row population task. Recall is the same for all methods because they share the same candidate generation module. -tuning 40.92 63.30 48.31 78.13    </figDesc><table><row><cell># seed</cell><cell>0</cell><cell>1</cell></row><row><cell>Method</cell><cell cols="2">MAP Recall MAP Recall</cell></row><row><cell>EntiTables [45]</cell><cell cols="2">17.90 63.30 42.31 78.13</cell></row><row><cell>Table2Vec [13]</cell><cell>-</cell><cell>63.30 20.86 78.13</cell></row><row><cell>TURL + fine</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Model evaluation on cell filling task.</figDesc><table><row><cell cols="2">Method P @ 1</cell><cell>P @ 3</cell><cell cols="2">P @ 5 P @ 10</cell></row><row><cell>Exact</cell><cell>51.36</cell><cell>70.10</cell><cell>76.80</cell><cell>84.93</cell></row><row><cell>H2H</cell><cell>51.90</cell><cell>70.95</cell><cell>77.33</cell><cell>85.44</cell></row><row><cell>H2V</cell><cell>52.23</cell><cell>70.82</cell><cell>77.35</cell><cell>85.58</cell></row><row><cell>TURL</cell><cell>54.80</cell><cell>76.58</cell><cell>83.66</cell><cell>90.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>Model evaluation on schema augmentation task.</figDesc><table><row><cell>Method</cell><cell cols="2">#seed column labels 0 1</cell></row><row><cell>kNN</cell><cell>80.16</cell><cell>82.01</cell></row><row><cell>TURL + fine-tuning</cell><cell>81.94</cell><cell>77.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 :</head><label>15</label><figDesc>Case study on schema augmentation. Here we show average precision (AP) for each example. Support Caption is the caption of the source table that kNN found to be most similar to the query table. Our model performs worse when there exist source tables that are very similar to the query table (e.g., comparing support caption vs query caption).</figDesc><table><row><cell>Method</cell><cell>Query Caption</cell><cell>Seed</cell><cell>Target</cell><cell>AP</cell><cell>Predicted</cell><cell>Support Caption</cell></row><row><cell>kNN Ours</cell><cell>2010 santos fc season out</cell><cell>pos.</cell><cell>name, moving to</cell><cell>1.0 0.58</cell><cell>moving to, name, player, moving from, to moving to, fee/notes, destination club, fee, loaned to</cell><cell>2007 santos fc season out -</cell></row><row><cell>kNN Ours</cell><cell>first ladies and gentlemen of panama list</cell><cell>no.</cell><cell>name, president</cell><cell>0.20 0.14</cell><cell>country, runner-up, champion, player, team team year, runner-up, spouse, name, father</cell><cell>first ladies of chile list of first ladies -</cell></row><row><cell>kNN Ours</cell><cell>list of radio stations in metro manila am stations</cell><cell>name</cell><cell>format, covered location</cell><cell>1.0 0.83</cell><cell>format, covered location, company, call sign, owner format, owner, covered location, city of license, call sign</cell><cell>list of radio stations in metro manila fm stations -</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We simply reweight the score of the top-1 prediction by our model with a factor of 0.8 and compare it with the top-1 prediction returned by Wikidata Lookup. The higher one is chosen as final prediction.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use the data released by<ref type="bibr" target="#b11">[11]</ref> (https://github.com/alan-turing-institute/SemAIDA). The number of instances is slightly different from the original paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank the anonymous reviewers for their helpful comments. Authors at the Ohio State University were sponsored in part by Google Faculty Award, the Army Research Office under cooperative agreements W911NF-17-1-0412, NSF Grant IIS1815674, NSF CAREER #1942980, Fujitsu gift grant, and Ohio Supercomputer Center <ref type="bibr" target="#b9">[9]</ref>. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards a hybrid imputation approach using web tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Ahmadov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maik</forename><surname>Thiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Eberius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wrembel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">IEEE/ACM 2nd International Symposium on Big Data Computing (BDC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An Introduction to Kernel and Nearest Neighbor Nonparametric Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomi</forename><forename type="middle">S</forename><surname>Altman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DBpedia: A Nucleus for a Web of Open Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?ren</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">G</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC/ASWC</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">TabEL: Entity Linking in Web Tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Sekhar Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanapon</forename><surname>Noraset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on The Semantic Web-ISWC 2015</title>
		<meeting>the 14th International Conference on The Semantic Web-ISWC 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9366</biblScope>
			<biblScope unit="page" from="425" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Webtables: exploring the power of tables on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><forename type="middle">Zhe</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VLDB Endowment</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="538" to="549" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uncovering the Relational Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><forename type="middle">Y</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><forename type="middle">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International Workshop on the Web and Databases</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06-13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Leveraging wikipedia table schemas for knowledge graph augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Cannaviccio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Ariemma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Workshop on the Web and Databases</title>
		<meeting>the 21st International Workshop on the Web and Databases</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ohio Supercomputer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Center</surname></persName>
		</author>
		<ptr target="http://osc.edu/ark:/19495/f5s1ph73" />
	</analytic>
	<monogr>
		<title level="j">Ohio Supercomputer Center</title>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ColNet: Embedding the Semantics of Web Tables for Column Type Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernesto</forename><surname>Jim?nez-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">A</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning Semantic Annotations for Tabular Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernesto</forename><surname>Jim?nez-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">A</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding Related Tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish Das</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lujun</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongrae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reynold</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012</title>
		<meeting>the 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<title level="m">ACM SIGMOD International Conference on Management of Data</title>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="817" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Lei Min Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balog</surname></persName>
		</author>
		<title level="m">Table2Vec: Neural Word and Entity Embeddings for Table Population and Retrieval. In SIGIR&apos;19</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Leveraging 2-hop Distant Supervision from Table Entity Pairs for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06007</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Matching Web Tables with Knowledge Base Entities: From Entity Lookups to Entity Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasilis</forename><surname>Efthymiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oktie</forename><surname>Hassanzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Rodriguez-Muro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><surname>Christophides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Termite: a system for tunneling through heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><forename type="middle">Castro</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Workshop on Exploiting Artificial Intelligence Techniques for Data Management</title>
		<meeting>the Second International Workshop on Exploiting Artificial Intelligence Techniques for Data Management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="233" to="259" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://developers.google.com/freebase/data" />
		<title level="m">Freebase Data Dumps</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">TAPAS: Weakly Supervised Table Parsing via Pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">Martin</forename><surname>Eisenschlos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madelon</forename><surname>Hulsebos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">Zeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Zgraggen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demiralp</forename><surname>Ccaugatay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C&amp;apos;esar A</forename><surname>Hidalgo</surname></persName>
		</author>
		<title level="m">Sherlock: A Deep Learning Approach to Semantic Data Type Detection. Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">TinyBERT: Distilling BERT for Natural Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiaoqi Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xusong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv abs/1909.10351</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SemTab 2019: Resources to Benchmark Tabular Data to Knowledge Graph Matching Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernesto</forename><surname>Jim?nez-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oktie</forename><surname>Hassanzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasilis</forename><surname>Efthymiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavitha</forename><surname>Srinivas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="514" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Large Public Corpus of Web Tables containing Time and Context Metadata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lehmberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Ritze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Meusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference Companion on World Wide Web</title>
		<meeting>the 25th International Conference Companion on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="75" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Suhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhai</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00584</idno>
		<title level="m">Deep Entity Matching with Pre-Trained Language Models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Annotating and searching web tables using entities, types and relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarawagisunita</forename><surname>Limayegirija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chakrabartisoumen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB 2010</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using Linked Data to Interpret Tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varish</forename><surname>Mulwad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">W</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zareen</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLD</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Knowledge Enhanced Contextual Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relation Extraction with Matrix Factorization and Universal Schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Matching HTML Tables to DBpedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Ritze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lehmberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WIMS &apos;15</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yoones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><forename type="middle">Di</forename><surname>Sekhavat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denilson</forename><surname>Paolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Merialdo</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.]. Knowledge Base Augmentation using Tabular Data</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding: A Survey of Approaches and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Qi Shan Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biwu</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding by Translating on Hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<idno>ArXiv abs/1307.7973</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Infogather: entity augmentation and attribute discovery by holistic matching with web tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Yakout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Ganjam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surajit</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2012 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="97" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Wen Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Entitables: Smart assistance for entityfocused tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="255" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Auto-completion for data cells in relational tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="761" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Web Table Extraction, Retrieval, and Augmentation: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Effective and efficient Semantic Table Interpretation using TableMiner+</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="921" to="957" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced Language Representation with Informative Entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
