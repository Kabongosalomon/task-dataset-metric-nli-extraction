<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">X-volution: On the Unification of Convolution and Self-attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanhong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Huawei Hisilicon</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Wang</surname></persName>
							<email>wang--hang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Huawei Hisilicon</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
							<email>nibingbing@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Huawei Hisilicon</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">X-volution: On the Unification of Convolution and Self-attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolution and self-attention are acting as two fundamental building blocks in deep neural networks, where the former extracts local image features in a linear way while the latter non-locally encodes high-order contextual relationships. Though essentially complementary to each other, i.e., first-/high-order, stat-of-the-art architectures, i.e., CNNs or transformers, lack a principled way to simultaneously apply both operations in a single computational module, due to their heterogeneous computing pattern and excessive burden of global dot-product for visual tasks. In this work, we theoretically derive a global self-attention approximation scheme, which approximates self-attention via the convolution operation on transformed features. Based on the approximate scheme, we establish a multi-branch elementary module composed of both convolution and self-attention operation, capable of unifying both local and non-local feature interaction. Importantly, once trained, this multi-branch module could be conditionally converted into a single standard convolution operation via structural re-parameterization, rendering a pure convolution styled operator named X-volution, ready to be plugged into any modern networks as an atomic operation. Extensive experiments demonstrate that the proposed X-volution, achieves highly competitive visual understanding improvements (+1.2% top-1 accuracy on ImageNet classification, +1.7 box AP and +1.5 mask AP on COCO detection and segmentation).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In deep learning era, convolution <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref> and self-attention <ref type="bibr" target="#b31">[32]</ref> are the two most important computational primitive of information extraction and encoding. On the one hand, convolution is a linear operation which extracts a local image feature via the discrete convolutional operation (actually a dot product) between a patch x centered at a given pixel/image feature with a window shaped filter W, i.e., Wx + B. On the other hand, self-attention operation is a high-order operator which encodes non-local contextual information via scaled dot product between a given location with all other positions in the image, i.e., sof tmax W Q x</p><formula xml:id="formula_0">T W K x W V x.</formula><p>It is natural that these two computing modules are complementary to each other, i.e., local and nonlocal information, and integrating both modules in a single operator is plausible, which is promising to benefit from the advantages of both schemes. Specifically, convolution adopts the inductive bias of local processing and isotropic property <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31]</ref>, which endow it with a well-conditioned optimization and translation equivariance property. Contrast to convolution, self-attention embodies low inductive bias releasing a larger space for self-attention to freely explore the inherent characteristics of a data-set, which prompts it to achieve a better performance and generalization <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36]</ref>. Furthermore, integrating the two together helps to overcome their respective weaknesses. For example, the local nature of convolution prevents it from establishing long-term relationships which are often useful for a better visual understanding <ref type="bibr" target="#b0">[1]</ref> In addition, the lack of local priors makes self-attention difficult to optimize and relies heavily on the pre-training on oversize data-sets (e.g., JFT300M <ref type="bibr" target="#b28">[29]</ref>, ImageNet-21k <ref type="bibr" target="#b4">[5]</ref>). However, these two operators belong to different computational patterns, resulting in them difficult to be integrated. Convolutional operator performs a predefined weighted average within a local window, while self-attention works by global weighting with dynamic coefficients. In addition, in visual domain, calculating the dot product between all positions in the image is computationally forbidden, which brings out more challenges for applying non-local operation in a similar framework as CNN. Thus, state-of-the-art networks either conduct convolution or self-attention solely, e.g., CNNs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref>, Transformers <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b1">2]</ref>, with no flexible or efficient way to jointly absorb both operations in a single atomic module which has great potential in working as a computational primitive for visual understanding.</p><p>In this work, we explicitly address these obstacles and develop a novel atomic operator, named X-volution, integrating both convolution and self-attention. First, we theoretically prove the feasibility of approximating the global self-attention by propagating contextual relationship from local region to non-local region. According to this idea, we develop a novel approximate self-attention scheme with the complexity of O(n), named Pixel Shift Self-Attention (PSSA). In contrast to concurrent self-attention schemes <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31]</ref>, the proposed PSSA converts the self-attention into a convolutional operation on transformed features which are obtained by sequential element-wise shift and elementwise dot product. Second, based on this approximation scheme, we establish a multi-branch network module to integrate convolution and self-attention simultaneously. The multi-branch topology makes the module not only possess well-conditioned optimization properties, but also acquire the ability to capture long-term relationships, thus showing stronger learning ability and better performance. More importantly, our carefully designed multi-branch structure could be conditionally transformed into a single standard convolution operation via network structural re-parameterization <ref type="bibr" target="#b5">[6]</ref>, rendering a pure convolution styled operator X-volution, ready to be plugged into any modern networks as an atomic operation. In fact, convolution and self-attention can be regarded as static convolution (i.e., content-independent kernel) and dynamic convolution (i.e., content-dependent kernel), respectively.</p><p>We experiment with the proposed X-volution in terms of both qualitative and quantitative evaluations on mainstream vision tasks. Extensive results demonstrate that our X-volution operator achieves very competitive improvements (i.e., image classification on ImageNet <ref type="bibr" target="#b24">[25]</ref>: +1.2% top1 accuracy, object detection on COCO <ref type="bibr" target="#b18">[19]</ref>: +1.7 box AP, instance segmentation on COCO: +1.5 mask AP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Convolutional networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30]</ref> have been proved to be powerful when facing computer vision problems such as object detection and image classification. However, recent works <ref type="bibr" target="#b33">[34]</ref> show that the effectiveness of the convolution operator is limited to its local receptive field and the self-attention mechanism with global information aggregation can achieve better performance. Driven by this understanding, a lot of works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref> have introduced various complex mechanisms to enhance the representation ability of the network. Recently, researchers realized that self-attention <ref type="bibr" target="#b31">[32]</ref> as a computational primitive can also handle computer vision problems with high performance, and a large number of transformers <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b16">17]</ref> emerge. Cordonnier et al. <ref type="bibr" target="#b3">[4]</ref> found that in fact, multi-head self-attention is able to learn the characteristics of convolution, and the two can be converted to each other under certain conditions. Instead of using convolution alone, Bello et al. <ref type="bibr" target="#b0">[1]</ref> combined convolution and self-attention through direct concatenation, and achieved a promising improvement. This shows that combining the two operators is of great help in improving performance. Wu et al. <ref type="bibr" target="#b35">[36]</ref> confirmed the above point from another angle. They introduced the convolution operation to vision transformers and further improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We propose a novel atomic operator, named X-volution, to integrate the fundamental convolution and self-attention operators into a single unified computing block, which is expected to gain very impressive performance improvements from both, i.e., local vs. non-local/linear vs. non-linear. In this section, we first revisit basic mathematical formulas of convolution and self-attention, and then we propose a simple approximation scheme of global self-attention, which is directly converted  <ref type="figure">Figure 1</ref>: The detailed structure of our proposed X-volution operator. X-volution is a training-inference decouple topology <ref type="bibr" target="#b5">[6]</ref>. Its training structure is shown in the middle with two main branches. Right branch is composed of cascaded convolution and BN, which can integrate convolution capabilities for X-volution. The left consists of a pixel shift self-attention, which provides approximated global self-attention features. Once trained, we conditionally re-parameterize it as an atomic operation as shown on the right. At inference phase, X-volution is actually a dynamic convolution operator, and its weight is composed of the attention map that needs to be dynamically calculated and the convolution weight that has been trained and solidified.</p><formula xml:id="formula_1">x 0 x i x i+1 x k x i+2 ? Approximated Self-Attention x 0 x k ?</formula><p>to a compatible pattern of convolution. Finally, we describe that in the inference phase how to conditionally merge the branches of convolution and the proposed self-attention approximation into a SINGLE convolutional style atomic operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Convolution and Self-Attention Operator Revisit</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">The Convolution Module</head><p>The convolutional operator is the de-factor computational primitive for building convolutional neural network (CNN), which estimates the output by linear weighting within a limited local region. Given a feature tensor X ? R C i ?H?W , Ci denotes the number of input channels, H is the height and W is the width. The estimation result Y ? R Co?H?W of a convolutional operator is defined by following formulation:</p><formula xml:id="formula_2">Y co,i,j def = Ci ci=0 (?i,?j )?? K W co,ci,?i+ K/2 ,?j + K/2 X ci,i+?i,j+?j + B co ,<label>(1)</label></formula><p>where Co denotes the number of output channels. W ? R Co?C i ?K?K refers to the convolution kernel, and W co,c i ,? i + K/2 ,? j + K/2 refers to the kernel scalar value at the specific location. K is the window size of convolution, B ? R Co denotes the bias vector, and ?K ? Z 2 represents the set of all possible offsets in the K ? K convolution window. It can be seen from Eq. 1 that convolution is actually a first-order linear weighting operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Self-Attention Module</head><p>The self-attention <ref type="bibr" target="#b31">[32]</ref> is a rising alternative computational primitive for vision tasks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b1">2]</ref>, whose core idea is to build long-term semantic interactions established by performing the intra-vector dot-product. Unlike convolution, self-attention can not process image tensor directly, where input feature tensor is firstly reshaped into a vector X ? R C?L . L represents the length of the vector and L = H ? W . W Q , W K , W V respectively represent the Query, Key and Value embedding transformation, and they are spatial-shared linear transformations. The prediction of self-attention is defined as below:</p><formula xml:id="formula_3">Y def = sof tmax W Q X T W K X W V X =W(X)X,<label>(2)</label></formula><p>whereW(X) represents the final equivalent coefficient matrix of self-attention, which can be considered as a dynamic (element-wise content-dependent) and spatially varying convolutional filter. Eq. 2 demonstrates that self-attention is a high-order global operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Approximation of Global Self-Attention</head><p>The global self-attention is the most primitive attention scheme, which possesses the advantage of excellent performance benefited from its global scope. However, its complexity is unacceptable i.e., O(n 2 ) (n denotes total pixel number), which makes its application in visual tasks severely limited.</p><p>The key problem becomes whether we could infer a proper approximate scheme ofW(X) in Eq. 2, namely, whether we could find a compatible computing pattern ofW(X) in terms of off-the-shelf operators such as convolution, single element-wise product? In this section, we show that after simple element-wise shift and dot-product, we could approximate the global self-attention operator in the form of convolution. Given a position in the feature tensor X, we denote its feature vector as x 0 , and its attention logit s 0 can be written as following formulation:</p><formula xml:id="formula_4">s0 = x t ?? ?t x0, xt = x j ?A ?j x0, xj Local Region + x i ?(?\A) ?i x0, xi N on?local Region ,<label>(3)</label></formula><p>where ? t = w q w k w v x t , ? denotes the global region, and A denotes a local region centered on x 0 . We illustrate the local region and non-local region on the left side of <ref type="figure">Fig. 1</ref>. The gray box in the figure denotes the global region of input feature X, and green box represents the local region centered on x 0 . In addition, non-local region is the region outside the local region. Since images hold strong Markovian property <ref type="bibr" target="#b12">[13]</ref>, x 0 can be approximately linearly represented by pixels in its local region:</p><formula xml:id="formula_5">x0 ? x k ?? ? k x k , where ? k is the linear weight.</formula><p>Substituting it into second item in Eq. 3 can obtain following formulation:</p><p>Notice that, x k , xi is the inner product between x k and xi, which measures the similarity between x k and xi. x i ?U (x k ) ?i? k x k , xi is the attention result of x k within its neighbourhood. Therefore, the global attention logit at x 0 can be approximated by weighted summing the attention result of the pixels within its neighbourhood, as shown in Eq. 7. Following above understanding, we can design an approximate operator that estimates the global attention through point-by-point contextual relationship propagation. Thus, we propose a global attention approximation scheme, Pixel Shift Self-Attention (PSSA), based on pixel shift and convolution to approximate the global attention. Specifically, we first shift the feature map by L pixels along given directions (i.e., left, right, up e.t.c.), and then we employ element-wise product between the original features and the shifted features to obtain the transformed features. In fact, the shift-product operation establishes the contextual relationship between the points in the neighborhood, and through hierarchical stacking, we can propagate the contextual relationship to the global region. Finally, we perform weighted summing (can be implemented by convolution operator) between these transformed features to get an approximate self-attention map. The complexity of shift, element-wise product and weighted summing are O(n), so the proposed PSSA is an operator with O(n) temporal complexity. It is worth noting that PSSA actually converts self-attention into a standard convolution operation on transformed features. Through hierarchical stacking, this structure can realize the estimation of global self-attention logit via contextual relationship propagation. The convolution employs the inductive bias of locality and isotropy endowing it the capability of translation equivariance <ref type="bibr" target="#b0">[1]</ref>. However, the local inherent instinct makes convolution failed to establish the long-term relationship which is necessary to formulate a Turing complete atomic operator <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref>. In contrast to convolution, self-attention discards mentioned inductive bias, so-called low-bias, and strives to discover natural patterns from a data-set without explicit model assumption. The lowbias principle gives self-attention the freedom to explore complex relationships (e.g., long-term dependency, anisotropic semantics, strong local correlations in CNNs <ref type="bibr" target="#b6">[7]</ref>, e.t.c.), resulting in the scheme usually requires pre-training on extra oversize data-sets (e.g., JFT-300M, ImageNet21K).</p><p>In addition, self-attention is difficult to optimize, requiring a longer training cycle and complex tricks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b1">2]</ref>. Witnessing this, several works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36]</ref> propose that convolution should be introduced into the self-attention mechanism to improve its robustness and performance. In short, different model assumptions are adopted to make convolution and self-attention complement each other in terms of optimization characteristics (i.e., well-condition/ill-condition), attention scope (i.e., local/long-term), and content dependence (content-dependent/independent) e.t.c..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">The Multi-Branch Topology for the Unification</head><p>There are several works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b0">1]</ref> attempting to combine convolution and self-attention, whereas the coarse topological combination (e.g., hierarchical stacking, concatenate) prevents them from getting an single atomic operation (applying convolution and attention in the same module) and makes the structure irregular. For instance, AANet <ref type="bibr" target="#b0">[1]</ref> directly concatenates the results processed by a convolution layer and a self-attention layer to obtain the combined results. It shows that a single convolution or a single self-attention will cause performance degradation, and the performance will be significantly improved when they exist at the same time.</p><p>Although challenging due to heterogeneous computing pattern of convolution and self-attention, in this work, we study the mathematical formulation of convolution and self-attention operator, (i.e., Eq. 1 and Eq. 2), and find the approximate form in Eq. 7 could be equivalent to a spatial varying convolution on certain dot-product map, observing that global element-wise interaction (dot-product) could be approximated by the propagation of local element-wise interaction. Thus, both operators could be treated in a unified computing pattern, i.e., convolution. From the other point of view, the convolution operation can be regarded as the spatially invariant bias of the selfattention. Perceiving this, we combine the operators into a multi-branch topology, shown in the <ref type="figure">Fig. 1</ref>, which could benefit from convolution and self-attention simultaneously. The multi-branch module is composed of two main branches. The branch on the left is composed of cascaded Shift Pixel Self-Attention and batch-normalization <ref type="bibr" target="#b11">[12]</ref>, playing the role to approximate the global self-attention operation. The right branch is designed as a convolutional branch composed of cascade convolution and batch-normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Conditionally Converting Multi-Branch Scheme to the Atomic X-volution</head><p>The multi-branch module achieves the functional combination of convolution and self-attention. However, it is only a coarse-grained combination of operators, which will make the network highly complex and irregular. From the perspective of hardware implementation, the multi-branch structure requires more caches to serve the processing of multiple paths. In contrast, a single atomic operation is more efficient and has lower memory overhead, which is hardware-friendly. For brevity, we omit the formula of batch-normalization here. Actually, batch-normalization can be regarded as a 1 ? 1 group convolution (its group is equal to the number of channels), which can be merged into the convolution/self-attention layer. In fact, we generally employ the PSSA by hierarchical stacking, and the weighted operation in the stacked structure can be omitted, as hierarchical stacking implies the operation of weighted neighbor pixels. The training phase formulation of our proposed multi-branch module can be written as below:</p><formula xml:id="formula_6">y 0 = xi?A ? i x 0 , x i P SSA Branch + xi?A w c x i + b c Conv Branch = xi?A w q w k w v x i x 0 , x i + xi?A w c x i + b c ,<label>(8)</label></formula><p>where w c represents the convolutional weight, and b c is its corresponding bias.</p><formula xml:id="formula_7">y 0 = xi?A w q w k w v x 0 , x i + w c x i + b c = xi?A w A (x 0 , x i ) + w c x i + b c ,<label>(9)</label></formula><p>where w A (x0, xi) def = w q w k w v x0, xi represents the content-dependent/dynamic coefficients coming from pixel shift self-attention branch. w c denotes the content-independent/static coefficients inherited from convolutional branch, and it will be fixed once the training is completed. Observing Eq. 9, we can find that after a simple transformation the multi-branch structure can be transformed into a convolution form. It is worth pointing out that the process is widely used in CNN and is called structural re-parameterization <ref type="bibr" target="#b5">[6]</ref>. We here first extend it to the merging of convolution and selfattention. According to Eq. 9, we equivalently convert the multi-branch module composed of convolution and self-attention into a dynamic convolution operator named X-voultion. Note that, our proposed X-volution can be plugged in mainstream networks (e.g., ResNet) as an atomic operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>For image classification, we test the proposed X-volution on ImageNet-1k <ref type="bibr" target="#b24">[25]</ref> (MIT License) benchmark, which contains 1.28M training images and 50K validation images. Input images are cropped to 224 ? 224 pixels with horizontal flipping. We use the SGD optimizer (initial learning rate: 0.1, momentum: 0.9, weight decay: 1 ? 10 ?4 ) with a total batch size of 256 to train the network for 100 epochs, and the learning rate is decreased by the factor of 10 every 30 epochs.</p><p>For object detection and instance segmentation, We conduct experiments on COCO 2017 <ref type="bibr" target="#b18">[19]</ref> (Commons Attribution 4.0 License) data-set, which contains 118k training images and 5k validation images. We adopt Faster R-CNN <ref type="bibr" target="#b23">[24]</ref> for detection and Mask R-CNN <ref type="bibr" target="#b7">[8]</ref> for segmentation, both equipped with the FPN <ref type="bibr" target="#b17">[18]</ref> neck. We employ the implementation of detectors from Detectron2 <ref type="bibr" target="#b36">[37]</ref> with its default settings. Concretely, a total batch size of 16 (2 images per GPU) is adopted to train the network with 1x (12 epochs) schedule, and the default multi-scale training strategy is utilized.</p><p>For all experiments, we choose ResNet <ref type="bibr" target="#b8">[9]</ref> as the backbone. Following BoTNet <ref type="bibr" target="#b27">[28]</ref>, we replace the final three 3 ? 3 convolutions with the proposed X-volution to verify its effectiveness, and details of operator replacement are shown in the <ref type="figure" target="#fig_1">Fig. 2</ref>. The detailed implementation of PSSA can be found in the supplementary material. Unless otherwise specified, the replacement only occurs in the last stage of ResNet, and other operators are evaluated with the same replacement for fair comparison. In specific, we re-evaluate the performance of AA-Convolution (presented by AA-Net [1]), Self-Attention (SA) <ref type="bibr" target="#b31">[32]</ref>, and Involution <ref type="bibr" target="#b15">[16]</ref> operators using the implementation from their source code under the same setting, and the results of our X-volution combined with other self-attention operators (X-volution(SA), X-volution(Inv)) are also provided. The reported results are averaged over five independent runs. We use 8 NVIDIA Tesla V100 GPUs for training. All the operators are implemented with the PyTorch <ref type="bibr" target="#b21">[22]</ref> deep learning framework, and the source code will be released for reproducibility. Note that, no other data augmentation or training skills are adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Image Classification</head><p>In Tab. 1, we compare our proposed X-volution operator with the fundamental convolution and self-attention operator using ResNet-34 and ResNet-50 as the backbone on ImageNet-1k. We   replace partial convolution operators with our proposed X-volution in the network, and the detailed replacement position is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. We can observe that our proposed X-volution leads to stable improvement for both ResNet structures, which illustrates the effectiveness of our multi-branch design. The self-attention branch provides our X-volution with non-local and anisotropic processing capabilities, which are very important for dealing with complex context relationships. Note that, the performance of stand-alone self-attention operator inserted into ResNet is worse than convolution, indicating that the naive introduction of self-attention operator has little effect on classification task.</p><p>We also investigate the effect of different replacement locations. Different stages have different numbers of building blocks, e.g., 3, 4, 6, 3 for ResNet-34 and ResNet-50, and also correspond to different feature resolution. Limited by computational resources, we only test our X-volution in the last three stages, replacing all residual blocks in this stage. Results show that the replacement in stage3 brings greatest improvement, i.e., +1.2% top-1 accuracy for ResNet-34 and +0.9% top-1 accuracy for ResNet-50. We suspect the inferior performance of the replacement in stage4 for ResNet-50 can be ascribed to the increased learnable parameters, which slow down the convergence of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Object Detection</head><p>Besides image classification, we also evaluate our proposed X-volution operator on object detection to verify its generalization ability. Tab. 2 reports the results of different operators on COCO 2017. Convolution (first row) denotes the original ResNet-50 baseline trained with ImageNet pretrained weights. The performance of all operators is obtained by only replacing the final three 3?3 convolutions of the last stage in ResNet-50 architecture, as shown in <ref type="figure" target="#fig_1">Fig. 2 (d)</ref>.</p><p>Based on the Faster R-CNN framework, three versions of X-volution all surpass their self-attention counterparts with considerable performance gains. In particular, our X-volution(SA) achieves the best performance with a significant gain of +1.7 box AP over ResNet-50. By combining low-order local features and high-order long-range dependencies, the proposed X-volution operator achieves higher  <ref type="bibr" target="#b15">[16]</ref> 39.6 (+0.5) 23.7 (+0.5) 42.9 (+0.5) 51.4 (+0. accuracy than convolution or self-attention alone. The results show that, a Turing complete atomic operator is helpful to visual understanding, and such property is neglected by existing computational operators. Moreover, the X-volution based on PSSA achieves comparable performance with the Xvolution(SA), indicating that the approximation works well in our X-volution module, which is more friendly to hardware implementation and computation. We also verified the effect of multi-branch convolution. Under the same training settings, the design of multi-branch convolution (second row in Tab. 2) leads to degraded performance, showing that the increase of network learnable parameters does not always lead to performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Instance Segmentation</head><p>For instance segmentation, we adopt Mask R-CNN framework with FPN neck and ResNet-50 backbone. Tab. 3 compares our X-volution operator against other atomic operators. We can observe that our proposed X-volution outperforms other operators by a large margin. Specifically, X-volution(SA) achieves 41.1 box AP and 37.2 mask AP, which brings 2.0 box AP and 1.5 mask AP gains over ResNet-50, and also improves Self-Attention by 1.7 box AP and 1.1 mask AP. The combination of the Involution <ref type="bibr" target="#b15">[16]</ref> performs marginally better than the Involution baseline (+0.3 box AP and +0.1 mask AP), and the X-volution composed of PSSA contributes to comparable performance with the global version X-volution. These results illustrate the superiority of our X-volution operator, in which the integration of local and global contextual information enables more precise instance segmentation prediction. Compared to AA-Convolution <ref type="bibr" target="#b0">[1]</ref>, which proposes a split attention mechanism, our multi-branch design of X-volution obtains a better performance, showing the complementarity and necessity of convolution and self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Effect of Different Kernel Size</head><p>To study the influence of convolution branch in our X-volution, we conduct experiments of different convolutional kernel size (i.e., ranges from 1 ? 1 to 9 ? 9). We show the results in <ref type="figure" target="#fig_4">Fig. 3 (a)</ref>. When reducing the kernel size to 1 ? 1, obvious performance decay occurs, which is possibly related to the feature resolution in the network. On the contrary, the performance of X-volution under this configuration is still acceptable, as self-attention branch provides the ability to build long-term interactions. We continue to increase the size of the convolution kernel (i.e., 5 ? 5, 7 ? 7), and steady improvements are observed on the corresponding network. When the size of the convolution kernel is set to 9 ? 9, the performance begins to decrease. It is worth noting that the improvement of X-volution over stand-alone convolution becomes smaller. This phenomenon is caused by two factors. First, the large kernel size increases the number of learnable parameters, which inevitably improves the capability of the network and thus brings the improvement. Second, increasing the size of the convolution kernel will enlarge the receptive field and gain the ability to establish relationship in a larger region. It can be concluded from this experiment that the convolution branch plays a critical role in our X-volution and the existence of the self-attention branch allows X-volution to achieve an excellent competitive performance. Average Precision (%) <ref type="bibr" target="#b34">35</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">The Optimization Property of X-volution</head><p>Convolution obtains well-conditioned optimization characteristics due to its local and isotropic inductive bias, but self-attention is a low-bias operator, which makes its optimization difficult and requires more training epochs. We show the AP convergence curve of different operators on COCO object detection in <ref type="figure" target="#fig_4">Fig. 3 (b)</ref>. In the initial stage (i.e., the 1-st epoch), the performance of convolution is obviously higher than that of X-volution, and the convergence speed of our X-volution is slightly faster than the single PSSA branch thanks to the addition of convolution. As training continues, our X-volution surpasses convolution and PSSA in the 3-th epoch, and maintains the lead in performance. Our designed PSSA lags behind convolution until the 9-th epoch, at which time the learning rate is decayed, and then these two operators maintain comparable performance. From the trend of the curve in <ref type="figure" target="#fig_4">Fig. 3</ref>, it can be concluded that combining the two operators can overcome the shortcoming of slow convergence of self-attention and obtain better optimization properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Relative Position Encoding</head><p>Positional encoding <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b25">26]</ref> is an vital component in self-attention. When processing, self-attention needs to transform the data into a vector, which destroys the position information in the data. This is particularly prominent in visual tasks. Contemporary transformers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b1">2]</ref> employ various position embedding methods (e.g., absolute position encoding, relative position encoding, e.t.c.) to recover the perception of positional information. In Tab. 4, we report the results of ablation study for relative position encoding. It can be observed that, for our X-volution, there is only a marginal improvement after adding the position encoding, compared to the approximate self-attention. We judge that the local processing properties of convolution can alleviate the problem of missing position information caused by self-attention to a certain extent. Moreover, relative position encoding possesses similar characteristics with local processing, which is directly introduced by the convolution branch in our X-volution. Based on this understanding, the standard X-volution is not equipped with position encoding, which also slightly reduces additional computational burden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper we study and analyze the fundamental principle of convolution and self-attention, which show complementary characteristics. We first theoretically derive a global self-attention approximation scheme PSSA. Then we propose a multi-branch topology to integrate the two operators in a coarse-grained manner, absorbing the advantages from both. Furthermore, we leverage the structural re-parameterization to perform fine-grained merging of the constructed multi-branch after training, and finally obtain a single convolutional style atomic operator X-volution which simplifies the topology. Extensive experiments on image classification, object detection and instance segmentation demonstrate the effectiveness and feasibility of our proposed operator.</p><p>Several opening problems for this work still remain. In future work, we will focus on how to further reduce the complexity of operator merging. Besides, the experiments employing the X-volution in various backbone should be conducted, which could further demonstrate the feasibly and efficiency of our operator. In addition, we will study and design a framework entirely composed of X-volution.</p><p>6 Appendix 6.1 More Implementation Details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Network Architecture</head><p>In Tab. 5, we show the detailed architecture we used for our X-voluiton model (X-volution, stage5) on ImageNet-1k classification. The only difference from ResNet-50 is replacement of 3?3 convolutions with the proposed X-volution in conv5_x. Other operators are evaluated with the same replacement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Implementation of X-volution</head><p>Our proposed X-volution is composed of two branches, one of which is convolution and the other branch is composed of self-attention. For the convolution branch, we use a 3 ? 3 convolution paralleled with an additional 5 ? 5 dilated convolution (stride=1, dilation=2). For the self-attention branch, we use our proposed PSSA, which is an approximation of global self-attention, shown in <ref type="figure" target="#fig_0">Fig.4</ref>. In PSSA, the input feature map is firstly shifted toward eight directions by L pixels, where L is set as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref> in the implementation. After the Query and Key transformation, we calculate the element-wise products between the original feature map and the shifted feature map to get the transformed features. Then we perform weighted addition on the transformed features to get an approximated self-attention map. A batch normalization layer is stacked before the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">More Detection Results on COCO</head><p>We also evaluate our proposed PSSA and X-volution with convolution baseline under multiple training schedules (i.e., 1x: 12 epochs, 2x: 24 epochs, 3x: 36 epochs, 6x: 72 epochs) <ref type="bibr" target="#b2">3</ref> . For all experiments, we train with a total batch size of 16 on 8 NVIDIA Tesla V100 GPUs. The learning rate is initially set as 0.2, and is divided by 10 with different learning schedules. The other experimental settings remain the same. Tab. 7 presents the detailed hyperparameters for different schedules.</p><p>Tab. 6 compares the results of different operators under different training schedules. Convolution denotes the ResNet-50 baseline. We can clearly observe that our X-volution brings steady and significant improvements over the convolution, suggesting that the combination of convolution and self-attention contributes to better visual understanding. Under 1x setting, PSSA achieves comparable performance comparing to convolution, and the performance of PSSA is obviously better than that of convolution as the training steps increase. Moreover, the gain of X-volution also increases with   more training epochs, and the most significant gain (i.e., +2.0 AP) is obtained under 3x setting. This phenomenon indicates that self-attention operator needs a longer training circle to achieve better performance than convolution, and the multi-branch topology allows the X-volution module to possess well-conditioned optimization properties from convolution and also benefit from long-range interactions introduced by self-attention, thus showing stronger learning ability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3. 4</head><label>4</label><figDesc>The Unification of Convolution and Self-Attention: X-volution 3.4.1 Convolution and Self-Attention are Complementary</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Replacement position for our X-volution operator. In ResNet-34, we replace the last Conv3?3 in the BasicBlock, and for ResNet-50, we replace the middle Conv3?3 in the BottleneckBlock.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2) 35.8 (+0.1) 18.0 (+0.6) 38.4 (+0.3) 51.3 (?0.1) X-volution(Inv) 39.9 (+0.8) 24.1 (+0.9) 42.9 (+0.5) 52.0 (+0.8) 35.9 (+0.2) 18.2 (+0.8) 38.5 (+0.4) 51.8 (+0.4) Self-Attention [32] 39.4 (+0.3) 24.1 (+0.9) 43.3 (+0.9) 49.8 (?1.4) 36.1 (+0.4) 18.1 (+0.7) 39.0 (+0.9) 51.0 (?0.4) X-volution(SA) 41.1 (+2.0) 25.4 (+2.2) 44.3 (+1.9) 53.0 (+1.8) 37.2 (+1.5) 19.2 (+1.8) 40.0 (+1.9) 53.1 (+1.7) PSSA 39.5 (+0.4) 23.4 (+0.2) 43.1 (+0.7) 50.7 (?0.5) 36.0 (+0.3) 17.1 (?0.3) 38.6 (+0.5) 51.3 (?0.1) X-volution 40.9 (+1.8) 24.8 (+1.6) 43.8 (+1.4) 53.2 (+2.0) 36.8 (+1.1) 18.5 (+1.1) 39.2 (+1.1) 52.7 (+1.3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>(a) Ablation study for different convolution kernel sizes.(b) Convergence speed of different kinds of operators on COCO (learning rate decays at 8-th and 11-st epoch).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>The detailed structure of our proposed PSSA operator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different operators on ImageNet-1K<ref type="bibr" target="#b4">[5]</ref> using ResNet-34 and ResNet-50.</figDesc><table><row><cell cols="2">Backbone Operator</cell><cell>top-1 top-5</cell></row><row><cell></cell><cell>Convolution</cell><cell>73.8 91.6</cell></row><row><cell></cell><cell>Self-Attention</cell><cell>73.2 91.4</cell></row><row><cell>ResNet-34</cell><cell cols="2">PSSA X-volution, stage3 75.0 92.4 73.6 91.5</cell></row><row><cell></cell><cell cols="2">X-volution, stage4 74.3 91.9</cell></row><row><cell></cell><cell cols="2">X-volution, stage5 74.2 91.7</cell></row><row><cell></cell><cell>Convolution</cell><cell>75.7 92.5</cell></row><row><cell></cell><cell>Self-Attention</cell><cell>75.3 92.2</cell></row><row><cell>ResNet-50</cell><cell cols="2">PSSA X-volution, stage3 76.6 93.3 75.5 92.5</cell></row><row><cell></cell><cell cols="2">X-volution, stage4 75.1 92.4</cell></row><row><cell></cell><cell cols="2">X-volution, stage5 75.9 92.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Object detection results of different operators on COCO val 2017.</figDesc><table><row><cell>Operator</cell><cell>AP bbox</cell><cell>AP bbox 50</cell><cell>AP bbox 75</cell><cell>AP bbox S</cell><cell>AP bbox M</cell><cell>AP bbox L</cell></row><row><cell>Convolution</cell><cell>38.6</cell><cell>59.7</cell><cell>42.0</cell><cell>23.6</cell><cell>42.3</cell><cell>50.0</cell></row><row><cell>Multi-Convolution</cell><cell>38.4 (?0.2)</cell><cell>59.0 (?0.7)</cell><cell>41.7 (?0.3)</cell><cell>22.2 (?1.4)</cell><cell>41.6 (?0.7)</cell><cell>49.9 (?0.1)</cell></row><row><cell cols="2">AA-Convolution [1] 39.4 (+0.8)</cell><cell>60.6 (+0.9)</cell><cell>42.7 (+0.7)</cell><cell>24.4 (+0.8)</cell><cell>42.9 (+0.6)</cell><cell>50.9 (+0.9)</cell></row><row><cell>Involution [16]</cell><cell>38.8 (+0.2)</cell><cell>59.9 (+0.2)</cell><cell>42.1 (+0.1)</cell><cell>23.6 (+0.0)</cell><cell>41.9 (?0.4)</cell><cell>50.2 (+0.2)</cell></row><row><cell>X-volution(Inv)</cell><cell>39.2 (+0.6)</cell><cell>60.2 (+0.5)</cell><cell>42.7 (+0.7)</cell><cell>23.6 (+0.0)</cell><cell>42.0 (?0.3)</cell><cell>50.9 (+0.9)</cell></row><row><cell>Self-Attention [32]</cell><cell>38.5 (?0.1)</cell><cell>60.5 (+0.8)</cell><cell>41.7 (?0.3)</cell><cell>24.2 (+0.6)</cell><cell>42.2 (?0.1)</cell><cell>48.6 (?1.4)</cell></row><row><cell>X-volution(SA)</cell><cell>40.3 (+1.7)</cell><cell>61.8 (+2.1)</cell><cell>43.7 (+1.7)</cell><cell>24.6 (+1.0)</cell><cell>43.9 (+1.6)</cell><cell>52.0 (+2.0)</cell></row><row><cell>PSSA</cell><cell>38.7 (+0.1)</cell><cell>60.4 (+0.7)</cell><cell>42.2 (+0.2)</cell><cell>23.8 (+0.2)</cell><cell>42.3 (+0.0)</cell><cell>49.1 (?0.9)</cell></row><row><cell>X-volution</cell><cell>40.1 (+1.5)</cell><cell>61.3 (+1.6)</cell><cell>43.6 (+1.6)</cell><cell>24.0 (+0.4)</cell><cell>43.6 (+1.3)</cell><cell>51.4 (+1.4)</cell></row></table><note>* ?(x.x) here denotes the performance gain over the convolution baseline.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Instance segmentation results of different operators on COCO val 2017. Convolution [1] 40.2 (+1.1) 25.0 (+1.8) 43.6 (+1.2) 52.0 (+0.8) 36.4 (+0.7) 18.8 (+1.4) 39.2 (+1.1) 52.5 (+1.1) Involution</figDesc><table><row><cell>Operator</cell><cell>AP bbox</cell><cell>AP bbox S</cell><cell>AP bbox M</cell><cell>AP bbox L</cell><cell>AP mask</cell><cell>AP mask S</cell><cell>AP mask M</cell><cell>AP mask L</cell></row><row><cell>Convolution</cell><cell>39.1</cell><cell>23.2</cell><cell>42.4</cell><cell>51.2</cell><cell>35.7</cell><cell>17.4</cell><cell>38.1</cell><cell>51.4</cell></row><row><cell>AA-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Effects of relative position encoding on COCO detection. Following<ref type="bibr" target="#b19">[20]</ref>, we add the position embedding as the bias to the product of Query and Key.</figDesc><table><row><cell cols="2">Operator Pos. AP bbox AP bbox S</cell><cell>AP bbox M</cell><cell>AP bbox L</cell></row><row><cell>PSSA</cell><cell cols="3">38.7 23.8 42.3 49.1</cell></row><row><cell>PSSA</cell><cell cols="3">39.1 24.1 42.7 49.5</cell></row><row><cell>X-volution</cell><cell cols="3">40.1 24.0 43.6 51.4</cell></row><row><cell>X-volution</cell><cell cols="3">40.2 24.0 43.8 51.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Architectures for ImageNet using ResNet-50.</figDesc><table><row><cell>stage</cell><cell>output</cell><cell></cell><cell cols="2">ResNet-50</cell><cell></cell><cell></cell><cell>Ours</cell></row><row><cell cols="2">conv1 112?112</cell><cell></cell><cell></cell><cell cols="4">7?7, 64, stride 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">3?3 max pool, stride 2</cell></row><row><cell cols="2">conv2_x 56 ? 56</cell><cell>? ?</cell><cell>1?1, 64 3?3, 64</cell><cell>? ? ?3</cell><cell></cell><cell>? ?</cell><cell>1?1, 64 3?3, 64</cell><cell>? ? ?3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1?1, 256</cell><cell></cell><cell></cell><cell></cell><cell>1?1, 256</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>1?1, 128</cell><cell>?</cell><cell></cell><cell>?</cell><cell>1?1, 128</cell><cell>?</cell></row><row><cell cols="2">conv3_x 28 ? 28</cell><cell>?</cell><cell>3?3, 128</cell><cell>? ?4</cell><cell></cell><cell>?</cell><cell>3?3, 128</cell><cell>? ?4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1?1, 512</cell><cell></cell><cell></cell><cell></cell><cell>1?1, 512</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>1?1, 256</cell><cell>?</cell><cell></cell><cell>?</cell><cell>1?1, 256</cell><cell>?</cell></row><row><cell cols="2">conv4_x 14 ? 14</cell><cell>?</cell><cell>3?3, 256</cell><cell>? ?6</cell><cell></cell><cell>?</cell><cell>3?3, 256</cell><cell>? ?6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1?1, 1024</cell><cell></cell><cell></cell><cell></cell><cell>1?1, 1024</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>1?1, 512</cell><cell>?</cell><cell>?</cell><cell></cell><cell>1?1, 512</cell><cell>?</cell></row><row><cell>conv5_x</cell><cell>7 ? 7</cell><cell>?</cell><cell>3?3, 512</cell><cell>? ?3</cell><cell>?</cell><cell cols="3">X-volution, 512</cell><cell>? ?3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1?1, 2048</cell><cell></cell><cell></cell><cell></cell><cell>1?1, 2048</cell></row><row><cell></cell><cell>1 ? 1</cell><cell></cell><cell cols="5">average pool, 1000-d fc, softmax</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>More detection results about different operators on COCO 2017 val under the multiple settings. +0.5) 62.8 (+1.3) 44.9 (+0.4) 25.4 (+0.9) 44.9 (+0.4) 52.3 (?0.2) X-volution 36 42.7 (+2.0) 63.9 (+2.4) 46.3 (+1.8) 26.5 (+2.0) 46.3 (+1.8) 54.8 (+2.3) +0.7) 63.1 (+1.4) 45.4 (+0.6) 25.8 (+1.6) 45.1 (+0.8) 53.4 (+0.3) X-volution 72 42.8 (+1.8) 64.0 (+2.3) 46.4 (+1.6) 26.9 (+2.7) 46.0 (+1.7) 55.0 (+1.9)</figDesc><table><row><cell>Operator</cell><cell>Epoch</cell><cell>AP bbox</cell><cell>AP bbox 50</cell><cell>AP bbox 75</cell><cell>AP bbox S</cell><cell>AP bbox M</cell><cell>AP bbox L</cell></row><row><cell>Convolution</cell><cell>12</cell><cell>38.6</cell><cell>59.7</cell><cell>42.0</cell><cell>23.6</cell><cell>42.3</cell><cell>50.0</cell></row><row><cell>PSSA</cell><cell>12</cell><cell cols="6">38.7 (+0.1) 60.4 (+0.7) 42.2 (+0.2) 23.8 (+0.2) 42.3 (+0.0) 49.1 (?0.9)</cell></row><row><cell>X-volution</cell><cell>12</cell><cell cols="6">40.1 (+1.5) 61.3 (+1.6) 43.6 (+1.6) 24.0 (+0.4) 43.6 (+1.3) 51.4 (+1.4)</cell></row><row><cell>Convolution</cell><cell>24</cell><cell>40.1</cell><cell>61.1</cell><cell>43.7</cell><cell>23.9</cell><cell>43.7</cell><cell>52.2</cell></row><row><cell>PSSA</cell><cell>24</cell><cell cols="6">40.4 (+0.3) 61.9 (+0.8) 44.1 (+0.4) 24.9 (+1.0) 44.1 (+0.4) 51.4 (?0.8)</cell></row><row><cell>X-volution</cell><cell>24</cell><cell cols="6">42.0 (+1.9) 63.8 (+2.7) 45.9 (+2.2) 26.4 (+2.5) 45.5 (+1.8) 54.2 (+2.0)</cell></row><row><cell>Convolution</cell><cell>36</cell><cell>40.7</cell><cell>61.5</cell><cell>44.5</cell><cell>24.5</cell><cell>44.5</cell><cell>52.5</cell></row><row><cell cols="3">PSSA 41.2 (Convolution 36 72 41.0</cell><cell>61.7</cell><cell>44.8</cell><cell>24.2</cell><cell>44.3</cell><cell>53.1</cell></row><row><cell>PSSA</cell><cell>72</cell><cell>41.7 (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Learning rate schedules for the 1x, 2x, 3x and 6x settings.</figDesc><table><row><cell cols="4">Setting Epochs Training Steps Learning Rate Schedule</cell></row><row><cell>1x</cell><cell>12</cell><cell>90k</cell><cell>[60k, 80k]</cell></row><row><cell>2x</cell><cell>24</cell><cell>180k</cell><cell>[120, 160k]</cell></row><row><cell>3x</cell><cell>36</cell><cell>270k</cell><cell>[210k, 250k]</cell></row><row><cell>6x</cell><cell>72</cell><cell>540k</cell><cell>[420k, 500k]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">x i ?(?\A) ?i x0, xi ? x i ?(?\A) ?i x k ?? ? k x k , xi = x i ?(?\A) x k ?? ?i? k x k , xi .(4)Without loss of generality, we can add terms in region A whose coefficient is zero. By design, non-local region is also within the receptive field of the boundary pixels of the local region. Thus, we can transform Eq. 4 into below formulation:x i ?(?\A) x k ?? ?i? k x k , xi = x i ?? x k ?? ?i? k x k , xi .(5)According to the Markovian property of the image, we can assume that for x k ? A, the interaction between x i (far away from x k ) and x k is weak. Thus, the Eq. 5 can be further simplified:x i ?? x k ?? ?i? k x k , xi = x k ?? x i ?U (x k ) ?i? k x k , xi ,(6)where U (x k ) denote the local region of x k . Substituting Eq. 6 into second item in Eq. 3, we can rewrite it as following formulation:x t ?? ?t x0, xt ? x i ?A ?i ? 1 x0, xi + x k ?? x i ?U (x k ) ?i? k x k , xi = x k ?A x i ?U (x k ) ?i? k x k , xi = x k ?A ? k x i ?U (x k ) w q w k w v xi x k , xi .(7)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">1x, 2x, 3x and 6x setting is adopted from Detectron2.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Transformer tracking. CoRR, abs/2103.15436</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the relationship between selfattention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Repvgg: Making vgg-style convnets great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/2101.03697</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
		<idno>abs/2012.09841</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Involution: Inverting the inherence of convolution for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>CoRR, abs/2103.06255, 2021. 6</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/2012.09760</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Thinking fast and slow: Efficient text-to-visual retrieval with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/2103.16553</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the turing completeness of modern neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Marinkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Barcel?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bottleneck transformers for visual recognition. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Scaling local self-attention for parameter efficient visual backbones. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><forename type="middle">A</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Eca-net: Efficient channel attention for deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Banggu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. CBAM: convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deformable DETR: deformable transformers for end-to-end object detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
