<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepType: Multilingual Entity Linking by Neural Type System Evolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Raiman</surname></persName>
							<email>raiman@openai.com</email>
							<affiliation key="aff0">
								<orgName type="institution">OpenAI San Francisco</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Raiman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">OpenAI San Francisco</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agilience</forename><surname>Paris</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">OpenAI San Francisco</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">France</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">OpenAI San Francisco</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeepType: Multilingual Entity Linking by Neural Type System Evolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The wealth of structured (e.g. Wikidata) and unstructured data about the world available today presents an incredible opportunity for tomorrow's Artificial Intelligence. So far, integration of these two different modalities is a difficult process, involving many decisions concerning how best to represent the information so that it will be captured or useful, and hand-labeling large amounts of data. DeepType overcomes this challenge by explicitly integrating symbolic information into the reasoning process of a neural network with a type system. First we construct a type system, and second, we use it to constrain the outputs of a neural network to respect the symbolic structure. We achieve this by reformulating the design problem into a mixed integer problem: create a type system and subsequently train a neural network with it. In this reformulation discrete variables select which parent-child relations from an ontology are types within the type system, while continuous variables control a classifier fit to the type system. The original problem cannot be solved exactly, so we propose a 2-step algorithm: 1) heuristic search or stochastic optimization over discrete variables that define a type system informed by an Oracle and a Learnability heuristic, 2) gradient descent to fit classifier parameters. We apply DeepType to the problem of Entity Linking on three standard datasets (i.e. WikiDisamb30, CoNLL (YAGO), TAC KBP 2010) and find that it outperforms all existing solutions by a wide margin, including approaches that rely on a human-designed type system or recent deep learning-based entity embeddings, while explicitly using symbolic information lets it integrate new entities without retraining. 1 e.g. Do we overfit to a particular set of symbolic structures use-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Online encyclopedias, knowledge bases, ontologies (e.g. Wikipedia, Wikidata, Wordnet), alongside image and video datasets with their associated label and category hierarchies (e.g. Imagenet <ref type="bibr" target="#b2">(Deng et al. 2009</ref>), Youtube-8M <ref type="bibr" target="#b0">(Abu-El-Haija et al. 2016)</ref>, Kinetics <ref type="bibr" target="#b8">(Kay et al. 2017)</ref>) offer an unprecedented opportunity for incorporating symbolic representations within distributed and neural representations in Artificial Intelligence systems. Several approaches exist for integrating rich symbolic structures within the behavior of neural networks: a label hierarchy aware loss function that relies on the ultrametric tree distance between labels (e.g. it is worse to confuse sheepdogs and skyscrapers than it is to confuse sheepdogs and poodles) (Wu, Tygert, and LeCun 2017), a loss function that trades off specificity for accuracy by incorporating hypo/hypernymy relations <ref type="bibr" target="#b3">(Deng et al. 2012)</ref>, using NER types to constrain the behavior of an Entity Linking system <ref type="bibr" target="#b11">(Ling, Singh, and Weld 2015)</ref>, or more recently integrating explicit type constraints within a decoder's grammar for neural semantic parsing <ref type="bibr" target="#b8">(Krishnamurthy, Dasigi, and Gardner 2017)</ref>. However, current approaches face several difficulties:</p><p>? Selection of the right symbolic information based on the utility or information gain for a target task. ? Design of the representation for symbolic information (hierarchy, grammar, constraints). ? Hand-labelling large amounts of data.</p><p>DeepType overcomes these difficulties by explicitly integrating symbolic information into the reasoning process of a neural network with a type system that is automatically designed without human effort for a target task. We achieve this by reformulating the design problem into a mixed integer problem: create a type system by selecting roots and edges from an ontology that serve as types in a type system, and subsequently train a neural network with it. The original problem cannot be solved exactly, so we propose a 2-step algorithm: 1. heuristic search or stochastic optimization over the discrete variable assignments controlling type system design, using an Oracle and a Learnability heuristic to ensure that design decisions will be easy to learn by a neural network, and will provide improvements on the target task, 2. gradient descent to fit classifier parameters to predict the behavior of the type system. In order to validate the benefits of our approach, we focus on applying DeepType to Entity Linking (EL), the task of resolving ambiguous mentions of entities to their referent entities in a knowledge base (KB) (e.g. Wikipedia). Specifically we compare our results to state of the art systems on three standard datasets (WikiDisamb30, CoNLL (YAGO), TAC KBP 2010). We verify whether our approach can work in multiple languages, and whether optimization of the type system for a particular language generalizes to other languages 1 by training our full system in a monolingual (English) and bilingual setup (English and French), and also evaluate our Oracle (performance upper bound) on German and Spanish test datasets. We compare stochastic optimization and heuristic search to solve our mixed integer problem by comparing the final performance of systems whose type systems came from different search methodologies. We also investigate whether symbolic information is captured by using DeepType as pretraining for Named Entity Recognition (NER) on two standard datasets (i.e. CoNLL 2003 <ref type="bibr" target="#b13">(Sang and Meulder 2003)</ref>, OntoNotes 5.0 (CoNLL 2012) <ref type="bibr" target="#b12">(Pradhan et al. 2012)</ref>).</p><p>Our key contributions in this work are as follows:</p><p>? A system for integrating symbolic knowledge into the reasoning process of a neural network through a type system, to constrain the behavior to respect the desired symbolic structure, and automatically design the type system without human effort.</p><p>? An approach to EL that uses type constraints, reduces disambiguation resolution complexity from O(N 2 ) to O(N ), incorporates new entities into the system without retraining, and outperforms all existing solutions by a wide margin.</p><p>We release code for designing, evolving, and training neural type systems 2 . Moreover, we observe that disambiguation accuracy reaches 99.0% on CoNLL (YAGO) and 98.6% on TAC KBP 2010 when entity types are predicted by an Oracle, suggesting that EL would be almost solved if we can improve type prediction accuracy. The rest of this paper is structured as follows. In Section 2 we introduce EL and EL with Types, in Section 3 we describe DeepType for EL, In Section 4 we provide experimental results for DeepType applied to EL and evidence of cross-lingual and cross-domain transfer of the representation learned by a DeepType system. In Section 5 we relate our work to existing approaches. Conclusions and directions for future work are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task</head><p>Before we define how DeepType can be used to constrain the outputs of a neural network using a type system, we will first define the goal task of Entity Linking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Linking</head><p>The goal is to recover the ground truth entities in a KB referred to in a document by locating mentions (text spans), and for each mention properly disambiguating the referent entity. Commonly, a lookup table that maps each mention to a proposal set of entities for each mention m: E m = {e 1 , . . . , e n } (e.g. "Washington" could mean Washington, D.C. or George Washington). Disambiguation is finding for each mention m the a ground truth entity e GT in E m . Typically, disambiguation operates according to two criteria: in a large corpus, how often does a mention point to an entity, LinkCount(m, e), and how often does entity e 1 cooccur with entity e 2 , an O(N 2 ) process, often named coherful only in English, or can we discover a knowledge representation that works across languages? 2 http://github.com/openai/deeptype ence <ref type="bibr" target="#b12">(Milne and Witten 2008;</ref><ref type="bibr" target="#b4">Ferragina and Scaiella 2010;</ref><ref type="bibr">Yamada et al. 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Linking with Types</head><p>In this work we extend the EL task to associate with each entity a series of types (e.g. Person, Place, etc.) that if known, would rule out invalid answers, and therefore ease linking (e.g. the context now enables types to disambiguate "Washington"). Knowledge of the types T associated with a mention can also help prune entities from the the proposal set, to produce a constrained set: E m,T ? E m . In a probabilistic setting it is also possible to rank an entity e in document x according to its likelihood under the type system prediction and under the entity model: P(e|x) ? P type (types(e)|x) ? P entity (e|x, types(e)).</p><p>(1)</p><p>In prior work, the 112 FIGER Types <ref type="bibr" target="#b10">(Ling and Weld 2012)</ref> were associated with entities to combine an NER tagger with an EL system <ref type="bibr" target="#b11">(Ling, Singh, and Weld 2015)</ref>. In their work, they found that regular NER types were unhelpful, while finer grain FIGER types improved system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DeepType for Entity Linking</head><p>DeepType is a strategy for integrating symbolic knowledge into the reasoning process of a neural network through a type system. When we apply this technique to EL, we constrain the behavior of an entity prediction model to respect the symbolic structure defined by types. As an example, when we attempt to disambiguate "Jaguar" the benefits of this approach are apparent: our decision can be based on whether the predicted type is Animal or Road Vehicle as shown visually in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>In this section, we will first define key terminology, then explain the model and its sub-components separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Terminology</head><p>Relation Given some knowledge graph or feature set, a relation is a set of inheritance rules that define membership or exclusion from a particular group. For instance the relation instance of(city) selects all children of the root city connected by instance of as members of the group, depicted by outlined boxes in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Type In this work a type is a label defined by a relation (e.g. IsHuman is the type applied to all children of Human connected by instance of).</p><p>Type Axis A set of mutually exclusive types (e.g.</p><formula xml:id="formula_0">IsHuman ? IsPlant = {}).</formula><p>Type System A grouping of type axes, A, along with a type labelling function: {t 1 , . . . , t k } = TypeLabeler(e, A). For instance a type system with two axes {IsA, Topic} assigns to George Washington: {Person, Politics}, and to Washington, D.C.: {Place, Geography}).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>To construct an EL system that uses type constraints we require: a type system, the associated type classifier, and a</p><p>The prey saw a jaguar cross in the jungle.</p><p>The man saw a Jaguar speed on the highway.  model for predicting and ranking entities given a mention. Instead of assuming we receive a type system, classifier, entity prediction model, we will instead create the type system and its classifier starting from a given entity prediction model and ontology with text snippets containing entity mentions (e.g. Wikidata and Wikipedia). For simplicity we use LinkCount(e, m) as our entity prediction model.</p><p>We restrict the types in our type systems to use a set of parent-child relations over the ontology in Wikipedia and Wikidata, where each type axis has a root node r and an edge type g, that sets membership or exclusion from the axis (e.g. r = human, e = instance of, splits entities into: human vs. non-human 3 ).</p><p>We then reformulate the problem into a mixed integer problem, where discrete variables control which roots r 1 , . . . , r k and edge types g 1 , . . . , g k among all roots R and edge types G will define type axes, while the continuous variables ? parametrize a classifier fit to the type system. Our goal in type system design is to select parent-child relations that a classifier easily predicts, and where the types improve disambiguation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Objective</head><p>To formally define our mixed integer problem, let us first denote A as the assignment for the discrete vari-3 Type "instance of:human" mimics the NER PER label. ables that define our type system (i.e. boolean variables defining if a parent-child relation gets included in our type system), ? as the parameters for our entity prediction model and type classifier, and S model (A, ?) as the disambiguation accuracy given a test corpus containing mentions M = (m 0 , e GT 0 , E m0 ), . . . , (m n , e GT n , E mn ) . We now assume our model produces some score for each proposed entity e given a mention m in a document D, defined EntityScore(e, m, D, A, ?). The predicted entity for a given mention is thus: e * = argmax e?Em EntityScore(e, m, D, A, ?). If e * = e GT , the mention is disambiguated. Our problem is thus defined as:</p><formula xml:id="formula_1">max A max ? S model (A, ?) = (m,eGT,Em)?M 1 eGT (e * ) |M | .<label>(2)</label></formula><p>This original formulation cannot be solved exactly 4 . To make this problem tractable we propose a 2-step algorithm:</p><p>1. Discrete Optimization of Type System: Heuristic search or stochastic optimization over the discrete variables of the type system, A, informed by a Learnability heuristic and an Oracle.</p><p>2. Type classifier: Gradient descent over continuous variables ? to fit type classifier and entity prediction model.</p><p>We will now explain in more detail discrete optimization of a type system, our heuristics (Oracle and Learnability heuristic), the type classifier, and inference in this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discrete Optimization of a Type System</head><p>The original objective S model (A, ?) cannot be solved exactly, thus we rely on heuristic search or stochastic optimization to find suitable assignments for A. To avoid training an entire type classifier and entity prediction model for each evaluation of the objective function, we instead use a proxy objective J for the discrete optimization 5 . To ensure that maximizing J(A) also maximizes S model (A, ?), we introduce a Learnability heuristic and an Oracle that quantify the disambiguation power of a proposed type system, an estimate of how learnable the type axes in the selected solution will be. We measure an upper bound for the disambiguation power by measuring disambiguation accuracy S oracle for a type classifier Oracle over a test corpus.</p><p>To ensure that the additional disambiguation power of a solution A translates in practice we weigh by an estimate of solution's learnability Learnability(A) improvements between S oracle and the accuracy of a system that predicts only according to the entity prediction model 6 , S greedy .</p><p>Selecting a large number of type axes will provide strong disambiguation power, but may lead to degenerate solutions that are harder to train, slow down inference, and lack higher-level concepts that provide similar accuracy with less axes. We prevent this by adding a per type axis penalty of ?.</p><p>Combining these three terms gives us the equation for J:</p><formula xml:id="formula_2">J(A) =(S oracle ? S greedy ) ? Learnability(A) + S greedy ? |A| ? ?.<label>(3)</label></formula><p>Oracle Our Oracle is a methodology for abstracting away machine learning performance from the underlying representational power of a type system A. It operates on a test corpus with a set of mentions, entities, and proposal sets:</p><formula xml:id="formula_3">m i , e GT i , E mi .</formula><p>The Oracle prunes each proposal set to only contain entities whose types match those of e GT i , yielding E m,oracle . Types fully disambiguate when |E m,oracle | = 1, otherwise we use the entity prediction model to select the right entity in the remainder set E mi,oracle :</p><p>Oracle(m) = argmax e?E m,oracle P entity (e|m, types(x)).</p><p>( <ref type="formula">4)</ref> If Oracle(m) = e GT , the mention is disambiguated. Oracle accuracy is denoted S oracle given a type system over a test corpus containing mentions M = (m 0 , e GT 0 , E m0 ), . . . , (m n , e GT n , E mn ) :</p><formula xml:id="formula_4">S oracle = (m,eGT,Em)?M 1 eGT (Oracle(m)) |M | . (5)</formula><p>Learnability To ensure that disambiguation gains obtained during the discrete optimization are available when we train our type classifier, we want to ensure that the types selected are easy to predict. The Learnability heuristic empirically measures the average performance of classifiers at predicting the presence of a type within some Learnabilityspecific training set. To efficiently estimate Learnability for a full type system we make an independence assumption and model it as the mean of the Learnability for each individual axis, ignoring positive or negative transfer effects between different type axes. This assumption lets us parallelize training of 5 Training of the type classifier takes ?3 days on a Titan X Pascal, while our Oracle can run over the test set in 100ms. <ref type="bibr">6</ref> For an entity prediction model based only on link counts, this means always picking the most linked entity.  <ref type="figure">Figure 3</ref>: Text window classifier in (a) serves as type Learnability estimator, while the network in (b) takes longer to train, but discovers long-term dependencies to predict types and jointly produces a distribution for multiple type axes.</p><p>simpler classifiers for each type axis. We measure the area under its receiver operating characteristics curve (AUC) for each classifier and compute the type system's learnability:</p><formula xml:id="formula_5">Learnability(A) = t?A AUC(t)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|A|</head><p>. We use a text window classifier trained over windows of 10 words before and after a mention. Words are represented with randomly initialized word embeddings; the classifier is illustrated in <ref type="figure">Figure 3a</ref>. AUC is averaged over 4 training runs for each type axis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type Classifier</head><p>After the discrete optimization has completed we now have a type system A. We can now use this type system to label data in multiple languages from text snippets associated with the ontology 7 , and supervize a Type classifier.</p><p>The goal for this classifier is to discover long-term dependencies in the input data that let it reliably predict types across many contexts and languages. For this reason we select a bidirectional-LSTM <ref type="bibr" target="#b9">(Lample et al. 2016</ref>) with word, prefix, and suffix embeddings as done in <ref type="bibr" target="#b0">(Andor et al. 2016)</ref>. Our network is shown pictorially in <ref type="figure">Figure 3b</ref>. Our classifier is trained to minimize the negative log likelihood of the per-token types for each type axis in the document D with L tokens: ? k i=1 log P i (t i,1 , . . . , t i,L |D). When using Wikipedia as our source of text snippets our label supervision is partial 8 , so we make a conditional independence assumption about our predictions and use Softmax as our output activation:</p><formula xml:id="formula_6">? k i=1 L j=1 log P i (t i,j |w j , D).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference</head><p>At inference-time we incorporate classifier belief into our decision process by first running it over the full context and obtaining a belief over each type axis for each input word w 0 , . . . , w L . For each mention m covering words w x , . . . , w y , we obtain the type conditional probability for all type axes i: {P i (?|w x , D), . . . , P i (?|w y , D)}. In multiword mentions we must combine beliefs over multiple tokens x . . . y: the product of the beliefs over the mention's tokens is correct but numerically unstable and slightly less performant than max-over-time 9 , which we denote for the i-th type axis: P i, * (?|m, D).</p><p>The score s e,m,D,A,? = EntityScore(e, m, D, A, ?) of an entity e given these conditional probability distributions P 1, * (?|m, D), . . . , P k, * (?|m, D), and the entities' types in each axis t 1 , . . . , t k can then be combined to rank entities according to how predicted they were by both the entity prediction model and the type system. The chosen entity e * for a mention m is chosen by taking the option that maximizes the score among the E m possible entities; the equation for scoring and e * is given below, with P Link (e|m) = LinkCount(m,e) j?Em LinkCount(m,j) , ? i a per type axis smoothing parameter, ? is a smoothing parameter over all types:</p><formula xml:id="formula_7">s e,m,D,A,? =P Link (e|m) ? 1 ? ? + ? ? k i=1 (1 ? ? i + ? i ? P i, * (t i |m, D)) .<label>(6)</label></formula><p>4 Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type System Discovery</head><p>In the following experiments we evaluate the behavior of different search methodologies for type system discovery: which method best scales to large numbers of types, achieves high accuracy on the target EL task, and whether the choice of search impacts learnability by a classifier or generalisability to held-out EL datasets. For the following experiments we optimize DeepType's type system over a held-out set of 1000 randomly sampled articles taken from the Feb. 2017 English Wikipedia dump, with the Learnability heuristic text window classifiers trained only on those articles. The type classifier is trained jointly on English and French articles, totalling 800 million tokens for training, 1 million tokens for validation, sampled equally from either language.</p><p>We restrict roots R and edges G to the most common 1.5?10 5 entities that are entity parents through wikipedia category or instance of edges, and eliminate type axes where Learnability(?) is 0, leaving 53,626 type axes.</p><p>Human Type System Baseline To isolate discrete optimization from system performance and gain perspective on the difficulty and nature of the type system design we incorporate a human-designed type system. Human designers have access to the full set of entities and relations in Wikipedia and Wikidata, and compose different inheritance rules through Boolean algebra to obtain higher level concepts (e.g. woman = IsHuman ? IsFemale, or animal = IsTaxon??{IsHuman?IsPlant} 10 ). The final human system uses 5 type axes 11 , and 1218 inheritance rules. <ref type="bibr">9</ref> The choice of max-over-time is empirically motivated: we compared product mean, min, max, and found that max was comparable to mean, and slightly better than the alternatives. <ref type="bibr">10</ref> Taxon is the general parent of living items in Wikidata. 11 IsA, Topic, Location, Continent, and Time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search methodologies</head><p>Beam Search and Greedy selection We iteratively construct a type system by choosing among all remaining type axes and evaluating whether the inclusion of a new type axis improves our objective: J(A ? {t j }) &gt; J(A). We use a beam size of b and stop the search when all solutions stop growing.</p><p>Cross-Entropy Method (CEM) (Rubinstein 1999) is a stochastic optimization procedure applicable to the selection of types. We begin with a probability vector P 0 set to p start , and at each iteration we sample M CEM vectors s from the Bernoulli distribution given by P i , and measure each sample's fitness with Eq. 3. The N CEM highest fitness elements are our winning population S t at iteration t. Our probabilities are fit to S t giving P t+1 = s?S t s NCEM . The optimization is complete when the probability vector is binary.</p><p>Genetic Algorithm The best subset of type axes can be found by representing type axes as genes carried by N population individuals in a population undergoing mutations and crossovers <ref type="bibr" target="#b5">(Harvey 2009</ref>) over G generations. We select individuals using Eq. 3 as our fitness function.</p><p>Search Methodology Performance Impact To validate that ? controls type system size, and find the best tradeoff between size and accuracy, we experiment with a range of values and find that accuracy grows more slowly below 0.00007, while system size still increases.</p><p>From this point on we keep ? = 0.00007, and we compare the number of iterations needed by different search methods to converge, against two baselines: the empty set and the mean performance of 100 randomly sampled sets of 128 types <ref type="table" target="#tab_0">(Table 1a</ref>). We observe that the performance of stochastic optimizers GA and CEM is similar to heuristic search, but requires orders of magnitude less function evaluations.</p><p>Next, we compare the behavior of the different search methods to a human designed system and state of the art approaches on three standard datasets (i.e. WIKI-DISAMB30 (WKD30) <ref type="bibr" target="#b4">(Ferragina and Scaiella 2010)</ref>  <ref type="bibr">12</ref> , CoNLL(YAGO) <ref type="bibr" target="#b6">(Hoffart et al. 2011)</ref>, and TAC KBP 2010 <ref type="bibr" target="#b7">(Ji et al. 2010)</ref>), along with test sets built by randomly sampling 1000 articles from Wikipedia's February 2017 dump in English, French, German, and Spanish which were excluded from training the classifiers. <ref type="table" target="#tab_0">Table 1c</ref> has Oracle performance for the different search methods on the test sets, where we report disambiguation accuracy per annotation. A LinkCount baseline is included that selects the mention's most frequently linked entity 13 . All search techniques' Oracle ac- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Lingual Transfer</head><p>Type systems are defined over Wikidata/Wikipedia, a multilingual knowledge base/encyclopaedia, thus type axes are language independent and can produce cross-lingual supervision. To verify whether this cross-lingual ability is useful we train a type system on an English dataset and verify whether it can successfully supervize French data. We also measure using the Oracle (performance upper bound) whether the type system is useful in Spanish or German. Oracle performance across multiple languages does not appear to degrade when transferring to other languages <ref type="table" target="#tab_0">(Table 1c</ref>). We also notice that training in French with an English type system still yields improvements over LinkCount for CEM, greedy, and human systems.</p><p>Because multi-lingual training might oversubscribe the model, we verified if monolingual would outperform bilingual training: we compare GA in English + French with only English (last row of <ref type="table" target="#tab_0">Table 1c</ref>). Bilingual training does not appear to hurt, and might in fact be helpful.</p><p>We follow-up by inspecting whether the bilingual word vector space led to shared representations: common nouns have their English-French translation close-by, while proper nouns do not (French and US politicians cluster separately).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named Entity Recognition Transfer</head><p>The goal of our NER experiment is to verify whether Deep-Type produces a type sensitive language representation useful for transfer to other downstream tasks. To measure this we pre-train a type classifier with a character-CNN and word embeddings as inputs, following <ref type="bibr" target="#b8">(Kim et al. 2015)</ref>, and replace the output layer with a linear-chain CRF <ref type="bibr" target="#b9">(Lample et al. 2016)</ref> to fine-tune to NER data. Our model's F1 scores when transferring to the CoNLL 2003 NER task and OntoNotes 5.0 (CoNLL 2012) split are given in <ref type="table" target="#tab_0">Table 1b</ref>. We compare with two baselines that share the architecture but are not pre-trained, along with the current state of the art <ref type="bibr" target="#b1">(Chiu and Nichols 2015)</ref>.</p><p>We see positive transfer on Ontonotes and CoNLL: our baseline Bi-LSTM strongly outperforms <ref type="bibr" target="#b1">(Chiu and Nichols 2015)</ref>'s baseline, while pre-training gives an additional 3-4 F1 points, with our best model outperforming the state of the art on the OntoNotes development split. While our baseline LSTM-CRF performs better than in the literature, our strongest baseline (CNN+LSTM+CRF) does not match the state of the art with a lexicon. We find that DeepType always improves over baselines and partially recovers lexicon performance gains, but does not fully replace lexicons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Neural Network Reasoning with Symbolic structures Several approaches exist for incorporating symbolic structures into the reasoning process of a neural network by designing a loss function that is defined with a label hierarchy. In particular the work of <ref type="bibr" target="#b3">(Deng et al. 2012</ref>) trades off specificity for accuracy, by leveraging the hyper/hyponymy relation to make a model aware of different granularity levels. Our work differs from this approach in that we design our type system within an ontology to meet specific accuracy goals, while they make the accuracy/specificity tradeoff at training time, with a fixed structure. More recently (Wu, Tygert, and LeCun 2017) use a hierarchical loss to increase the penalty for distant branches of a label hierarchy using the ultrametric tree distance. We also aim to capture the most important aspects of the symbolic structure and shape our loss function accordingly, however our loss shaping is a result of discrete optimization and incorporates a learnability heuristic to choose aspects that can easily be acquired.</p><p>A different direction for integrating structure stems from constraining model outputs, or enforcing a grammar. In the work of <ref type="bibr" target="#b11">(Ling, Singh, and Weld 2015)</ref>, the authors use NER and FIGER types to ensure that an EL model follows the constraints given by types. We also use a type system and constrain our model's output, however our type system is task-specific and designed by a machine with a disambiguation accuracy objective, and unlike the authors we find that types improve accuracy. The work of (Krishnamurthy, Dasigi, and Gardner 2017) uses a type-aware grammar to constrain the decoding of a neural semantic parser. Our work makes use of type constraints during decoding, however the grammar and types in their system require human engineering to fit each individual semantic parsing task, while our type systems are based on online encyclopaedias and ontologies, with applications beyond EL.</p><p>Neural Entity Linking Current approaches to entity linking make extensive use of deep neural networks, distributed representations. In (Globerson et al. 2016) a neural network uses attention to focus on contextual entities to disambiguate. While our work does not make use of attention, RNNs allow context information to affect disambiguation decisions. In the work of (Yamada et al. 2016) and <ref type="bibr">(Yamada et al. 2017)</ref>, the authors adopt a distributed representation of context which either models words and entities, or documents and entities such that distances between vectors informs disambiguation. We also rely on word and document vectors produced by RNNs, however entities are not explicitly represented in our neural network, and we use context to predict entity types, thereby allowing us to incorporate new entities without retraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work we introduce DeepType, a method for integrating symbolic knowledge into the reasoning process of a neural network. We've proposed a mixed integer reformulation for jointly designing type systems and training a classifier for a target task, and empirically validated that when this technique is applied to EL it is effective at integrating symbolic information in the neural network reasoning process. When pre-training with DeepType for NER, we observe improved performance over baselines and a new state of the art on the OntoNotes dev set, suggesting there is cross-domain transfer: symbolic information is incorporated in the neural network's distributed representation. Furthermore we find that type systems designed by machines outperform those designed by humans on three benchmark datasets, which is attributable to incorporating learnability and target task performance goals within the design process. Our approach naturally enables multilingual training, and our experiments show that bilingual training improves over monolingual, and type systems optimized for English operate at similar accuracies in French, German, and Spanish, supporting the claim that the type system optimization leads to the discovery of high level cross-lingual concepts useful for knowledge representation. We compare different search techniques, and observe that stochastic optimization has comparable performance to heuristic search, but with orders of magnitude less objective function evaluations.</p><p>The main contributions of this work are a joint formulation for designing and integrating symbolic information into neural networks, that enable us to constrain the out-puts to obey symbolic structure, and an approach to EL that uses type constraints. Our approach reduces EL resolution complexity from O(N 2 ) to O(N ), while allowing new entities to be incorporated without retraining, and we find on three standard datasets (WikiDisamb30, CoNLL (YAGO), TAC KBP 2010) that our approach outperforms all existing solutions by a wide margin, including approaches that rely on a human-designed type system <ref type="bibr" target="#b11">(Ling, Singh, and Weld 2015)</ref> and the more recent work by Yamada et al. for embedding words and entities <ref type="bibr">(Yamada et al. 2016), or document and</ref><ref type="bibr">entities (Yamada et al. 2017)</ref>. As a result of our experiments, we observe that disambiguation accuracy using Oracles reaches 99.0% on CoNLL (YAGO) and 98.6% on TAC KBP 2010, suggesting that EL would be almost solved if we can close the gap between type classifiers and the Oracle.</p><p>The results presented in this work suggest many directions for future research: we may test how DeepType can be applied to other problems where incorporating symbolic structure is beneficial, whether making type system design more expressive by allowing hierarchies can help close the gap between model and Oracle accuracy, and seeing if additional gains can be obtained by relaxing the classifier's conditional independence assumption.</p><p>Acknowledgments We would like to thank the anonymous reviewers for their valuable feedback. In addition, we thank John Miller, Andrew Gibiansky, and Szymon Sidor for thoughtful comments and fruitful discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A Training details and hyperparameters Optimization</head><p>Our models are implemented in Tensorflow and optimized with Adam with a learning rate of 10 ?4 , ? 1 = 0.9, ? 2 = 0.999, = 10 ?8 , annealed by 0.99 every 10,000 iterations.</p><p>To reduce over-fitting and make our system more robust to spelling changes we apply Dropout to input embeddings and augment our data with noise: swap input words with a special &lt;UNK&gt; word, remove capitalization or a trailing "s." In our NER experiments we add Gaussian noise during training to the LSTM weights with ? = 10 ?6 .</p><p>We use early stopping in our NER experiments when validation F1 score stops increasing. Type classification model selection is different as the models did not overfit, thus we instead stop training when no more improvements in F1 are observed on held-out type-training data (? 3 days on one Titan X Pascal).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>Character representation Our character-convolutions have character filters with (width, channels): {(1, 50), (2, 75), (3, 75), (4, 100), <ref type="bibr">(5, 200), (6, 200), (7, 200)</ref>}, a maximum word length of 40, and 15-dimensional character embeddings followed by 2 highway layers. We learn 6-dimensional embeddings for 2 and 3 character prefixes and suffixes. Text Window Classifier The text window classifiers have 5-dimensional word embeddings, and use Dropout of 0.5. Empirically we find that two passes through the dataset with a batch size of 128 is sufficient for the window classifiers to converge. Additionally we train multiple type axes in a single batch, reaching a training speed of 2.5 type axes/second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Wikipedia Link Simplification</head><p>Link statistics collected on large corpuses of entity mentions are extensively used in entity linking. These statistics provide a noisy estimate of the conditional probability of an entity e for a mention m P(e|m). Intra-wiki links in Wikipedia provide a multilingual and broad coverage source of links, however annotators often create link anaphoras: "king" ? Charles I of England. This behavior increases polysemy ("king" mention has 974 associated entities) and distorts link frequencies ("queen" links to the band Queen 4920 times, Elizabeth II 1430 times, and monarch only 32 times). Problems with link sparsity or anaphora were previously identified, however present solutions rely on pruning rare links and thus lose track of the original statistics <ref type="bibr" target="#b4">(Ferragina and Scaiella 2010;</ref><ref type="bibr" target="#b5">Hasibi, Balog, and Bratsberg 2016;</ref><ref type="bibr" target="#b11">Ling, Singh, and Weld 2015)</ref>. We propose instead to detect anaphoras and recover the generic meaning through the Wikidata property graph: if a mention points to entities A and B, with A being more linked than B, and A is B's parent in the Wikidata property graph, then replace B with A. We define A to be the parent of B if they connect through a sequence of Wikidata properties {instance of, subclass of, is a list of}, or through a single edge in {occupation, position held, series 16 }. The simplification process is repeated until no more updates occur. This transformation reduces the number of associated entities for each mention ("king" senses drop from 974 to 143) and ensures that the semantics <ref type="bibr">15</ref> The choice of pstart affects the system size at the first step of the CEM search: setting it too low leads to poor search space exploration, while too high increase the cost of the objective function evaluation. Empirically we know that for a given ? the solution will have an expected size s. Setting pstart = s |R| leads to sufficient exploration to reach the performance of larger pstart.   After simplification we find that the mean number of senses attached to polysemous mentions drops from 4.73 to 3.93, while over 10,670,910 links undergo changes in this process <ref type="figure" target="#fig_4">(Figure 4)</ref>. <ref type="table" target="#tab_2">Table 3</ref> indicates that most changes result from mentions containing entities and their immediate parents. This simplification method strongly reduces the number of entities tied to each Wikipedia mention in an automatic fashion across multiple languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Multilingual Training Representation</head><p>Multilingual data creation is a side-effect of the ontologybased automatic labeling scheme. In <ref type="table" target="#tab_3">Table 4</ref> we present nearest-neighbor words for words in multiple languages. We note that common words (he, Argentinian, hypothesis) remain close to their foreign language counterpart, while proper nouns group with country/language-specific terms. We hypothesize that common words, by not fulfilling a role as a label, can therefore operate in a language independent way to inform the context of types, while proper nouns will have different type requirements based on their labels, and thus will not converge to the same representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Effect of System Size Penalty</head><p>We measure the effect of varying ? on type system discovery when using CEM for our search. The effect averaged on 10 trials for a variety of ? penalties is shown in <ref type="figure">Figure  6</ref>. In particular we notice that there is a crossover point in the performance characteristics when selecting ?, where a looser penalty has diminishing returns in accuracy around ? = 10 ?4 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Learnability Heuristic behavior</head><p>To better understand the behavior of the population of classifiers used to obtain AUC scores for the Learnability heuristic we investigate whether certain type axes are systematically easier or harder to predict, and summarize our results in <ref type="figure">Figure 7</ref>. We find that type axes with a instance of edge have on average higher AUC scores than type axes relying on wikipedia category. Furthermore, we also wanted to ensure that our methodology for estimating learnability was not flawed or if variance in our measurement was correlated with AUC for a type axis. We find that there is no obvious relation between the standard deviation of the AUC scores for a type axis and the AUC score itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Multilingual Part of Speech Tagging</head><p>Finally the usage of multilingual data allows some amount of subjective experiments. For instance in <ref type="figure">Figure 8</ref> we show some samples from the model trained jointly on english and french correctly detecting the meaning of the word "car" across three possible meanings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Human Type System</head><p>To assist humans with the design of the system, the rules are built interactively in a REPL, and execute over the 24 million entities in under 10 seconds, allowing for real time feedback in the form of statistics or error analysis over an evaluation corpus. On the evaluation corpus, disambiguation mistakes can be grouped according to the ground truth type, allowing a per type error analysis to easily detect areas where more granularity would help. Shown below are the 5 different type axes designed by humans.    <ref type="figure">Figure 6</ref>: Effect of varying ? on CEM type system discovery: Solution size (a) and iterations to convergence (b) grow exponentially with penalty decrease, while accuracy plateaus (c) around ? = 10 ?4 . Objective function increases as penalty decreases, since solution size is less penalized (d). Standard deviation is shown as the red region around the mean.       </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example model output: "jaguar" refers to different entities depending on context. Predicting the type associated with each word (e.g. animal, region, etc.) helps eliminate options that do not match, and recover the true entity. Bar charts give the system's belief over the type-axis "IsA", and the table shows how types affects entity probabilities given by Wikipedia links. jaguar cross in the jungle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Defining group membership with a knowledge graph relation: children of root (city) via edge (instance of).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The prey saw a jaguar cross in the jungle.The prey saw a jaguar cross in the jungle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>16 e.g. Return of the Jedi ? series Star Wars</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Mention Polysemy change after simplification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Effect of varying ? on CEM type system discovery</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Most instance of type-axes have higher AUC scores than wikipedia categories (a). The standard deviation for AUC scoring with text window classifiers is below 0.1 (b), AUC is not correlated with AUC's standard deviation. Model trained jointly on monolingual POS corpora detecting the multiple meanings of "car" (shown in bold) in a mixed English-French sentence.Cun, Y. 2017. Hierarchical loss for classification. arXiv preprint arXiv:1709.01062. [Yamada et al. 2016] Yamada, I.; Shindo, H.; Takeda, H.; and Takefuji, Y. 2016. Joint learning of the embedding of words and entities for named entity disambiguation. arXiv preprint arXiv:1601.01343. [Yamada et al. 2017] Yamada, I.; Shindo, H.; Takeda, H.; and Takefuji, Y. 2017. Learning distributed representations of texts and entities from knowledge base. arXiv preprint arXiv:1705.02494.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Method comparisons. Highest value in bold, excluding oracles. NER F1 score comparison for DeepType pretraining vs. baselines. Entity Linking model Comparison. Significant improvements over prior work denoted by * for p &lt; 0.05, and * * for p &lt; 0.01.</figDesc><table><row><cell cols="4">(a) Type system discovery method comparison Approach Evals Accuracy Items</cell><cell>(b) Model</cell><cell></cell><cell>CoNLL 2003 Dev Test</cell><cell>OntoNotes Dev Test</cell></row><row><cell cols="2">BeamSearch 5.12 ? 10 7 Greedy 6.40 ? 10 6</cell><cell>97.84 97.83</cell><cell>130 130</cell><cell cols="3">Bi-LSTM (Chiu and Nichols 2015)</cell><cell>-76.29</cell><cell>-77.77</cell></row><row><cell>GA CEM</cell><cell>116, 000 43, 000</cell><cell>96.959 96.26</cell><cell>128 89</cell><cell cols="3">Bi-LSTM-CNN + emb + lex (Chiu and Nichols 2015)</cell><cell>94.31 91.62 84.57 86.28</cell></row><row><cell>Random</cell><cell cols="2">N/A 92.9 ? 0.28</cell><cell>128</cell><cell cols="2">Bi-LSTM (Ours)</cell><cell>89.49 83.40 82.75 81.03</cell></row><row><cell>No types</cell><cell>0</cell><cell>92.10</cell><cell>0</cell><cell cols="3">Bi-LSTM-CNN (Ours)</cell><cell>90.54 84.74 83.17 82.35</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Bi-LSTM-CNN (Ours) + types 93.54 88.67 85.11 83.12</cell></row><row><cell>(c) Model</cell><cell></cell><cell cols="2">enwiki</cell><cell>frwiki</cell><cell>dewiki</cell><cell>eswiki WKD30 CoNLL</cell><cell>TAC 2010</cell></row><row><cell cols="3">M&amp;W(Milne and Witten 2008)</cell><cell></cell><cell></cell><cell></cell><cell>84.6</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">TagMe (Ferragina and Scaiella 2010) 83.224</cell><cell></cell><cell>80.711</cell><cell>90.9</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">(Globerson et al. 2016)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>91.7</cell><cell>87.2</cell></row><row><cell cols="2">(Yamada et al. 2016)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>91.5</cell><cell>85.2</cell></row><row><cell cols="2">NTEE (Yamada et al. 2017) LinkCount only</cell><cell cols="5">-89.064  *  *  92.013 92.013  *  *  89.980 82.710 manual 94.331  *  *  92.967 91.888  *  *  93.108  *  *  90.743  *  -87.7 68.614 81.485</cell></row><row><cell></cell><cell cols="5">manual (oracle) 97.734 greedy 93.725  *  *  92.984 98.026 98.632</cell><cell>98.178 95.872 92.375  *  *  94.151  *  *  90.850  *  98.217 98.601</cell></row><row><cell>Ours</cell><cell cols="5">greedy (oracle) 98.002 CEM 93.707  *  *  92.415 97.222 97.915 CEM (oracle) 97.500 96.648 97.480 GA 93.684  *  *  92.027</cell><cell>98.246 97.293 92.247  *  *  93.962  *  *  90.302  *  98.982 98.278 97.599 96.481 99.005 96.767 92.062  *  *  94.879  *  *  90.312  *</cell></row><row><cell></cell><cell cols="3">GA (oracle) 97.297 GA (English only) 93.029  *  *</cell><cell cols="2">96.783 97.408</cell><cell>97.609 96.268 91.743  *  *  93.701  *  *  -98.461 96.663</cell></row><row><cell cols="5">curacy significantly improve over LinkCount, and achieve</cell><cell cols="2">accuracy we query the public web API 14 available in Ger-</cell></row><row><cell cols="5">near perfect accuracy on all datasets (97-99%); furthermore</cell><cell cols="2">man and English, while other methods can be compared on</cell></row><row><cell cols="5">we notice that performance between the held-out Wikipedia</cell><cell cols="2">CoNLL(YAGO) and TAC KBP 2010. Models trained on a</cell></row><row><cell cols="5">sets and standard datasets sets is similar, supporting the</cell><cell cols="2">human type system outperform all previous approaches to</cell></row><row><cell cols="5">claim that the discovered type systems generalize well. We</cell><cell cols="2">entity linking, while type systems discovered by machines</cell></row><row><cell cols="5">note that machine discovered type systems outperform hu-</cell><cell cols="2">lead to even higher performance on all datasets except En-</cell></row><row><cell cols="5">man designed systems: CEM beats the human type sys-</cell><cell cols="2">glish Wikipedia.</cell></row><row><cell cols="5">tem on English Wikipedia, and all search method's type</cell><cell></cell></row><row><cell cols="5">systems outperform human systems on WIKI-DISAMB30,</cell><cell></cell></row><row><cell cols="3">CoNLL(YAGO), and TAC KBP 2010.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Search Methodology Learnability Impact To under-</cell><cell></cell></row><row><cell cols="5">stand whether the type systems produced by different search</cell><cell></cell></row><row><cell cols="5">methods can be trained similarly well we compare the type</cell><cell></cell></row><row><cell cols="5">system built by GA, CEM, greedy, and the one constructed</cell><cell></cell></row><row><cell cols="5">manually. EL Disambiguation accuracy is shown in Table</cell><cell></cell></row><row><cell cols="5">1c, where we compare with recent deep-learning based ap-</cell><cell></cell></row><row><cell cols="5">proaches (Globerson et al. 2016), or recent work by Ya-</cell><cell></cell></row><row><cell cols="5">mada et al. for embedding word and entities (Yamada et</cell><cell></cell></row><row><cell cols="5">al. 2016), or documents and entities (Yamada et al. 2017),</cell><cell></cell></row><row><cell cols="5">along with count and coherence based techniques Tagme</cell><cell></cell></row><row><cell cols="5">(Ferragina and Scaiella 2010) and Milne &amp; Witten (Milne</cell><cell></cell></row><row><cell cols="5">and Witten 2008). To obtain Tagme's Feb. 2017 Wikipedia</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Hyperparameters for type system discovery search.</figDesc><table><row><cell>Method</cell><cell>Parameter</cell><cell>Value</cell></row><row><cell>Greedy</cell><cell>b</cell><cell>1</cell></row><row><cell>Beam Search</cell><cell>b</cell><cell>8</cell></row><row><cell>CEM</cell><cell>M CEM p start N CEM</cell><cell>1000 |R| ? 0.001 15 50 200</cell></row><row><cell></cell><cell>G</cell><cell>200</cell></row><row><cell>GA</cell><cell>N population mutation probability</cell><cell>1000 0.5</cell></row><row><cell></cell><cell>crossover probability</cell><cell>0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Link change statistics per iteration during English Wikipedia Anaphora Simplification.</figDesc><table><row><cell cols="3">Step Replacements Links changed</cell></row><row><cell>1</cell><cell>1,109,408</cell><cell>9,212,321</cell></row><row><cell>2</cell><cell>13922</cell><cell>1,027,009</cell></row><row><cell>3</cell><cell>1229</cell><cell>364,500</cell></row><row><cell>4</cell><cell>153</cell><cell>40,488</cell></row><row><cell>5</cell><cell>74</cell><cell>25,094</cell></row><row><cell>6</cell><cell>4</cell><cell>1,498</cell></row><row><cell cols="3">of multiple specific links are aggregated (number of "queen"</cell></row><row><cell cols="3">links to monarch increase from 32 to 3553).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Top-k Nearest neighbors (cosine distance) in shared English-French word vector space. Argentine (0.315) him (0.398) Montebourg (0.419) Cheney (0.495) hypoth?se (0.497)</figDesc><table><row><cell>k Argentinian</cell><cell>lui</cell><cell>Sarkozy</cell><cell>Clinton</cell><cell>hypothesis</cell></row><row><cell>1 argentin (0.259)</cell><cell>he (0.333)</cell><cell>Bayron (0.395)</cell><cell cols="2">Reagan (0.413) paradox (0.388)</cell></row><row><cell cols="2">2 Argentina (0.313) il (0.360)</cell><cell>Peillon (0.409)</cell><cell>Trump (0.441)</cell><cell>Hypothesis (0.459)</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Additional set of Top-k Nearest neighbors (cosine distance) in shared English-French word vector space.</figDesc><table><row><cell>k feu</cell><cell>computer</cell></row><row><cell cols="2">1 killing (0.585) Computer (0.384)</cell></row><row><cell cols="2">2 terrible (0.601) computers (0.446)</cell></row><row><cell cols="2">3 beings (0.618) informatique (0.457)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Human Type Axis: IsA</figDesc><table><row><cell>Activity</cell></row><row><cell>Aircraft</cell></row><row><cell>Airport</cell></row><row><cell>Algorithm</cell></row><row><cell>Alphabet</cell></row><row><cell>Anatomical structure</cell></row><row><cell>Astronomical object</cell></row><row><cell>Audio visual work</cell></row><row><cell>Award</cell></row><row><cell>Award ceremony</cell></row><row><cell>Battle</cell></row><row><cell>Book magazine article</cell></row><row><cell>Brand</cell></row><row><cell>Bridge</cell></row><row><cell>Character</cell></row><row><cell>Chemical compound</cell></row><row><cell>Clothing</cell></row><row><cell>Color</cell></row><row><cell>Concept</cell></row><row><cell>Country</cell></row><row><cell>Crime</cell></row><row><cell>Currency</cell></row><row><cell>Data format</cell></row><row><cell>Date</cell></row><row><cell>Developmental biology period</cell></row><row><cell>Disease</cell></row><row><cell>Electromagnetic wave</cell></row><row><cell>Event</cell></row><row><cell>Facility</cell></row><row><cell>Family</cell></row><row><cell>Fictional character</cell></row><row><cell>Food</cell></row><row><cell>Gas</cell></row><row><cell>Gene</cell></row><row><cell>Genre</cell></row><row><cell>Geographical object</cell></row><row><cell>Geometric shape</cell></row><row><cell>Hazard</cell></row><row><cell>Human</cell></row><row><cell>Human female</cell></row><row><cell>Human male</cell></row><row><cell>International relations</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Human Type Axis: IsA (continued)</figDesc><table><row><cell>Kinship</cell></row><row><cell>Lake</cell></row><row><cell>Language</cell></row><row><cell>Law</cell></row><row><cell>Legal action</cell></row><row><cell>Legal case</cell></row><row><cell>Legislative term</cell></row><row><cell>Mathematical object</cell></row><row><cell>Mind</cell></row><row><cell>Molecule</cell></row><row><cell>Monument</cell></row><row><cell>Mountain</cell></row><row><cell>Musical work</cell></row><row><cell>Name</cell></row><row><cell>Natural phenomenon</cell></row><row><cell>Number</cell></row><row><cell>Organization</cell></row><row><cell>Other art work</cell></row><row><cell>People</cell></row><row><cell>Person role</cell></row><row><cell>Physical object</cell></row><row><cell>Physical quantity</cell></row><row><cell>Plant</cell></row><row><cell>Populated place</cell></row><row><cell>Position</cell></row><row><cell>Postal code</cell></row><row><cell>Radio program</cell></row><row><cell>Railroad</cell></row><row><cell>Record chart</cell></row><row><cell>Region</cell></row><row><cell>Religion</cell></row><row><cell>Research</cell></row><row><cell>River</cell></row><row><cell>Road vehicle</cell></row><row><cell>Sea</cell></row><row><cell>Sexual orientation</cell></row><row><cell>Software</cell></row><row><cell>Song</cell></row><row><cell>Speech</cell></row><row><cell>Sport</cell></row><row><cell>Sport event</cell></row><row><cell>Sports terminology</cell></row><row><cell>Strategy</cell></row><row><cell>Taxon</cell></row><row><cell>Taxonomic rank</cell></row><row><cell>Title</cell></row><row><cell>Train station</cell></row><row><cell>Union</cell></row><row><cell>Unit of mass</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Human Type Axis: IsA (continued)</figDesc><table><row><cell>Value</cell></row><row><cell>Vehicle</cell></row><row><cell>Vehicle brand</cell></row><row><cell>Volcano</cell></row><row><cell>War</cell></row><row><cell>Watercraft</cell></row><row><cell>Weapon</cell></row><row><cell>Website</cell></row><row><cell>Other</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Human Type Axis: Topic</figDesc><table><row><cell>Archaeology</cell></row><row><cell>Automotive industry</cell></row><row><cell>Aviation</cell></row><row><cell>Biology</cell></row><row><cell>Botany</cell></row><row><cell>Business other</cell></row><row><cell>Construction</cell></row><row><cell>Culture</cell></row><row><cell>Culture-comics</cell></row><row><cell>Culture-dance</cell></row><row><cell>Culture-movie</cell></row><row><cell>Culture-music</cell></row><row><cell>Culture-painting</cell></row><row><cell>Culture-photography</cell></row><row><cell>Culture-sculpture</cell></row><row><cell>Culture-theatre</cell></row><row><cell>Culture arts other</cell></row><row><cell>Culture ceramic art</cell></row><row><cell>Culture circus</cell></row><row><cell>Culture literature</cell></row><row><cell>Economics</cell></row><row><cell>Education</cell></row><row><cell>Electronics</cell></row><row><cell>Energy</cell></row><row><cell>Engineering</cell></row><row><cell>Environment</cell></row><row><cell>Family</cell></row><row><cell>Fashion</cell></row><row><cell>Finance</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 12 :</head><label>12</label><figDesc>Human Type Axis: Time</figDesc><table><row><cell>Post-1950</cell></row><row><cell>Pre-1950</cell></row><row><cell>Other</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 13 :</head><label>13</label><figDesc>Human Type Axis: Location</figDesc><table><row><cell>Africa</cell></row><row><cell>Antarctica</cell></row><row><cell>Asia</cell></row><row><cell>Europe</cell></row><row><cell>Middle East</cell></row><row><cell>North America</cell></row><row><cell>Oceania</cell></row><row><cell>Outer Space</cell></row><row><cell>Populated place unlocalized</cell></row><row><cell>South America</cell></row><row><cell>Other</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">There are ? 2 2.4?10 7 choices if each Wikipedia article can be a type within our type system.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Wikidata's ontology has cross-links with Wikipedia, IMDB, Discogs, MusicBrainz, and other encyclopaedias with snippets.8  We obtain type labels only on the intra-wiki link anchor text.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">We apply the preprocessing and link pruning as<ref type="bibr" target="#b4">(Ferragina and Scaiella 2010)</ref> to ensure the comparison is fair.13  Note that LinkCount accuracy is stronger than the one found in<ref type="bibr" target="#b4">(Ferragina and Scaiella 2010)</ref> or<ref type="bibr" target="#b12">(Milne and Witten 2008)</ref> because newer Wikipedia dumps improve link coverage and reduce link distribution noisiness.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">https://tagme.d4science.org/tagme/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Youtube-8m: A large-scale video classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>El-Haija</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<idno>arXiv:1603.06042</idno>
	</analytic>
	<monogr>
		<title level="m">Globally normalized transition-based neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Andor et al. 2016</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichols</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08308</idno>
		<title level="m">Named entity recognition with bidirectional lstm-cnns</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hedging your bets: Optimizing accuracyspecificity trade-offs in large scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3450" to="3457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tagme: on-the-fly annotation of short text fragments (by wikipedia entities)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Scaiella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ringaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM international conference on Information and knowledge management</title>
		<meeting>the 19th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="621" to="631" />
		</imprint>
	</monogr>
	<note>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the reproducibility of the tagme entity linking system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hasibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Bratsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<meeting><address><addrLine>Bratsberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="436" to="449" />
		</imprint>
	</monogr>
	<note>European Conference on Artificial Life</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust Disambiguation of Named Entities in Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="782" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Overview of the tac 2010 knowledge base population track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Text Analysis Conference</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="3" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural semantic parsing with type constraints for semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<idno>arXiv:1508.06615</idno>
	</analytic>
	<monogr>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lample</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Finegrained entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<editor>AAAI. Citeseer</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Design challenges for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Singh</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="315" to="328" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM conference on Information and knowledge management</title>
		<meeting>the 17th ACM conference on Information and knowledge management<address><addrLine>Witten</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="127" to="190" />
		</imprint>
	</monogr>
	<note>The crossentropy method for combinatorial and continuous optimization</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F T K</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>and Le</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
