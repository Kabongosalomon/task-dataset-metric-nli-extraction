<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deeply Interleaved Two-Stream Encoder for Referring Video Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Feng</surname></persName>
							<email>fengguang.gg@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
							<email>zhanglihe@dlut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deeply Interleaved Two-Stream Encoder for Referring Video Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Referring video segmentation aims to segment the corresponding video object described by the language expression. To address this task, we first design a twostream encoder to extract CNN-based visual features and transformer-based linguistic features hierarchically, and a vision-language mutual guidance (VLMG) module is inserted into the encoder multiple times to promote the hierarchical and progressive fusion of multi-modal features. Compared with the existing multi-modal fusion methods, this two-stream encoder takes into account the multigranularity linguistic context, and realizes the deep interleaving between modalities with the help of VLGM. In order to promote the temporal alignment between frames, we further propose a language-guided multi-scale dynamic filtering (LMDF) module to strengthen the temporal coherence, which uses the language-guided spatial-temporal features to generate a set of position-specific dynamic filters to more flexibly and effectively update the feature of current frame. Extensive experiments on four datasets verify the effectiveness of the proposed model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper aims to address a task called referring video segmentation. Given a video clip and a referring expression, the goal of this task is to segment the corresponding entity (object) in the video sequence according to the description of query language. Traditional semi-supervised video object segmentation uses the ground-truth mask of the first frame (or a few frames) as the guide to segment the corresponding foreground object(s) in the remaining video sequence. Similarly, referring video segmentation considers language expression as interactive information to guide the segmentation of video object. Compared with the groundtruth mask, the language labeling is more flexible and economy. This task can be applied to language-driven video ? Corresponding Author editing, human-robot interaction, etc.</p><p>Referring video segmentation contains two crucial issues. They are how to achieve the modality consistency between linguistic and visual information and how to leverage the temporal coherency between frames. For the first issue, the referring image/video segmentation methods that have emerged in recent years adopt straightforward concatenation-convolution <ref type="bibr" target="#b6">[7]</ref>, recurrent LSTM <ref type="bibr" target="#b14">[16]</ref>, convolution LSTM <ref type="bibr" target="#b12">[14]</ref>, dynamic filters <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b22">24]</ref>, cross-modal attention <ref type="bibr">[4, 5, 8-11, 22, 23, 25, 27]</ref> to aggregate linguistic and visual features. According to the position of multimodal fusion in the network, they can be divided into two categories: decoder fusion ( <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>) and encoder fusion ( <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>). The former uses linguistic feature to separately guide visual features of different levels in the decoder for cross-modal embedding. The latter realizes the progressive guidance of language to the multi-modal features in the encoder <ref type="bibr" target="#b4">[5]</ref>. However, during the process of linguistic feature extraction, both of them ignore the semantic hierarchy of language information similar to the multi-level property embodied by visual features, which may weaken the consistency of the multiple-granularity cross-modal alignment.</p><p>Regarding the second issue, most of the existing methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b23">25]</ref> directly employ a I3D encoder <ref type="bibr" target="#b0">[1]</ref> to extract visual features for each frame, which implicitly characterizes the temporal relevance among different frames. The visual features are then handled the same way as image referring segmentation methods do. These video based methods actually focus on the design of cross-modal fusion while neglecting the inter-frame relationship modeling. Moreover, they do not utilize the language to guidance inter-frame feature interaction. These flaws inevitably weaken the temporal coherence among the features of frames. In <ref type="bibr" target="#b9">[10]</ref>, the language-guided channel attention is designed to re-weight and combine the features of current frame and reference frame, in which their attention weights are query-specific and are shared across the whole video. This design of temporal modeling does not consider the inter-frame adaptability and the intra-frame inhomogeneity well. To address the above-mentioned problems, we introduce the vision-language interleaved encoder and the languageguided dynamic filtering mechanism. Firstly, from the perspective of vision-language fusion, we adopt CNN encoder and transformer encoder to extract visual and linguistic features, respectively. Meanwhile, a vision-language mutual guidance (VLMG) module is repeatedly embedded between the two encoding streams to gradually transfer hierarchical cross-modal information to each other. Thus, the cross-modal alignment can be achieved at multiple levels of granularity. Secondly, for the temporal modeling, we propose a language-guided multi-scale dynamic filtering (LMDF) module, which exploits the spatial-temporal features to learn a set of position-specific and frame-specific dynamic filters under the guidance of language, and then utilize these filters to update the multi-level features of current frame. This module can promote the network to steadily and accurately capture the visual clues referred by the language query along the video.</p><p>Our main contributions are as follows:</p><p>? We propose a vision-language interleaved two-stream encoder, which leverages the vision-language mutual guidance module to effectively realize the multi-level information interaction between vision and language encoding steams. The interleaved encoder can produce a compact multi-modal representation.</p><p>? We design a language-guided multi-scale dynamic filtering module, which utilizes the language-guided spatial-temporal context to generate region-aware dynamic filters to update the features of current frame, thereby ensuring the temporal coherence of multimodal feature learning.</p><p>? Extensive evaluations on four referring video segmentation datasets (A2D, J-HMDB, Ref-DAVIS2017 and Ref-Youtube-VOS) demonstrate that our method surpasses the previous state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Vision-Language Interaction</head><p>Early method <ref type="bibr" target="#b6">[7]</ref> utilizes concatenation and convolution to fuse linguistic and visual features. Subsequently, Liu et al. <ref type="bibr" target="#b14">[16]</ref> and Li et al. <ref type="bibr" target="#b12">[14]</ref> adopt recurrent LSTM and convolutional LSTM to gradually combine the concatenated multi-modal features in the decoder, respectively. These methods directly concatenate visual and linguistic features, which dose not explicitly consider the relationship between each pixel and each word.</p><p>To solve this problem, some methods exploit attention mechanism to model the cross-modal relationship. For example, Shi et al. <ref type="bibr" target="#b21">[23]</ref> use the visual features as a guidance to learn the keywords corresponding to each pixel, which can suppress the noise in the query expression. Ye et al. <ref type="bibr" target="#b25">[27]</ref> and Seo et al. <ref type="bibr" target="#b20">[22]</ref> employ non-local module to model the relationship between the pixel-word mixed features. Wang et al. <ref type="bibr" target="#b23">[25]</ref> propose an asymmetric cross-attention module, which utilizes two affinity matrices to update visual and linguistic features, respectively. Hu et al. <ref type="bibr" target="#b7">[8]</ref> build a bidirectional relationship decoder to realize the mutual guidance between language and vision. Hui et al.</p><p>[11] rely on the dependency parsing tree to re-weight the edges of the word graph, thereby suppressing the useless edges in the initial fully connected graph. Huang et al. <ref type="bibr" target="#b8">[9]</ref> first perceive all entities in the image according to the category and appearance information and then employ the relation words to model the relationships among all entities. Ding et al. <ref type="bibr" target="#b3">[4]</ref> design a transformer-based query generation module at the decoding end to understand the vision-language context. Some other methods employ the dynamic filters generated by the sentence to match the visual features, thereby strengthening the response of the language-related visual region. Gavrilyuk et al. <ref type="bibr" target="#b5">[6]</ref> directly apply a dynamic filter to weight all channels of feature map and sum them to yield the pixel-wise segmentation map. Similarly, Margffoy-Tuay et al. <ref type="bibr" target="#b16">[18]</ref> learn a set of dynamic filters for each word to re-weight the visual features. Wang et al. <ref type="bibr" target="#b22">[24]</ref> introduce deformable convolution <ref type="bibr" target="#b1">[2]</ref> into the process of dynamic filtering. In addition. McIntosh et al. <ref type="bibr" target="#b17">[19]</ref> design a capsule network to perform vision-language coding.</p><p>A common feature of all the above methods is that they fuse visual and linguistic features at the decoding end of the network. The interaction between visual feature of each scale and language feature is isolated. Recently, the encoder fusion strategy <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">20]</ref> is applied to referring segmentation task. It achieves the continuous guidance of language to multi-scale visual features. Regardless of the encoder fusion or the decoder fusion, their language encoding processes do not consider the boosting effect of visual information of multiple levels, and ignore the semantic hierarchy of linguistic information. Integrating the same language fea- tures with visual features of different levels may weaken the consistency of cross-modal matching. Different from them, we propose a vision-language interleaved two-stream encoder, which can extract hierarchical language information with the help of multi-level visual features and implement the bidirectional cross-modal interaction at different levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Temporal Feature Modeling</head><p>Modeling temporal information between frames is important for referring video segmentation. A straightforward idea is to use 3D convolution to capture temporal cues of the video. Therefore, some methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b23">25]</ref> directly employ the I3D network <ref type="bibr" target="#b0">[1]</ref> to encode video sequences. In the testing phase, the fixed convolution parameters are not easily generalized to all video clips, which limits the capability of temporal modeling. To solve this problem, Seo et al. <ref type="bibr" target="#b20">[22]</ref> design a non-local based memory attention to learn the inter-frame correlation. Hui et al. <ref type="bibr" target="#b9">[10]</ref> consider the crossmodal property of referring video segmentation task and utilize language feature to build channel attention, which is applied to the weighted fusion of frames. In this paper, we leverage the language-guided spatial-temporal information to learn a set of position-specific and frame-specific dynamic filters, which can fully exploit the inter-modality and inter-frame information interaction to progressively combine the cross-modal aligned features at multiple scales for current frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The overall structure of the proposed model is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Firstly, the video features from the CNN-based encoder and language features from the transformer-based encoder are deeply interweaved through the vision-language mutual guidance (VLMG) module. In the pipeline of network, after the initial interweaving, the features flowed into the trailing end of the encoder and the whole decoder are already the multi-modal mixed features. Next, we propose a language-guided multi-scale dynamic filtering (LMDF) module to fuse temporal information across frames. The multiple-scale spatial-temporal features output by LMDF and the visual features from the first two encoder blocks of the visual encoder are fed to the decoder for progressive fusion and upsampling, thereby generating the final segmentation result. Note that, for the reference frame (I t?1 ) and the current frame (I t ), their encoder parameters are shared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Vision-Language Interleaved Encoder</head><p>We design a deeply interleaved two-stream encoder to achieve the mutual guidance and embedding between visual and linguistic features. It can not only encode the hierarchical semantic context of the sentence, but also gradually realize the mixing of multi-modal information from the feature extractors at different levels.</p><p>Specifically, we take ResNeSt <ref type="bibr" target="#b28">[30]</ref> as the visual encoder. It contains five feature blocks, and their features are </p><formula xml:id="formula_0">defined as {V i } 5 i=1 . The query expression is denoted as R = {r t } T t=1 ,</formula><p>where t indicates the t-th word and T is the total number of words. We utilize the Bert-embedding <ref type="bibr" target="#b2">[3]</ref> to obtain the initial representation L 0 = {l 0 t } T t=1 of R. L 0 is fed into three cascaded transformer blocks to encode the linguistic context {L j } 3 j=1 . To establish the connection between the two encoders, we propose a vision-language mutual guidance (VLMG) module to interlacedly guide their feature extraction. For the convenience of description, we define the sizes of V i and L j as C ?H ?W and C ?T , respectively. H, W and C denote the height, width and channel number, respectively. The feature V i is first reshaped into a matrix representation with size C ? (HW ). The vision-to-language affinity matrix between V i and L j can be calculated as follows:</p><formula xml:id="formula_1">A vl = (W 1 v V i ) (W 1 l L j ), A vl = softmax(A vl ),<label>(1)</label></formula><p>where W 1 v , W 1 l ? R C1?C are the learnable parameters. A vl ? R (HW )?T describes the similarity between each pixel of V i and each word of L j . The softmax function is used to normalize the affinity matrix along the first dimension. We utilize A vl to project the visual feature V i to the language space as follows:</p><formula xml:id="formula_2">V i = V i A vl .<label>(2)</label></formula><p>Next, the features V i ? R C?T and L j ? R C?T are fed into the standard transformer structure to learn their coembedding:</p><formula xml:id="formula_3">L j = MSA(LN( V i + L j )) + ( V i + L j ), L j+1 = FFN(LN( L j )) + L j ,<label>(3)</label></formula><p>where MSA denotes the multi-head self-attention module. LN represents the LayerNorm layer. FFN is the feed forward network. We further use L j+1 ? R C?T and V i to learn the affinity matrix of language-to-vision:</p><formula xml:id="formula_4">A lv = (W 2 v V i ) (W 2 l L j+1 ), A lv = softmax(A lv ),<label>(4)</label></formula><p>where W 2 v , W 2 l are the learned parameters. Similarly, the column-wise normalized affinity matrix A lv ? R T ?(HW ) is utilized to map the mixed multi-modal feature L j+1 to the vision space:</p><formula xml:id="formula_5">L j+1 = L j+1 A lv .<label>(5)</label></formula><p>Finally, we adopt concatenation Cat(?, ?) and convolution Conv to incorporate the feature L j+1 ? R C?H?W into the visual encoder: <ref type="formula">(6)</ref> where the Norm represents the L2-normalize, and its purpose is to normalize the feature maps to the same order of magnitude to avoid preference bias. The updated multimodal feature V i is fed to the next convolution block of the visual encoder for further feature extraction. <ref type="figure" target="#fig_2">Fig. 3</ref> illustrates the detailed structure of VLMG. By embedding VLMG between the visual CNN encoder and the linguistic transformer encoder, the multi-modal interaction process runs through the entire network. And most of the previous global attention methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">25]</ref> need to calculate an affinity matrix with the size of HW ? HW , so increasing the size of the input image will lead to a sharp increase in computational cost. The size of the affinity matrix calculated in VLMG is much smaller than HW ? HW (e.g. (HW ) ? T in Eq. 1 and Eq. 4, T ? T in Eq. 3). This also makes our encoder more suitable for real applications.</p><formula xml:id="formula_6">V i = Norm(V i ) + Norm(Conv(Cat(V i , L j+1 ))),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Language-Guided Multi-Scale Dynamic Filtering</head><p>For referring video segmentation, the language expression contains important spatial-temporal information related to the video. Therefore, we can use it to guide the cross-frame feature fusion. Let V r ? R C?H?W and V c ? R C?H?W indicate the features of the reference frame and the current frame, respectively. They are generated by Eq. 6. To capture useful spatial-temporal information from the reference frame and the current frame through the guidance of linguistic features, we firstly calculate the affinity matrix A r ? R (HW )?T between V r and L 0 , as:</p><formula xml:id="formula_7">A r = softmax((W 3 v V r ) (W 3 l L 0 )).<label>(7)</label></formula><p>The column-wise normalized similarity matrix is used to map the reference feature V r to the language space, as:</p><formula xml:id="formula_8">V r = V r A r .<label>(8)</label></formula><p>We further adopt the transformer block to mix V r ? R C?T and L 0 ? R C?T and obtain the reference-frame modulated language feature L r ? R C?T :</p><formula xml:id="formula_9">L r = MSA(LN( V r + L 0 )) + ( V r + L 0 ), L r = MLP(LN( L r )) + L r .<label>(9)</label></formula><p>Then, we superpose the current frame V c to further compute a multi-frame modulated language feature L c ? R C?T as follows:</p><formula xml:id="formula_10">V c = V c (softmax((W 4 v V c ) (W 4 l L r ))), L c = MSA(LN( V c + L r )) + ( V c + L r ), L c = MLP(LN( L c )) + L c .<label>(10)</label></formula><p>Thus, under the guidance of language, the feature L c successively fuses the language related spatial-temporal information from the reference frame and the current frame.</p><p>In order to generate the position-specific dynamic filters to accurately calibrate the current-frame feature based on the inter-frame and inter-modality information, we in advance learn the position-adaptive guidance feature based on the feature L c :</p><formula xml:id="formula_11">? V c = L c (softmax(((W 5 v V c ) (W 5 l L c )) )),<label>(11)</label></formula><p>where</p><formula xml:id="formula_12">? V c ? R C?H?W . ? v c i,j ? R C and v c i,j ? R C rep- resent the vectors of position (i, j) in ? V c and V c , respec-</formula><p>tively. Then ? v c i,j is used to generate a set of dynamic kernels to filter the features of the neighborhood around (i, j):</p><formula xml:id="formula_13">v d = 1 k=?1 1 l=?1 (w k,l ? v c i,j ) v c i+k?d,j+l?d ,<label>(12)</label></formula><p>where w k,l ? R C?C is the learnable parameter. denotes the element-wise multiplication. Actually, Eq.12 can be understood as a 3?3 depth-wise convolution with dilated rate d. During the process of filtering, the multi-scale neighborhood is adopted. We then concatenate v c and multiple v d of different dilated rates and follow a convolution layer to combine them. Finally, the resulted cross-frame multimodal features from stage3?stage5 and the visual features from stage1?stage2 are feed into the decoder (feature pyramid network) to predict the final segmentation result.</p><p>Comparison with Other Temporal Models. Our LMDF is different from other temporal modeling methods, e.g. CMDy <ref type="bibr" target="#b22">[24]</ref> and LGFS <ref type="bibr" target="#b9">[10]</ref>. 1) They exploit the global guidance features of language after max or average pooling to generate the filters. The pooling operation causes all spatial positions of visual feature to share the same language guidance. While the LMDF combines the intermodality and inter-frame information to yield the positionadaptive language guidance for dynamic filtering, which is more flexible and adaptive to appearance changes. 2) The LMDF learns the frame-specific filters, and implements the cross-modal and cross-frame interaction twice. The preinteraction process before dynamic kernel generation sequentially mixes the language features with the multi-modal features of reference frame and current frame, which can effectively obtain spatial-temporal guidance information.</p><p>Next, the re-interaction process of dynamic filtering further guarantees the temporal coherence of features for segmentation prediction. While LGFS <ref type="bibr" target="#b9">[10]</ref> uses the global language features to generate query-specific filters, which are shared along the whole video. And, it achieves the cross-modal and cross-frame interaction once. The CMDy <ref type="bibr" target="#b22">[24]</ref> uses 3D convolution to obtain the multi-frame mixed feature for each frame. The mixed temporal information without language guidance may confuse spatial features of the current frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>In order to illustrate the effectiveness of the proposed method, we conduct extensive experiments on four referring video segmentation datasets, which are A2D Sentences <ref type="bibr" target="#b5">[6]</ref>, J-HMDB Sentences <ref type="bibr" target="#b5">[6]</ref>, Refer-DAVIS2017 <ref type="bibr" target="#b11">[13]</ref>, and Refer-Youtube-VOS <ref type="bibr" target="#b20">[22]</ref>. Gavrilyuk et al. <ref type="bibr" target="#b5">[6]</ref> extended the original Actor-Action Dataset (A2D) <ref type="bibr" target="#b24">[26]</ref> and J-HMDB <ref type="bibr" target="#b10">[12]</ref> datasets to the referring video segmentation task by adding additional natural language descriptions. A2D Sentences is composed of 3,782 videos with 6,655 referring expressions. And this videos contain 8 different actions that are preformed by 7 actor classes. Following the previous setting <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">25]</ref>, we split A2D into two parts, 3,017 of which are used for training and 737 for testing. Each video contains 3 to 5 frame pixel-level segmentation masks. For J-HMDB Sentences, it consists of 928 videos with 21 different action classes. Each video contains a corresponding language expression. And the main purpose of this dataset is to evaluate the generalization ability of the model. Generally, previous methods directly use the model trained on the A2D dataset to test all the videos in this dataset. Similarly, Refer-DAVIS2017 is based on DAVIS2017 <ref type="bibr" target="#b19">[21]</ref>. It contains 90 videos, 60 for training and 30 for testing. Refer-Youtube-VOS is by far the largest dataset, it contains a total of 3,978 videos and about 15,000 language expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Experiment setting. We exploit the pytorch platform to implement our network, and adopt a Nvidia RTX 3090 GPU to train and test it. During training, we use the SGD optimizer to optimize the whole network. And we set the initial learning rate, momentum and weight decay to 1e-3, 0.9 and 5e-4, respectively. The batch size is set to 16 and the in- put frame is resized to 320?320. The maximum length of the referring expression is limited to 20. The dilated rates of LMDS are set d = 1, 3, 5. For Refer-DAVIS2017 and Refer-Youtube-VOS, we use the referring image segmentation dataset RefCOCO <ref type="bibr" target="#b27">[29]</ref> to assist their training. And random affine transformation is used to for data augmentation. The maximum number of iterations for A2D Sentences, Refer-DAVIS2017, and Refer-Youtube-VOS are set to 75,000, 150,000, and 300,000, respectively. And after 50,000, 80,000, and 200,000 iterations, the learning rate is reduced by 5 times. J-HMDB Sentences is only used to verify the generalization ability of the model. Evaluation metrics. Following previous works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">22]</ref>, we take Prec@X, Overall IoU, mAP, Mean IoU (mean region similarity (J)), mean contour accuracy (F), and the average of and J and F (J &amp;F) to evaluate our method. Where X ? {0.5, 0.6, 0.7, 0.8, 0.9}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-arts</head><p>We compare the performance of our method and the previous state-of-the-art methods on four different datasets. First of all, for the A2D Sentences dataset, we can find that the proposed method is significantly better than the previous methods. This also reflects the effectiveness of the two-stream encoder and language-guided dynamic filtering. For all the evaluation metrics in Tab. 1, our model achieves an average absolute gain of more than 5.5%. Especially on Prec@0.6, Prec@0.7 and Prec@0.8, our method obtains absolute enhancement of 7.1%, 7.9% and 8.6% compared with the second-best method. For the metric Prec@0.9, our method improves it from 9.8% to 15.1%, which is 1.5 times the previous performance. This also proves that our network can more accurately perceive the boundary of the object. Following the previous methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">17]</ref>, we further use the J-HMDB Sentences dataset to verify the generalization  <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b20">22]</ref> usually take advantage of RefCOCO <ref type="bibr" target="#b27">[29]</ref> for data augmentation. Our model achieves the gain of 6.5% and 5.3% in terms of J and F. Finally, Tab. 4 illustrates the accuracy on the large-scale dataset Refer-Youtube-VOS. Our model surpasses other methods in all evaluation metrics. We also present some visual cases in <ref type="figure" target="#fig_3">Fig. 4</ref>. It can be seen that our method can produce accurate segmentation masks, even when the language expression does not contain location information or the length of query is various.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We carefully evaluate the proposed components on the Refer-DAVIS2017 dataset. The quantitative results are shown in Tab. 5 and Tab. 6. Vision-Language Interleaved Encoder. We first remove the vision-language mutual guidance module (VLMG) and language-guided multi-scale dynamic filtering module (LMDF) from the overall network to construct the basic encoder fusion network (EFN). In EFN, the initial vector representation (L 0 ) of the language is directly inserted into the visual encoder three times by tile and convolution to achieve the fusion of language and vision (as used in <ref type="bibr" target="#b4">[5]</ref>). In Tab 5, EFN * indicates that the transformer structure is used to extract the multi-granularity language context of L 0 . By comparing EFN * and EFN, we can find that the hierarchical language information in EFN * can be better integrated with the multi-level visual information, thereby improving the performance of the network. Next, we introduce VLMG to realize the mutual interweaving of vision and language (marked as Dual in Tab. 5). The experimental results show that the dual interleaved encoding strategy can further bring 3.5%, 3.0%, and 3.3% gains in terms of J , F, and J &amp;F. Language-Guided Multi-Scale Dynamic Filtering. By comparing the third and fourth rows in Tab 5, we can find that the LMDF achieves the performance improvement by 2.5%, 2.1%, and 2.3% in terms of J , F, and J &amp;F, respectively. This indicates that this module can accurately capture the temporal coherence between frames. Different Settings of LMDF. We evaluate the design options about LMDF, and the results are shown in Tab. 6. By comparing the second (full LMDF) and third columns, it can be seen that learn multi-scale semantic context can produce more accurate segmentation mask. The LMDF uses Eq. 11 to compute the position-adaptive guidance feature for dynamic filtering. While CMDy <ref type="bibr" target="#b22">[24]</ref> and LGFS <ref type="bibr" target="#b9">[10]</ref> use max or average pooling to obtain the global guidance feature. Because their source codes are not available, we adjust the setting of LMDF to imitate the global language guidance for experimental comparison. Specifically, we firstly use max pooling to deal with the multimodal spatial-temporal feature L c (generated by Eq. 10). And then, the pooled vector is used to weight the feature of current frame V c by element-wise multiplication. Thus, the global guidance information for each position is obtained. Lastly, we generate dynamic kernels and filter the current frame by Eq. 12. The results (marked as MaxPool) show that the pooling operation indeed weakens the local perception required by the segmentation task. In addition, LMDF interacts linguistic features with visual features in advance (Eq. 7?Eq. 10), which makes the dynamic filters better align spatial-temporal grouping cues. The comparison between the second column and w/o pre-interaction also confirms this statement. Unlike LGFS <ref type="bibr" target="#b9">[10]</ref>, which shares weight parameters across all spatial positions, LMDF learns the position-adaptive dynamic kernels. For comparison, we utilize L c to generate the position-independent dynamic kernels. Specifically, we conduct max pooling on L c to obtain the global guidance vector, based on which the dynamic filters are generated the same way as in <ref type="bibr" target="#b9">[10]</ref>.</p><p>The results of the rightmost column in Tab. 6 show that the position-independent strategy leads to performance degradation. Qualitative cases. <ref type="figure">Fig. 5</ref> gives some representative examples to illustrate the benefits of our proposed module. We can find that the dual interleaved encoder can help the network accurately locate the referred object region, and the LMDF can utilize the temporal coherence to optimize the local details of the object. Thus, our model obtains a prediction mask that is closer to the ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a vision-language interleaved dual encoder and a language-guided multi-scale dynamic filtering mechanism to address the referring video segmentation. Specifically, our method utilizes a language encoder, a visual encoder and a vision-language mutual guidance (VLMG) module to complete the progressive interweaving of different levels of multi-modal features between the two encoders. In addition, in order to fully depict the temporal coherence in the video, we further propose a language-guided multi-scale dynamic filtering (LMDF) module, which can learn adaptive spatial-temporal information with the guidance of language to promote the fea- Query: "a black colored" Query: "a big man on the right in a black jacket" ture update of current frame. Extensive experiments on four referring video segmentation datasets demonstrate that the proposed model significantly outperforms the state-of-theart methods. The ablation study also verifies the effectiveness of the proposed modules. Query: "a black swan"</p><p>Query: "a girl on the left holding two cell phones"</p><p>Query: "a blonde haired girl dancing in a blue dress"</p><p>Image EFN EFN * Dual Dual+LMDF GT <ref type="figure">Figure 5</ref>. Visual examples of the proposed modules.</p><p>[11] Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang, and Jizhong Han. Linguistic structure guided context modeling for referring image segmentation.</p><p>In European Conference on Computer Vision, pages 59-75. Springer, 2020. 1, 2</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Three multi-modal fusion mechanisms. (a) Decoder fusion. (b) Unidirectional encoder fusion. (c) Bidirectional encoder fusion, which realizes the hierarchical cross-modal interaction between vision and language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The overall architecture of our model. It mainly consists of the transformer-based linguistic encoder, CNN-based visual encoder, vision-language mutual guidance (VLMG) module, language-guided multi-scale dynamic filtering (LMDF) module. It denotes the current frame. It?1 denotes the reference frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Vision-language mutual guidance (VLMG) module. Vto-L: Vision-to-language mapping. L-to-V: Language-to-vision mapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visual examples of referring video segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluation on A2D Sentences. -denotes no data available. * denotes utilizing additional optical flow input.</figDesc><table><row><cell>Method</cell><cell cols="6">Precision prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0.9 0.5:0.95 mAP</cell><cell>Overall</cell><cell>IoU</cell><cell>Mean</cell></row><row><cell>Hu et al.16 [7]</cell><cell>34.8</cell><cell>23.6</cell><cell>13.3</cell><cell>3.3</cell><cell>0.1</cell><cell>13.2</cell><cell>47.4</cell><cell></cell><cell>35.0</cell></row><row><cell>Li et al.17 [15]</cell><cell>38.7</cell><cell>29.0</cell><cell>17.5</cell><cell>6.6</cell><cell>0.1</cell><cell>16.3</cell><cell>51.5</cell><cell></cell><cell>35.4</cell></row><row><cell>Gavrilyuk et al.18 [6]</cell><cell>47.5</cell><cell>34.7</cell><cell>21.1</cell><cell>8.0</cell><cell>0.2</cell><cell>19.8</cell><cell>53.6</cell><cell></cell><cell>42.1</cell></row><row><cell>Gavrilyuk et al.18 [6]  *</cell><cell>50.0</cell><cell>37.6</cell><cell>23.1</cell><cell>9.4</cell><cell>0.4</cell><cell>21.5</cell><cell>55.1</cell><cell></cell><cell>42.6</cell></row><row><cell>ACGA19 [25]</cell><cell>55.7</cell><cell>45.9</cell><cell>31.9</cell><cell>16.0</cell><cell>2.0</cell><cell>27.4</cell><cell>60.1</cell><cell></cell><cell>49.0</cell></row><row><cell>VT-Capsule20 [19]</cell><cell>52.6</cell><cell>45.0</cell><cell>34.5</cell><cell>20.7</cell><cell>3.6</cell><cell>30.3</cell><cell>56.8</cell><cell></cell><cell>46.0</cell></row><row><cell>CMDy20 [24]</cell><cell>60.7</cell><cell>52.5</cell><cell>40.5</cell><cell>23.5</cell><cell>4.5</cell><cell>33.3</cell><cell>62.3</cell><cell></cell><cell>53.1</cell></row><row><cell>PRPE20 [20]</cell><cell>63.4</cell><cell>57.9</cell><cell>48.3</cell><cell>32.2</cell><cell>8.3</cell><cell>38.8</cell><cell>66.1</cell><cell></cell><cell>52.9</cell></row><row><cell>CMSA-V21 [28]</cell><cell>48.7</cell><cell>43.1</cell><cell>35.8</cell><cell>23.1</cell><cell>5.2</cell><cell>-</cell><cell>61.8</cell><cell></cell><cell>43.2</cell></row><row><cell>LGFS21 [10]</cell><cell>65.4</cell><cell>58.9</cell><cell>49.7</cell><cell>33.3</cell><cell>9.1</cell><cell>39.9</cell><cell>66.2</cell><cell></cell><cell>56.1</cell></row><row><cell>CMPC-V21 [17]</cell><cell>65.5</cell><cell>59.2</cell><cell>50.6</cell><cell>34.2</cell><cell>9.8</cell><cell>40.4</cell><cell>65.3</cell><cell></cell><cell>57.3</cell></row><row><cell>Ours</cell><cell>70.2</cell><cell>66.3</cell><cell>58.5</cell><cell>42.8</cell><cell>15.1</cell><cell>46.9</cell><cell>71.4</cell><cell></cell><cell>59.8</cell></row><row><cell cols="8">Table 2. Quantitative evaluation on J-HMDB Sentences. -denotes no data available.</cell><cell></cell></row><row><cell>Method</cell><cell cols="6">Precision prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0.9 0.5:0.95 mAP</cell><cell>Overall</cell><cell>IoU</cell><cell>Mean</cell></row><row><cell>Hu et al.16 [7]</cell><cell>63.3</cell><cell>35.0</cell><cell>8.5</cell><cell>0.2</cell><cell>0.0</cell><cell>17.8</cell><cell>54.6</cell><cell></cell><cell>52.8</cell></row><row><cell>Li et al.17 [15]</cell><cell>57.8</cell><cell>33.5</cell><cell>10.3</cell><cell>0.6</cell><cell>0.0</cell><cell>17.3</cell><cell>52.9</cell><cell></cell><cell>49.1</cell></row><row><cell>Gavrilyuk et al.18 [6]</cell><cell>69.9</cell><cell>46.0</cell><cell>17.3</cell><cell>1.4</cell><cell>0.0</cell><cell>23.3</cell><cell>54.1</cell><cell></cell><cell>54.2</cell></row><row><cell>ACGA19 [25]</cell><cell>75.6</cell><cell>56.4</cell><cell>28.7</cell><cell>3.4</cell><cell>0.0</cell><cell>28.9</cell><cell>57.6</cell><cell></cell><cell>58.4</cell></row><row><cell>VT-Capsule20 [19]</cell><cell>67.7</cell><cell>51.3</cell><cell>28.3</cell><cell>5.1</cell><cell>0.0</cell><cell>26.1</cell><cell>53.5</cell><cell></cell><cell>55.0</cell></row><row><cell>CMDy20 [24]</cell><cell>74.2</cell><cell>58.7</cell><cell>31.6</cell><cell>4.7</cell><cell>0.0</cell><cell>30.1</cell><cell>55.4</cell><cell></cell><cell>57.6</cell></row><row><cell>PRPE20 [20]</cell><cell>69.1</cell><cell>57.2</cell><cell>31.9</cell><cell>6.0</cell><cell>0.1</cell><cell>29.4</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>CMSA-V21 [28]</cell><cell>76.4</cell><cell>62.5</cell><cell>38.9</cell><cell>9.0</cell><cell>0.1</cell><cell>-</cell><cell>62.8</cell><cell></cell><cell>58.1</cell></row><row><cell>LGFS21 [10]</cell><cell>78.3</cell><cell>63.9</cell><cell>37.8</cell><cell>7.6</cell><cell>0.0</cell><cell>33.5</cell><cell>59.8</cell><cell></cell><cell>60.4</cell></row><row><cell>CMPC-V21 [17]</cell><cell>81.3</cell><cell>65.7</cell><cell>37.1</cell><cell>7.0</cell><cell>0.0</cell><cell>34.2</cell><cell>61.6</cell><cell></cell><cell>61.7</cell></row><row><cell>Ours</cell><cell>87.4</cell><cell>79.1</cell><cell>58.6</cell><cell>18.2</cell><cell>0.3</cell><cell>44.1</cell><cell>68.0</cell><cell></cell><cell>66.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Quantitative evaluation on Refer-DAVIS2017 val.</figDesc><table><row><cell>Mtehod</cell><cell>J</cell><cell>F</cell><cell>J &amp;F</cell></row><row><cell>Khoreva et al.18 [13]</cell><cell>37.3</cell><cell>41.3</cell><cell>39.3</cell></row><row><cell>URVOS20 [22]</cell><cell>41.23</cell><cell>47.01</cell><cell>44.12</cell></row><row><cell>Ours</cell><cell>47.71</cell><cell>52.33</cell><cell>50.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Quantitative evaluation on Refer-Youtube-VOS val.</figDesc><table><row><cell>Mtehod</cell><cell>J</cell><cell>F</cell><cell>J &amp;F</cell></row><row><cell>URVOS20 [22]</cell><cell>45.27</cell><cell>49.19</cell><cell>47.23</cell></row><row><cell>CMPC-V21 [17]</cell><cell>45.64</cell><cell>49.32</cell><cell>47.48</cell></row><row><cell>Ours</cell><cell>48.44</cell><cell>50.67</cell><cell>49.56</cell></row><row><cell cols="4">ability of the model trained on the A2D Sentences dataset.</cell></row><row><cell cols="4">As shown in Tab. 2, we can find that the proposed model</cell></row><row><cell cols="4">achieves the best performance under all evaluation metrics.</cell></row><row><cell cols="4">Particularly, our network shows a very significant improve-</cell></row><row><cell cols="4">ment of 19.7% on Prec@0.7. For the Refer-DAVIS2017,</cell></row><row><cell cols="4">it only contains 60 training videos, so the previous meth-</cell></row><row><cell>ods</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on the Refer-DAVIS2017 val.EFN EFN * Dual LMDF prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0.9</figDesc><table><row><cell>J</cell><cell>F</cell><cell>J &amp;F</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Comparison of different settings of LMDF on the Refer-DAVIS2017 val dataset. d denotes the dilated rate.</figDesc><table><row><cell>Metrics</cell><cell>d = 1, 3, 5</cell><cell>d = 1</cell><cell>MaxPool</cell><cell>w/o pre-interaction</cell><cell>w/ weight-sharing</cell></row><row><cell>J</cell><cell>47.71</cell><cell>46.62</cell><cell>46.41</cell><cell>45.73</cell><cell>46.50</cell></row><row><cell>F</cell><cell>52.33</cell><cell>51.26</cell><cell>51.68</cell><cell>50.23</cell><cell>51.05</cell></row><row><cell>J &amp;F</cell><cell>50.02</cell><cell>48.94</cell><cell>49.05</cell><cell>47.98</cell><cell>48.78</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vision-language transformer and query generation for referring segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="16321" to="16330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder fusion network with co-attention embedding for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="15506" to="15515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Actor and action video segmentation from a sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5958" to="5966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="108" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bi-directional relationship inferring network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="4424" to="4433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Referring image segmentation via cross-modal progressive comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10488" to="10497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Collaborative spatial-temporal modeling for language-queried video actor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video object segmentation with language referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Referring image segmentation via recurrent refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaican</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chun</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5745" to="5753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tracking by natural language specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6495" to="6503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent multimodal interaction for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1271" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cross-modal progressive comprehension for referring segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic multimodal instance segmentation guided by natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Margffoy-Tuay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Botero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arbel?ez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual-textual capsule routing for text-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Mcintosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9942" to="9951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Polar relative positional encoding for video-language segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Urvos: Unified referring video object segmentation network with a large-scale benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonguk</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Key-word-aware network for referring expression image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengcan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanman</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingbo</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="38" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Context modulated dynamic networks for actor and action video segmentation with language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Asymmetric cross-guided attention network for actor and action video segmentation from natural language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3939" to="3948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Can humans fly? action understanding with multiple classes of actors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Hang</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2264" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-modal self-attention network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10502" to="10511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Referring segmentation in images and videos with cross-modal self-attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
