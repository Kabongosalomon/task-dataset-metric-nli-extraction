<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AEI: Actors-Environment Interaction with Adaptive Attention for Temporal Action Proposals Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Vo</surname></persName>
							<email>khoavoho@uark.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AICV Lab</orgName>
								<orgName type="institution">University of Arkansas Fayetteville</orgName>
								<address>
									<region>AR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyekang</forename><surname>Joo</surname></persName>
							<email>hkjoo@cs.umd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashu</forename><surname>Yamazaki</surname></persName>
							<email>kyamazak@uark.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AICV Lab</orgName>
								<orgName type="institution">University of Arkansas Fayetteville</orgName>
								<address>
									<region>AR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><surname>Truong</surname></persName>
							<email>sangt@uark.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AICV Lab</orgName>
								<orgName type="institution">University of Arkansas Fayetteville</orgName>
								<address>
									<region>AR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
							<email>kkitani@cs.cmu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Science VNU-HCM</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">National University</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngan</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AICV Lab</orgName>
								<orgName type="institution">University of Arkansas Fayetteville</orgName>
								<address>
									<region>AR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ho</forename><surname>Chi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>City</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vietnam</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ho</forename><surname>Chi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>City</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vietnam</forename></persName>
						</author>
						<title level="a" type="main">AEI: Actors-Environment Interaction with Adaptive Attention for Temporal Action Proposals Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>KHOA VO ET AL.: AEI WITH ADAPTIVE ATTENTION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Humans typically perceive the establishment of an action in a video through the interaction between an actor and the surrounding environment. An action only starts when the main actor in the video begins to interact with the environment, while it ends when the main actor stops the interaction. Despite the great progress in temporal action proposal generation, most existing works ignore the aforementioned fact and leave their model learning to propose actions as a black-box. In this paper, we make an attempt to simulate that ability of a human by proposing Actor Environment Interaction (AEI) network to improve the video representation for temporal action proposals generation. AEI contains two modules, i.e., perception-based visual representation (PVR) and boundary-matching module (BMM). PVR represents each video snippet by taking human-human relations and humans-environment relations into consideration using the proposed adaptive attention mechanism. Then, the video representation is taken by BMM to generate action proposals. AEI is comprehensively evaluated in ActivityNet-1.3 and THUMOS-14 datasets, on temporal action proposal and detection tasks, with two boundary-matching architectures (i.e., CNN-based and GCN-based) and two classifiers (i.e., Unet and P-GCN). Our AEI robustly outperforms the state-of-the-art methods with remarkable performance and generalization for both temporal action proposal generation and temporal action detection. Source code is available at 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Temporal action proposals generation (TAPG) is one of the most important problems in video analysis and video understanding <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36]</ref>. Particularly, TAPG is the fundamental step for other downstream tasks, including temporal action detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>, action recognition <ref type="bibr" target="#b20">[21]</ref>, and video dense captioning <ref type="bibr" target="#b21">[22]</ref>. Given an untrimmed video, TAPG aims to propose temporal segments with specific starting and ending timestamps for each action of interest appearing in the video.</p><p>Recently, state-of-the-art (SOTA) methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref> follow a paradigm where a set of possible starting and ending timestamps of all actions are detected separately, then a proposal evaluation module is employed to evaluate every possible pair of starting and ending timestamps by predicting its confidence score. A non-maximum suppression (NMS) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31]</ref> function is finally used to eliminate redundant candidate proposals based on their confidence scores and overlapping metrics.</p><p>As we observe, a human has a capacity to perceive an action being established in a video <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref> in two steps. First, the main actors at each temporal period are identified; then, the interactions between main actors and the environment are observed to specify when the action starts and ends. Despite good achievements on benchmarking datasets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>, the SOTA approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref> disregard the above perception process of humans by only applying a backbone network (pre-trained on action recognition task) to extract the video representation, leading to a potential loss of some proposals. For instance, in <ref type="figure">Fig. 1</ref>, the works in the literature take the whole spatial region of video frames (e.g., red boxes) to propose action intervals; this, however, may lead to inaccurate results because the background occupies much bigger region than the actor who performs the action (e.g., blue boxes). In <ref type="figure">Fig. 1</ref>, the "rope skipping" action can trick an action proposal model into missing the time at which this action starts or ends due to a subtle difference in shape between between "jumping" and "standing".</p><p>In this paper, we propose a novel Actor Environment Interaction network (AEI) in an attempt to simulate and explore the capability of human-perception process. Our AEI consists of a visual representation module (PVR) and a boundary-matching module (BMM). The PVR is comprised of three components: (i) environment spectator; (ii) actors spectator; and (iii) actors-environment interaction spectator. As illustrated in <ref type="figure">Fig. 1</ref>, the environment spectator processes the entire spatial dimensions of a snippet (red boxes) to capture the global environmental information. The actors spectator focuses on actors (blue and light gray boxes around humans) to capture local appearance and motion information of actors. Additionally, we introduce a novel adaptive attention mechanism (AAM) in the actors spectator to select the main actors (blue boxes) who mainly commit the action. Given a video snippet, the features corresponding to the global environment and the local main actors are first extracted by the first two spectators. The relationship between the environment and the main actors is then modeled by the third component (i.e., the actors-environment interaction spectator).</p><p>Our contributions are summarized as follows: ? We propose a video representation network, AEI, which follows the human-perception process to understand human action. ? We introduce a novel adaptive attention mechanism (AAM) that simultaneously selects main actors and eliminates inessential actor(s) and then extracts semantic relations between main actors. ? We investigate the effectiveness of the proposed AEI by implementing the BMM under two network architectures: CNN-based and GCN-based. ? Our proposed AEI network achieves the SOTA performance on common benchmarking datasets of ActivityNet-1.3 and THUMOS-14 in both TAPG and TAD tracks with a large margin compared to the previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Temporal Action Proposal Generation (TAPG) TAPG aims to propose intervals that tightly contain actions in a long untrimmed video. Previous works can be divided into two main groups: anchor-based and boundary-based. Anchor-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>, which are inspired by anchor-based object detection methods in 2D images <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, predefine a set of fixed segments and try to fit them into ground-truth action segments in the video. Although a regression network may be applied in some of those methods to refine the proposals, a finite number of anchors cannot fit all ground-truth actions with diverse lengths. Boundary-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45]</ref> address this problem by localizing the starting and ending timestamps of all actions appearing in the video and matching them by a boundary-matching module. Our boundary-matching module belongs to the second group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Networks</head><p>Attention Networks (AN) have a long history in the artificial neural networks literature <ref type="bibr" target="#b17">[18]</ref>. We can divide AN into two main groups: Soft-Attention Networks (Soft-AN) and Hard-Attention Networks (Hard-AN). <ref type="bibr" target="#b0">[1]</ref> was one of the first Soft-AN that was applied to machine translation. Because of its differentiable architecture, which helps the whole model learn in an end-to-end fashion, Soft-AN has become an essential component in a large number of applications (e.g., speech <ref type="bibr" target="#b6">[7]</ref>, NLP <ref type="bibr" target="#b9">[10]</ref>, computer vision <ref type="bibr" target="#b5">[6]</ref>). Hard-AN was first introduced in <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b7">[8]</ref> for digit and object classifications, respectively. Hard-AN aims to mask out irrelevant elements of the inputs to reduce the distractions. This is an advanced benefit over Soft-AN; however, Hard-AN in <ref type="bibr" target="#b43">[44]</ref> is indifferentiable. Recently, <ref type="bibr" target="#b31">[32]</ref> proposes a Hard-AN that can be trained by normal gradient back-propagation, with a fundamental observation that the L2-norm values of more important features are usually higher than those of less important features in a feature map. In this work, we propose AAM, a module to leverage both the differentiable Hard-AN <ref type="bibr" target="#b31">[32]</ref> and the self-attention network <ref type="bibr" target="#b38">[39]</ref> to select the main actors of the video and to learn the relations between main actors, respectively. 3 Our Proposed AEI</p><formula xml:id="formula_0">Given an input video V = {v i } N i=1 ,</formula><p>where N is the number of frames, we follow the common paradigm from previous works to divide it into a sequence of snippets, each of which consists of ? consecutive frames from the video, resulting in a total of T = N ? snippets. Let ? (.) be an encode function to extract visual representation of a ? -frame snippet s i , the entire video is presented as:</p><formula xml:id="formula_1">f i = ? (s i ), and F = { f i } T i=1<label>(1)</label></formula><p>Prior works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45</ref>] employ a pre-trained backbone network (e.g., C3D network <ref type="bibr" target="#b18">[19]</ref> or Two-Stream network <ref type="bibr" target="#b36">[37]</ref>) to model ? (.). However, simply applying those networks for video representation may have some drawbacks as mentioned in Section 1. In Section 3.1, our proposed perception-based visual representation (PVR) is discussed as an alternative to the former strategy. Then, boundary-matching module for temporal action proposals generation is discussed in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Perception-based Visual Representation (PVR)</head><p>The PVR module aims to extract video visual representation based on how a human perceives an action, i.e., identifying the main actors at each temporal period and interactions between main actors and the environment to specify when the action starts and ends. PVR consists of three main components: (i) environment spectator; (ii) actors spectator; and (iii) actorsenvironment interaction spectator. The overall architecture of PVR is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>(i) Environment Spectator aims to extract global semantic information of the input ?frame snippet. To extract both the spatial and temporal details of the snippet, we adopt a 3D network pre-trained on action recognition benchmarking datasets as a backbone feature extractor. The snippet is processed through all convolutional blocks of the 3D network to obtain a feature map M; then, an average pooling operator is employed to produce a spatiotemporal feature vector f e .</p><p>(ii) Actors Spectator aims to semantically extract main actor(s) representation. An action cannot happen in the absence of a human (actor). However, when an action occurs, it does not necessarily signal that every actor in the scene has committed the action. First, the // embed f e to common space with every f a i in F a set S a andF a to empty list // S a will store scores of every actor setF a to empty list // F a will store selected main actors actors spectator detects all existing actors in the snippet by an actor localization module. Then, an adaptive attention mechanism (AAM) is proposed to adaptively select an arbitrary number of main actor(s) and extract their mutual relationships to represent them as a single feature vector. Actor Localization: To localize all actors in a ? -frame snippet, we apply a human detector onto the middle frame of it with the assumption that, with a small ? , the actors would not move fast enough to be mis-located. We denote</p><formula xml:id="formula_2">for each f a i in F a d? f a i ? MLP ?a ( f a i ) // embed f a i to common space with f e s a i ?</formula><formula xml:id="formula_3">B = {b i } N B i=1</formula><p>as a set of detected human bounding boxes, where N B ? 0. Afterwards, each of the detected bounding boxes, b i , is aligned onto feature map M (obtained by the 3D network backbone from environment spectator) using RoIAlign <ref type="bibr" target="#b15">[16]</ref> and then average-pooled into a single feature vector f a i . Finally, we obtain a set of actor features F a = { f a i } N B i=1 . Adaptive Attention Mechanism (AAM): Given N B detected actors, there are only a few of detected actors (called main actors) who actually contribute to the action if it presents. Because the number of main actors is unknown and continuously changes throughout the input video, we propose an adaptive attention mechanism (AAM) that inherits the merits from adaptive hard attention to select an arbitrary number of main actors and a soft self-attention mechanism <ref type="bibr" target="#b38">[39]</ref> to extract relationships among them. AAM is described by pseudocode in Algorithm 1 and illustrated in <ref type="figure">Fig. 3</ref>; more details are provided in the supplementary.  (iii) Actors-Environment Interaction Spectator This module aims to model the relations between environment feature f e and actors representation feature f a , and then combine them into a single feature f . Herein, we employ the self-attention model <ref type="bibr" target="#b38">[39]</ref> where f e and f a are the inputs. We denote f i as a visual representation for snippet s i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Boundary-Matching Module (BMM)</head><p>BMM is responsible for generating action proposals, which are boundary-pairs of every possible action of interest appearing in the video. Our BMM contains three components: base module, temporal evaluation module, and proposal evaluation module as illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>. The base module aims to model the semantic relationship between snippets. The temporal evaluation module assesses each snippet 0 ? i ? T in the video to estimate probabilities that any action starts or ends there, corresponding to P S i and P E i , respectively. Meanwhile, the proposal evaluation module evaluates every interval [i, j] in the video to estimate its actionness score, corresponding to P A i,d , where d = j ? i. At the inference stage, we search through P S and P E to select temporal locations i whose P S i or P E i are local maximums to form sets of potential starting and ending temporal locations, respectively. Then, starting and ending locations (s, e) from those lists that satisfy timing constraint (e.g. s ? e ? T ) are paired and become a candidate proposal with a score s = P S s ? P E e ? P A s,e?s . Based on the timestamps and scores of candidate proposals, we finally apply NMS <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31]</ref> to produce the final set of action proposals.</p><p>In our paper, we have conducted BMM under two different network architectures: CNNbased and GCN-based. Our CNN-based BMM, called AEI-B, is leveraged by <ref type="bibr" target="#b25">[26]</ref> where the base module is comprised of 1D convolutional layers to learn and extract the temporal relations between snippets. On the other hand, our GCN-based BMM, called AEI-G, is leveraged by <ref type="bibr" target="#b44">[45]</ref> to extract not only local relations, but also the relations of snippets that share close semantic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Methodology</head><p>Given a list of N G ground-truth action segments G = {g i = (g s i , g e i )} N G i=1 of input video V, we generate the ground-truth starting labels L S ? [0, 1] T , ending labels L E ? [0, 1] T and actionness labels L A ? [0, 1] T ?D (D is a pre-defined maximum proposal length). L S i (or L E i ) carrying a value of 1 means that there is a ground-truth starting (or ending) boundary of any action at i and vice versa. Likewise, L A i,d carrying a value of 1 means that there is a ground-truth action starts at i with a length of d. To train our AEI network with the ground-truth labels, we define the loss function L AEI as follows:</p><formula xml:id="formula_4">L AEI = L start (P S , L S ) + L end (P E , L E ) + L actionness (P A , L A )</formula><p>We use weighted binary log-likelihood loss L wb for L start and L end , which is defined as follows:</p><formula xml:id="formula_5">L wb (P, L) = N ? i=1 L i N + log P i + (1 ? L i ) N ? log(1 ? P i )<label>(2)</label></formula><p>where N + and N ? are the number of positives and negatives in ground-truth labels, respectively. Conversely, L actionness (P, L) = L wb (P, L) + ? L 2 (P, L), where L 2 is the mean squared error loss and ? is set to 10.</p><p>To reduce time cost in training phase of our proposed AEI network, actors features set F a and environment feature f e for actors spectator and environment spectator, respectively, are extracted in advance. Then, the AAM and Interaction Spectator of PVR module is trained with BMM module in an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments Datasets</head><p>We conduct experiments on ActivityNet-1.3 <ref type="bibr" target="#b8">[9]</ref> and THUMOS-14 <ref type="bibr" target="#b19">[20]</ref> datasets. The former contains 20,000 videos with 200 annotated activities while the latter consists of 414 videos with 20 actions. For both datasets, we follow previous works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> to preprocess videos with the snippet length set to ? = 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head><p>In TAPG, AR@AN and AUC are popular metrics to benchmark the performance of competing methods. The former is the average recall (AR) computed with the average number of proposals (AN) kept by each video. The latter is the score of the area under AR versus the AN curve. In ActivityNet-1.3, AR@100 and AUC are mainly used. On the other hand, in THUMOS-14, only AR@AN is used to compare between methods; however, multiple AN is selected from a list of [50, 100, 200, 500, 1000].</p><p>In TAD, both ActivityNet-1.3 and THUMOS-14 use mean Average Precision (mAP) to benchmark methods in this problem. ActivityNet-1.3 uses tIoU thresholds of {0.5, 0.75, 0.95} and average mAP, while THUMOS-14 uses tIoU thresholds of {0.3, 0.4, 0.5, 0.6, 0.7} for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>For all experiments on both ActivityNet-1.3 <ref type="bibr" target="#b8">[9]</ref> and THUMOS-14 <ref type="bibr" target="#b19">[20]</ref>, we employ C3D <ref type="bibr" target="#b18">[19]</ref> network pre-trained on Kinetics-400 <ref type="bibr" target="#b20">[21]</ref> as the backbone network to extract features from video snippets. The features extracted from C3D backbone have 2048 dimensions. <ref type="table">Table 2</ref>: TAD comparisons on ActivityNet-1.3 in terms of mAP@tIoU and mAP, where the proposals are combined with video-level classification results generated by <ref type="bibr" target="#b42">[43]</ref>. In the actors spectator, for actor localization, we employ a Faster-RCNN model <ref type="bibr" target="#b33">[34]</ref> pre-trained on COCO <ref type="bibr" target="#b26">[27]</ref> dataset to detect humans as discussed in 3.1. To train our AEI network, we utilize Adam optimizer with the initial learning rate set to 0.0001 and 0.001 for ActivityNet-1.3 and THUMOS-14, respectively.</p><p>In TAPG, Soft-NMS <ref type="bibr" target="#b2">[3]</ref> is applied in post-processing for all experiments on ActivityNet-1.3, while on THUMOS-14, both Soft-NMS <ref type="bibr" target="#b2">[3]</ref> and NMS <ref type="bibr" target="#b30">[31]</ref> are evaluated. In TAD, following <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, we use NMS <ref type="bibr" target="#b30">[31]</ref> for post-processing on both datasets.</p><p>In the following experiments, we highlight the best performance in bold and the secondbest performance in italic. <ref type="table" target="#tab_1">Table 1</ref> demonstrates the comparison on ActivityNet-1.3 validation and testing sets. Based on the results, it can be observed that the performances of our methods, AEI-B and AEI-G, are standing out against those of SOTA methods in terms of AR@100 and AUC by large margins. <ref type="table" target="#tab_3">Table 3</ref> presents the comparison of SOTA TAPG methods on THUMOS-14 dataset. Compared to SOTA approaches, our AEI obtains better performance on all AR@ANs metrics regardless of the architecture of BMM. From <ref type="table" target="#tab_1">Table 1</ref>, 3, we empirically observe that AEI-G, which employs GCN to model the relationship between snippets, marginally surpasses AEI-B on TAPG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Temporal Action Proposal Generation (TAPG)</head><p>Generalizability is also considered as an important criterion to evaluate a method in TAPG. Following the same experiment setup in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref>, our AEI is trained on Unseen+Seen and Seen training sets, separately, and then is evaluated on the Seen and Unseen validation sets, separately as illustrated in <ref type="figure" target="#fig_5">Fig. 5</ref>. The performances on Seen validation set is shown in the first two charts, whereas the performances on Unseen validation set is given in the last two charts. <ref type="figure" target="#fig_5">Fig. 5</ref> shows that our AEI achieves good performances on Seen validation set with an acceptable drop on Unseen validation set on both training configurations, suggesting that our AEI is highly generalizable to unseen action types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Temporal Action Detection (TAD)</head><p>Following the experiment settings in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45]</ref>, we adopt top-1 video-level classification results generated by the method in <ref type="bibr" target="#b42">[43]</ref> on ActivityNet-1.3 to label the proposals produced by our method. We use top-2 video-level classification results generated by UntrimmedNet <ref type="bibr" target="#b41">[42]</ref> to label proposals generated by our method on THUMOS-14. <ref type="table">Table 2</ref> illustrates TAD performance comparison between AEI and other SOTA methods on ActivityNet-1.3 validation set. The results emphasize that our methods outperform SOTA methods in spite of CNN-based BMM or GCN-based BMM. The experiment results on   Method @50 @100 @200 @500 @1000 T-TAG <ref type="bibr" target="#b10">[11]</ref> 18.55 29.00 39.61 --CTAP <ref type="bibr" target="#b12">[13]</ref> 32.49 42.61 51.97 --BSN <ref type="bibr" target="#b24">[25]</ref> 37. <ref type="bibr" target="#b45">46</ref>   THUMOS-14 test set in <ref type="table" target="#tab_5">Table 4</ref> demonstrate that our AEI-B and AEI-G are superior to other SOTA methods on most of the metrics regardless of UntrimmedNet <ref type="bibr" target="#b41">[42]</ref> or P-GCN <ref type="bibr" target="#b45">[46]</ref> classifiers. From <ref type="table">Table 2</ref> and 4, we empirically notice that both AEI-B and AEI-G obtain comparable TAD performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We further conduct a detailed ablation study on THUMOS-14 dataset to evaluate the contributions of different components of the proposed AEI framework. We conduct two ablation studies as shown in <ref type="figure" target="#fig_6">Fig. 6</ref> on TAPG and THUMOS-14 in terms of AR@ANs. First, we evaluate the contribution of each spectator to the overall performance of our proposed PVR (described in Section 3.1), i.e., environment spectator, actors spectator, and interaction spectator. As illustrated in <ref type="figure" target="#fig_6">Fig. 6 (a)</ref>, "Environment spectator only", which only focuses on global information, plays an important role in TAPG, whereas "Actors spectator only", which takes only local information of the main actor(s) into account, achieves ade- . quate performance. "W/o interaction spectator", which withdraws the interaction spectator by simply fusing global and local information using an averaging operation, gives an undesired performance that is even lower than "Environment only". The complete proposed model, e.g., "AEI (all spectators)", gives the best result thanks to the interaction spectator adaptively fusing global feature from environment spectator and local feature from actors spectator.</p><p>In addition, we also evaluate the effectiveness of main actor selection and feature fusion in our proposed AAM. <ref type="figure" target="#fig_6">Fig. 6 (b)</ref> shows the performance of the network without each of these components. In the "AEI w/o feature fusion" settings, we use an average pooling layer to fuse features obtained from main actor selection component. As illustrated, both configurations achieve similar performance with AN below 600, while main actor selection component plays a slightly more significant role than feature fusion component. This implies that having an appropriate main actor selection contributes significantly to the entire network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we proposed a novel actors-environment interaction (AEI) network to simulate human perceiving ability in the temporal action proposals generation. Our AEI contains two modules: perception-based visual representation (PVR) and boundary-matching (BMM). PVR aims to extract visual representation of each snippet. To achieve the human perceiving ability, PVR is equipped with three spectators, each of which learns to perceive input snippet at a unique aspect, e.g. environment, main actors, and actors-environment interactions. An adaptive attention mechanism (AAM) is proposed in actors spectator to select an arbitrary number of main actor(s) in the snippet as well as learning the relationships between them.</p><p>Extensive experiments are conducted on ActivityNet-1.3 and THUMOS-14 datasets on TAPG and TAD tasks, which demonstrate that our proposed AEI outperforms SOTA methods regardless of BMM architecture (e.g., CNN-based or GCN-based). These results prove that replicating human perceiving ability in video understanding is a promising track to follow and further explore in the future.</p><p>Beside three proposed spectators in PVR, we can include additional spectators to observe human body parts and the interaction between them with objects to better handle egocentric videos, in which the main actor who perform the action does explicitly appear.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of our proposed PVR. Given a ? -snippet, the corresponding snippet visual representation is obtained by three modules: (i) environment spectator to extract global environment feature; (ii) actors spectator to extract local actor representation; and (iii) actors-environment interaction spectator to model the relationship between the environment feature and the actor feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Algorithm 1 :</head><label>31</label><figDesc>Illustration of our proposed AAM. Given an environment feature f e and a set of actor features F a , this module aims to select main actor features, followed by fusing arbitrary main actor features to obtain an actor visual representation f a . Adaptive Attention Mechanism (AAM) to extract representation of main actors in a snippet. Data: Feature vector f e and features set F a represent environment and all actors that appear in input snippet, respectively. Result: Feature vector f a represents main actors in input snippet. f e ? MLP ?e ( f e )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The overall architecture of our proposed AEI, consisting of perception-based visual representation module (PVR), and boundary-matching module (BMM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Generalizability evaluation and comparisons with BMN<ref type="bibr" target="#b25">[26]</ref> and DBG<ref type="bibr" target="#b22">[23]</ref> on ActivityNet 1.3 in terms of AR@100 and AUC. Methods are trained on Unseen+Seen (blue columns) and Seen training sets (orange columns), respectively; and are evaluated on Seen (first two charts) and Unseen (last two charts) validation sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>TAPG comparisons on different AEI configurations: (a) either only environment or only actors spectator or both; (b) either only main actor selection or only feature fusion or both.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>TAPG comparisons in terms of AR@AN and AUC between our AEI and other SOTA methods on ActivityNet-1.3. AR@100 (val) 74.16 74.54 75.27 75.01 76.65 76.52 76.31 75.65 76.72 77.25 77.24 AUC (val) 66.17 66.43 66.51 67.10 68.23 68.26 68.35 68.15 69.16 69.43</figDesc><table><row><cell>Metrics</cell><cell>BSN MGG [25] [29]</cell><cell>MR [47]</cell><cell cols="4">BMN DBG BSN++ TSI++ AEN ABN AEI-B AEI-G [26] [23] [38] [28] [41] [40]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>69.47</cell></row><row><cell>AUC (test)</cell><cell>66.26 66.47</cell><cell>-</cell><cell>67.19 68.57</cell><cell>-</cell><cell>68.85 68.99 69.26 69.94</cell><cell>70.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>TAPG comparisons on THUMOS-14 in terms of AR@AN, where SNMS represents Soft-NMS<ref type="bibr" target="#b2">[3]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>TAD comparisons on THUMOS-14 in term of mAP@tIoU.<ref type="bibr" target="#b44">[45]</ref> 22.9 37.6 51.6 60.4 66.<ref type="bibr" target="#b3">4</ref> MR</figDesc><table><row><cell cols="6">indicates P-GCN classifier (2 nd group);</cell></row><row><cell cols="6">otherwise, UntrimmedNet classifier.</cell></row><row><cell>Method</cell><cell>0.7</cell><cell>0.6</cell><cell>0.5</cell><cell>0.4</cell><cell>0.3</cell></row><row><cell>T-TAP[4]</cell><cell>6.3</cell><cell cols="4">14.1 24.5 35.3 46.3</cell></row><row><cell>BSN [25]</cell><cell cols="5">20.0 28.4 36.9 45.0 53.5</cell></row><row><cell>BMN [26]</cell><cell cols="5">20.5 29.7 38.8 47.4 56.0</cell></row><row><cell>MGG [29]</cell><cell cols="5">21.3 29.5 37.4 46.8 53.9</cell></row><row><cell>DBG [23]</cell><cell cols="5">21.7 30.2 39.8 49.4 57.8</cell></row><row><cell>GTAD [45]</cell><cell cols="5">23.4 30.8 40.2 47.6 54.5</cell></row><row><cell>TSI++[28]</cell><cell cols="5">22.4 33.2 42.6 52.1 61.0</cell></row><row><cell>GTAN [30]</cell><cell>-</cell><cell>-</cell><cell cols="3">38.8 47.2 57.8</cell></row><row><cell>AEI-B</cell><cell cols="5">23.4 35.9 44.7 52.7 58.7</cell></row><row><cell>AEI-G</cell><cell cols="5">22.9 34.2 43.4 51.6 57.4</cell></row><row><cell>BSN  *  [25]</cell><cell>-</cell><cell>-</cell><cell cols="3">49.1 57.8 63.6</cell></row><row><cell>GTAD  [47]</cell><cell cols="5">28.5 38.0 45.4 50.7 53.9</cell></row><row><cell>AEI-B  *</cell><cell cols="5">22.4 37.8 52.1 60.6 67.3</cell></row><row><cell>AEI-G  *</cell><cell cols="5">22.3 37.9 52.0 60.4 67.6</cell></row></table><note>***</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This material is based upon work supported by the National Science Foundation under Award No. OIA-1946391; partially funded by Gia Lam Urban Development and Investment Company Limited, Vingroup and supported by Vingroup Innovation Foundation (VINIF) under project code VINIF.2019.DA19.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Understanding human action. integrating meanings, mechanisms, causes, and contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bazhanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Scholz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Soft-nmsimproving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An attentive survey of attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sneha</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gungor</forename><surname>Polatkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Ramanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Mithal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02874</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Describing multimedia content using attention-based encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1875" to="1886" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Saccader: Improving accuracy of hard attention models for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gamaleldin</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bernard Ghanem Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Galassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Torroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cascaded Boundary Regression for Temporal Action Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01180</idno>
		<imprint>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
	<note>arXiv e-prints, art</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ctap: Complementary temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Motion-appearance comemory networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhou</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Computational Models for Cognitive Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ghosh</surname></persName>
		</author>
		<ptr target="https://books.google.com/books?id=fnPtDwAAQBAJ" />
		<imprint>
			<date type="published" when="2020" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
		<idno type="DOI">10.1109/34.730558</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2012.59</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densecaptioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fast learning of temporal action proposal via dense boundary generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-04" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="11499" to="11506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tsi: Temporal scale invariant network for action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilan</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchen</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient non-maximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neubeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICPR.2006.479</idno>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="850" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Differential attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Badri</forename><surname>Patro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7680" to="7688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolov3: An incremental improvement. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">BSN++: complementary boundary regressor with scale-balanced relation modeling for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Abn: Agent-aware boundary networks for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashu</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngan</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2021.3110973</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="126431" to="126445" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Agent-environment network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet-Khoa</forename><surname>Vo-Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashu</forename><surname>Kamazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP39728.2021.9415101</idno>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2160" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">CUHK &amp; ETHZ &amp; SIAT submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1608.00797</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bottom-up temporal action localization with mutual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peisen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="539" to="555" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
