<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Self-Supervised Low-Rank Network for Single-Stage Weakly and Semi-Supervised Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwen</forename><surname>Pan</surname></persName>
							<email>junwenpan@tju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
							<email>zhupengfei@tju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwen</forename><surname>Pan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Nanjing University of Information Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Self-Supervised Low-Rank Network for Single-Stage Weakly and Semi-Supervised Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor) ? Corresponding author * These authors contributed equally.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Weakly-supervised Learning ? Semi- supervised Learning ? Semantic Segmentation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation with limited annotations, such as weakly supervised semantic segmentation (WSSS) and semi-supervised semantic segmentation (SSSS), is a challenging task that has attracted much attention recently. Most leading WSSS methods employ a sophisticated multi-stage training strategy to estimate pseudo-labels as precise as possible, but they suffer from high model complexity. In contrast, there exists another research line that trains a single network with image-level labels in one training cycle. However, such a single-stage strategy often performs poorly because of the compounding effect caused by inaccurate pseudo-label estimation. To address this issue, this paper presents a Self-supervised Low-Rank Network (SLRNet) for single-stage WSSS and SSSS. The SLRNet uses cross-view self-supervision, that is, it simultaneously predicts several complementary attentive LR representations from different views of an image to learn precise pseudo-labels. Specifically, we reformulate the LR representation learning as</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a collective matrix factorization problem and optimize it jointly with the network learning in an end-to-end manner. The resulting LR representation deprecates noisy information while capturing stable semantics across different views, making it robust to the input variations, thereby reducing overfitting to self-supervision errors. The SLRNet can provide a unified single-stage framework for various labelefficient semantic segmentation settings: 1) WSSS with image-level labeled data, 2) SSSS with a few pixel-level labeled data, and 3) SSSS with a few pixel-level labeled data and many image-level labeled data. Extensive experiments on the Pascal VOC 2012, COCO, and L2ID datasets demonstrate that our SLRNet outperforms both state-of-the-art WSSS and SSSS methods with a variety of different settings, proving its good generalizability and efficacy.</p><p>Keywords Weakly-supervised Learning ? Semisupervised Learning ? Semantic Segmentation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation is a fundamental computer vision task that aims to assign a label to each pixel, promoting the development of many downstream tasks, such as scene parsing, autonomous driving, and medical image analysis <ref type="bibr" target="#b6">(Chen et al., 2018;</ref><ref type="bibr" target="#b63">Zhou et al., 2019;</ref><ref type="bibr" target="#b18">Havaei et al., 2017)</ref>. Recently, deep learning based semantic segmentation models <ref type="bibr" target="#b36">(Long et al., 2015;</ref><ref type="bibr" target="#b6">Chen et al., 2018)</ref>, trained with large-scale data labeled at pixel level, have achieved impressive progress. However, such supervised approaches require intensive manual annotations that are time-consuming and expensive, which have inspired many investigations about learning with low-cost annotations, such as semi-supervised  <ref type="bibr" target="#b2">(Araslanov and Roth, 2020)</ref>, (II) Cross pseudo supervision for SSSS , (III) PseudoSeg supervision for SSSS <ref type="bibr" target="#b65">(Zou et al., 2021)</ref>, and (IV) The SLRNet with MVMC and CVLR can mitigate the compounding effect of pseudo supervision error. '?' means the forward operation, ' ' means pseudo supervision, '/' on '?' means stop-gradient and pseudo-label generation. t denotes the transformation operator, s and w are short for "strong" and "weak" respectively. semantic segmentation (SSSS) with limited amounts of labeled data, weakly supervised semantic segmentation (WSSS) with bounding boxes <ref type="bibr" target="#b10">(Dai et al., 2015)</ref>, scribbles <ref type="bibr" target="#b33">(Lin et al., 2016)</ref>, points <ref type="bibr" target="#b3">(Bearman et al., 2016)</ref>, and image-level labels <ref type="bibr" target="#b25">(Kolesnikov and Lampert, 2016)</ref>. Nevertheless, there is a considerable gulf between weakly supervised and semi-supervised approaches.</p><p>Most popular image-level WSSS methods <ref type="bibr" target="#b1">(Ahn et al., 2019;</ref><ref type="bibr" target="#b14">Dong et al., 2020;</ref><ref type="bibr" target="#b50">Sun et al., 2020)</ref> resort to multiple training and refinement stages to obtain more accurate pseudo-labels while avoiding error accumulation. These methods often start from a weakly supervised localization, such as a class activation map (CAM) <ref type="bibr" target="#b62">(Zhou et al., 2016)</ref>, which highlights the most discriminative regions in an image. In this approach, diverse enhanced CAM-generating networks <ref type="bibr" target="#b30">(Lee et al., 2019;</ref><ref type="bibr" target="#b54">Wang et al., 2020b;</ref><ref type="bibr" target="#b50">Sun et al., 2020)</ref> and CAM-refinement procedures <ref type="bibr" target="#b0">(Ahn and Kwak, 2018;</ref><ref type="bibr" target="#b1">Ahn et al., 2019;</ref><ref type="bibr" target="#b45">Shimoda and Yanai, 2019)</ref> have been designed to expand the highlighted area to the entire object or eliminate the wrongly highlighted area. Although these multi-stage methods can produce more accurate pseudo-labels, they suffer from the need of a large number of hyper-parameters and complex training procedures. Single-stage WSSS methods <ref type="bibr" target="#b61">(Zheng et al., 2015;</ref><ref type="bibr" target="#b40">Papandreou et al., 2015)</ref> have received less attention because their segmentation is less accurate than that of multi-stage methods. Recently, <ref type="bibr" target="#b2">Araslanov and Roth (2020)</ref> proposed a simple single-stage WSSS model that generates pixel-level pseudo-labels online as self-supervision ( <ref type="figure" target="#fig_0">Fig. 1 (I)</ref>). However, its accuracy is still not comparable with that of multi-stage approaches. In contrast, the simple online pseudo-supervision scheme has made promising progress in SSSS ( <ref type="figure" target="#fig_0">Fig. 1(II)</ref>   and (III) <ref type="bibr" target="#b65">(Zou et al., 2021)</ref>).</p><p>We argue that the cause of the inferior performance of the online pseudo supervised WSSS is the compounding effect of errors caused by online inaccurate pseudo supervision. Like multi-stage refinements, online pseudo-label supervision should gradually improve the semantic fidelity and completeness during the training process. However, this also increases the risk that errors are mimicked and accumulated with the gradient flows being backpropagated from the top to the lower layers. Consistency learning is widely used as additional supervision to semi-supervised learning <ref type="bibr" target="#b39">(Ouali et al., 2020;</ref>. However, in practice, existing consistency-based methods are not applicable to image-level weakly supervised settings. First, they require pixel-level supervision to avoid the collapsing solution <ref type="bibr" target="#b8">(Chen and He, 2021)</ref>. Second, the dominance of consistency harms the region expansion for WSSS.</p><p>To this end, we propose the Self-supervised Low-Rank Network (SLRNet) for single-stage WSSS and SSSS. As illustrated in <ref type="figure" target="#fig_0">Fig. 1(IV)</ref>, the SLRNet simultaneously predicts several segmentation masks for various augmented versions of one image, which are jointly calibrated and refined by a multi-view mask calibration (MVMC) module to generate one pseudo-mask for self-supervision. The pseudo-mask leverages the complementary information from various augmented views, which enforces the cross-view consistency on the predictions. To further regularize the network, the SLRNet introduces the LR inductive bias implemented by a cross-view low-rank (CVLR) module. The CVLR exploits the collective matrix factorization to jointly decompose the learned representations from different views into sub-matrices while recovering a clean LR signal subspace. Through the dictionary shared over different views, a variety of related features from various views can be refined and amplified to eliminate the ambiguities or false predictions. Thereby, the input features of the decoder deprecate noisy information, and this can effectively prevent the network from overfitting to the false pseudo-labels. Additionally, instead of directly randomly initializing the sub-matrices, a latent space regularization is designed to improve the optimization efficiency.</p><p>The SLRNet is an efficient and elegant framework that generalizes well to different label-efficient segmentation settings without additional training phases.</p><p>For instance, to simultaneously utilize image-level and pixel-level labels, previous SSSS methods <ref type="bibr" target="#b30">(Lee et al., 2019;</ref><ref type="bibr" target="#b56">Wei et al., 2018)</ref> have to generate and refine pseudo-labels offline using WSSS model, which are bundled with pixel-level labels to train a network in the next stage. Such a multi-stage scheme provides a marginal improvement over dedicated SSSS algorithms <ref type="bibr" target="#b39">(Ouali et al., 2020;</ref><ref type="bibr" target="#b65">Zou et al., 2021)</ref> with unlabeled data. In contrast, the SLRNet directly introduces additional pixel-level supervision while combining it with image-level data without extra cost. In other words, the online pseudo-mask generation takes into account both image-level and pixel-level labels in a single training phase and is undoubtedly more accurate. To the best of our knowledge, the SLRNet is the first attempt to bridge these tasks into a unified single-stage scheme, allowing it to maximize exploiting various annotations with a limited budget.</p><p>In our experiments, we first validate the performance of SLRNet in an image-level WSSS setting on several datasets, including Pascal VOC 2012 <ref type="bibr">(Everingham et al., 2010)</ref>, COCO <ref type="bibr" target="#b34">(Lin et al., 2014)</ref>, and L2ID . Extensive experiments demonstrate that the cross-view supervision and the CVLR help improve semantic fidelity and completeness of the generated segmentation masks. Notably, the SLRNet also establishes new state-of-the-arts for various label-efficient semantic segmentation tasks, including 1) WSSS with image-level labeled data, 2) SSSS with pixel-level and image-level labeled data and 3) SSSS with pixel-level labeled and unlabeled data. Moreover, the SLRNet achieves the best performance at the WSSS Track of CVPR 2021 Learning from Limited and Imperfect Data (L2ID) Challenge , outperforming other competitors by large margins of ? 9.35% in terms of mIoU.</p><p>The main contributions of this work are summarized as follows: 1) We propose an effective cross-view self-supervision scheme, incorporating the CVLR module, to alleviate the compounding effect of self-supervision errors for the online pseudo-label training. 2) We present a plug-and-play collective matrix factorization method with latent space regularization for multi-view LR representation learning, which can be readily embedded into any Siamese networks for end-to-end training. 3) The SLRNet provides a unified framework that can be well generalized to learn a segmentation model from different limited annotations in various WSSS and SSSS settings. 4) The SLRNet achieves leading performance compared to a variety of state-of-the-art methods on Pascal VOC 2012, COCO, and L2ID datasets for both WSSS and SSSS tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This section reviews a variety of methods related to the proposed SLRNet, including the WSSS, the SSSS, the LR representation, and the self-supervised learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Weakly Supervised Semantic Segmentation</head><p>In the past years, various variants of WSSS methods have been developed and evolved rapidly, which can be categorized into multi-stage and single-stage classes.</p><p>Most multi-stage WSSS methods with image-level supervision start from the CAM <ref type="bibr" target="#b62">(Zhou et al., 2016)</ref>. These methods refine the CAM obtained from a pre-trained CAM-generating (classification) network to generate segmentation pseudo-labels, with the aim of expanding the highlighted area to the entire object or eliminating the false highlighted area. The prevailing method is to consider the semantic completeness and fidelity of the seed region when training the CAM-generating network using, for instance, atrous convolution <ref type="bibr" target="#b56">(Wei et al., 2018)</ref>, stochastic feature selection <ref type="bibr" target="#b30">(Lee et al., 2019)</ref>, the idea of erasing <ref type="bibr" target="#b20">Hou et al., 2017)</ref>, cross-image affinity , and the equivariant for various scaled input images <ref type="bibr" target="#b54">(Wang et al., 2020b)</ref>. Although the seed obtained from an improved CAM-generating network is better, most of these methods still need extra CAMrefinement procedures, such as random walk <ref type="bibr" target="#b0">(Ahn and Kwak, 2018)</ref>, region growing , or an additional network for distillation. Moreover, some of these methods utilize class-agnostic saliency maps to obtain background cues. The multi-stage methods use a series of algorithms to improve the WSSS accuracy by carefully tuning the hyperparameters of each stage, leading to a rapid increase in complexity.</p><p>In contrast to the multi-stage methods, the singlestage approaches train WSSS models using only one training cycle <ref type="bibr" target="#b43">(Pinheiro and Collobert, 2015;</ref><ref type="bibr" target="#b40">Papandreou et al., 2015;</ref><ref type="bibr" target="#b61">Zheng et al., 2015)</ref>, but they cannot perform favorably because of their inferior segmentation accuracy. Recently, <ref type="bibr" target="#b2">(Araslanov and Roth, 2020)</ref> proposed a simple yet effective single-stage model, i.e., they train a segmentation network with image-level labels and produce refined masks online as self-supervision. However, this self-trained model is still unable to compete with the latest multi-stage methods <ref type="bibr" target="#b1">Ahn et al. (2019)</ref>; <ref type="bibr" target="#b30">Lee et al. (2019)</ref>; <ref type="bibr" target="#b54">Wang et al. (2020b)</ref> in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semi-Supervised Semantic Segmentation</head><p>Generally, in semi-supervised learning, only a small subset of training images are assumed to have annotations, and a large number of unlabeled data are exploited to improve performance. Early SSSS models <ref type="bibr" target="#b23">(Hung et al., 2018;</ref><ref type="bibr" target="#b47">Souly et al., 2017)</ref> based on generative adversarial networks learn a discriminator between the prediction and the ground truth mask or generate additional training data. Recently, consistency based approaches have been extensively explored. These approaches enforce the predictions to be consistent, either using transformed input images <ref type="bibr" target="#b16">(French et al., 2020;</ref><ref type="bibr" target="#b65">Zou et al., 2021)</ref>, perturbed feature representations <ref type="bibr" target="#b39">(Ouali et al., 2020)</ref>, or different networks . PseudoSeg <ref type="bibr" target="#b65">(Zou et al., 2021)</ref> and CrossPseudo  realize the idea of consistency by designing pseudo-labels online to encourage the cross-view consistency, i.e., one view generates pseudo-labels for supervising another view, instead of explicitly enforcing the prediction consistency. In this paper, our experiments illustrate that this explicit consistency impairs the expansion of highlighted regions under a weakly supervised setting.</p><p>Another WSSS-based line of research involves harnessing low-cost image-level supervision. As described in Sec. 2.1, WSSS approaches generate segmentation pseudo-labels that can then be used to train a segmentation network together with the human-annotated pixel-level labels <ref type="bibr" target="#b30">(Lee et al., 2019;</ref><ref type="bibr" target="#b56">Wei et al., 2018;</ref><ref type="bibr" target="#b31">Li et al., 2018)</ref>. In contrast, our SLRNet provides a unified single-stage WSSS and SSSS framework without offline pseudo-label generation or multi-stage training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Low-Rank Representation</head><p>The LR representation seeks a compact data structure and has been widely applied to subspace clustering <ref type="bibr" target="#b35">(Liu et al., 2012)</ref>, dictionary learning , matrix decomposition <ref type="bibr" target="#b4">(Cabral et al., 2013)</ref>, and deep network approximation <ref type="bibr" target="#b51">(Tai et al., 2016)</ref>. Liu et al. proposed a robust subspace clustering model using an LR representation <ref type="bibr" target="#b35">(Liu et al., 2012)</ref>; Ma et al. proposed a discriminative LR dictionary learning algorithm for face recognition ; and Ricardo et al. proposed a unified approach to bilinear factorization and nuclear norm regularization for LR matrix decomposition <ref type="bibr" target="#b4">(Cabral et al., 2013)</ref>. Because of the redundancy of convolutional filters, LR regularization is imposed to speed up convolutional neural networks <ref type="bibr" target="#b51">(Tai et al., 2016)</ref>. <ref type="bibr">Recently, Geng et al. (2021)</ref> and <ref type="bibr" target="#b32">Li et al. (2019)</ref> introduced the LR reconstruction into segmentation as an alternative to the self-attention mechanism. Both of them merely endeavor to denoise variance and capture the invariant representations between related pixels in a single feature map, while ignoring the inconsistency and noise in cross-view feature maps, and thus can bring a rather marginal effect towards alleviating the error accumulation in those settings with limited supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Self-Supervised Learning</head><p>Self-supervised learning aims at designing pretext tasks to learn general feature representations from large-scale unlabeled data without human-annotated labels. Classical pretext tasks include image and video generation <ref type="bibr" target="#b42">(Pathak et al., 2016;</ref><ref type="bibr" target="#b27">Ledig et al., 2017)</ref>, spatial or temporal context learning <ref type="bibr" target="#b13">(Doersch et al., 2015;</ref><ref type="bibr" target="#b29">Lee et al., 2017)</ref>, and free semantic label-based methods <ref type="bibr" target="#b48">(Stretcu and Leordeanu, 2015)</ref>. Recently, contrastive learning has become popular, whose core idea is to attract positive pairs and repulse negative pairs <ref type="bibr" target="#b19">He et al., 2020)</ref>. Siamese architectures for contrastive learning can model transformation invariance by weight-sharing, i.e., two views of the same sample should produce the consistent outputs <ref type="bibr" target="#b8">(Chen and He, 2021)</ref>. The pre-trained features by self-supervised methods are transferred to the downstream tasks for further learning. For downstream tasks requiring dense predictions (e.g., segmentation and detection), there are also a flurry of recent work <ref type="bibr">(Xie et al., 2021a,b;</ref><ref type="bibr" target="#b53">Wang et al., 2021;</ref><ref type="bibr" target="#b38">O. Pinheiro et al., 2020)</ref> exploring pixel-level contrastive learning that model local invariance by considering the correspondence between local representations.</p><p>Considering the label-efficient segmentation with limited supervision signal, it is intuitive to introduce self-supervision as additional constraint to narrow the gap between label-efficient and fully-supervised settings. SEAM <ref type="bibr" target="#b54">(Wang et al., 2020b</ref>) exploited a Siamese network to model scale equivariance, while CIAN  and MCIS  mined the cross-image affinity from image pairs. Here, self-supervision is manifested in distinct dimensions: along with pixel-level prediction consistency over different transformed views, free pseudo ground-truth calibrated by the MVMC can also be regarded as self-supervision. Additionally, the CVLR enforces consistency between cluster assignments produced for different views, which is also related to self-supervised SWaV <ref type="bibr" target="#b5">(Caron et al., 2020)</ref>.</p><formula xml:id="formula_0">CVLR Enc MVMC Dec T (v) T (u) Enc Dec Lreg L cls Lseg Lseg L cls T (v) T (u)</formula><p>Share parameters</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CVLR Module Framework</head><formula xml:id="formula_1">C (v) C (u) E (v) E (u) D X (v) X (u) + ? y y Optional Collective MF Initialize</formula><p>Linear projector</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear projector</head><p>Auxiliary Predictor</p><formula xml:id="formula_2">d ? h (v) w (v) k ? h (v) w (v) d ? k d ? h (v) w (v) =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs Outputs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2:</head><p>Left: Architecture of the proposed SLRNet. The SLRNet is a multi-branch network with shared parameters, which simultaneously predicts the masks of multiple views (v and u) from one image. The MVMC generates a pseudo segmentation mask as self-supervision (using loss L seg ) by calibrating the false activations in these predicted masks. Meanwhile, the outputs are also constrained by image-level loss L cls and explicit cross-view regularization L reg . Right: The CVLR module. The CVLR is implemented by the cross-view matrix factorization, which reduces the multi-view features into a shared dictionary (D) and a set of code matrices (C v ), where the noises are removed during the reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>This section introduces the proposed SLRNet in details. First, we introduce the unified framework of SLRNet for label-efficient semantic segmentation in Sec. 3.1. Then, we introduce how to design the cross-view supervision in Sec. 3.2. Among it, to reduce the compounding effect caused by self-supervision errors, the MVMC method is proposed to provide cross-view pseudo-label supervision. Finally, Sec. 3.3 introduces the CVLR model, among which we introduce the inductive bias of the LR property into the neural network using the collective matrix factorization method to further mitigate the compounding effect of errors. The CVLR module is then integrated into the network for end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework of the SLRNet</head><p>Similar to typical fully-supervised semantic segmentation, the SLRNet consists of one network trained in one stage without complicated steps for both WSSS and SSSS tasks in a unified framework. Specifically, the SLRNet expands the established encoder-decoder segmentation network <ref type="bibr" target="#b6">(Chen et al., 2018)</ref> into a simple shared-weight Siamese structure <ref type="figure">(Fig. 2 left)</ref>. The SLR-Net takes m views I (v) from an image I augmented by transformations T (v) as input. For explanation clarity, the superscript v denotes the index of view v ? V and |V| denotes the total number of views. The encoder net-work processes these views and produces feature maps</p><formula xml:id="formula_3">X (v) ? R h (v) ?w (v) ?d . The CVLR module in Sec. 3.3</formula><p>jointly factorizes the high-dimensional noisy features X (v) from different views into sub-matrices and reconstructs the LR featuresX <ref type="bibr">(v)</ref> . Afterwards, the features with the LR property are fed to the decoder to predict the segmentation logitsM <ref type="bibr">(v)</ref> . From a unified perspective, we consider three typical types of data under established label-efficient settings:</p><formula xml:id="formula_4">pixel-level labeled data D p ? {(I i , M i )} |Dc| i=1 , image-level (i.e. classification) labeled data D c ? {(I i , y i )} |Dc| i=1 , and unlabeled data D u ? {I i } |Du| i=1</formula><p>. During training, the samples in D p use the manually labeled pixel-wise mask M, and those in D c and D u use the estimated pseudo-labels M produced by the MVMC module described in Sec. 3.2 as ground-truth to construct the following pixellevel cross entropy (CE) loss:</p><formula xml:id="formula_5">L seg = i?Dp v?V L CE (M (v) i , M i ) + i?Dc?Du v?V L CE (M (v) i ,M i ).</formula><p>(1)</p><p>We apply normalized global weighted pooling with focal mask penalty <ref type="bibr" target="#b2">(Araslanov and Roth, 2020)</ref> on the mask logits M (v) to obtain class score? y (v) = pool(M (v) ). Besides, we employ the binary cross entropy (BCE) <ref type="bibr" target="#b41">(Paszke et al., 2019)</ref> for multi-label one-versus-all classification defined as follows:</p><formula xml:id="formula_6">L cls = i?Dc v?V L BCE ? (v) i , y i .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-view Supervision</head><p>The SLRNet employs pixel-level pseudo-labels generated online for self-supervision. How to generate the desired pseudo-maskM is a pivotal question. A naive solution is to simply utilize the decoder output of a trained model after confidence thresholding, as suggested by <ref type="bibr" target="#b64">Zoph et al. (2020)</ref>; <ref type="bibr" target="#b46">Sohn et al. (2020)</ref>. However, in a label-efficient segmentation regime, especially in image-level WSSS, the generated hard/soft pseudomask is fairly coarse. As the network becomes deeper and deeper, errors are prone to be accumulated as the gradient flows are backpropagated from the top to the lower layers, and thus yield inferior performance <ref type="bibr" target="#b6">(Chen et al., 2018;</ref><ref type="bibr" target="#b2">Araslanov and Roth, 2020)</ref>. We use two distinct yet efficient insights to generate better pseudolabel masks: First, we use the complementary information through multi-view fusion to eliminate potential errors in the decoder outputs; Second, we utilize explicit and implicit cross-view supervision to regularize the network to produce more consistent outputs.</p><p>Multi-view Mask Calibration. Most previous WSSS approaches employ offline multi-scale ensemble (i.e. test-time augmentation) and complicated postprocessing steps (e.g. <ref type="bibr" target="#b0">Ahn and Kwak (2018)</ref>; <ref type="bibr" target="#b1">Ahn et al. (2019)</ref>; <ref type="bibr" target="#b14">Dong et al. (2020)</ref>) to refine the coarse outputs as pseudo-labels. These approaches require multi-stage training that increases model complexity. Here, we present an efficient and effective online MVMC scheme for mask calibration. In the early training steps, the network's output is prone to activate partial regions of interest or too many background regions, whereas we can use the output of different augmented versions to calibrate the false predictions. Specifically, the network simultaneously predicts a set of masks</p><formula xml:id="formula_7">{M (v) ? R h (v) ?h (v) ?d } v?V</formula><p>for various transformed versions of the same image whose spatial pixels are not aligned because of various geometric transformations. First, the outputM (v) is transformed respectively by the inverse geometric transformations T <ref type="bibr">(v)</ref> . Note that we assume that T (v) is "differentiable", e.g., we use bilinear interpolation and flipping. Then, the fused masks are processed by the refinement procedure R.</p><p>The whole calibration process can be formulated as:</p><formula xml:id="formula_8">M = R sof tmax v?V T (v) (M (v) ) |V| .<label>(3)</label></formula><p>Because a classic refinement algorithm such as dense CRF <ref type="bibr" target="#b26">(Kr?henb?hl and Koltun, 2011)</ref> slows down the training process, we refine the coarse masks with respect to appearance affinities through pixel-adaptive convolution <ref type="bibr" target="#b49">(Su et al., 2019;</ref><ref type="bibr" target="#b2">Araslanov and Roth, 2020)</ref>.</p><p>To generate the one-hot hard pseudo-labels, we retain the pseudo-labels of pixels with confidence higher than a threshold ?, and ignore the pixels with low confidence or those belonging to multiple categories.</p><p>Implicit and Explicit Cross-view Supervision. The pseudo-masks produced by the MVMC utilize the complementary information from different views, and hence the pseudo supervised segmentation loss in Eq. 1 implicitly enforces prediction consistency. In contrast to PseudoSeg <ref type="bibr" target="#b65">(Zou et al., 2021)</ref> and CPS , which explicitly realize cross-view supervision, the proposed approach implicitly implements cross-view consistency regularization and exploits the MVMC to filter out false pseudo-labels. Extensively investigations reveal that pure cross-view supervision is prone to cause mode collapse in unsupervised settings <ref type="bibr" target="#b8">(Chen and He, 2021;</ref><ref type="bibr" target="#b7">Chen et al., 2020)</ref>. In practice, our experiments in Sec. 4.6 also show that the dominance of explicit cross-view supervision can compromise the semantic completeness in WSSS. Therefore, a proper and adjustable cross-view supervision strength is crucial in the settings with limited supervision. Here, we define an explicit consistency loss, which is tuned by a scaling factor ? reg , to compensate for the implicit cross-view consistency:</p><formula xml:id="formula_9">L mask reg = i?D v,u?V,v =u c?yi s T (v) M (v) i,c , T (u) M(u) i,c ,<label>(4)</label></formula><p>where s(?, ?) is a distance measure between two output masks, D = D p ? D c ? D u denotes all samples, and (v, u) denotes the pairs composed of different views. We consider only the mask of category c contained in the classification label y i for the i-th example. For unlabeled sample i ? D u , all categories are involved in the computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cross-view Low-Rank Module</head><p>Although the MVMC can provide more accurate pseudo-label mask and regularization effect, there still exist a lot of clutter and incompleteness in pseudomasks. To further reduce the compounding effect of self-supervision errors, we introduce an additional inductive bias, i.e., the LR property of the cross-view deep embeddings, and formulate it into an optimization objective in terms of collective matrix factorization. <ref type="figure">Fig. 2 (right)</ref> illustrates the architecture of the CVLR module. Its essence lies in capturing the invariant LR feature representations from differently transformed views to reduce the accumulated errors introduced by distorted and noisy self-supervision.</p><p>Algorithm 1: Collective MF</p><formula xml:id="formula_10">Input: X = {X (v) ? R d?h (v) w (v) } v?V , D ? R d?k , C = {C (v) ? R k?h (v) w (v) } v?V 1 for t ? 1 to T do 2 D ? v?V X (v) C (v) S ?1 , where S = diag( v?V C (v) 1n); 3 C ? {sof tmax 1 ? D norm ? X (v) , axis = 0 } v?V , where Dnorm = d1 d1 2 ; ...; d k d k 2 ; 4 end 5X ? {X (v) = DC (v) } v?V ;</formula><p>Output:X Matrix Factorization (MF). Before introducing the proposed CVLR module, we first review the key concept of LR MF. The MF reduces a matrix into constituent parts, which disentangles latent structures in high-dimensional complex data. Given n data features of dimension d denoted as X = [x 1 ; . . . ; x n ] ? R d?n , we assume that there is a low-dimensional subspace hidden in X. Therefore, X can be decomposed into a dictionary matrix D = [d 1 ; . . . ; d k ] ? R d?k and an associated code matrix C = [c 1 ; . . . ; c n ] ? R k?n :</p><formula xml:id="formula_11">X =X + E = DC + E,<label>(5)</label></formula><p>where we denote the reconstructed feature asX ? R n?d and the noise matrix as E ? R n?d . E is discarded in the reconstruction. We assume k min(n, d), thusX has the LR property: rank(X) min(rank(D), rank(C)) k min(d, n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(6)</head><p>Vector Quantization (VQ). VQ is a classic data compression method, which can be formulated as an optimization objective in terms of MF:</p><formula xml:id="formula_12">min D,C X ? DC F s.t. c ij ? {0, 1}, i c ij = 1,<label>(7)</label></formula><p>where vector c j is a one-hot encoding, which implies a crisp partitioning.</p><p>Collective MF. We investigate the idea of collective MF to learn shared latent factors from |V| matrices of multiview features. Different views use the same LR representation as part of the approximation, enabling feature sharing and interaction. The objective function of Collective MF can be formalized as:</p><formula xml:id="formula_13">min D,{C (v) } v?V X (v) ? DC (v) F + r(C (v) ) ,<label>(8)</label></formula><p>where r is the regularization term. To minimize the objective function, we utilize the same alternating minimization method as VQ, which is also equivalent to K-means clustering <ref type="bibr">(Gray and Neuhoff, 1998)</ref>. We now describe a single iteration on a set of flattened deep feature matrices from various transformed views {X (v) ? R d?h (v) ?w (v) } v?V . We exploit the weighted mean over varied augmented features to update the shared dictionary matrix D (Line 2 in Algorithm 1), where the weight C (v) S ?1 is calculated over the global codes. Factor matrix</p><formula xml:id="formula_14">C (v) ? R k?h (v) w (v)</formula><p>is computed via the softmax-normalized attention <ref type="bibr" target="#b51">(Vaswani et al., 2017)</ref> with temperature coefficient ? instead of the hard maximum, which enables the gradients to be backpropagated. Here we normalize each vector in D and set ? = 1 by default. As described in Algorithm 1, the CVLR updates the factor matrices D and {C (v) } v?V alternately. After T iterations, the converged D and {C (v) } v?V are used to approximate input features {X (v) } v?V . As discussed above, the re-estimated matrices {X (v) } v?V are endowed with the LR property.</p><p>With the above optimization, the features of various transformed views interact with each other iteratively and are compressed into a shared dictionary D. Then, these condensed semantics are propagated into individual views in the final reconstruction step. From an intuitive perspective, as illustrated in <ref type="figure">Fig. 2</ref>, the reestimated LR features amplify and refine the related features from complementary views, eliminate ambiguity or false responses, and thus produce more complete and clean activation regions.</p><p>Training with Back-Propagation. The above optimization is differentiable with respect to its parameters, which makes it suitable to incorporate with a convolutional neural network for end-to-end training. However, the alternating minimization introduces recurrent neural network-like behavior, and randomly initialized factor matrices require multiple iterations, which will degrade performance due to the vanishing gradient in practice. One possible solution is to stop the gradient flow in the iterations to avoid gradient instability and memorize the previous D to initialize the current optimization to reduce the number of iterations T <ref type="bibr" target="#b32">(Li et al., 2019)</ref>. In contrast, the gradient flow is allowed in CVLR and factor C (v) is initialized with the feature map produced by the network. Given feature map</p><formula xml:id="formula_15">Z (v) ? R k?h (v) w (v)</formula><p>, sof tmax(Z (v) , axis = 0) can be regarded as a good initialization of C <ref type="bibr">(v)</ref> . In contrast to random initialization, the optimization swiftly converges within a small number of iterations T . Latent Space Regularization. For specific scenarios, there are diverse MF variants with various regularizations on D and C, such as non-negativity <ref type="bibr" target="#b28">(Lee and Seung, 1999)</ref>, orthogonality <ref type="bibr" target="#b12">(Ding et al., 2006)</ref> and latent space enforcement <ref type="bibr" target="#b21">(Hu et al., 2013)</ref>. Here, although multi-view features are compressed into a shared dictionary, features with the same semantic meaning may still be assigned to different elements in D. To this end, we regularize the latent space of code matrix C (v) to be the pixel-level classification space by fully exploiting the limited supervision. That is, we set the dimension k of the factor matrices to the number of categories provide auxiliary supervision on the initialization of C <ref type="bibr">(v)</ref> . In this way, each element in D is assigned a specific meaning, ensuring cross-view invariance of the reconstructed LR representations. In addition, a cross-view regularization loss on the code matrix C is designed to encourage the cross-view consistency as follows:</p><formula xml:id="formula_16">L fact reg = v,u?V,i =j s T (v) C (v) , T (u) C (u) ,<label>(9)</label></formula><p>where s(?, ?) is a similarity measure, T (v) and T (u) are inverse geometric transformations.</p><p>Detailed CVLR Module Design. <ref type="figure">Fig. 2</ref> presents the architecture of CVLR, which collaborates with the networks via two linear projectors and a skip connection. Specifically, a learnable linear transformation maps the inputs to a feature space, and another one maps the approximationX to the input space. To produce the feature Z (v) as the initialization of C (v) , the auxiliary prediction head is composed of two convolutional layers supervised by the loss functions defined in Eq. 1 and Eq. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Implementation Details. In our experiments, WideResNet-38  pre-trained on ImageNet <ref type="bibr" target="#b11">(Deng et al., 2009</ref>) and atrous spatial pyramid pooling (ASPP) <ref type="bibr" target="#b6">(Chen et al., 2018)</ref> form our encoder. The decoder consists of three convolutional layers and a stochastic gate <ref type="bibr" target="#b2">(Araslanov and Roth, 2020)</ref>, which mixes shallow and deep features. We trained our model for 20 epochs with a stochastic gradient descent optimizer using a weight decay of 5 ? 10 ?4 . The learning rate was set to 5 ? 10 ?3 for randomly initialized parameters and 5 ? 10 ?4 for pre-trained parameters. We use L1 distance as the similarity measure s(?, ?) in Eq. 4 and Eq. 9. The final loss function can be expressed as</p><formula xml:id="formula_17">Method CRF train(%) val(%)</formula><p>CAM <ref type="bibr" target="#b0">(Ahn and Kwak, 2018)</ref> 48.0 46.8 SCE <ref type="bibr" target="#b5">(Chang et al., 2020)</ref> 50.9 49.6 SEAM <ref type="bibr" target="#b54">(Wang et al., 2020b)</ref> 56.8 -CAM+RW <ref type="bibr" target="#b0">(Ahn and Kwak, 2018)</ref> 59.7 -SCE+RW <ref type="bibr" target="#b5">(Chang et al., 2020)</ref> 63.4 61.2 1-Stage <ref type="bibr" target="#b2">(Araslanov and Roth, 2020)</ref> 64.7 63.4 1-Stage <ref type="bibr" target="#b2">(Araslanov and Roth, 2020)</ref> 66 </p><formula xml:id="formula_18">L = ? seg L seg + ? cls L cls + ? reg L mask reg + ? reg L fact reg .</formula><p>In the first five epochs, the factors of the loss functions were set to ? seg = 0 , ? cls = 1, and ? reg = 4. After that, they were set to the default values ? seg = 1 , ? cls = 1 and ? reg = 4.</p><p>Evaluation Metric. We report the results in terms of the mean of the class-wise intersection over union (mIoU) for all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment I: Learning WSSS from Pascal VOC Dataset</head><p>Experimental Setup. To evaluate the effectiveness of our SLRNet for WSSS, we conduct experiments on the Pascal VOC 2012 dataset <ref type="bibr">(Everingham et al., 2010)</ref>, which is a widely-used WSSS benchmark. Following the previous standard practice, we take additional annotations from <ref type="bibr" target="#b17">(Hariharan et al., 2011)</ref> to build the augmented training set. In total, 10,582 images are used for training, and 1,449 images are kept for validation. Note that, only image-level annotations are available during weakly-supervised training.</p><p>Pseudo-mask Quality. Most of the advanced methods refine the pseudo-masks and distill them into a fullysupervised segmentation network at the last stage. Although the SLRNet does not need intermediate pseudomasks for further training, Tab. 1 compares our pseudomask quality with previous state-of-the-arts. We use image-level ground-truth to filter out false-positive errors following prior practice (for this experiment only). Our method achieves superior performance against improved CAM-generating methods <ref type="bibr" target="#b54">(Wang et al., 2020b;</ref><ref type="bibr" target="#b5">Chang et al., 2020)</ref>, multi-stage CAM-refinement methods <ref type="bibr" target="#b0">(Ahn and Kwak, 2018)</ref>, and single-stage methods <ref type="bibr" target="#b2">(Araslanov and Roth, 2020)</ref>.  , the current best-performing multi-stage model with saliency maps, our SLRNet obtains an improvement of 1.0% on the val set. Compared with SEAM+CONTA <ref type="bibr" target="#b14">(Dong et al., 2020)</ref>, that is the current best-performing multi-stage models with WideResNet38 backbone, our SLRNet achieves an mIoU improvement of 1.1%. Note that the compared multi-stage methods without saliency detection are trained in at least three stages, which improve performance at the cost of a considerable increase in model complexity. Essentially, CONTA <ref type="bibr" target="#b14">(Dong et al., 2020)</ref> is a refinement approach that employs additional networks to revise the masks produced by AffinityNet. SEAM <ref type="bibr" target="#b54">(Wang et al., 2020b)</ref> and <ref type="bibr">SCE (Chang et al., 2020)</ref> are improved CAM-generating networks which produce the CAMs as AffinityNet's input. Multi-stage methods improve performance at the cost of a considerable increase in model complexity. Our SLRNet significantly outperforms previous single-stage models with the help of simple cross-view supervision and the lightweight CVLR. Besides, although it is trivial to train an additional network for distillation, we still provide a simple distilled result for reference.</p><p>Qualitative Analysis. <ref type="figure">Fig. 3</ref> shows some typical qualitative results produced by our SLRNet. Although only trained with image-level supervision, the SLRNet successfully produces high-quality segmentation results for objects of various sizes in various scenarios. In the right side of <ref type="figure">Fig. 3</ref>, we also provide some typical failure cases with clutter or incompleteness, such as interweaving objects and misleading appearance cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment II: Learning WSSS from COCO Dataset</head><p>Experimental Setup. COCO <ref type="bibr" target="#b34">(Lin et al., 2014)</ref> contains 80 classes, 80k, and 40k images for training and validation. Although pixel-level labels are provided in the COCO dataset, we only used image-level class labels in the training process. Note that we only sample 50% (40k) of the training images for training to reduce computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Backbone Sup. val(%)test(%)</head><p>Multi stage (+saliency)</p><p>SEC <ref type="bibr" target="#b25">(Kolesnikov and Lampert, 2016)</ref> VGG16 I, S 50.7 51.7 MDC <ref type="bibr" target="#b56">(Wei et al., 2018)</ref> VGG16 I, S 60.4 60.8 DSRG  ResNet101 I, S 61.4 63.2 FickleNet <ref type="bibr" target="#b30">(Lee et al., 2019)</ref> ResNet101 I, S 64.9 65.3 CIAN  ResNet101 I, S 64.3 65.3 MCIS  ResNet101 I, S 66.2 66.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi stage</head><p>AffinityNet <ref type="bibr" target="#b0">(Ahn and Kwak, 2018)</ref> ResNet38 I 61.7 63.7 IRN <ref type="bibr" target="#b1">(Ahn et al., 2019)</ref> ResNet50 I 63.5 64.8 IAL <ref type="bibr" target="#b52">(Wang et al., 2020a)</ref> ResNet38 I 64.3 65.4 SSDD <ref type="bibr" target="#b45">(Shimoda and Yanai, 2019)</ref> ResNet38 I 64.9 65.5 SEAM <ref type="bibr" target="#b54">(Wang et al., 2020b)</ref> ResNet38 I 64.5 65.7 SCE <ref type="bibr" target="#b5">(Chang et al., 2020)</ref> ResNet101 I 66.1 65.9 CONTA <ref type="bibr" target="#b14">(Dong et al., 2020)</ref> ResNet38 I 66.1 66.7</p><p>Single stage EM-Adapt <ref type="bibr" target="#b40">(Papandreou et al., 2015)</ref>VGG16 I 38.2 39.6 MIL-LSE <ref type="bibr" target="#b43">(Pinheiro and Collobert, 2015)</ref> Overfeat I 42.0 40.6 CRF-RNN <ref type="bibr" target="#b61">(Zheng et al., 2015)</ref> VGG16 I 52.8 53.7 1-Stage <ref type="bibr" target="#b2">(Araslanov and Roth, 2020)</ref>   <ref type="table">Table 2</ref>: Results for WSSS on the Pascal VOC validation and test sets. Cues used for training are given in the "Sup." column, including image-level labels (I) and saliency detection (S). * indicates our multi-stage extension: we simply train a DeeplabV3+ network with pseudo-labels generated by SLRNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone mIoU(%)</p><p>BFBP <ref type="bibr" target="#b44">(Saleh et al., 2016)</ref> VGG16 20.4 SEC <ref type="bibr" target="#b25">(Kolesnikov and Lampert, 2016)</ref> VGG16 22.4 IAL <ref type="bibr" target="#b52">(Wang et al., 2020a)</ref> ResNet38 27.7 SEAM * <ref type="bibr" target="#b54">(Wang et al., 2020b)</ref> ResNet38 31.9 IRNet * <ref type="bibr" target="#b1">(Ahn et al., 2019)</ref> ResNet50 32.6 SEAM+CONTA <ref type="bibr" target="#b14">(Dong et al., 2020)</ref>      is built upon the object detection track of ImageNet Large Scale Visual Recognition Competition (ILSVRC) <ref type="bibr" target="#b11">(Deng et al., 2009)</ref>, which contains 349,319 images with image-level labels from 200 categories. The evaluation is performed on the validation and test sets, which include 4,690 and 10,000 images, respectively. We obtain the pseudo-labels using our single-stage model with mIoU=52.5% on the validation set. Following the previous practice, we train a standalone Deeplabv3+ <ref type="bibr" target="#b6">(Chen et al., 2018)</ref> network using pseudo-labels generated by our SLRNet.</p><p>Experimental Results. Tab. 4 lists the final results with the mIoU criterion for WSSS track of L2ID challenge, where the top performing methods are included. Our model significantly outperforms the winner of LID 2019, which utilizes saliency maps. In contrast to the champion of LID 2020, the SLRNet is trained in only one cycle to generate pseudo-labels, while the winner integrates the equivariant attention <ref type="bibr" target="#b54">(Wang et al., 2020b)</ref> and the co-attention  to train the classification network, and use the online attention accumulation <ref type="bibr" target="#b24">(Jiang et al., 2019)</ref> to generate object localization maps. Besides, it trains the AffinityNet to refine GAIN <ref type="bibr" target="#b31">(Li et al., 2018)</ref> VGG16 I, P, S 60.5 62.1 DSRG  VGG16 I, P, S 64.3 -MDC <ref type="bibr" target="#b56">(Wei et al., 2018)</ref> VGG16 I, P, S 65.7 67.6 FickleNet <ref type="bibr" target="#b30">(Lee et al., 2019)</ref> VGG16 I, P, S 65.8 -GANSeg <ref type="bibr" target="#b47">(Souly et al., 2017)</ref> VGG16 I, P 65.8 -CCT <ref type="bibr" target="#b39">(Ouali et al., 2020)</ref> ResNet50 I, P 73.2 -PseudoSeg <ref type="bibr" target="#b65">(Zou et al., 2021)</ref>    <ref type="bibr">(Everingham et al., 2010)</ref>, which is a standard benchmark of SSSS. Following the prior practice, we took 1,449 images with pixellevel labels from the VOC training set and an additional 9,133 images with image-level labels from SBD <ref type="bibr" target="#b17">(Hariharan et al., 2011)</ref> to construct the augmented training set. Note that the finely labeled and weakly labeled data are mixed and fed to the network in one training cycle. Intuitively, the finely labeled samples deserve larger weights. We oversample the finely-labeled data by 5? and multiply their loss by a scaling factor of 2. We do not use any additional post-processing methods during testing.</p><p>Experimental Results. Tab. 5 lists the comparison results of that our SLRNet to a variety of state-of-the-art methods on the Pascal VOC dataset, where our SLR-Net is trained on only 13.8% of images with pixel-level annotations. It achieves an mIoU of 75.1%, which significantly surpasses the WSSS-based methods <ref type="bibr" target="#b56">Wei et al., 2018;</ref><ref type="bibr" target="#b30">Lee et al., 2019)</ref>, GANbased methods <ref type="bibr" target="#b47">(Souly et al., 2017)</ref>, and consistencybased methods <ref type="bibr" target="#b39">(Ouali et al., 2020;</ref><ref type="bibr" target="#b65">Zou et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experiment V: SSSS with Pixel-level Labeled Dataset and Unlabeled Dataset</head><p>Method Backbone mIoU (%)</p><p>GANSeg <ref type="bibr" target="#b47">(Souly et al., 2017)</ref> VGG16 64.1 AdvSemSeg <ref type="bibr" target="#b23">(Hung et al., 2018)</ref> ResNet101 68.4 CCT <ref type="bibr" target="#b39">(Ouali et al., 2020)</ref> ResNet50 69.4 PseudoSeg <ref type="bibr" target="#b65">(Zou et al., 2021)</ref> ResNet101 <ref type="formula" target="#formula_6">72</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study</head><p>We conduct ablation experiments on the Pascal VOC dataset for WSSS settings. To demonstrate the improvement source of our SLRNet, we use mean false discovery rate (mFDR) and mean false negative rate (mFNR) to indicate the semantic fidelity and completeness respectively, which are defined as</p><formula xml:id="formula_19">mF DR = 1 C C c=1 F P c T P c + F P c ,<label>(10)</label></formula><p>and</p><formula xml:id="formula_20">mF N R = 1 C C c=1 F N c T P c + F N c ,<label>(11)</label></formula><p>where T P c , F P c , and F N c denote the number of true positive, false positive and false negative predictions of class c respectively.  <ref type="table">Table 7</ref>: Evaluation on different compositions of data augmentation applied on multiple branches. Here, means that multiple views use the different random transformations (e.g. for color distortion, randomly different distortion strength for different views). No CRF or any other post-processing is used.</p><p>Data Augmentation. To understand the effect of individual data augmentation for our SLRNet, we consider several geometric and appearance augmentations in . Furthermore, we pay more attention to the invertible and differentiable geometric transformations, such as resizing and flipping. First, the images are randomly cropped to 321 ? 321. Then, we apply target transformations to different branches. We study the compositions of three kinds of transformations: re-scaling with fixed rates, random horizontal flip and random color distortion (e.g. brightness, contrast, saturation, and hue). Stronger color distortion cannot improve even hurts performance under supervised settings <ref type="bibr" target="#b30">Lee et al., 2021)</ref>. Therefore, we set the maximum strength of color distortion to 0.3 for brightness, contrast, and saturation, and 0.1 for hue component.</p><p>Tab. 7 lists evaluation results on the the Pascal VOC val set under different compositions of transformations. We observe that combination of three different augmentations produces the best performance (64.07%). When composing more augmentations, cross-view supervision is expected to become much stronger. We also note that re-scaling contributes a significantly greater improvement than other augmentations. There is a significant mIoU drop (2.54%) without re-scaling. In contrast, using the same color distortion and flipping for different views leads to a slight mIoU drop (0.08%). The combination of different color distortion and flipping only achieves a minor improvement (0.65%) compared with single-view baseline. Furthermore, it is worth pointing out that although adding more views has higher complexity, this cannot improve the performance. This indicates that the crucial improvement sources are our cross-view design and the LR reconstruction, which require simple augmentations to provide perturbations.  .</p><p>Cross-view Supervision. Cross-view supervision aims to mitigate the compounding effect of self-supervision errors by introducing additional regularization. We adjust the loss coefficient ? reg that explicitly controls the strength of cross-view supervision. As shown in <ref type="figure">Fig. 4</ref>, we observe that cross-view supervision mainly improves the segmentation quality by reducing mF DR, i.e., preventing the accumulation of false positives in self-supervision to improve the semantic fidelity. This improvement is maximized with ? reg = 4 in our experiments. (For clarification, the scale of L reg is much smaller than L seg .) It is worth pointing out that higher strength of cross-view supervision increases mF N R, i.e., hurts semantic completeness. Collapsing solutions, where the predictions are all "background", will occur when explicit cross-view supervision dominates <ref type="bibr" target="#b8">(Chen and He, 2021;</ref><ref type="bibr" target="#b5">Caron et al., 2020)</ref>. Therefore in the task with limited supervision, choosing appropriate cross-view strength is the crux of performance improvement.</p><p>To further validate efficacy of the proposed implicit cross-view supervision, we also compare the MVMC with other cross-view supervision mechanisms. As shown in <ref type="figure">Fig. 4 (right)</ref>, we substitute the cross pseudo supervision  or Kullback-Leibler divergence for MVMC, achieving much worse results. Generally, these explicit consistency methods are applied to SSSS that require pixel-level supervision to avoid collapsing solutions <ref type="bibr" target="#b8">(Chen and He, 2021)</ref>.</p><p>The CVLR Module. To study the effectiveness of each part in the CVLR, we conduct thorough ablation experiments on it. Firstly, as listed in Tab. 8, substituting the shared dictionary matrix with separate ones causes the most severe decay in performance, attesting the significance of the cross-view LR mechanism.  <ref type="table">Table 8</ref>: Ablation on shared dictionary D and latent space regularization. We also provide comparisons between CVLR and other LR modules in terms of iterations (#Iter.) and back-propagation gradients (Grad.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image GT</head><p>C k,? Cperson,? C bg,? <ref type="figure">Fig. 5</ref>: Visualization of code C k,? for specific item in dictionary D.</p><p>Second, the latent space regularization also contributes considerable performance improvement, as it further enforces the semantic stability and consistency of the elements in the shared dictionary and reduces optimization steps. Furthermore, Tab. 8 compares the CVLR with the EMAU <ref type="bibr" target="#b32">(Li et al., 2019) and</ref><ref type="bibr">the Hamburger (Geng et al., 2021)</ref> modules that learn single-view LR representations with specific designs, demonstrating advantages of the proposed CVLR.</p><p>To understand and verify the behavior of CVLR, as in <ref type="figure">Fig. 2 (right)</ref>, we visualize the feature map before and after the module. The CVLR emphasizes and refines the relevant features from complementary views through reducing ambiguous activation and completing regions. Meanwhile, the proposed latent space regularization allows a close look at the collective matrix factorization, making the factor matrices well interpretable. <ref type="figure">Fig. 5</ref> visualizes the optimal factor matrix C (single view is selected) that represents the corresponding probabilities of all pixels to the selected k-th element in the dictionary. The code matrix of specific categories completely highlights the corresponding semantic regions and finely delineates the boundaries.</p><p>Another experiment is conducted to explore the impact of hyper-parameters in the CVLR module,  <ref type="figure">Fig. 7</ref>: Visualization of factor C k,? with different iteration number T . The first two rows are from the early training step (randomly selected, epoch=7) and the last two rows are from the last training step (epoch=20).</p><p>including optimization step T and latent space dimension d. As shown in <ref type="figure" target="#fig_2">Fig. 6 (left)</ref>, we observe that the iteration steps improve the segmentation performance mainly by reducing mF N R, i.e., improving the semantic completeness. We also find that single-step optimization is enough and more iterations cannot improve performance. To empirically analyze this observation, <ref type="figure">Fig. 7</ref> visualizes the factor matrix C k,? with different number of iterations at different training step. More iterations yield evident effects in the early epochs, while becoming insignificant as the network is converged. Inspired by SimSiam <ref type="bibr" target="#b8">(Chen and He, 2021)</ref>, we conjecture that the multi-step alternation (inner loop) is optional because the SGD steps provide much more outer loops and the optimization trajectory of the collective MF is memorized by the network parameters. Furthermore, as illustrated in <ref type="figure" target="#fig_2">Fig. 6 (right)</ref>, either too small or too large latent space dimension will negatively impact the segmentation performance and d = 256 is the optimal choice in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper has presented a simple yet effective SLRNet for single-stage WSSS and SSSS under online pseudolabel supervision. To overcome the compounding effect of self-supervision errors, we have developed a Siamese network based architecture that makes full use of crossview supervision and the LR property of the features. Specifically, we have designed the MVMC that aids with explicit cross-view consistency to provide a flexible cross-view supervision solution. Then, we have built a lightweight CVLR module that can be readily integrated into the network for end-to-end training. The CVLR learns an interpretable global cross-view LR representation, which complements cross-view supervision to improve semantic completeness while ensuring semantic fidelity. Despite its simplicity, extensive evaluations have demonstrated that the proposed SLRNet has achieved favorable performance superior to that of both leading single-and multi-stage WSSS and SSSS methods in terms of complexity and accuracy. Our future work is to extend the proposed single-stage model to other label-efficient tasks without the need of considerable training cycles and post-processing techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Overview of pseudo supervision architectures: (I) Single pseudo supervision for WSSS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Ablation on the iteration number T of CVLR and dimension d of the dictionary matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Qualitative results on Pascal VOC 2012 val set. left: simple cases. right: cases with clutter or incompleteness. The SLRNet produces high-quality segmentation results for challenging scenes with varying object sizes and semantic contents.</figDesc><table><row><cell>Ours</cell><cell>Ours+CRF</cell><cell>Ground-truth</cell><cell>Ours</cell><cell>Ours+CRF</cell><cell>Ground-truth</cell></row><row><cell cols="3">Fig. 3: Experimental Results. Tab. 2 compares the SLRNet</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">with a variety of leading single-and multi-stage WSSS</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">methods. Among them, the single-stage SLRNet</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">achieves the best performance on both val (67.2%)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">and test (67.6%) sets. Compared with MCIS</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results for WSSS on COCO validation set. * denotes results provided by<ref type="bibr" target="#b14">Dong et al. (2020)</ref>.</figDesc><table><row><cell>Experimental Results. Tab. 3 compares our approach</cell></row><row><cell>and current top-leading WSSS methods with image-</cell></row><row><cell>level supervision on the COCO dataset. We can observe</cell></row><row><cell>that our method achieves mIoU score of 35.0% on</cell></row><row><cell>the val set, outperforming all the competitors. When</cell></row><row><cell>compared with IRNet+CONTA (Dong et al., 2020),</cell></row><row><cell>the current best-performing method, our approach</cell></row><row><cell>obtains the improvement of 1.6% with 50% training</cell></row><row><cell>samples and simple single-stage training. Our SLRNet</cell></row><row><cell>demonstrates a powerful efficiency and efficacy advan-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Experimental results for WSSS on L2ID vali-</cell></row><row><cell>dation and test set.</cell></row><row><cell>tage when training on large-scale datasets. In contrast,</cell></row><row><cell>most recent approaches, including SEAM (Wang et al.,</cell></row><row><cell>2020b), IRNet (Ahn et al., 2019) and CONTA (Dong</cell></row><row><cell>et al., 2020), require training in three or four stages</cell></row><row><cell>and search a large number of hyperparameters for each</cell></row><row><cell>stage. Moreover, the intermediate results of each stage</cell></row><row><cell>must be stored on the hard disk which means a huge</cell></row><row><cell>amount of space and time consumption for large-scale</cell></row><row><cell>datasets, severely reducing the practicability of WSSS.</cell></row><row><cell>4.3 Experiment III: Performance on the WSSS Track</cell></row><row><cell>of the L2ID Challenge</cell></row><row><cell>Experimental Setup. The L2ID challenge dataset</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Experimental results for semi-supervised se-</cell></row><row><cell>mantic segmentation on Pascal VOC validation and</cell></row><row><cell>test set with 9k image-level labeled data (I) and 1.4k</cell></row><row><cell>pixel-level labeled data (P). We also indicate additional</cell></row><row><cell>saliency supervision (S).</cell></row><row><cell>the pseudo-labels, and leverages the CRF to refine the</cell></row><row><cell>final segmentation results. Our model achieves 52.3%</cell></row><row><cell>mIoU on the validation set and 49.03% mIoU on the</cell></row><row><cell>test set, respectively, setting a new state-of-the-art on</cell></row><row><cell>the L2ID challenge through a simple framework.</cell></row><row><cell>4.4 Experiment IV: SSSS with Image-level Labeled</cell></row><row><cell>Dataset and Pixel-level Labeled Dataset</cell></row><row><cell>Experimental Setup. To evaluate the proposed method</cell></row><row><cell>in the semi-supervised setting, we conduct experiments</cell></row><row><cell>on the Pascal VOC 2012 dataset</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Experimental results for semi-supervised se-</cell></row><row><cell>mantic segmentation on the Pascal VOC validation set</cell></row><row><cell>with 1.4k pixel-level labeled data (P) and 9k unlabeled</cell></row><row><cell>data.</cell></row><row><cell>Experimental Setup. We also conduct experiments on</cell></row><row><cell>the Pascal VOC 2012 dataset (Everingham et al., 2010)</cell></row><row><cell>using 1,449 pixel-level labeled images and an additional</cell></row><row><cell>9,133 unlabeled images.</cell></row><row><cell>Experimental Results. Tab. 6 compares the perfor-</cell></row><row><cell>mance of our SLRNet with previous state-of-the-art</cell></row><row><cell>SSSS methods. It is worth pointing out that we exactly</cell></row><row><cell>use the same network and hyperparameters as in</cell></row><row><cell>the image-level WSSS setting. Notwithstanding, the</cell></row><row><cell>SLRNet still outperforms all the other dedicated SSSS</cell></row><row><cell>models, illustrating the good versatility and generality</cell></row><row><cell>of our approach.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="4981" to="4990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Weakly supervised learning of instance segmentation with inter-pixel relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2209" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Single-stage semantic segmentation from image labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4252" to="4261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unifying nuclear norm and bilinear factorization approaches for low-rank matrix decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bernardino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="2488" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A ; Neurips</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="8988" to="8997" />
		</imprint>
	</monogr>
	<note>Weakly-supervised semantic segmentation via sub-category exploration</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="833" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semisupervised semantic segmentation with cross pseudo supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">BoxSup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Orthogonal nonnegative matrix t-factorizations for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Causal intervention for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hanwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jinhui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiansheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qianru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ljv</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cki</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Everingham M</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>IJCV</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">CIAN: cross-image affinity net for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="10762" to="10769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Is attention better than matrix decomposition? In: ICLR Gray R, Neuhoff D (1998) Quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename><forename type="middle">T</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Bmvc Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2325" to="2383" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Semi-supervised semantic segmentation needs strong, varied perturbations</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Anal</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-erasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="547" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised sentiment analysis with emotional signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd International World Wide Web Conference, WWW &apos;13</title>
		<meeting><address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05-13" />
			<biblScope unit="page" from="607" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="7014" to="7023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adversarial learning for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Integral object mining via online attention accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV, IEEE</publisher>
			<biblScope unit="page" from="2070" to="2079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="695" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="issue">6755</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">FickleNet: Weakly and semi-supervised semantic image segmentation using stochastic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J ; Neurips</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><forename type="middle">E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="5267" to="5276" />
		</imprint>
	</monogr>
	<note>Improving transferability of representations via augmentationaware self-supervision</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Tell me where to look: Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="9215" to="9223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Expectation-maximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="9166" to="9175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Scribble-Sup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Sparse representation for face recognition based on discriminative low-rank dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR, IEEE</publisher>
			<biblScope unit="page" from="2586" to="2593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised learning of dense visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>O Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benmalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Golemo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<editor>Larochelle H, Ranzato M, Hadsell R, Balcan MF, Lin H</editor>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4489" to="4500" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation with cross-consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="12671" to="12681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pho</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1713" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Built-in foreground/background prior for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Msa</forename><surname>Akbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ECCV</title>
		<imprint>
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page" from="413" to="432" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self-supervised difference detection for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shimoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5207" to="5216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<editor>Larochelle H, Ranzato M, Hadsell R, Balcan M, Lin H</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semi supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5689" to="5697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Multiple frames matching for object discovery in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Stretcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<editor>Xie X, Jones MW, Tam GKL</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="186" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Pixel-adaptive convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="11166" to="11175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Mining crossimage semantics for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="347" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Convolutional neural networks with low-rank regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E ; Iclr</forename><surname>Weinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NeurIPS</publisher>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation by iterative affinity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1736" to="1749" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Dense contrastive learning for self-supervised visual pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3024" to="3033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="12272" to="12281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="6488" to="6496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="7268" to="7277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">LID 2020: The learning from imperfect data challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dobko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Viniavskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dobosevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">11724</biblScope>
		</imprint>
	</monogr>
	<note>CoRR abs/2010.11724</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the ResNet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Detco: Unsupervised contrastive learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="8392" to="8401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="16684" to="16693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lapedriza?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="321" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Rethinking pre-training and self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<editor>Larochelle H, Ranzato M, Hadsell R, Balcan M, Lin H</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Pseudoseg: Designing pseudo labels for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
