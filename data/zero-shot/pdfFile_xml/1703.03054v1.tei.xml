<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<email>xiaodan1@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Lee</surname></persName>
							<email>lslee@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
							<email>epxing@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite progress in visual perception tasks such as image classification and detection, computers still struggle to understand the interdependency of objects in the scene as a whole, e.g., relations between objects or their attributes. Existing methods often ignore global context cues capturing the interactions among different object instances, and can only recognize a handful of types by exhaustively training individual detectors for all possible relationships. To capture such global interdependency, we propose a deep Variation-structured Reinforcement Learning (VRL) framework to sequentially discover object relationships and attributes in the whole image. First, a directed semantic action graph is built using language priors to provide a rich and compact representation of semantic correlations between object categories, predicates, and attributes. Next, we use a variation-structured traversal over the action graph to construct a small, adaptive action set for each step based on the current state and historical actions. In particular, an ambiguity-aware object mining scheme is used to resolve semantic ambiguity among object categories that the object detector fails to distinguish. We then make sequential predictions using a deep RL framework, incorporating global context cues and semantic embeddings of previously extracted phrases in the state vector. Our experiments on the Visual Relationship Detection (VRD) dataset and the large-scale Visual Genome dataset validate the superiority of VRL, which can achieve significantly better detection results on datasets involving thousands of relationship and attribute types. We also demonstrate that VRL is able to predict unseen types embedded in our action graph by learning correlations on shared graph nodes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Despite progress in visual perception tasks such as image classification and detection, computers still struggle to understand the interdependency of objects in the scene as a whole, e.g., relations between objects or their attributes. Existing methods often ignore global context cues capturing the interactions among different object instances, and can only recognize a handful of types by exhaustively training individual detectors for all possible relationships. To capture such global interdependency, we propose a deep Variation-structured Reinforcement Learning (VRL) framework to sequentially discover object relationships and attributes in the whole image. First, a directed semantic action graph is built using language priors to provide a rich and compact representation of semantic correlations between object categories, predicates, and attributes. Next, we use a variation-structured traversal over the action graph to construct a small, adaptive action set for each step based on the current state and historical actions. In particular, an ambiguity-aware object mining scheme is used to resolve semantic ambiguity among object categories that the object detector fails to distinguish. We then make sequential predictions using a deep RL framework, incorporating global context cues and semantic embeddings of previously extracted phrases in the state vector. Our experiments on the Visual Relationship Detection (VRD) dataset and the large-scale Visual Genome dataset validate the superiority of VRL, which can achieve significantly better detection results on datasets involving thousands of relationship and attribute types. We also demonstrate that VRL is able to predict unseen types embedded in our action graph by learning correlations on shared graph nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Although much progress has been made in image classification <ref type="bibr" target="#b6">[7]</ref>, detection <ref type="bibr" target="#b19">[20]</ref> and segmentation <ref type="bibr" target="#b14">[15]</ref>, we are still far from reaching the goal of holistic scene understanding-that is, a model capable of recognizing the interactions and relationships between objects, and describing their attributes. While objects are the core building blocks of an image, it is often the relationships and attributes that determine the holistic interpretation of the scene. For example in <ref type="figure" target="#fig_0">Fig. 1</ref>, the left image can be understood as "a man standing on a yellow and green skateboard", and the right image as "a woman wearing a blue wet suit and kneeling on a surfboard". Being able to extract and exploit such visual information would benefit many realworld applications such as image search <ref type="bibr" target="#b18">[19]</ref>, question answering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref>, and fine-grained recognition <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b3">4]</ref>. Visual relationships are a pair of localized objects connected via a predicate; for example, predicates can be actions ("kick"), comparative ("smaller than"), spatial ("near to"), verbs ("wear"), or prepositions ("with"). Attributes describe a localized object, e.g., with color ("yellow") or state ("standing"). Detecting relationships and attributes is more challenging than traditional object detection <ref type="bibr" target="#b19">[20]</ref> due to the following reasons: (1) There are a massive number of possible relationship and attribute types (e.g., 13,894 relationship types in Visual Genome <ref type="bibr" target="#b12">[13]</ref>), resulting in a greater skew of rare and infrequent types. (2) Each object can be associated with many relationships and attributes, making it inefficient to exhaustively search all possible relationships  <ref type="figure">Figure 2</ref>. An overview of the VRL framework that sequentially detects relationships ("subject-predicate-object") and attributes ("subjectattribute"). First, we build a directed semantic action graph G to configure the whole action space. In each step, the input state consists of the current subject and object instances ("sub:man", "obj:skateboard") and a history phrase embedding, which captures the search paths that have already been traversed by the agent. A variation-structured traversal scheme over G dynamically constructs three small action sets ?a, ?p, ?c. The agent predicts three actions: (1) ga ? ?a, an attribute of the subject; (2) gp ? ?p, the predicate between the subject and object; and (3) gc ? ?c, the next object category of interest ("obj:helmet"). The new state consists of the new subject/object instances ("sub:man", "obj:helmet") and an updated history phrase embedding.</p><p>for each pair of objects. (3) A global, holistic perspective of the image is essential to resolve semantic ambiguities (e.g., "woman wearing wetsuit" vs. "woman wearing shirt"). Existing approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16]</ref> only predict a limited set of relationship types (e.g., 13 in Visual Phrase <ref type="bibr" target="#b21">[22]</ref>) and ignore semantic interdependencies between relationships and attributes by evaluating each region within a scene separately <ref type="bibr" target="#b15">[16]</ref>. It is impractical to exhaustively search all possibilities for each region, and also deviates from human perception. Therefore, it is preferable to have a more principled decision-making framework, which can discover all relevant relationships and attributes within a small number of search steps. To address the aforementioned issues, we propose a deep Variation-structured Reinforcement Learning (VRL) framework which sequentially detects relationship and attribute instances by exploiting global context cues.</p><p>First, we use language priors to build a directed semantic action graph G, where the nodes are nouns, attributes, and predicates, connected by directed edges that represent semantic correlations (see <ref type="figure">Fig. 2</ref>). This graph provides a highly-informative, compact representation that enables the model to learn rare relationships and attributes from frequent ones using shared graph nodes. For example, the semantic meaning of "riding" learned from "person-ridingbicycle" can help predict the rare phrase "child-ridingelephant". This generalizing ability allows VRL to handle a considerable number of possible relationship types.</p><p>Second, existing deep reinforcement learning (RL) models <ref type="bibr" target="#b23">[24]</ref> often require several costly episodes of trial and error to converge, even with a small action space, and our large action space would exacerbate this problem. To efficiently discover all relationships and attributes in a small number of steps, we introduce a novel variation-structured traversal scheme over the action graph which constructs small, adaptive action sets ? a , ? p , ? c for each step based on the current state and historical actions: ? a contains candidate attributes to describe an object; ? p contains candidate predicates for relating a pair of objects; and ? c contains new object instances to mine in the next step. Since an object instance may belong to multiple object categories which the object detector cannot distinguish, we introduce an ambiguity-aware object mining scheme to assign each object with the most appropriate category given the global scene context. Our variation-structured traversal scheme offers a very promising technique for extending the applications of deep RL to complex real-world tasks.</p><p>Third, to incorporate global context cues for better reasoning, we explicitly encode the semantic embeddings of previously extracted phrases in the state vector. It makes a better tradeoff between increasing the input dimension and utilizing more historical context, compared to appending history frames <ref type="bibr" target="#b27">[28]</ref> or binary action vectors <ref type="bibr" target="#b1">[2]</ref> as in previous RL methods.</p><p>Extensive experiments on the Visual Relationship Detection (VRD) dataset <ref type="bibr" target="#b15">[16]</ref> and Visual Genome dataset <ref type="bibr" target="#b12">[13]</ref> demonstrate that the proposed VRL outperforms state-ofthe-art methods for both relationship and attribute detection, and also has good generalization capabilities for predicting unseen types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Visual relationship and attribute detection. There has been an increased interest in the problem of visual relationship detection <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13]</ref>. However, most existing approaches <ref type="bibr" target="#b21">[22]</ref>  <ref type="bibr" target="#b12">[13]</ref> can detect only a handful of pre-defined, frequent types by training individual detectors for each relationship. Recently, Lu et al. <ref type="bibr" target="#b15">[16]</ref> leveraged word embeddings to handle large-scale relationships. However, their model still ignores the structured correlations between objects and relationships. Furthermore, some methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b13">14]</ref> organized predictions into a scene graph which can provide a structured representation for describing the objects, their attributes and relationships in each image. In particular, Johnson et al. <ref type="bibr" target="#b9">[10]</ref> introduced a conditional random field model for reasoning about possible groundings of scene graphs while Schuster et al. <ref type="bibr" target="#b22">[23]</ref> proposed a rule-based and classifier-based scene graph parser. In contrast, the proposed VRL makes the first attempt to sequentially discover objects, relationships and attributes by fully exploiting global interdependency.</p><p>Deep reinforcement learning. Integrating deep learning methods with reinforcement learning (RL) <ref type="bibr" target="#b10">[11]</ref> has recently shown very promising results on decision-making problems. For example, Mnih et al. <ref type="bibr" target="#b17">[18]</ref> proposed using deep Q-networks to play ATARI games. Silver et al. <ref type="bibr" target="#b23">[24]</ref> proposed a new search algorithm based on the integration of Monte-Carlo tree search with deep RL, which beat the world champion in the game of Go. Other efforts applied deep RL to various real-world tasks, e.g., robotic manipulation <ref type="bibr" target="#b5">[6]</ref>, indoor navigation <ref type="bibr" target="#b27">[28]</ref>, and object proposal generation <ref type="bibr" target="#b1">[2]</ref>. Our work deals with real-world scenes that are much more complex than ATARI games or images taken in some constrained scenarios, and investigates how to make decisions over a larger action space (e.g., thousands of attribute types). To handle such a large action space, we propose a variation-structured traversal scheme over the whole action graph to decrease the number of possible actions in each step, which substantially reduces the number of trials and thus speeds up the convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Variation-structured Reinforcement Learning</head><p>We propose a novel VRL framework which formulates the problem of detecting visual relationships and attributes as a sequential decision-making process. An overview is provided in <ref type="figure">Fig. 2</ref>. The key components of VRL, including the directed semantic action graph, the variation-structured traversal scheme, the state space, and the reward function, are detailed in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Directed Semantic Action Graph</head><p>We build a directed semantic graph G = (V, E) to organize all possible object nouns, attributes, and relationships into a compact and semantically meaningful representation (see <ref type="figure">Fig. 2</ref>). The nodes V consist of the set of all candidate object categories C, attributes A, and predicates P. Object categories in C are nouns, and may be people, places, or parts of objects. Attributes in A can describe color, shape, or pose. Relationships are directional, i.e. they relate a subject noun and an object noun via a predicate. Predicates in P can be spatial (e.g., "inside of"), compositional (e.g. "part of") or action (e.g., "swinging").</p><p>The directed edges E consist of attribute phrases E A ? C ? A and predicate phrases E P ? C ? P ? C. An attribute phrase (c, a) ? E A represents an attribute a ? A belonging to a noun c ? C. For example, the attribute phrase "young girl" can be represented by ("girl", "young") ? E A . A predicate phrase (c, p, c ) ? E P represents a subject noun c ? C and an object noun c ? C related by a predicate p ? P . For example, the predicate phrase "a man is swinging a bat" can be represented by ("man", "swinging", "bat") ? E P .</p><p>The recently released Visual Genome dataset <ref type="bibr" target="#b12">[13]</ref> provides a large-scale annotation of images containing 18,136 unique object categories, 13,041 unique attributes, and 13,894 unique relationships. We then select the types that appear at least 30 times in Visual Genome dataset, resulting in 1,750 object-, 8,561 attribute-, and 13,823 relationshiptypes. From these attribute and relationship types, we build a directed semantic action graph by extracting all unique object category words, attribute words, and predicate words as the graph nodes. Our directed action graph thus contains |C| = 1750 object nodes, |A| = 1049 attribute nodes, and |P| = 347 predicate nodes. On average, each object word is connected to 5 attribute words and 15 predicate words. This semantic action graph serves as the action space for VRL, as we will see in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Variation-structured RL</head><p>Instead of learning in the entire action space as in traditional deep RL <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>, we propose a novel variationstructured traversal scheme over the semantic action graph that dynamically constructs small action sets for each step.</p><p>First, VRL uses an object detector to get a set S of candidate object instances, and then sequentially assigns relationships and attributes to each instance s ? S. For our experiments, we used state-of-the-art Faster R-CNN <ref type="bibr" target="#b19">[20]</ref> as the object detector, where the network parameters were initialized using the pre-trained VGG-16 ImageNet model <ref type="bibr" target="#b24">[25]</ref>.</p><p>Since subject instances in an image often have multiple relationships and attributes, we do a breadth-first search: we predict all relationships and attributes with respect to the current subject instance of interest, and then move onto the next instance. We start from the subject instance with the most confident classification score. To prevent the agent from being trapped in a single search path (e.g., in a small local region), the agent selects a new starting subject instance if it has traversed through 5 neighboring objects in the breadth-first search.</p><p>The same object in multiple scenarios may be described  The state vector f is a concatenation of (1) a 4096-dim feature of the whole image, taken from the fc6 layer of the pre-trained VGG-16 ImageNet model <ref type="bibr" target="#b24">[25]</ref>; (2) two 4096-dim features of the subject s and object s instances, taken from the conv5 3 layer of the trained Faster R-CNN object detector; and (3) a 9600-dim history phrase embedding, which is created by concatenating four 2400-dim semantic embeddings from a Skip-thought language model <ref type="bibr" target="#b11">[12]</ref> of the last two relationship phrases (relating s and s ) and the last two attribute phrases (describing s) that were predicted by VRL. A variation-structured traversal scheme over the directed semantic action graph produces a smaller action space from the whole action space, which originally consists of |A| = 1049 attributes, |P| = 347 predicates, and |C| = 1750 object categories plus one terminal trigger. From this variation-structured action space, the model selects actions with the highest predicted Q-values in state f .  by different, semantically ambiguous noun categories that cannot be distinguished by the object detector. To address this semantic ambiguity, we introduce an ambiguity-aware object mining scheme which leverages scene contexts captured by extracted relationships and attributes to help determine the most appropriate object category.</p><p>Variation-structured action space. The directed semantic graph G serves as the action space for VRL. For any object instance s ? S in an image, denote its object category by s c ? C and its bounding box by B(s) = (s x , s y , s w , s h ) where (s x , s y ) is the center coordinate, s w is the width, and s h is the height. Given the current subject instance s and object instance s , we select three actions g a ? A, g p ? P, g c ? C according to the VRL network as follows:</p><p>(1) Select an attribute g a describing s from the set ? a = {a : (s c , a) ? E A \H A (s)}, where H A (s) denotes the set of previously mined attribute phrases for s.</p><p>(2) Select a predicate g p relating the subject noun s c and object noun s c from ? p = {p : (s c , p, s c ) ? E P }.</p><p>(3) To select the next object instances ? S in the image, we select its corresponding object category g c from a set ? c ? C, which is constructed using an ambiguity-aware object mining scheme as follows (also illustrated in <ref type="figure">Fig. 5</ref>). Let N (s) ? S be the set of objects neighboring s, where a neighbor of s is defined to be any objects ? S such that |s x ? s x | &lt; 0.5(s w + s w ) and |s y ? s y | &lt; 0.5(s h + s h ). For each objects, let C(s) ? C be the set of object categories ofs whose confidence scores are at most 0.1 less than that of the most confident category. Let ? c = s?N (s)\H S C(s) ? {Terminal}, where H S is the set of previously extracted object instances and Terminal is a terminal trigger indicating the end of the object mining scheme for this subject instance. If N (s)\H s is empty or the terminal trigger is activated, then we select a new subject instance following the breadth-first scheme. The terminal trigger allows the number of object mining steps for each subject instance to be dynamically specified and limited to a small number.</p><p>In each step, the VRL selects actions from the adaptive action sets ? a , ? p , and ? c , which we call the variationstructured action space due to their dynamic structure.</p><p>State space. A detailed overview of the state feature extraction process is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Given the current subject s and object s instances in each time step, the state vector f is a concatenation of (1) the feature vectors of s and s ;</p><p>(2) the feature vector of the whole image; and (3) a history phrase embedding vector, which is created by concatenating the semantic embeddings of the last two relationship phrases (relating s and s ) and the last two attribute phrases (describing s) that were mined via the variation-structured traversal scheme. More specifically, each phrase (e.g., "person riding bicycle") is embedded into a 2400-dim vector using a pre-trained Skip-thought language model <ref type="bibr" target="#b11">[12]</ref>, thus resulting in a 9600-dim history phrase embedding.</p><p>The feature vector of the whole image provides global  <ref type="figure">Figure 5</ref>. Illustration of ambiguity-aware object mining. The image on the left shows the subject instance (red box) and its neighboring object instances (green boxes). The action set ?c contains candidate object categories of each neighboring object which the object detector cannot distinguish (e.g., "hat" vs. "helmet"), and a terminal trigger indicating the end of the object mining scheme for this subject instance.</p><p>context cues which not only help in recognizing relationships and attributes, but also allow the agent to be aware of other uncovered objects. The history phrase embedding captures the search paths and scene contexts that have already been traversed by the agent.</p><p>Rewards: Suppose we have groundtruth labels, which consist of the set? of object instances in the image, and attribute phrases? A and predicate phrases? P describing the objects in?. Given a predicted object instance s ? S, we say that a groundtruth object? ?? overlaps with s if they have the same object category (i.e., s c =? c ? C), and their bounding boxes have at least 0.5 Intersection-over-Union (IoU) overlap.</p><p>We define the following reward functions to reflect the detection accuracy of taking action (g a , g p , g c ) in state f , where the current subject and object instances are s and s , respectively:</p><p>(1) R a (f , g a ) returns +1 if there exists a groundtruth object? ?? that overlaps with s, and the predicted attribute relationship (s c , g a ) is in the groundtruth set? A . Otherwise, it returns -1.</p><p>(2) R p (f , g p ) returns +1 if there exists?,? ?? that overlap with s and s respectively, and (s c , g p , s c ) ?? P . Otherwise, it returns -1.</p><p>(3) R c (f , g c ) returns +5 if the next object instances ? S corresponding to category g c ? C overlaps with a new groundtruth object? ? S. Otherwise, it returns -1. Thus, it encourages faster exploration over all objects in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Deep Variation-structured RL</head><p>We optimize three policies to select three actions for each state by maximizing the sum of discounted rewards, which can be formulated as a decision-making process in the deep RL framework. Due to the high-dimensional continuous image data and a model-free environment, we resort to the deep Q-Network (DQN) framework proposed by <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, which generalizes well to unseen inputs. The detailed architecture of our Q-network is illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. Specifically, we use DQN to estimate three Q-value sets, parametrized by network weights ? a , ? p , ? c , which correspond to the action sets A, P, C. In each training episode, we use angreedy strategy to select actions g a , g p , g c in the variationstructured action space ? a , ? p , ? c , where the agent selects random actions with probability , and selects actions with the highest estimated Q-values with probability 1 ? . During testing, we directly select the best actions with highest estimated Q-values in ? a , ? p , ? c . The agent sequentially determines the best actions to discover objects, relationships, and attributes in the given image, until either the maximum search step is reached or there are no remaining uncovered object instances.</p><p>We also utilize a replay memory to store experience from past episodes. In each step, we draw a random minibatch from the replay memory to perform the Q-learning update. The replay memory helps stabilize the training by smoothing the training distribution over past experiences and reducing correlation between training samples <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>.</p><formula xml:id="formula_0">Given a transition sample (f , f , g a , g p , g c , R a , R p , R c ), the network weights ? (t) a , ? (t) p , ? (t)</formula><p>c are updated as follows:</p><formula xml:id="formula_1">? (t+1) a =? (t) a + ?(Ra + ? max g a Q(f , g a ; ? (t)? a ) ? Q(f , ga; ? (t) a )) ? (t) a Q(f , ga; ? (t) a ), ? (t+1) p =? (t) p + ?(Rp + ? max g p Q(f , g p ; ? (t)? p ) ? Q(f , gp; ? (t) p )) ? (t) p Q(f , gp; ? (t) p ), ? (t+1) c =? (t) c + ?(Rc + ? max g c Q(f , g c ; ? (t)? c ) ? Q(f , gc; ? (t) c )) ? (t) c Q(f , gc; ? (t) c ),<label>(1)</label></formula><p>where g a , g p , g c represent the actions that can be taken in state f , ? is the learning rate, and ? is the discount factor. The target network weights ?</p><formula xml:id="formula_2">(t)? a , ? (t)? p , ? (t)? c</formula><p>are copied every ? steps from the online network, and kept fixed in all other steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Dataset. We conduct our experiments on the Visual Relationship Detection (VRD) dataset <ref type="bibr" target="#b15">[16]</ref> and the Visual Genome dataset <ref type="bibr" target="#b12">[13]</ref>. VRD <ref type="bibr" target="#b15">[16]</ref> contains 5000 images (4000 for training, 1000 for testing) with 100 object categories and 70 predicates. In total, the dataset contains 37,993 relationship instances with 6,672 relationship types, out of which 1,877 relationships occur only in the test set and not in the training set. For the Visual Genome Dataset <ref type="bibr" target="#b12">[13]</ref>, we experiment on 87,398 images (out of which 5000 are held out for validation, and 5000 for testing), containing 703,839 relationship instances with 13,823 relationship types and 1,464,446 attribute instances with  <ref type="bibr" target="#b25">[26]</ref>. Each epoch ends after performing an episode on all training images. We use a mini-batch size of 64 images. The maximum search step for each image is empirically set to 300. During -greedy training, is annealed linearly from 1 to 0.1 over the first 20 epochs, and is fixed to 0.1 in the remaining epochs. The discount factor ? is set to 0.9, and the network parameters ? are copied after every ? = 10000 steps. The learning rate ? is initialized to 0.0007 and decreased by a factor of 10 after every 10 epochs. Only the top 100 candidate object instances, ranked by objectness confidence scores by the trained object detector, are selected for mining relationships and attributes in an image, in order to balance efficiency and effectiveness. On VRD <ref type="bibr" target="#b15">[16]</ref>, VRL takes about 8 hours to train an object detector with 100 object categories, and two days to converge. On the Visual Genome dataset <ref type="bibr" target="#b12">[13]</ref>, VRL takes between 4 to 5 days to train an object detector with 1750 object categories, and one week to converge. On average, it takes 300ms to feed-forward one image into VRL. More details about the dataset are provided in Sec. 4. The implementations are based on the publicly available Torch7 platform on a single NVIDIA GeForce GTX 1080.</p><p>Evaluation. Following <ref type="bibr" target="#b15">[16]</ref>, we use recall@100 and recall@50 as our evaluation metrics. Recall@x computes the fraction of times the correct relationship or attribute instance is covered in the top x confident predictions, which are ranked by the product of objectness confidence scores for the relevant object instances (i.e., confidence scores of the object detector) and Q-values of the selected predicates or attributes. As discussed in <ref type="bibr" target="#b15">[16]</ref>, we do not use the mean average precision (mAP), which is a pessimistic evaluation metric because the dataset cannot exhaustively annotate all  possible relationships and attributes in an image.</p><p>Following <ref type="bibr" target="#b15">[16]</ref>, we evaluate on three tasks: (1) In relationship phrase detection <ref type="bibr" target="#b21">[22]</ref>, the goal is to predict a "subject-predicate-object" phrase, where the localization of the entire relationship has at least 0.5 overlap with a groundtruth bounding box. <ref type="bibr" target="#b1">(2)</ref> In relationship detection, the goal is to predict a "subject-predicate-object" phrase, where the localizations of the subject and object instances have at least 0.5 overlap with their corresponding groundtruth boxes. (3) In attribute detection, the goal is to predict a "subject-attribute" phrase, where the subject's localization has at least 0.5 overlap with a groundtruth box.</p><p>Baseline models. First, we compare our model with state-of-the-art approaches, Visual Phrases <ref type="bibr" target="#b21">[22]</ref>, Joint CNN+R-CNN <ref type="bibr" target="#b24">[25]</ref> and Lu et al. <ref type="bibr" target="#b15">[16]</ref>. Note that the latter two methods use R-CNN <ref type="bibr" target="#b4">[5]</ref> to extract object proposals. Their results on VRD are reported in <ref type="bibr" target="#b15">[16]</ref>, and we also experiment their methods on the Visual Genome dataset. Lu et al. V only <ref type="bibr" target="#b15">[16]</ref> trains individual detectors for object and predicate categories separately, and then combines their confidences to generate a relationship prediction. Furthermore, we train and compare with the following models: "Faster R-CNN <ref type="bibr" target="#b19">[20]</ref>" directly detects each unique relationship or attribute type, following Visual Phrases <ref type="bibr" target="#b21">[22]</ref>. "Faster R-CNN V only <ref type="bibr" target="#b19">[20]</ref>" model is similar to Lu et al. V only <ref type="bibr" target="#b15">[16]</ref>, with the only difference being that Faster R-CNN is used for object detection. "Joint CNN+RPN <ref type="bibr" target="#b24">[25]</ref>" extracts proposals using the pre-trained RPN <ref type="bibr" target="#b19">[20]</ref> model on VOC 2012 <ref type="bibr" target="#b2">[3]</ref> and then performs the classification. "Joint CNN+Trained RPN <ref type="bibr" target="#b19">[20]</ref>" trains a separate RPN model on our dataset to generate proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VRD:%person%wear%shirt VRD:%person%wear%helmet</head><p>Our%VRL:%person% hold%phone Our%VRL:%person% wear%shirt <ref type="figure">Figure 6</ref>. Qualitative comparison between VRD <ref type="bibr" target="#b15">[16]</ref> and our VRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with State-of-the-art Models</head><p>Comparison of results with baseline methods on VRD and Visual Genome are reported in Tables 1, 2, and 3.</p><p>Shared Detectors vs Individual Detectors. The compared models can be categorized into two classes: <ref type="bibr" target="#b0">(1)</ref> Models that train individual detectors for each predicate or attribute type, i.e., Visual Phrases <ref type="bibr" target="#b21">[22]</ref>, Joint CNN+R-CNN <ref type="bibr" target="#b24">[25]</ref>, Joint CNN+RPN <ref type="bibr" target="#b24">[25]</ref>, Faster R-CNN <ref type="bibr" target="#b19">[20]</ref>, Joint CNN+Trained RPN <ref type="bibr" target="#b19">[20]</ref>. <ref type="bibr" target="#b1">(2)</ref> Models that train shared detectors for predicate or attribute types, and then combine their results with object detectors to generate the final prediction, i.e., Lu et al. V only <ref type="bibr" target="#b15">[16]</ref>, Faster R-CNN V only <ref type="bibr" target="#b19">[20]</ref>, Lu et al. <ref type="bibr" target="#b15">[16]</ref> and our VRL. Since the space of all possible relationships and attributes is often large, there are insufficient training examples for infrequent relationships, leading to poor average performance of the models that use individual detectors.</p><p>RPN vs R-CNN. In all cases, we obtain performance improvements using RPN network <ref type="bibr" target="#b19">[20]</ref> over R-CNN <ref type="bibr" target="#b4">[5]</ref> for proposal generation. Additionally, training the proposal network on VRD and VG datasets can also increase the recalls over the pre-trained networks on other datasets.</p><p>Language Priors. Unlike baselines that simply train classifiers from visual cues, VRL and Lu et al. <ref type="bibr" target="#b15">[16]</ref> leverage language priors to facilitate prediction. Lu et al. <ref type="bibr" target="#b15">[16]</ref> uses semantic word embeddings to finetune the likelihood of a predicted relationship, while VRL follows a variationalstructured traversal scheme over a directed semantic action graph built from language priors. Both VRL and Lu et al. <ref type="bibr" target="#b15">[16]</ref> achieve significantly better performance than other baselines, which demonstrates the necessity of language priors for relationship and attribute detection. Moreover, VRL still shows substantial improvements comparison to Lu et al. <ref type="bibr" target="#b15">[16]</ref>. Therefore, VRL's directed semantic action graph provides a more compact and rich representation of semantic correlations than the word embeddings used in Lu et al. <ref type="bibr" target="#b15">[16]</ref>. The significant performance improvement is also due to the sequential reasoning of RL.</p><p>Qualitative Comparisons We show some qualitative comparison with Lu et al. <ref type="bibr" target="#b15">[16]</ref> in <ref type="figure">Fig. 6, and</ref>   <ref type="figure">Figure 7</ref>. Comparison between "VRL w/o ambiguity" and VRL. Using ambiguity-aware object mining, VRL successfully resolves vague predictions into more concrete ones ("man" ? "skier" and "stick" ? "bat"). results of VRL in <ref type="figure" target="#fig_4">Fig. 8</ref>. Our VRL generates a rich understanding of the image, including the localization and recognition of objects, and the detection of object relationships and attributes. For instance, VRL can correctly detect interactions ("person on elephant", "man riding motor"), spatial layouts ("picture hanging on wall", "car on road"), parts of objects ("person has shirt", "wheel of motor"), and attribute descriptions ("television old", "woman standing").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Discussion</head><p>We give further analysis on the key components of VRL, and report the results in <ref type="table" target="#tab_9">Table 4</ref>.</p><p>Reinforcement Learning vs. Random Walk. The variant "RL" is a standard deep RL model that selects three actions over the entire action space instead of the variationstructured action space. We compare RL with a simple "Random Walk" traversal scheme where in each step, the agent randomly selects one object instance in the image, and predicts relationships and attributions for the two mostrecently selected instances."Random Walk" only achieves slightly better results than "Joint+Trained RPN <ref type="bibr" target="#b19">[20]</ref>" and performs much worse than the remaining variants, again demonstrating the benefit of sequential mining in RL.</p><p>Variation-structured traversal scheme. VRL achieves a remarkably higher recall compared to RL (e.g., 13.34% vs 6.23% on relationship detection and 26.43% vs 12.47% on attribute detection, in terms of Recall@100). Thus, we conclude that using a variation-structured traversal scheme to dynamically configure the small action set for each state can accelerate and stabilize the learning procedure, by dramatically decreasing the number of possible actions. For example, the number of predicate actions (347) can be dropped to <ref type="bibr" target="#b14">15</ref>    in the past and stabilize search trajectories that might get stuck in repetitive cycles. (2) "VRL w/ historical actions" directly stores a historical action vector in the state <ref type="bibr" target="#b1">[2]</ref>. Each historical action vector is the concatenation of four (|C|+|A|+|P|)-dim action vectors corresponding to the last four actions taken, where each action vector is zero in all elements except the indices corresponding to the three actions taken in C, A, P. This variant still causes the recall to drop, demonstrating that semantic phrase embeddings learned by language models can capture richer history cues (e.g., relationship similarity). Ambiguity-aware object mining. "VRL w/o ambiguity" only considers the top-1 predicted category of each object for the action set ? c . It obtains lower recall than VRL, suggesting that incorporating semantically ambiguous categories into ? c can help identify a more appropriate category for each object under different scene contexts. <ref type="figure">Fig. 7</ref> illustrates two examples where VRL successfully resolves vague predictions of "VRL w/o ambiguity" into more concrete ones ("man"?"skier" and "stick"?"bat").</p><p>Spatial actions. Similar to <ref type="bibr" target="#b1">[2]</ref>, we experiment using spatial actions in the deep RL setting to sequentially extract object instances. The variant "VRL w/ directional actions" replaces the 1751-dim object category action vector with a 9-dim action vector indexed by directions (N, NE, E, SE, S, SW, W, NW) plus one terminal trigger. In each step, the agent selects a neighboring object instance with the highest confidence whose center lies in one of the eight directions w.r.t. that of the subject instance. The diverse spatial layouts of object instances across different images make it difficult to learn a spatial action policy, and causes this variant to perform poorly.</p><p>Long Short-Term Memory "VRL w/ LSTM" is a variant where all fully-connected layers in <ref type="figure" target="#fig_1">Fig. 3</ref> are replaced with LSTM <ref type="bibr" target="#b7">[8]</ref> layers, which have shown promising results in capturing long-term dependencies. However, "VRL w/ LSTM" has no noticeable performance improvements over VRL, while requiring much more training time. This shows that history phrase embeddings can sufficiently model historical context for sequential prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Zero-shot Learning</head><p>We also compare VRL with Lu et al. <ref type="bibr" target="#b15">[16]</ref> in the zeroshot learning setting (see <ref type="table" target="#tab_4">Tables 1 and 2</ref>). A promising model should be capable of predicting unseen relationships, since the training data will not cover all possible relationship types. Lu et al. <ref type="bibr" target="#b15">[16]</ref> uses word embeddings to project similar relationships onto unseen ones, while our VRL uses a large semantic action graph to learn similar relationships on shared graph nodes. Our VRL achieves &gt; 5% performance improvements over Lu et al. <ref type="bibr" target="#b15">[16]</ref> on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>We proposed a novel deep variation-structured reinforcement learning framework for detecting visual relationships and attributes. The VRL sequentially discovers the relationship and attribute instances following a variation-structured traversal scheme on a directed semantic action graph. It incorporates global interdependency to facilitate predictions in local regions. Our experiments on VRD and the Visual Genome dataset demonstrate the power and efficiency of our model over baselines. As future work, a larger directed action graph can be built using natural language sentences. Additionally, VRL can be generalized into an unsupervised learning framework to learn from a massive number of unlabeled images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>In each example (left and right), we show the bounding boxes of objects in the image (top), and the relationships and attributes recognized by our proposed VRL framework (bottom). Only the top few results are illustrated for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Network architecture of deep VRL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The VRL does a sequential breadth-first search, predicting all relationships and attributes with respect to the current subject instance before moving onto the next instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Examples of relationship and attribute detection results generated by VRL on the Visual Genome dataset. We show the top predictions for each image: the localized objects (top) and a semantic graph describing their relationships and attributes (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Results for relationship phrase detection (Phr.) and relationship detection (Rel.) on the VRD dataset. R@100 and R@50 are abbreviations for Recall@100 and Recall@50.</figDesc><table><row><cell>Method</cell><cell cols="4">Phr. R@100 Phr. R@50 Rel. R@100 Rel. R@50</cell></row><row><cell>Visual Phrases [22]</cell><cell>0.07</cell><cell>0.04</cell><cell>-</cell><cell>-</cell></row><row><cell>Joint CNN+R-CNN [25]</cell><cell>0.09</cell><cell>0.07</cell><cell>0.09</cell><cell>0.07</cell></row><row><cell>Joint CNN+RPN [25]</cell><cell>2.18</cell><cell>2.13</cell><cell>1.17</cell><cell>1.15</cell></row><row><cell>Lu et al. V only [16]</cell><cell>2.61</cell><cell>2.24</cell><cell>1.85</cell><cell>1.58</cell></row><row><cell>Faster R-CNN [20]</cell><cell>3.31</cell><cell>3.24</cell><cell>-</cell><cell>-</cell></row><row><cell>Joint CNN+Trained RPN [20]</cell><cell>3.51</cell><cell>3.17</cell><cell>2.22</cell><cell>1.98</cell></row><row><cell>Faster R-CNN V only [20]</cell><cell>6.13</cell><cell>5.61</cell><cell>5.90</cell><cell>4.26</cell></row><row><cell>Lu et al. [16]</cell><cell>17.03</cell><cell>16.17</cell><cell>14.70</cell><cell>13.86</cell></row><row><cell>Our VRL</cell><cell>22.60</cell><cell>21.37</cell><cell>20.79</cell><cell>18.19</cell></row><row><cell>Lu et al. [16] (zero-shot)</cell><cell>3.76</cell><cell>3.36</cell><cell>3.28</cell><cell>3.13</cell></row><row><cell>Our VRL (zero-shot)</cell><cell>10.31</cell><cell>9.17</cell><cell>8.52</cell><cell>7.94</cell></row><row><cell cols="5">8,561 attribute types. There are 2,015 relationship types</cell></row><row><cell cols="5">that occur in the test set but not in the training set, which</cell></row><row><cell cols="4">allows us to evaluate VRL on zero-shot learning.</cell><cell></cell></row><row><cell cols="5">Implementation Details. We train a deep Q-network</cell></row><row><cell cols="4">for 60 epochs with a shared RMSProp optimizer</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Results for relationship detection on Visual Genome.</figDesc><table><row><cell>Method</cell><cell cols="4">Phr. R@100 Phr. R@50 Rel. R@100 Rel. R@50</cell></row><row><cell>Joint CNN+R-CNN [25]</cell><cell>0.13</cell><cell>0.10</cell><cell>0.11</cell><cell>0.08</cell></row><row><cell>Joint CNN+RPN [25]</cell><cell>1.39</cell><cell>1.34</cell><cell>1.22</cell><cell>1.18</cell></row><row><cell>Lu et al. V only [16]</cell><cell>1.66</cell><cell>1.54</cell><cell>1.48</cell><cell>1.20</cell></row><row><cell>Faster R-CNN [20]</cell><cell>2.25</cell><cell>2.19</cell><cell>-</cell><cell>-</cell></row><row><cell>Joint CNN+Trained RPN [20]</cell><cell>2.52</cell><cell>2.44</cell><cell>2.37</cell><cell>2.23</cell></row><row><cell>Faster R-CNN V only [20]</cell><cell>5.79</cell><cell>5.22</cell><cell>4.87</cell><cell>4.36</cell></row><row><cell>Lu et al. [16]</cell><cell>10.23</cell><cell>9.55</cell><cell>7.96</cell><cell>6.01</cell></row><row><cell>Our VRL</cell><cell>16.09</cell><cell>14.36</cell><cell>13.34</cell><cell>12.57</cell></row><row><cell>Lu et al. [16] (zero-shot)</cell><cell>1.20</cell><cell>1.08</cell><cell>1.13</cell><cell>0.97</cell></row><row><cell>Our VRL (zero-shot)</cell><cell>7.98</cell><cell>6.53</cell><cell>7.14</cell><cell>6.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Results for attribute detection on Visual Genome.</figDesc><table><row><cell>Method</cell><cell cols="2">Attribute Recall@100 Attribute Recall@50</cell></row><row><cell>Joint CNN+R-CNN [25]</cell><cell>2.38</cell><cell>1.97</cell></row><row><cell>Joint CNN+RPN [25]</cell><cell>3.48</cell><cell>2.63</cell></row><row><cell>Faster R-CNN [20]</cell><cell>7.36</cell><cell>5.22</cell></row><row><cell>Joint CNN+Trained RPN [20]</cell><cell>9.77</cell><cell>8.35</cell></row><row><cell>Our VRL</cell><cell>26.43</cell><cell>24.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>on average. History phrase embedding. To validate the effectiveness of history phrase embeddings, we evaluate two variants of VRL: (1) "VRL w/o history phrase" does not incorporate history phrase embedding into state features. This variant causes the recall to drop by over 4% compared to the original VRL. Thus, leveraging history phrase embeddings can help inform the current state what has happened</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 .</head><label>4</label><figDesc>Performance of VRL and its variants on Visual Genome.</figDesc><table><row><cell>Method</cell><cell cols="4">Rel. R@100 Rel. R@50 Attr. R@100 Attr. R@50</cell></row><row><cell>Joint CNN+Trained RPN [20]</cell><cell>2.37</cell><cell>2.23</cell><cell>9.77</cell><cell>8.35</cell></row><row><cell>Random Walk</cell><cell>3.67</cell><cell>3.09</cell><cell>10.21</cell><cell>8.59</cell></row><row><cell>RL</cell><cell>6.23</cell><cell>5.10</cell><cell>12.47</cell><cell>10.09</cell></row><row><cell>VRL w/o history phrase</cell><cell>9.05</cell><cell>8.12</cell><cell>20.09</cell><cell>19.45</cell></row><row><cell>VRL w/ directional actions</cell><cell>10.66</cell><cell>9.85</cell><cell>20.31</cell><cell>18.62</cell></row><row><cell>VRL w/ historical actions</cell><cell>11.98</cell><cell>10.01</cell><cell>23.02</cell><cell>22.15</cell></row><row><cell>VRL w/o ambiguity</cell><cell>12.01</cell><cell>11.20</cell><cell>24.78</cell><cell>22.46</cell></row><row><cell>Our VRL</cell><cell>13.34</cell><cell>12.57</cell><cell>26.43</cell><cell>24.87</cell></row><row><cell>VRL w/ LSTM</cell><cell>13.86</cell><cell>13.07</cell><cell>25.98</cell><cell>25.01</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Active object localization with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2488" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for robotic manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Holly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00633</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reinforcement learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="237" to="285" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07332</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">On support relations and semantic scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05834</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="852" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning, Neural Information Processing Systems Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Query adaptive similarity for large scale object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wengert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1610" to="1617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Viske: Visual knowledge extraction and question answering by visual verification of relation phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1456" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognition using visual phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1745" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generating semantically precise scene graphs from textual descriptions for improved image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Vision and Language</title>
		<meeting>the Fourth Workshop on Vision and Language</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="70" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reasoning about object affordances in a knowledge base representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="408" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Target-driven visual navigation in indoor scenes using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05143</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
