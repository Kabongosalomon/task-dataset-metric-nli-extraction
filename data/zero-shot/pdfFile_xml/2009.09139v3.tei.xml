<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 CONDITIONALLY ADAPTIVE MULTI-TASK LEARNING: IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS &amp; LESS DATA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Pilault</surname></persName>
							<email>jonathan.pilault@polymtl.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Polytechnique Montreal &amp; Mila</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amine</forename><surname>El Hattami</surname></persName>
							<email>amine.elhattami@polymtl.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Polytechnique Montreal &amp; Mila</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
							<email>christopher.pal@polymtl.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Polytechnique Montreal &amp; Mila</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Element AI</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CIFAR AI Chair</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 CONDITIONALLY ADAPTIVE MULTI-TASK LEARNING: IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS &amp; LESS DATA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-Task Learning (MTL) networks have emerged as a promising method for transferring learned knowledge across different tasks. However, MTL must deal with challenges such as: overfitting to low resource tasks, catastrophic forgetting, and negative task transfer, or learning interference. Often, in Natural Language Processing (NLP), a separate model per task is needed to obtain the best performance. However, many fine-tuning approaches are both parameter inefficient, i.e., potentially involving one new model per task, and highly susceptible to losing knowledge acquired during pretraining. We propose a novel Transformer based Adapter consisting of a new conditional attention mechanism as well as a set of task-conditioned modules that facilitate weight sharing. Through this construction, we achieve more efficient parameter sharing and mitigate forgetting by keeping half of the weights of a pretrained model fixed. We also use a new multi-task data sampling strategy to mitigate the negative effects of data imbalance across tasks. Using this approach, we are able to surpass single task fine-tuning methods while being parameter and data efficient (using around 66% of the data for weight updates). Compared to other BERT Large methods on GLUE, our 8-task model surpasses other Adapter methods by 2.8% and our 24-task model outperforms by 0.7-1.0% models that use MTL and single task fine-tuning. We show that a larger variant of our single multi-task model approach performs competitively across 26 NLP tasks and yields state-of-the-art results on a number of test and development sets. Our code is publicly available at https://github.com/CAMTL/CA-MTL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The introduction of deep, contextualized Masked Language Models (MLM) 1 trained on massive amounts of unlabeled data has led to significant advances across many different Natural Language Processing (NLP) tasks <ref type="bibr" target="#b53">(Peters et al., 2018;</ref><ref type="bibr" target="#b44">Liu et al., 2019a)</ref>. Much of these recent advances can be attributed to the now well-known BERT approach <ref type="bibr" target="#b20">(Devlin et al., 2018)</ref>. Substantial improvements over previous state-of-the-art results on the GLUE benchmark <ref type="bibr" target="#b75">(Wang et al., 2018)</ref> have been obtained by multiple groups using BERT models with task specific fine-tuning. The "BERT-variant + fine-tuning" formula has continued to improve over time with newer work constantly pushing the state-of-the-art forward on the GLUE benchmark. The use of a single neural architecture for multiple NLP tasks has shown promise long before the current wave of BERT inspired methods <ref type="bibr" target="#b16">(Collobert &amp; Weston, 2008)</ref> and recent work has argued that autoregressive language models (ARLMs) trained on large-scale datasets -such as the GPT family of models <ref type="bibr" target="#b58">(Radford et al., 2018)</ref>, are in practice multi-task learners <ref type="bibr" target="#b5">(Brown et al., 2020)</ref>. However, even with MLMs and ARLMs trained for multi-tasking, single task fine-tuning is usually also employed to achieve state-of-the-art performance on specific tasks of interest. Typically this fine-tuning process may entail: creating a task-specific fine-tuned model <ref type="bibr" target="#b20">(Devlin et al., 2018)</ref>, training specialized model components for task-specific predictions <ref type="bibr">(Houlsby et al., 2019)</ref> or fine-tuning a single multi-task architecture <ref type="bibr" target="#b45">(Liu et al., 2019b)</ref>. uncertainty-based sampling algorithm. Each task has its own decoder. The input embedding layer and the lower Transformer layers are frozen. The upper Transformer layer and Conditional Alignment module are modulated with the task embedding.</p><p>Single-task fine-tuning overall pretrained model parameters may have other issues. Recent analyses of such MLM have shed light on the linguistic knowledge that is captured in the hidden states and attention maps <ref type="bibr" target="#b13">(Clark et al., 2019b;</ref><ref type="bibr" target="#b49">Merchant et al., 2020)</ref>. Particularly, BERT has middle Transformer <ref type="bibr" target="#b73">(Vaswani et al., 2017)</ref> layers that are typically the most transferable to a downstream task <ref type="bibr" target="#b44">(Liu et al., 2019a)</ref>. The model proxies the steps of the traditional NLP pipeline in a localizable way ) -with basic syntactic information appearing earlier in the network, while high-level semantic information appearing in higher-level layers. Since pretraining is usually done on large-scale datasets, it may be useful, for a variety of downstream tasks, to conserve that knowledge. However, single task fine-tuning causes catastrophic forgetting of the knowledge learned during MLM <ref type="bibr" target="#b31">(Howard &amp; Ruder, 2018)</ref>. To preserve knowledge, freezing part of a pretrained network and using Adapters for new tasks have shown promising results <ref type="bibr">(Houlsby et al., 2019)</ref>.</p><p>Inspired by the human ability to transfer learned knowledge from one task to another new task, Multi-Task Learning (MTL) in a general sense <ref type="bibr" target="#b6">(Caruana, 1997;</ref><ref type="bibr" target="#b61">Rajpurkar et al., 2016b;</ref><ref type="bibr" target="#b63">Ruder, 2017)</ref> has been applied in many fields outside of NLP. <ref type="bibr" target="#b7">Caruana (1993)</ref> showed that a model trained in a multi-task manner can take advantage of the inductive transfer between tasks, achieving a better generalization performance. MTL has the advantage of computational/storage efficiency <ref type="bibr" target="#b86">(Zhang &amp; Yang, 2017)</ref>, but training models in a multi-task setting is a balancing act; particularly with datasets that have different: (a) dataset sizes, (b) task difficulty levels, and (c) different types of loss functions. In practice, learning multiple tasks at once is challenging since negative transfer <ref type="bibr" target="#b76">(Wang et al., 2019a)</ref>, task interference <ref type="bibr" target="#b81">(Wu et al., 2020;</ref> and catastrophic forgetting <ref type="bibr" target="#b65">(Serr? et al., 2018)</ref> can lead to worse data efficiency, training stability and generalization compared to single task fine-tuning.</p><p>Using Conditionally Adaptive Learning, we seek to improve pretraining knowledge retention and multi-task inductive knowledge transfer. Our contributions are the following:</p><p>? A new task conditioned Transformer that adapts and modulates pretrained weights (Section 2.1). ? A novel way to prioritize tasks with an uncertainty based multi-task data sampling method that helps balance the sampling of tasks to avoid catastrophic forgetting (Section 2.2).</p><p>Our Conditionally Adaptive Multi-Task Learning (CA-MTL) approach is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. To the best of our knowledge, our work is the first to explore the use of a latent representation of tasks to modularize and adapt pretrained architectures. Further, we believe our work is also the first to examine uncertainty sampling for large-scale multi-task learning in NLP. We show the efficacy of CA-MTL by: (a) testing on 26 different tasks and (b) presenting state-of-the-art results on a number of test sets as well as superior performance against both single-task and MTL baselines. Moreover, we further demonstrate that our method has advantages over (c) other adapter networks, and (d) other MTL sampling methods. Finally, we provide ablations and separate analysis of the MT-Uncertainty Sampling technique in section 4.1 and of each component of the adapter in 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODOLOGY</head><p>This section is organized according to the two main MTL problems that we will tackle: (1) How to modularize a pretrained network with latent task representations? (2) How to balance different tasks in MTL? We define each task as:</p><formula xml:id="formula_0">T i {p i (y i |x i , z i ), L i ,p i (x i )},</formula><p>where z i is task i's learnable shallow embedding, L i is the task loss, andp i (x i ) is the empirical distribution of the training data pair {x i , y i }, for i ? {1, . . . , T } and T the number of supervised tasks. The MTL objective is:</p><formula xml:id="formula_1">min ?(z),?1,...,? T T i=1 L i (f ?(zi),?i (x i ), y i )<label>(1)</label></formula><p>where f is the predictor function (includes encoder model and decoder heads), ?(z) are learnable generated weights conditioned on z, and ? i are task-specific parameters for the output decoder heads. z is constructed using an embedding lookup table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TASK CONDITIONED TRANSFORMER</head><p>Our task conditioned Transformer architecture is based on one simple concept. We either add conditional layers or modulate existing pretrained weights using a task representation by extending Feature Wise Linear Modulation <ref type="bibr" target="#b52">(Perez et al., 2018)</ref> functions in several ways depending on the Transformer layer. We define our framework below. Definition 1 (Conditional Weight Transformations). Given a neural network weight matrix W, we compute transformations of the form ?(W|z i ) = ? i (z i )W + ? i (z i ), where ? i and ? i are learned functions that transform the weights based on a learned vector embedding z i , for task i. Definition 2 (Conditionally Adaptive Learning). In our setting, Conditionally Adaptive Learning is the process of learning a set of ?s for the conditionally adaptive modules presented below along with a set of task embedding vectors z i for T tasks, using a multi-task loss (see equation 1).</p><p>In the subsections that follow: We introduce a new Transformer Attention Module using blockdiagonal Conditional Attention that allows the original query-key based attention to account for task-specific biases (section 2.1.1). We propose a new Conditional Alignment method that aligns the data of diverse tasks and that performs better than its unconditioned and higher capacity predecessor (section 2.1.2). We adapt layer normalization statistics to specific tasks using a new Conditional Layer Normalization module (section 2.1.3). We add a Conditional Bottleneck that facilitates weight sharing and task-specific information flow from lower layers (section 2.1.4). In our experiments we provide an ablation study of these components <ref type="table" target="#tab_1">(Table 1)</ref> examining performance in terms of GLUE scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">CONDITIONAL ATTENTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: Conditional Attention Module</head><p>Given d, the input dimensions, the query Q, the key K, and the value V as defined in <ref type="bibr" target="#b73">Vaswani et al. (2017)</ref>, we redefine the attention operation:</p><formula xml:id="formula_2">Attention(Q, K, V, z i )) = softmax M (z i ) + QK T ? d V M (z i ) = N n=1 A n (z i ), A n (z i ) = A n ? i (z i ) + ? i (z i )</formula><p>where is the direct sum operator (see section A.6), N is the number of block matrices A n ? R (L/N )?(L/N ) along the diagonal of the attention matrix, L is the input sequence, M (z i ) = diag(A 1 , . . . , A N ) is a block diagonal conditional matrix. Note that A n is constructed using L/N trainable and randomly initialized L/N dimensional vectors. While the original attention matrix depends on the hidden states h, M (z i ) is a learnable weight matrix that only depends on the task embedding</p><formula xml:id="formula_3">z i ? R d . ? i , ? i : R d ? R L 2 /N 2</formula><p>are Feature Wise Linear Modulation <ref type="bibr" target="#b52">(Perez et al., 2018)</ref> functions. We also experimented with full-block Conditional Attention ? R L?L . Not only did it have N 2 more parameters compared to the block-diagonal variant, but it also performed significantly worse on the GLUE development set (see FBA variant in <ref type="table" target="#tab_1">Table 10</ref>). It is possible that GLUE tasks derive a certain benefit from localized attention that is a consequence of M (z i ). With M (z i ), each element in a sequence can only attend to other elements in its subsequence of length L/N . In our experiments we used N = d/L. The full Conditional Attention mechanism used in our experiments is illustrated in <ref type="figure">Figure 2</ref>. <ref type="bibr" target="#b81">Wu et al. (2020)</ref> showed that in MTL having T separate alignment modules R 1 , . . . , R T increases BERT LARGE avg. scores on five GLUE tasks (CoLA, MRPC, QNLI, RTE, SST-2) by 2.35%. Inspired by this work, we found that adding a task conditioned alignment layer between the input embedding layer and the first BERT Transformer layer improved multi-task model performance. However, instead of having T separate alignment matrices R i for each T task, one alignment matrixR is generated as a function of the task embedding z i . As in <ref type="bibr" target="#b81">Wu et al. (2020)</ref>, we tested this module on the same five GLUE tasks and with BERT LARGE . Enabling task conditioned weight sharing across covariance alignment modules allows us to outperforms BERT LARGE by 3.61%. This is 1.26 % higher than having T separate alignment matrices. InsertingR into BERT, yields the following encoder functionf :f</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">CONDITIONAL ALIGNMENT</head><formula xml:id="formula_4">= T t=1 g ?i (E(x i )R(z i )B),R(z i ) = R? i (z i ) + ? i (z i )<label>(2)</label></formula><p>where x i ? R d is the layer input, g ?i is the decoder head function for task i with weights ? i , E the frozen BERT embedding layer, B the BERT Transformer layers and R the linear weight matrix of a single task conditioned alignment matrix. ? i , ? i : R d ? R d are Feature Wise Linear Modulation functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">CONDITIONAL LAYER NORMALIZATION (CLN)</head><p>We extend the Conditional Batch Normalization idea from de <ref type="bibr" target="#b18">Vries et al. (2017)</ref> to Layer Normalization <ref type="bibr" target="#b1">(Ba et al., 2016)</ref>. For task T i , i ? {1, . . . , T }:</p><formula xml:id="formula_5">h i = 1 ? (a i ? ?) * ? i (z i ) + ? i (z i ),? i (z i ) = ? ? i (z i ) + ?<label>(3)</label></formula><p>where h i is the CLN output vector, a i are the preceding layer activations associated with task i, ? and ? are the mean and the variance of the summed inputs within each layer as defined in <ref type="bibr" target="#b1">Ba et al. (2016)</ref>. Conditional Layer Normalization is initialized with BERT's Layer Normalization affine transformation weights and bias ? and ? from the original formulation: h = 1 ? (a ? ?) * ? + ? . During training, the weight and bias functions of ? i ( * ) and ? i ( * ) are always trained, while the original Layer Normalization weight may be kept fixed. This module was added to account for task specific rescaling of individual training cases. Layer Normalization normalizes the inputs across features. The conditioning introduced in equation 2.1.3 allows us to modulate the normalization's output based on a task's latent representation. We created a task conditioned two layer feed-forward bottleneck layer (CFF up/down in <ref type="figure">Figure 3</ref>). The conditional bottleneck layer follows the same transformation as in equation 2. The module in <ref type="figure">Figure 3a</ref> is added to the top most Transformer layers of CA-MTL BASE and uses a CLN. For CA-MTL LARGE this module is the main building block of the skip connection added alongside all Transformer layers seen in <ref type="figure">Figure 3b</ref>. The connection at layer j takes in the matrix sum of the Transformer layer output at j and the previous connection's output at j ? 1. The Conditional bottleneck allows lower layer information to flow upwards depending on the task. Our intuition for introducing this component is related to recent studies  that showed that the "most important layers for a given task appear at specific positions". As with the other modules described so far, each task adaptation is created from the weights of a single shared adapter that is modulated by the task embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">CONDITIONAL BOTTLENECK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MULTI-TASK UNCERTAINTY SAMPLING</head><p>MT-Uncertainty Sampling is a task selection strategy that is inspired by Active Learning techniques. Our algorithm 1 is outlined in the Appendix, Section A.2. Similar to Active Learning, our algorithm first evaluates the model uncertainty. MT-Uncertainty Sampling uses Shannon Entropy, an uncertainty measure, to choose training examples by first doing forward pass through the model with b ? T input samples. For an output classification prediction with C i possible classes and probabilities (p i,1 , . . . , p i,Ci ), the Shannon Entropy H i , for task T i and i ? {1, . . . , T }, our uncertainty measure U(x) are given by:</p><formula xml:id="formula_6">H i = H i (f ?(zi),?i (x)) = ? Ci c=1 p c log p c , U(x i ) = H i (f ?(zi),?i (x)) H ? H i (4) H = max i?{1,...,T }H i = max 1 b x?xi H i , H i = ? Ci c=1 1 C i log 1 C i<label>(5)</label></formula><p>whereH i is the average Shannon Entropy across b samples of task t, H i , the Shannon entropy of choosing classes with uniform distribution and?, the maximum of each task's average entropy over b samples. H i is normalizing factor that accounts for differing number of prediction classes (without the normalizing factor H i , tasks with a binary classification C i = 1 were rarely chosen). Further, to limit high entropy outliers and to favor tasks with highest uncertainty, we normalize with?. The measure in eq. 4 allows Algorithm 1 to choose b samples from b ? T candidates to train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>Multi-Tasking in NLP. To take advantage of the potential positive transfer of knowledge from one task to another, several works have proposed carefully choosing which tasks to train as an intermediate step in NLP before single task fine-tuning <ref type="bibr" target="#b3">(Bingel &amp; S?gaard, 2017;</ref><ref type="bibr" target="#b37">Kerinec et al., 2018;</ref><ref type="bibr" target="#b76">Wang et al., 2019a;</ref><ref type="bibr" target="#b67">Standley et al., 2019;</ref><ref type="bibr" target="#b57">Pruksachatkun et al., 2020;</ref><ref type="bibr" target="#b54">Phang et al., 2018)</ref>. The intermediate tasks are not required to perform well and are not typically evaluated jointly. In this work, all tasks are trained jointly and all tasks used are evaluated from a single model. In Natural Language Understanding (NLU), it is still the case that to get the best task performance one often needs a separate model per task <ref type="bibr" target="#b14">(Clark et al., 2019c;</ref><ref type="bibr" target="#b48">McCann et al., 2018)</ref>. At scale, Multilingual NMT systems <ref type="bibr" target="#b0">(Aharoni et al., 2019)</ref> have also found that MTL model performance degrades as the number of tasks increases. We notice a similar trend in NLU with our baseline MTL model. Recently, approaches in MTL have tackled the problem by designing task specific decoders on top of a shared model <ref type="bibr" target="#b45">(Liu et al., 2019b)</ref> or distilling multiple single-task models into one <ref type="bibr" target="#b14">(Clark et al., 2019c)</ref>. Nonetheless, such MTL approaches still involves single task fine-tuning. In this paper, we show that it is possible to achieve high performance in NLU without single task fine-tuning.</p><p>Adapters. Adapters are trainable modules that are attached in specific locations of a pretrained network. They provide another promising avenue to limit the number of parameters needed when confronted with a large number of tasks. This approach is useful with pretrained MLM models that have rich linguistic information <ref type="bibr" target="#b71">(Tenney et al., 2019b;</ref><ref type="bibr" target="#b13">Clark et al., 2019b;</ref><ref type="bibr" target="#b44">Liu et al., 2019a;</ref>. Recently, <ref type="bibr">Houlsby et al. (2019)</ref> added an adapter to a pretrained BERT model by fine-tuning the layer norms and adding feed forward bottlenecks in every Transformer layer. However, such methods adapt each task individually during the fine-tuning process. Unlike prior work, our method harnesses the vectorized representations of tasks to modularize a single pretrained model across all tasks. <ref type="bibr" target="#b68">Stickland et al. (2019)</ref> and <ref type="bibr" target="#b69">Tay et al. (2020)</ref> also mix both MTL and adapters with BERT and T5 encoder-decoder <ref type="bibr" target="#b59">(Raffel et al., 2019</ref>) respectively by creating local task modules that are controlled by a global task agnostic module. The main drawback is that a new set of non-shared parameters must be added when a new task is introduced. CA-MTL shares all parameters and is able to re-modulate existing weights with a new task embedding vector.</p><p>Active Learning, Task Selection and Sampling. Our sampling technique is similar to the ones found in several active learning algorithms <ref type="bibr" target="#b10">(Chen et al., 2006)</ref> that are based on Shannon entropy estimations. <ref type="bibr" target="#b62">Reichart et al. (2008)</ref> and <ref type="bibr" target="#b32">Ikhwantri et al. (2018)</ref> examined Multi-Task Active Learning (MTAL), a technique that chooses one informative sample for T different learners (or models) for each T tasks. Instead we choose T tasks samples for one model. Moreover, the algorithm weights each sample by the corresponding task score, and the Shannon entropy is normalized to account for various losses (see equation 5). Also, our algorithm is used in a large scale MTL setup ( 2 tasks). Recently, Glover &amp; Hokamp (2019) explored task selection in MTL using learning policies based on counterfactual estimations <ref type="bibr" target="#b9">(Charles et al., 2013)</ref>. However, such method considers only fixed stochastic parameterized policies while our method adapts its selection criterion based on model uncertainty throughout the training process.</p><p>Hypernetworks. CA-MTL is a hypernetwork adapter. The method to generate task-conditioned adapter weights is inspired by von Oswald et al. <ref type="bibr">(2020)</ref>. Hypernetwork layers have also been finetuned along with pretrained models. For example, Ponti et al. <ref type="formula" target="#formula_1">(2021)</ref> uses stochastic variational inference <ref type="bibr" target="#b28">Hoffman et al. (2013)</ref> to produce language and task latent codes that conditionally generates the weights of a BERT prediction head, a single hypernetwork linear layer shared across multiple languages and tasks. Unlike previous methods however, CA-MTL conditionally modulates pretrained weights and biases, attention matrices, hidden representations and normalization statistics with task embeddings. Further, CA-MTL can preserve the pretraining knowledge by freezing the underlying Transformer model. Finally, we show a synergy between our hypernetwork adapter and our active task sampling technique (see section 2.2) that allows CA-MTL to continue surpassing fully tuned models as we scale the number of tasks (see <ref type="figure" target="#fig_4">figure 7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND RESULTS</head><p>We show that our adapter of section 2 achieve parameter efficient transfer for 26 NLP tasks. Our implementation of CA-MTL is based on HuggingFace <ref type="bibr" target="#b80">(Wolf et al., 2019</ref>  <ref type="figure">Figure 4</ref>: MT-Uncertainty vs. other task sampling strategies: median dev set scores on 8 GLUE tasks and using BERTBASE. Data for the Counterfactual and Task Size policy ? |task| (eq. 6) is from <ref type="bibr" target="#b24">Glover &amp; Hokamp (2019)</ref>.</p><p>Our MT-Uncertainty sampling strategy, from section 2.2, is compared to 3 other task selection schemes: a) Counterfactual b) Task size c) Random. We used a BERT BASE (no adapters) on 200k iterations and with the same hyperparameters as in <ref type="bibr" target="#b24">Glover &amp; Hokamp (2019)</ref>. For more information on Counterfactual task selection, we invite the reader to consult the full explanation in <ref type="bibr" target="#b24">Glover &amp; Hokamp (2019)</ref>. For T tasks and the dataset D i for tasks i ? {1, . . . , T }, we rewrite the definitions of Random ? rand and Task size ? |task| sampling: In <ref type="figure">Figure 4</ref>, we see from the results that MT-Uncertainty converges faster by reaching the 80% average GLUE score line before other task sampling methods. Further, MT-Uncertainty maximum score on 200k iterations is at 82.2, which is 1.7% higher than Counterfactual sampling. The datasets in the GLUE benchmark offers a wide range of dataset sizes. This is useful to test how MT-Uncertainty manages a jointly trained low resource task (CoLA) and high resource task (MNLI). <ref type="figure">Figure 5</ref> explains how catastrophic forgetting is curtailed by sampling tasks before performance drops. With ? rand , all of CoLA's tasks are sampled by iteration 500, at which point the larger MNLI dataset overtakes the learning process and CoLA's dev set performance starts to diminish. On the other hand, with MT-Uncertainty sampling, CoLA is sampled whenever Shannon entropy is higher than MNLI's. The model first assesses uncertain samples using Shannon Entropy then decides what data is necessary to train on. This process allows lower resource tasks to keep performance steady.</p><formula xml:id="formula_7">? rand = 1/T, ? |task| = |D i | T i=1 |D i | ?1<label>(6)</label></formula><p>We provide evidence in <ref type="figure" target="#fig_6">Figure 8</ref> of A.2 that MT-Uncertainty is able to manage task difficulty -by choosing the most difficult tasks first. In <ref type="table" target="#tab_1">Table 1</ref>, we present the results of an ablation study to determine which elements of CA-MTL BERT-BASE had the largest positive gain on average GLUE scores. Starting from a MTL BERT BASE baseline trained using random task sampling (? rand ). Apart for the Conditional Adapter, each module as well as MT-Uncertainty lift overall performance and reduce variance across tasks. Please note that we also included accuracy/F1 scores for QQP, MRPC and Pearson/ Spearman correlation for STS-B to calculate score standard deviation Task ?. Intuitively, when negative task transfer occurs between two tasks, either (1) task interference is bidirectional and scores are both impacted, or (2) interference is unidirectional and only one score is impacted. We calculate Task ? to characterize changes in the dynamic range of performance across multiple tasks. We do this to asses the degree to which performance improvements are distributed across all tasks or only subsets of tasks. As we can see from <ref type="table" target="#tab_1">Table 1</ref>, Conditional Attention, Conditional Alignment, Conditional Layer Normalization, MT-Uncertainty play roles in reducing Task ? and increasing performance across tasks. This provides partial evidence of CA-MTL's ability to mitigating negative task transfer. We show that Conditional Alignment can learn to capture covariate distribution differences with task embeddings co-learned from other adapter components of CA-MTL. In <ref type="figure" target="#fig_3">Figure 6</ref>, we arrive at similar conclusions as <ref type="bibr" target="#b81">Wu et al. (2020)</ref>, who proved that negative task transfer is reduced when task covariances are aligned. The authors provided a "covariance similarity score" to gauge covariance alignment. For task i and j with m i and m j data samples respectively, and given d dimensional inputs to the first Transformer layer X i ? R mi?d and X j ? R mj ?d , we rewrite the steps to calculate the covariance similarity score between task i and j: (a) Take the covariance matrix X i X i , (b) Find its best rank-r i approximation U i,ri D i,ri U i,ri , where r i is chosen to contain 99% of the singular values. (c) Apply steps (a), (b) to X j , and compute the covariance similarity score CovSim i,j :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ABLATION AND MODULE ANALYSIS</head><formula xml:id="formula_8">CovSim i,j := (U i,ri D 1/2 i,ri ) U j,rj D 1/2 j,rj F U i,ri D 1/2 i,ri F ? U j,rj D 1/2 j,rj F . CovSim i = 1 T ? 1 j =i CovSim i,j<label>(7)</label></formula><p>Since we are training models with T tasks, we take the average covariance similarity score CovSim i between task i and all other tasks. We measure CovSim i using equation 7 between 9 single-task models trained on individual GLUE tasks. For each task in <ref type="figure" target="#fig_3">Figure 6</ref>, we measure the similarity score on the MTL trained BERT BASE baseline, e.g., CoLA (MTL), or CA-MTL BERT-BASE model, e.g., MNLI (CA-MTL). Our score improvement measure is the % difference between a single task model and MTL or CA-MTL on the particular task. We find that covariance similarity increases for 9 tasks and that performance increases for 7 out 9 tasks. These measurements confirm that the Conditional Alignment is able to align task covariance, thereby helping alleviate task interference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">JOINTLY TRAINING ON 8 TASKS: GLUE</head><p>In <ref type="table" target="#tab_2">Table 2</ref>, we evaluate the performance of CA-MTL against single task fine-tuned models, MTL as well as the other BERT-based adapters on GLUE. As in <ref type="bibr">Houlsby et al. (2019)</ref>, MNLI m and MNLI mm are treated as separate tasks. Our results indicate that CA-MTL outperforms both the BASE adapter, PALS+Anneal Sampling <ref type="bibr" target="#b68">(Stickland et al., 2019)</ref>, and the LARGE adapter, Adapters-256 <ref type="bibr">(Houlsby et al., 2019)</ref>. Against single task (ST) models, CA-MTL is 1.3% higher than BERT BASE , with 5 out 9 tasks equal or greater performance, and 0.7% higher than BERT LARGE , with 3 out 9 tasks equal or greater performance. ST models, however, need 9 models or close to 9? more parameters for all 9 tasks. We noted that CA-MTL BERT-LARGE 's average score is driven by strong RTE scores. While RTE benefits from MTL, this behavior may also be a side effect of layer freezing. In <ref type="table" target="#tab_1">Table 10</ref>, we see that CA-MTL has gains over ST on more and more tasks as we gradually unfreeze layers. In <ref type="table" target="#tab_3">Table 3</ref> we examine the ability of our method to quickly adapt to new tasks. We performed domain adaptation on SciTail (Khot et al., 2018) and SNLI <ref type="bibr" target="#b4">(Bowman et al., 2015)</ref> datasets, using a CA-MTL BASE model trained on GLUE and a new linear decoder head. We tested several pretrained and randomly initialized task embeddings in a zero-shot setting. The complete set of experiments with all task embeddings can be found in the Appendix, Section A.4. We then selected the best task embedding for our results in <ref type="table" target="#tab_3">Table 3</ref>. STS-B and MRPC MTL-trained task embeddings performed best on SciTail and SNLI respectively. CA-MTL BERT-BASE has faster adaptation than MT-DNN SMART <ref type="bibr" target="#b33">(Jiang et al., 2020)</ref> as evidenced by higher performances in low-resource regimes (0.1% and 1% of the data). When trained on the complete dataset, CA-MTL BERT-BASE is on par with MT-DNN SMART . Unlike MT-DNN SMART however, we do not add context from a semantic similarity model -MT-DNN SMART is built off HNN <ref type="bibr" target="#b27">(He et al., 2019)</ref>. Nonetheless, with a larger model, CA-MTL surpasses MT-DNN SMART on the full SNLI and SciTail datasets in <ref type="table" target="#tab_6">Table 6</ref>. Effects of Scaling Task Count. In <ref type="figure" target="#fig_4">Figure 7</ref> we continue to test if CA-MTL mitigates task interference by measuring GLUE average scores when progressively adding 9 GLUE tasks, 8 Super-GLUE tasks , 6 MRQA tasks <ref type="bibr" target="#b23">(Fisch et al., 2019)</ref>. Tasks are described in Appendix section A.9. The results show that adding 23 tasks drops the performance of our baseline MTL BERT BASE (? rand ). MTL BERT increases by 4.3% when adding MRQA but, with 23 tasks, the model performance drops by 1.8%. The opposite is true when CA-MTL modules are integrated into the model. CA-MTL continues to show gains with a large number of tasks and surpasses the baseline MTL model by close to 4% when trained on 23 tasks. 24-task CA-MTL. We jointly trained large MTL baselines and CA-MTL models on GLUE/Super-GLUE/MRQA and Named Entity Recognition (NER) WNUT2017 <ref type="bibr" target="#b19">(Derczynski et al., 2017)</ref>. Since some dev. set scores are not provided and since RoBERTa results were reported with a median score over 5 random seeds, we ran our own single seed ST/MTL baselines (marked "ReImp") for a fair comparison. The dev. set numbers reported in <ref type="bibr" target="#b47">Liu et al. (2019c)</ref> are displayed with our baselines in <ref type="table" target="#tab_10">Table 9</ref>. Results are presented in <ref type="table" target="#tab_4">Table 4</ref>. We notice in <ref type="table" target="#tab_4">Table 4</ref> that even for large models, CA-MTL provides large gains in performance on average over both ST and MTL models. For the BERT based models, CA-MTL provides 2.3% gain over ST and higher scores on 17 out 24 tasks. For RoBERTa based models, CA-MTL provides 1.2% gain over ST and higher scores on 15 out 24 tasks. We remind the reader that this is achieved with a single model. Even when trained with 16 other tasks, it is interesting to note that the MTL baseline perform better than the ST baseline on Super GLUE where most tasks have a small number of samples. Also, we used NER to test if we could still outperform the ST baseline on a token-level task, significantly different from other tasks. Unfortunately, while CA-MTL performs significantly better than the MTL baseline model, CA-MTL had not yet overfit on this particular task and could have closed the gap with the ST baselines with more training cycles. Comparisons with other methods. In <ref type="table" target="#tab_5">Table 5</ref>, CA-MTL BERT is compared to other Large BERT based methods that either use MTL + ST, such as MT-DNN <ref type="bibr" target="#b45">(Liu et al., 2019b)</ref>, intermediate tasks + ST, such as STILTS <ref type="bibr" target="#b54">(Phang et al., 2018)</ref> or MTL model distillation + ST, such as BAM! <ref type="bibr" target="#b14">(Clark et al., 2019c)</ref>. Our method scores higher than MT-DNN on 5 of 9 tasks and by 1.0 % on avg. Against STILTS, CA-MTL realizes a 0.7 % avg. score gain, surpassing scores on 6 of 9 tasks. We also show that CA-MTL RoBERTa is within only 1.6 % of a RoBERTa ensemble of 5 to 7 models per task and that uses intermediate tasks. Using our 24-task CA-MTL large RoBERTa-based model, we report NER F1 scores on the WNUT2017 test set in <ref type="table" target="#tab_6">Table 6a</ref>. We compare our result with RoBERTa LARGE and XLM-R LARGE <ref type="figure">(Nguyen et al., 2020)</ref> the current state-of-the-art (SOTA). Our model outperforms XLM-R LARGE by 1.6%, reaching a new state-of-the-art. Using domain adaptation as described in Section 4.4, we report results on the SciTail test set in <ref type="table" target="#tab_6">Table 6b</ref> and SNLI test set in <ref type="table" target="#tab_6">Table 6b</ref>. For SciTail, our model matches the current SOTA 2 ALUM , a RoBERTa large based model that additionally uses the SMART <ref type="bibr" target="#b33">(Jiang et al., 2020)</ref> fine-tuning method. For SNLI, our model outperforms SemBert, the current SOTA 3 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">TRANSFER TO NEW TASKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">JOINTLY TRAINING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We believe that our experiments here have helped demonstrate the potential of task conditioned adaptive learning within a single model that performs multiple tasks. In a large-scale 24-task NLP experiment, CA-MTL outperforms fully tuned single task models by 2.3% for BERT Large and by 1.2% for RoBERTa Large using 1.12 times the number of parameters, while single task fine-tuning approach requires 24 separately tuned single task models or 24 times the number of parameters. When a BERT vanilla MTL model sees its performance drop as the number of tasks increases, CA-MTL scores continue to climb. Performance gains are not driven by a single task as it is often the case in MTL. Each CA-MTL module that adapts a Transformer model is able to reduce performance variances between tasks, increasing average scores and aligning task covariances. This evidence shows that CA-MTL is able to mitigate task interference and promote more efficient parameter sharing. We showed that MT-Uncertainty is able to avoid degrading performances of low resource tasks. Tasks are sampled whenever the model sees entropy increase, helping avoid catastrophic forgetting. Overall, CA-MTL offers a promising avenue to dynamically adapt and modularize knowledge embedded in large monolithic pretrained models. Extending such ideas will be an objective for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 SUMMARY OF ACRONYMS Acronyms of datasets and descriptions can be found below in section A.9. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 UNCERTAINTY SAMPLING: ALGORITHM AND ADDITIONAL RESULTS</head><p>Algorithm 1: Multi-task Uncertainty Sampling Input: Training data D t for task t ? [1, . . . , T ]; batch size b; C t possible output classes for task t; f := f ?(zi),?i our model with weights ?, ? i ; Output: B -multi-task batch of size b</p><formula xml:id="formula_9">1 B ? ? 2 for t ? 1 to T do 3 Generate x t := {x t,1 , . . . , x t,b } i.i.d. ? D t 4 for i ? 1 to b do 5 H t,i ? ? Ci c=1 p c (f (x t,i )) log p c (f (x t,i ))</formula><p>Entropy of each sample  Return: With B , solve eq. 1 with gradient descent; updated model f An advantage of our MT-Uncertainty Sampling approach is its ability to manage task difficulty. This is highlighted in <ref type="figure" target="#fig_6">Figure 8</ref>. In this experiment, we estimated task difficulty using the Evolutionary Data Measures (EDM) 4 proposed by <ref type="bibr" target="#b15">Collins et al. (2018)</ref>. The task difficulty estimate relies on multiple dataset statistics such as the data size, class diversity, class balance and class interference. Interestingly, estimated task difficulty correlates with the first instance that the selection of a specific task occurs. Supposing that QNLI is an outlier, we notice that peaks in the data occur whenever tasks are first selected by MT Uncertainty sampling. This process follows the following order: 1. MNLI 2. CoLA 3. RTE 4. QQP 5. MRPC 6.SST-2, which is the order from highest task difficulty to lowest task difficulty using EDM. As opposed to Curriculum Learning <ref type="bibr" target="#b2">(Bengio et al., 2009)</ref>, MT-Uncertainty dynamically prioritizes the most difficult tasks. As also discovered in MTL vision work <ref type="bibr" target="#b26">(Guo et al., 2018)</ref>, this type of prioritization on more difficult tasks may explain MT-Uncertainty's improved performance over other task selection methods. In MTL, heuristics to balance tasks during training is typically done by weighting each task's loss differently. We see here how MT-Uncertainty is able to prioritize task difficulty. While the EDM difficulty measure is shown to correlate well with model performance, it lacks precision. As reported in <ref type="bibr" target="#b15">Collins et al. (2018)</ref>, the average score achieved on the Yahoo Answers dataset is 69.9% and its difficulty is 4.51. The average score achieved on Yelp Full is 56.8%, 13.1% less than Yahoo Answers and its difficulty is 4.42. The authors mention that "This indicates that the difficulty measure in its current incarnation may be more effective at assigning a class of difficulty to datasets, rather than a regression-like value".</p><formula xml:id="formula_10">B ? B ? x t and D t ? D t \ x t 13 if D t = ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 OTHER RELATED WORK</head><p>Multi-Tasking in NLP and other fields. MTL weight sharing algorithms such as Mixture-of-Experts (MoE) have found success in NLP <ref type="bibr" target="#b42">(Lepikhin et al., 2020)</ref>. CA-MTL can complement MoE since the Transformers multi-headed attention can be seen as a form of MoE <ref type="bibr" target="#b51">(Peng et al., 2020)</ref>. In Vision, MTL can also improve with optimization <ref type="bibr" target="#b64">(Sener &amp; Koltun, 2018)</ref> or gradient-based approaches <ref type="bibr" target="#b11">(Chen et al., 2017;</ref>.</p><p>Active Learning, Task Selection and Sampling. <ref type="bibr" target="#b32">Ikhwantri et al. (2018)</ref> examined multi-task active learning for neural semantic role labeling in a low resource setting, using entity recognition as the sole auxiliary task. They used uncertainty sampling for active learning and found that 12% less data could be used compared to passive learning. <ref type="bibr" target="#b62">Reichart et al. (2008)</ref> has examined different active learning techniques for the two task annotation scenario, focusing on named entity recognition and syntactic parse tree annotations. In contrast, here we examine the larger scale data regime, the modularization of a multi-task neural architecture, and the many task ( 2) setting among other differences. Other than MTAL <ref type="bibr" target="#b62">(Reichart et al., 2008;</ref><ref type="bibr" target="#b32">Ikhwantri et al., 2018)</ref>, <ref type="bibr" target="#b36">Kendall et al. (2017)</ref> leveraged model uncertainty to balance MTL losses but not to select tasks as is proposed here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 ZERO-SHOT RESULTS ON SCITAIL AND SNLI</head><p>Before testing models on domain adaptation in section 4.4, we ran zero-shot evaluations on the development set of SciTail and SNLI. <ref type="table" target="#tab_9">Table 8</ref> outlines 8-task CA-MTL BERT-BASE 's zero-shot transfer abilities when pretrained on GLUE with our MTL approach. We expand the task embedding layer to accommodate an extra task and explore various embedding initialization. We found that reusing STS-B and MRPC task embeddings worked best for SciTail and SNLI respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 MORE EXPERIMENTAL DETAILS</head><p>We used a batch size of 32 and a seed of 12 in all experiments. We used Adam <ref type="bibr" target="#b40">(Kingma &amp; Ba, 2015)</ref> as the optimizer with a learning rate of 2e-5. We applied a learning rate decay with warm up over the first 10% of the training steps. Unless otherwise specified, we used 5 epochs, a seed of 12 and a sequence length of 128. Additional details are outlined in section . Our data prepossessing and linear decoder heads are the same as in <ref type="bibr" target="#b20">Devlin et al. (2018)</ref>. We used the same dropout rate of 0.1 in all layers. To run our experiments, we used either four NVIDIA P100 GPU for base models or four NVIDIA V100 GPU for larger ones. We did not perform parameter search. We do not use ensemble of models or task-specific tricks <ref type="bibr" target="#b20">(Devlin et al., 2018;</ref><ref type="bibr" target="#b45">Liu et al., 2019b;</ref><ref type="bibr" target="#b14">Clark et al., 2019c)</ref>. All models are either 12 Transformer layers for BASE and 24 Transformer layers for LARGE. Apart from CA-MTL, models trained in multi-task learning (BERT or RoBERTa without adapters) used random task sampling. For <ref type="table" target="#tab_1">Table 1</ref> and <ref type="figure" target="#fig_4">Figure 7</ref>, all BERT-based model have half their layers frozen (untrained) for a fair comparison of ablation results. For the 24-task MTL and CA-MTL models in <ref type="table" target="#tab_4">Tables 4 and 5</ref>, we increased the input sequence length to 256 and used 8 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 THE DIRECT SUM OPERATOR</head><p>In section 2.1.1, we used the direct sum operator ?. This operation allows us to create a block diagonal matrix. The direct sum of a matrix A ? R n?m and B ? R p?q results in a matrix of size (m + p) ? (n + q), defined as:</p><formula xml:id="formula_11">A ? B = A 0 0 B = ? ? ? ? ? ? ? ? ? a 11 ? ? ? a 1n 0 ? ? ? 0 . . . . . . . . . . . . . . . . . . a m1 ? ? ? a mn 0 ? ? ? 0 0 ? ? ? 0 b 11 ? ? ? b 1q . . . . . . . . . . . . . . . . . . 0 ? ? ? 0 b p1 ? ? ? b pq ? ? ? ? ? ? ? ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 BASELINES AND OTHER EXPERIMENTAL RESULTS</head><p>In this section, we present our baseline results for BERT, RoBERTa, CA-MTL as well as other models. Our single task results (ST) that we ran ourselves surpass other paper's reported scores in <ref type="table" target="#tab_10">Table 9</ref>. <ref type="bibr" target="#b47">Liu et al. (2019c)</ref> reports random seed median scores for RoBERTa. However, our RoBERTa ST baseline matches or surpasses the original paper's scores 4 out 7 times on the development set when scores are comparable (QQP F1 and STS-B spearman are not reported).    SuperGLUE has a more diverse task format than GLUE, which is mostly limited to sentence and sentence-pair classification. We follow the same preprocessing procedure as in . All tasks are binary classification tasks, except CB (three classes). Also, WiC and WSC are span based classification tasks. We used the same modified MRQA dataset and preprocessing steps that were used in <ref type="bibr" target="#b35">Joshi et al. (2019)</ref>. All MRQA tasks are span prediction tasks which seeks to identify start and end tokens of an answer span in the input text. SNLI is a natural inference task where we predict three classes. Examples of three target labels are: Entailment, Contradiction, and Neutral (irrelevant). SciTail is a textual entailment dataset. The hypotheses in SciTail are created from multiple-choice science exams and the answer candidates (premise) are extracted from the web using information retrieval tools. SciTail is a binary true/false classification tasks that seeks to predict whether the premise entails the hypothesis. The two datasets are used only for domain adaptation in this study (see section A.4 for the details of our approach).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>CA-MTL base architecture with our</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure</head><label></label><figDesc>Figure 3: a) Conditional Bottleneck for CA-MTLBASE. b) Conditional Bottleneck for CA-MTLLARGE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>train entropy CoLA train entropy Figure 5: CoLA/MNLI Dev set scores and Entropy for ? rand (left) and MT-Uncertainty (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Task performance vs. avg. covariance similarity scores (eq. 7) for MTL and CA-MTL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>ON 24 TASKS: GLUE/SUPER-GLUE, MRQA AND WNUT2017 Effects of adding more datasets on avg GLUE scores. Experiments conducted on 3 epochs. When 23 tasks are trained jointly, performance of CA-MTLBERT-BASE continues to improve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Task composition of MT-Uncertainty sampling and estimated task difficulty using EDM: number of training samples per task at each iteration for batch size of 32. The occurrence of first peaks and estimated difficulty follow the same order: From highest to lowest: MNLI &gt; CoLA &gt; RTE &gt; QQP = MRPC &gt; SST-2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). Hyperparameters and our experimental set-up are outlined in A.5. To preserve the weights of the pretrained model, CA-MTL's bottom half Transformer layers are frozen in all experiments (except in section 4.4). We also tested different layer freezing configurations and found that freezing half the layers worked best on average (see Section A.8).</figDesc><table><row><cell>4.1 MULTI-TASK UNCERTAINTY SAMPLING</cell><cell></cell></row><row><cell></cell><cell>0.82</cell></row><row><cell></cell><cell>0.80</cell></row><row><cell></cell><cell>0.78</cell></row><row><cell>Average score</cell><cell>0.72 0.74 0.76</cell></row><row><cell></cell><cell>0.70</cell><cell>MT-Uncertainty</cell></row><row><cell></cell><cell></cell><cell>Couterfactual</cell></row><row><cell></cell><cell>0.68</cell><cell>Task size</cell></row><row><cell></cell><cell></cell><cell>Random</cell></row><row><cell></cell><cell>0.66</cell></row><row><cell></cell><cell>0</cell><cell>25000 50000 75000 100000 125000 150000 175000 200000</cell></row><row><cell></cell><cell></cell><cell>Training iteration</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Model ablation study a on the GLUE dev set. All models have the bottom half layers frozen. Task ?=scores standard deviation across tasks.</figDesc><table><row><cell>Model changes</cell><cell>Avg Task ? % data GLUE GLUE used</cell></row><row><cell cols="2">BERTBASE MTL (? rand ) 80.61 14.41 100</cell></row><row><cell cols="2">+ Conditional Attention 82.41 10.67 100</cell></row><row><cell cols="2">+ Conditional Adapter 82.90 11.27 100</cell></row><row><cell>+ CA and CLN</cell><cell>83.12 10.91 100</cell></row><row><cell>+ MT-Uncertainty (CA-MTLBERT-BASE)</cell><cell>84.03 10.02 66.3</cell></row><row><cell cols="2">a CA=Conditional Alignment, CLN=Conditional Layer Normal-</cell></row><row><cell>ization,</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Adapters with layer freezing vs. ST/MT on GLUE test set. F1 scores are reported for QQP/MRPC, Spearman's correlation for STS-B, accuracy on the matched/mismatch sets for MNLI, Matthew's correlation for CoLA and accuracy for other tasks. * Individual scores not available. ST=Single Task, MTL=Multitask, g.e.= greater or equal to. Results from: 1 Devlin et al. (2018) 2 Stickland et al. (2019). 3 Houlsby et al. (2019) .</figDesc><table><row><cell>Method</cell><cell>Type</cell><cell cols="4">Total params params/task g.e. ST CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B Avg Trained # tasks GLUE</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Base Models -Test Server Results</cell></row><row><cell>BERTBASE 1</cell><cell cols="2">ST 9.0?</cell><cell>100%</cell><cell>-</cell><cell>52.1 84.6/83.4 88.9 90.5 71.2 66.4 93.5 85.8 79.6</cell></row><row><cell>BERTBASE 2</cell><cell cols="2">MTL 1.0?</cell><cell>11.1%</cell><cell>2</cell><cell>51.2 84.0/83.4 86.7 89.3 70.8 76.6 93.4 83.6 79.9</cell></row><row><cell>PALs+Anneal Samp. 2</cell><cell cols="2">MTL 1.13?</cell><cell>12.5%</cell><cell>4</cell><cell>51.2 84.3/83.5 88.7 90.0 71.5 76.0 92.6 85.8 80.4</cell></row><row><cell cols="3">CA-MTLBERT-BASE (ours) MTL 1.12?</cell><cell>5.6 %</cell><cell>5</cell><cell>53.1 85.9/85.8 88.6 90.5 69.2 76.4 93.2 85.3 80.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Large Models -Test Server Results</cell></row><row><cell>BERTLARGE 1</cell><cell cols="2">ST 9.0?</cell><cell>100%</cell><cell>-</cell><cell>60.5 86.7/85.9 89.3 92.7 72.1 70.1 94.9 86.5 82.1</cell></row><row><cell>Adapters-256 3</cell><cell cols="2">ST 1.3?</cell><cell>3.6%</cell><cell>3</cell><cell>59.5 84.9/85.1 89.5 90.7 71.8 71.5 94.0 86.9 80.0</cell></row><row><cell cols="3">CA-MTLBERT-LARGE (ours) MTL 1.12?</cell><cell>5.6%</cell><cell>3</cell><cell>59.5 85.9/85.4 89.3 92.6 71.4 79.0 94.7 87.7 82.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Domain adaptation results on dev. sets for BASE models.</figDesc><table><row><cell>% data used</cell><cell>SciTail 0.1% 1% 10% 100% 0.1% 1% 10% 100% SNLI</cell></row><row><cell>BERTBASE 1</cell><cell>51.2 82.2 90.5 94.3 52.5 78.1 86.7 91.0</cell></row><row><cell>MT-DNN 1</cell><cell>81.9 88.3 91.1 95.7 81.9 88.3 91.1 95.7</cell></row><row><cell cols="2">MT-DNNSMART 2 82.3 88.6 91.3 96.1 82.7 86.0 88.7 91.6</cell></row><row><cell>CA-MTLBERT</cell><cell>83.2 88.7 91.4 95.6 82.8 86.2 88.0 91.5</cell></row></table><note>1 Liu et al. (2019b), 2 Jiang et al. (2020)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>24-task CA-MTL vs. ST and vs. 24-task MTL with frozen layers on GLUE, SuperGLUE, MRQA and NER development sets. ST=Single Task, MTL=Multitask, g.e.= greater or equal to. Details in section A.5.</figDesc><table><row><cell>Model</cell><cell cols="3">Task Grouping GLUE SuperGLUE MRQA NER</cell><cell>Avg</cell><cell cols="2"># tasks e.g. ST Params Total</cell></row><row><cell cols="2">BERT-LARGE models</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>STReImp</cell><cell>84.5</cell><cell>68.9</cell><cell cols="2">79.7 54.1 76.8</cell><cell>-</cell><cell>24?</cell></row><row><cell cols="2">MTLReImp 83.2</cell><cell>72.1</cell><cell cols="3">77.8 42.2 76.4 9/24</cell><cell>1?</cell></row><row><cell cols="2">CA-MTL 86.6</cell><cell>74.1</cell><cell cols="4">79.5 49.0 79.1 17/24 1.12?</cell></row><row><cell cols="3">RoBERTa-LARGE models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>STReImp</cell><cell>88.2</cell><cell>76.5</cell><cell cols="2">83.6 57.8 81.9</cell><cell>-</cell><cell>24?</cell></row><row><cell cols="2">MTLReImp 86.0</cell><cell>78.6</cell><cell cols="3">80.7 49.3 80.7 7/24</cell><cell>1?</cell></row><row><cell cols="2">CA-MTL 89.4</cell><cell>80.0</cell><cell cols="4">82.4 55.2 83.1 15/24 1.12?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Our 24-task CA-MTL vs. other large models on GLUE. F1 is reported for QQP/MRPC, Spearman's corr. for STS-B, Matthew's corr. for CoLA and accuracy for other tasks. *Split not available.</figDesc><table><row><cell>Model</cell><cell cols="5">GLUE tasks CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B</cell><cell>Avg</cell></row><row><cell cols="4">BERT-LARGE based models on Dev set.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MT-DNN</cell><cell cols="6">63.5 87.1/86.7 91.0 92.9 89.2 83.4 94.3 90.6 85.6</cell></row><row><cell>STILTS **</cell><cell>62.1</cell><cell>86.1*</cell><cell cols="4">92.3 90.5 88.5 83.4 93.2 90.8 85.9</cell></row><row><cell>BAM!</cell><cell>61.8</cell><cell>87.0*</cell><cell>-</cell><cell>92.5</cell><cell>-82.8 93.6 89.7</cell><cell>-</cell></row><row><cell cols="7">24-task CA-MTL 63.8 86.3/86.0 92.9 93.4 88.1 84.5 94.5 90.3 86.6</cell></row><row><cell cols="4">RoBERTa-LARGE based models on Test set.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">RoBERTA** with 67.8 91.0/90.8 91.6 95.4 74.0 87.9 97.5 92.5 87.3 Ensemble</cell></row><row><cell cols="7">24-task CA-MTL 62.2 89.0/88.4 92.0 94.7 72.3 86.2 96.3 89.8 85.7</cell></row></table><note>**Uses intermediate task fine-tuning + ST.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>CA-MTL test performance vs. SOTA.</figDesc><table><row><cell>(a) WNUT2017 RoBERTaLARGE XLM-RLARGE CA-MTLRoBERTa (ours) 58.0 F1 56.9 57.1</cell><cell>(b) SciTail MT-DNN ALUMRoBERTa ALUMRoBERTa-SMART CA-MTLRoBERTa (ours) 96.8 % Acc 94.1 96.3 96.8</cell><cell>(c) SNLI MT-DNN MT-DNNSMART SemBERT CA-MTLRoBERTa (ours) 92.1 % Acc 91.6 91.7 91.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>List of acronyms used in this paper.</figDesc><table><row><cell>Acronym</cell><cell>Description</cell></row><row><cell>ARLM</cell><cell>Autoregressive Language Models</cell></row><row><cell>CA-MTL</cell><cell>Conditional Adaptive Multi-Task Learning: our architecture</cell></row><row><cell>CFF</cell><cell>Conditional Feed-Forward: a feed-forward layer modulated by a conditioning vector</cell></row><row><cell>CLN</cell><cell>Conditional Layer Normalization in section 2.1.3</cell></row><row><cell>EDM</cell><cell>Evolutionary Data Measures (Collins et al., 2018): a task difficulty estimate</cell></row><row><cell>GLUE</cell><cell>General Language Understanding Evaluation Wang et al. (2018): a benchmark with multiple datasets</cell></row><row><cell>QA</cell><cell>Question Answering</cell></row><row><cell>MT</cell><cell>Multi-Task</cell></row><row><cell>MTAL</cell><cell>Multi-Task Active Learning: finding the most informative instance for multiple learners (or models)</cell></row><row><cell>MLM</cell><cell>Masked Language Model: BERT Devlin et al. (2018) is an example of an MLM</cell></row><row><cell>MTL</cell><cell>Multi-Task Learning: "learning tasks in parallel while using a shared representation" (Caruana, 1997)</cell></row><row><cell>MRQA</cell><cell>Machine Reading for Question Answering Fisch et al. (2019): a benchmark with multiple datasets</cell></row><row><cell>NER</cell><cell>Named Entity Recognition</cell></row><row><cell>NLP</cell><cell>Natural Language Processing</cell></row><row><cell>SOTA</cell><cell>State of the art</cell></row><row><cell>ST</cell><cell>Single Task fine-tuning: all weights are typically updated</cell></row><row><cell>ST-A</cell><cell>ST with Adapter modules: one adapter per task is trained and pretrained weights are optionally updated</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Compute? ? max i?{1,...,T } [H t ] B ? top_b({U t,i |t ? [1, . . . , T ], i ? [1, . . . , b]})</figDesc><table><row><cell></cell><cell>then</cell><cell></cell></row><row><cell>14</cell><cell>Reload D t</cell><cell></cell></row><row><cell>15</cell><cell>end</cell><cell></cell></row><row><cell>16</cell><cell>for i ? 1 to b do</cell><cell></cell></row><row><cell>17</cell><cell>Compute: U t,i ? H t,i /H t</cell><cell>Uncertainty normalized with max entropy</cell></row><row><cell>18</cell><cell>end</cell><cell></cell></row><row><cell cols="2">19 end</cell><cell></cell></row><row><cell cols="3">20 Entropy of task with highest average entropy</cell></row><row><cell cols="2">21 Update U t,i ? U t,i /?</cell><cell>Normalize each sample's uncertainty measure</cell></row><row><cell></cell><cell></cell><cell>b samples w/ highest uncertainty</cell></row></table><note>22</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>CA-MTL is flexible and extensible to new tasks. However, CA-MTL is sensitive to the new task's embedding. We tested multiple task embeddings that worked best on either SciTail or SNLI by checking performance in a zero shot setting or using 0% of the data.</figDesc><table><row><cell>Initialization of new</cell><cell>SciTail</cell><cell>SNLI</cell></row><row><cell>task embedding layer</cell><cell cols="2">0% of data 0% of data</cell></row><row><cell>CoLA's embeddings</cell><cell>43.0</cell><cell>34.0</cell></row><row><cell>MNLI's embeddings</cell><cell>24.2</cell><cell>33.0</cell></row><row><cell>MRPC's embeddings</cell><cell>34.5</cell><cell>45.5</cell></row><row><cell>STS-B's embeddings</cell><cell>46.9</cell><cell>33.2</cell></row><row><cell>SST-2's embeddings</cell><cell>25.8</cell><cell>34.2</cell></row><row><cell>QQP's embeddings</cell><cell>31.7</cell><cell>37.3</cell></row><row><cell>QNLI's embeddings</cell><cell>32.0</cell><cell>38.0</cell></row><row><cell>RTE's embeddings</cell><cell>32.3</cell><cell>40.6</cell></row><row><cell>WNLI's embeddings</cell><cell>29.0</cell><cell>30.4</cell></row><row><cell>Average</cell><cell>28.7</cell><cell>37.7</cell></row><row><cell>Random initialization</cell><cell>46.8</cell><cell>34.0</cell></row><row><cell>Xavier initialization</cell><cell>29.8</cell><cell>37.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>F1 scores are reported for QQP/MRPC, Spearman's correlation for STS-B, accuracy on the matched/mismatch sets for MNLI, Matthew's correlation for CoLA and accuracy for other tasks. ST=Single Task, MTL=Multitask. *QNLI v1 (we report v2) **F1 score or Spearman's correlation is not reported. ***Unknown random seeds. Results from: 1 Stickland et al. (2019) 2 Liu et al. (2019b) 3 Phang et al. (2018) 4 Liu et al. (2019c).</figDesc><table><row><cell>Method</cell><cell cols="10">Total params params/task CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B Avg Trained GLUE</cell></row><row><cell></cell><cell></cell><cell cols="3">Base Models -Dev set Results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PALs+Anneal Samp. 1</cell><cell>1.13?</cell><cell>12.5%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>GLUE<ref type="bibr" target="#b75">(Wang et al., 2018)</ref> dataset description. References: 1 Warstadt et al. (2018), 2 Socher et al. (2013), 3 Dolan &amp; Brockett (2005), 4 Cer et al. (2017), 5 Williams et al. (2018), 6 Wang et al. (2018), 7 Levesque(2011)</figDesc><table><row><cell cols="2">Acronym Corpus</cell><cell cols="2">|Train| Task</cell><cell>Domain</cell></row><row><cell>CoLA 1</cell><cell>Corpus of Linguistic Acceptability</cell><cell>8.5K</cell><cell>acceptability</cell><cell>miscellaneous</cell></row><row><cell>SST-2 2</cell><cell>Stanford Sentiment Treebank</cell><cell>67K</cell><cell>sentiment detection</cell><cell>movie reviews</cell></row><row><cell>MRPC 3</cell><cell>Microsoft Research Paraphrase Corpus</cell><cell>3.7K</cell><cell cols="2">paraphrase detection news</cell></row><row><cell>STS-B 4</cell><cell cols="2">Semantic Textual Similarity Benchmark 7K</cell><cell>textual similarity</cell><cell>miscellaneous</cell></row><row><cell>QQP</cell><cell>Quora Question Pairs</cell><cell>364K</cell><cell cols="2">paraphrase detection online QA</cell></row><row><cell>MNLI 5</cell><cell>Multi-Genre NLI</cell><cell>393K</cell><cell>inference</cell><cell>miscellaneous</cell></row><row><cell>RTE 6</cell><cell>Recognition Textual Entailment</cell><cell>2.5K</cell><cell cols="2">inference/entailment news, Wikipedia</cell></row><row><cell>WNLI 7</cell><cell>Winograd NLI</cell><cell>634</cell><cell>coreference</cell><cell>fiction books</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Super-GLUE (Wang et al., 2019b) dataset description. References: 1 Clark et al. (2019a), 2 de Marneffe et al. (2019), 3 Gordon et al. (2012), 4 Khashabi et al. (2018), 5 Zhang et al. (2018), 6 Wang et al. (2019b), 7 Poliak et al. (2018), 8 Levesque(2011)</figDesc><table><row><cell cols="2">Acronym Corpus</cell><cell cols="2">|Train| Task</cell><cell>Domain</cell></row><row><cell>BoolQ 1</cell><cell>Boolean Questions</cell><cell>9.4K</cell><cell>acceptability</cell><cell>Google queries, Wikipedia</cell></row><row><cell>CB 2</cell><cell>CommitmentBank</cell><cell>250</cell><cell>sentiment detection</cell><cell>miscellaneous</cell></row><row><cell>COPA 3</cell><cell>Choice of Plausible Alternatives</cell><cell>400</cell><cell>paraphrase detection</cell><cell>blogs, encyclopedia</cell></row><row><cell cols="3">MultiRC 4 Multi-Sentence Reading Comprehension 5.1K</cell><cell>textual similarity</cell><cell>miscellaneous</cell></row><row><cell cols="2">ReCoRD 5 Reading Comprehension</cell><cell>101K</cell><cell>paraphrase detection</cell><cell>news</cell></row><row><cell></cell><cell>and Commonsense Reasoning</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RTE 6</cell><cell>Recognition Textual Entailment</cell><cell>2.5K</cell><cell>inference</cell><cell>news, Wikipedia</cell></row><row><cell>WiC 7</cell><cell>Word-in-Context</cell><cell>6K</cell><cell cols="2">word sense disambiguation WordNet, VerbNet</cell></row><row><cell>WSC 8</cell><cell>Winograd Schema Challenge</cell><cell>554</cell><cell>coreference resolution</cell><cell>fiction books</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>MRQA (Fisch et al., 2019) dataset description. References: 1 Rajpurkar et al. (2016a), 2 Trischler et al. (2017), 3 Joshi et al. (2017), 4 Dunn et al. (2017), 5 Yang et al. (2018), 6 Kwiatkowski et al. (2019)</figDesc><table><row><cell>Acronym</cell><cell>Corpus</cell><cell cols="2">|Train| Task</cell><cell>Domain</cell></row><row><cell>SQuAD 1</cell><cell cols="2">Stanford QA Dataset 86.6K</cell><cell cols="2">crowdsourced questions Wikipedia</cell></row><row><cell>NewsQA 2</cell><cell>NewsQA</cell><cell>74.2K</cell><cell cols="2">crowdsourced questions news</cell></row><row><cell>TriviaQA 3</cell><cell>TriviaQA</cell><cell>61.7K</cell><cell>trivia QA</cell><cell>web snippets</cell></row><row><cell>SearchQA 4</cell><cell>SearchQA</cell><cell cols="2">117.4K Jeopardy QA</cell><cell>web snippets</cell></row><row><cell>HotpotQA 5</cell><cell>HotpotQA</cell><cell>72.9K</cell><cell cols="2">crowdsourced questions Wikipedia</cell></row><row><cell cols="2">Natural Questions 6 Natural Questions</cell><cell cols="2">104.7K search logs</cell><cell>Wikipedia</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>SNLI (Bowman et al., 2015)  andSciTail (Khot et al., 2018)  datasets description. Stanford Natural Language Inference 550.2k inference human-written English sentence pairs SciTail 2 Science and Entailment 23.5K entailment Science question answering</figDesc><table><row><cell>Acronym Corpus</cell><cell>|Train| Task</cell><cell>Domain</cell></row><row><cell>SNLI 1</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://leaderboard.allenai.org/scitail/submissions/public on 09/27/2020 3 https://nlp.stanford.edu/projects/snli/ on 09/27/2020</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/Wluper/edm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was supported by the Canada CIFAR AI Chairs Program, NSERC and PROMPT. Experiments in this article were conducted with Compute Canada and MILA computational infrastructure and we thank them for their support. We would like to thank Colin Raffel, Sandeep Subramanian, and Nicolas Gontier for their useful feedback and the anonymous reviewers for helpful comments, discussions and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Massively multilingual neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1388</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1388" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3874" to="3884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Layer normalization. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<ptr target="http://arxiv.org/abs/1607.06450" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Identifying beneficial task relations for multi-task learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>S?gaard</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/E17-2026" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="164" to="169" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sastry</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2005</biblScope>
		</imprint>
	</monogr>
	<note>et al. Language models are few-shot learners. arXiv</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1007379606734</idno>
		<ptr target="https://doi.org/10.1023/A:1007379606734" />
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multitask learning: A knowledge-based source of inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Machine Learning</title>
		<meeting>the Tenth International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?igo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2001</idno>
		<ptr target="https://www.aclweb.org/anthology/S17-2001" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Counterfactual reasoning and learning systems: The example of computational advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3207" to="3260" />
			<date type="published" when="2013-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An empirical study of the behavior of active learning for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Schein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N06-1016" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Main Conference</title>
		<meeting>the Human Language Technology Conference of the NAACL, Main Conference<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>abs/1711.02257</idno>
		<ptr target="http://arxiv.org/abs/1711.02257" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BoolQ: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>doi: 10.18653/ v1/N19-1300</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1300" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2924" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What does BERT look at? an analysis of BERT&apos;s attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4828</idno>
		<ptr target="https://www.aclweb.org/anthology/W19-4828" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08" />
			<biblScope unit="page" from="276" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bam! born-again multi-task networks for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1907.04829</idno>
		<ptr target="http://arxiv.org/abs/1907.04829" />
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evolutionary data measures: Understanding the difficulty of text classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolai</forename><surname>Rozanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K18-1037</idno>
		<ptr target="https://www.aclweb.org/anthology/K18-1037" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Computational Natural Language Learning</title>
		<meeting>the 22nd Conference on Computational Natural Language Learning<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="380" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.1145/1390156.1390177</idno>
		<ptr target="https://doi.org/10.1145/1390156.1390177" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The commitmentbank: Investigating projection in naturally occurring discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>Tonhauser</surname></persName>
		</author>
		<idno type="DOI">10.18148/sub/2019.v23i2.601</idno>
		<ptr target="https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Sinn und Bedeutung</title>
		<meeting>Sinn und Bedeutung</meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="107" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7237-modulating-early-visual-processing-by-language.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6594" to="6604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Results of the WNUT2017 shared task on novel and emerging entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marieke</forename><surname>Van Erp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4418</idno>
		<ptr target="https://www.aclweb.org/anthology/W17-4418" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Noisy User-generated Text</title>
		<meeting>the 3rd Workshop on Noisy User-generated Text<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="140" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brockett</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/I05-5002" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>
		<meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Searchqa: A new q&amp;a dataset augmented with context from a search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Ugur</forename><surname>G?ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>abs/1704.05179</idno>
		<ptr target="http://arxiv.org/abs/1704.05179" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MRQA 2019 shared task: Evaluating generalization in reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5801</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-5801" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Machine Reading for Question Answering</title>
		<meeting>the 2nd Workshop on Machine Reading for Question Answering<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Task selection policies for multitask learning. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Glover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.06214" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/S12-1052" />
	</analytic>
	<monogr>
		<title level="m">*SEM 2012: The First Joint Conference on Lexical and Computational Semantics</title>
		<meeting><address><addrLine>Montr?al, Canada, 7-8</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="394" to="398" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic task prioritization for multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A hybrid neural network model for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-6002</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-6002" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing</title>
		<meeting>the First Workshop on Commonsense Inference in Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="13" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v14/hoffman13a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Parameter-efficient transfer learning for NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1902.00751</idno>
		<ptr target="http://arxiv.org/abs/1902.00751" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1031</idno>
		<ptr target="https://www.aclweb.org/anthology/P18-1031" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-task active learning for neural semantic role labeling on low resource conversational corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fariz</forename><surname>Ikhwantri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Louvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Kurniawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bagas</forename><surname>Abisena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfan</forename><surname>Valdi Rachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahmad</forename><surname>Farizki Wicaksono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahendra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP</title>
		<meeting>the Workshop on Deep Learning Approaches for Low-Resource NLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SMART: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.197</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.197" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="2177" to="2190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>doi: 10.18653/ v1/P17-1147</idno>
		<ptr target="https://www.aclweb.org/anthology/P17-1147" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Spanbert: Improving pre-training by representing and predicting spans. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.10529" />
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. CoRR, abs/1705.07115</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1705.07115" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">When does deep multi-task learning work for loosely related document classification tasks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Kerinec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chlo?</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>S?gaard</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5401</idno>
		<ptr target="https://www.aclweb.org/anthology/W18-5401" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Looking beyond the surface: A challenge set for reading comprehension over multiple sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1023</idno>
		<ptr target="https://www.aclweb.org/anthology/N18-1023" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="252" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scitail: A textual entailment dataset from science question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tushar Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">N</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Gshard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16668</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/aaaiss/aaaiss2011-6.html#Levesque11" />
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning. AAAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Linguistic knowledge and transferability of contextual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno>abs/1903.08855</idno>
		<ptr target="http://arxiv.org/abs/1903.08855" />
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Multi-task deep neural networks for natural language understanding. CoRR, abs/1901.11504</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1901.11504" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Adversarial training for large neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">The natural language decathlon: Multitask learning as question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08730</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amil</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elahe</forename><surname>Rahimtoroghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14448</idno>
		<title level="m">What happens to bert embeddings during fine-tuning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Bertweet: A pre-trained language model for english tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10200</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A mixture of h -1 heads is better than h heads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.587</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.587" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="6566" to="6577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Dissecting contextual word embeddings: Architecture and representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno>abs/1808.08949</idno>
		<ptr target="http://arxiv.org/abs/1808.08949" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>F?vry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>abs/1811.01088</idno>
		<ptr target="http://arxiv.org/abs/1811.01088" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Collecting diverse natural language inference problems for sentence representation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparajita</forename><surname>Haldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Edward</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">Steven</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1007</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-1007" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="67" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Parameter space factorization for zero-shot learning across tasks and languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinela</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Parovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00374</idno>
		<ptr target="https://aclanthology.org/2021.tacl-1.25" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="410" to="428" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Phu Mon Htut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">Yuanzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00628</idno>
		<title level="m">Intermediate-task transfer learning with pretrained models for natural language understanding: When and why does it work? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
		<ptr target="https://www.aclweb.org/anthology/D16-1264" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multi-task active learning for linguistic annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Tomanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udo</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="861" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno>abs/1706.05098</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Multi-task learning as multi-objective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno>abs/1810.04650</idno>
		<ptr target="http://arxiv.org/abs/1810.04650" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting with hard attention to the task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didac</forename><surname>Suris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/serra18a.html" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4555" to="4564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D13-1170" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Which tasks should be learned together in multi-task learning? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Standley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<idno>abs/1905.07553</idno>
		<ptr target="http://arxiv.org/abs/1905.07553" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Projected attention layers for efficient adaptation in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asa</forename><forename type="middle">Cooper</forename><surname>Stickland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Someone</forename><surname>Bert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pals</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/stickland19a.html" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.05891</idno>
		<title level="m">Hypergrid: Efficient multi-task transformers with grid-wise decomposable hyper projections</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">BERT rediscovers the classical NLP pipeline. CoRR, abs/1905.05950</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.05950" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">What do you learn from context? probing for sentence structure in contextualized word representations. CoRR, abs/1905.06316</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Thomas</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.06316" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">NewsQA: A machine comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-2623</idno>
		<ptr target="https://www.aclweb.org/anthology/W17-2623" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Attention is all you need. CoRR, abs/1706.03762</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Continual learning with hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Johannes Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Henning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">F</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Sacramento</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJgwNerKvB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
		<ptr target="https://www.aclweb.org/anthology/W18-5446" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Can you tell me how to get past sesame street? sentence-level pretraining beyond language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavendra</forename><surname>Pappagari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Thomas</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roma</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuning</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>abs/1905.00537</idno>
		<ptr target="http://arxiv.org/abs/1905.00537" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1805.12471" />
		<title level="m">Neural network acceptability judgments. CoRR, abs/1805.12471</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
		<ptr target="https://www.aclweb.org/anthology/N18-1101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.03771" />
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Understanding and improving information transfer in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hongyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R?</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SylzhkBtDB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">HotpotQA: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1259</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-1259" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Gradient surgery for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06782</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Record: Bridging the gap between human and machine commonsense reading comprehension</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1810.12885</idno>
		<ptr target="http://arxiv.org/abs/1810.12885" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">A survey on multi-task learning. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1707.08114</idno>
		<ptr target="http://arxiv.org/abs/1707.08114.81.708-taskCA-MTLBERT-BASE(ours" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
				<title level="m">A.8 SOME RESULTS ON LAYER FREEZING AND WITH FULL BLOCK ATTENTION</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Results in Table 10 reveal that as we freeze more layers, performance tends to decrease. However, since we wanted to preserve as much pretrained knowledge as possible, we chose to keep at least 50% of layers frozen. While this has slightly lowered our performance on 9 GLUE tasks, we believe that keeping as much of the original pretrained weights is beneficial when increasing the total number of tasks in MTL to 24 or more tasks. However</title>
	</analytic>
	<monogr>
		<title level="m">All experiments in this section were run for only 5 epochs, exclusively on the GLUE dataset for the large BERT-based 8-task CA-MTL model</title>
		<imprint/>
	</monogr>
	<note>we did not explore this hypothesis more</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">3) for various layer freezing configurations. F1 scores are reported for QQP/MRPC, Spearman&apos;s correlation for STS-B, accuracy on the matched/mismatch sets for MNLI, Matthew&apos;s correlation for CoLA and accuracy for other tasks. FBA = Full Block Attention Method % frozen # tasks GLUE layers g</title>
	</analytic>
	<monogr>
		<title level="m">ST CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B Avg LARGE Models -Dev set Results ST BERT-LARGE (ours)</title>
		<editor>8-task CA-MTL BERT-LARGE</editor>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">0</biblScope>
		</imprint>
	</monogr>
	<note>see section 4.</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">DATASET DESCRIPTION The datasets that were used for the domain adaptation experiments were SciTail 5 and SNLI 6</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">We jointly trained a CA-MTL RoBERTa-LARGE model on 9 GLUE tasks, 8 Super-GLUE 7 tasks, 6 MRQA 8 tasks, and on WNUT2017 9</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">All GLUE tasks are binary classification, except STS-B (regression) and MNLI (three classes)</title>
		<editor>Devlin et al.</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>We used the same GLUE data preprocessing as in</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
