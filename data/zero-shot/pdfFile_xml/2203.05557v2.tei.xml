<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conditional Prompt Learning for Vision-Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
							<email>kaiyang.zhou@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
							<email>jingkang001@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
							<email>ziwei.liu@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Conditional Prompt Learning for Vision-Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the rise of powerful pre-trained vision-language models like CLIP, it becomes essential to investigate ways to adapt these models to downstream datasets. A recently proposed method named Context Optimization (CoOp) introduces the concept of prompt learning-a recent trend in NLP-to the vision domain for adapting pre-trained visionlanguage models. Specifically, CoOp turns context words in a prompt into a set of learnable vectors and, with only a few labeled images for learning, can achieve huge improvements over intensively-tuned manual prompts. In our study we identify a critical problem of CoOp: the learned context is not generalizable to wider unseen classes within the same dataset, suggesting that CoOp overfits base classes observed during training. To address the problem, we propose Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector). Compared to CoOp's static prompts, our dynamic prompts adapt to each instance and are thus less sensitive to class shift. Extensive experiments show that CoCoOp generalizes much better than CoOp to unseen classes, even showing promising transferability beyond a single dataset; and yields stronger domain generalization performance as well. Code is available at https://github.com/ KaiyangZhou/CoOp.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent research in large-scale vision-language pretraining has achieved striking performance in zero-shot image recognition <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b40">40]</ref>, demonstrating a potential in learning open-world visual concepts for such a paradigm.</p><p>The key design lies in how visual concepts are modeled. In traditional supervised learning where labels are discretized, each category is associated with a randomly initialized weight vector that is learned to minimize the distance with images containing the same category. Such a learning Corresponding author method focuses on closed-set visual concepts, limiting the model to a pre-defined list of categories, and is unscalable when it comes to new categories unseen during training.</p><p>In contrast, for vision-language models 1 like CLIP <ref type="bibr" target="#b40">[40]</ref> and ALIGN <ref type="bibr" target="#b24">[24]</ref>, the classification weights are diametrically generated by a parameterized text encoder (e.g., a Transformer <ref type="bibr" target="#b48">[48]</ref>) through prompting <ref type="bibr" target="#b34">[34]</ref>. For instance, to differentiate pet images containing different breeds of dogs and cats, one can adopt a prompt template like "a photo of a {class}, a type of pet" as input to the text encoder, and as a result, class-specific weights for classification can be synthesized by filling in the "{class}" token with real class names. Compared to discrete labels, vision-language models' source of supervision comes from natural language, which allows open-set visual concepts to be broadly explored and has been proven effective in learning transferable representations <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b40">40]</ref>.</p><p>With the rise of such powerful vision-language models, the community has recently started to investigate potential solutions to efficiently adapt these models to downstream datasets <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b63">63]</ref>. To fit web-scale data, such as the 400 million pairs of images and texts used by CLIP, visionlanguage models are purposefully designed to have high capacity, entailing that the model size would be enormous, typically with hundreds of millions of parameters or even billions. Therefore, fine-tuning the entire model, as often adopted in deep learning research <ref type="bibr" target="#b18">[18]</ref>, is impractical and might even damage the well-learned representation space.</p><p>A safer approach is to tune a prompt by adding some context that is meaningful to a task, like "a type of pet" for the pet dataset mentioned above, which has been found effective in improving performance <ref type="bibr" target="#b40">[40]</ref>. However, prompt engineering is extremely time-consuming and inefficient as it has to be based on trial and error, and does not guarantee an optimal prompt either. To automate prompt engineering, Zhou et al. <ref type="bibr" target="#b63">[63]</ref> have recently explored the concept of prompt learning-a recent trend in NLP <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b60">60]</ref>-for adapting pre-trained vision-language models. Their approach, Context Optimization (CoOp), turns con-                 text words in a prompt into a set of learnable vectors, taking advantage of the differentiable nature of neural networks. With only a few labeled images for learning, CoOp achieves huge improvements over intensively-tuned manual prompts across a wide range of image recognition datasets.</p><formula xml:id="formula_0">K G 1 n T U K x p m k p Y D c t J O i M C h M U h F l G O V Q e 3 G p o S B 0 T q 4 g s j A n A n R S t T v W + M g y G Z 5 K Z b / c 4 J b t V l R E a L 0 S q c 0 U d k V 9 O 9 a Q d 8 W i 0 k x P k o r l R W k g p z e D p i X H R u L m G e C M K a C G r y w g V D G 7 K 6 Y z Y k U x 9 r F s T E n F x h 0 q U X L D l F x u s q m U c 0 N S X X t W w f C 2 X t v g f O C H Q z / 4 N O i d 9 t</formula><formula xml:id="formula_1">L I f G J Q z 8 A u K K K O B U M O N F p W Y F o U t y z U I L c 5 I x H V d N v s E n l k n x X C r 7 5 Y A b t t t R k U z r T Z b Y k x m B h d 7 W a v J f W l j C / H V c 8 b w o g e X 0 N m h e C g w S 1 6 8 A p 1 w x C m J j A a G K 2 3 / F d E E U o W D f S i 8 l y X p 3 q L J S A F d y 3 W c T K Z d A E m 0 8 O 8 F g e 1 6 7 4 G I 8 C k 5 H / o f x 4 G z Y z n I P P U c v 0 B A F 6 B U 6 Q 2 / Q O Z o i 6 o D z y f n s f H G / u t / d H + 7 P 2 6 O u 0 / Y 8 Q 7 1 y f / 0 G A p s F g w = = &lt; / l a t e x i t &gt; CoCoOp &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n J G H b + f S e Y b F H t G p v Q f s L k Y u V T 0 = " &gt; A A A D M n i c j V L L b t Q w F H X C q 6 R Q W l i y s Z h W G j a j Z B a F Z a V u u g G K x L S V k m j k O E 7 H G j u O 7 J u Z j q L 8 R 7 f w D f w</formula><formula xml:id="formula_2">V f 4 J V w W J J r n O e c U r A U t M D Z / f o M I w S W S 3 q a R A f 4 r / N u G m i W a r A d N h 3 l g 0 j Y D f Q R l e a p X U F m v A c 2 5 d Y k l U d j z y v 4 z l s w E 3 9 u u f d J b c z O u p / Z U W L 1 q G F s z V s 5 5 K s O l U f i t o q f / p N v / d s i a k g x j B T 1 9 P 9 g T / y 2 8 L b I F i D A V r X u V 3 f X p Q q W k q W Q + s S B n 4 B c U U 0 c C p Y 7 U W l Y Q W h c 3 L N Q g t z I p m J q z a / x k e W S X G m t H 1 y w C 3 b n a i I N G Y l E 3 t S E p i Z T a 0 h / 6 W F J W R v 4 4 r n R Q k s p 3 d B W S k w K N z c A Z x y z S i I l Q W E a m 6 / F d M Z 0 Y S C v S m 9 l E T 2 / q G S p Q C u 1 b L P J k r N g S S m 9 u w G g 8 1 9 b Y O L 8 S g 4 H v k f x 4 O T 4 X q X O + g l e o W G K E B v 0 A k 6 Q + d o g q i j n V v</formula><formula xml:id="formula_3">= " &gt; A A A D S 3 i c j V L B b t N A E F 2 7 F E q A 0 s K R y 4 q k U r h E d i R K 2 1 N R L 1 y A I p G 2 U m x F 6 / W 6 X m X X a + 2 O k 0 a W v 4 C v 4 Q r f w A f w H d w Q B 9 Z u Q E 7 C g Z H W e v N</formula><formula xml:id="formula_4">I C f h w E h r f f P N z D f r T x t n g h v w v O + O u 3 N v 9 / 6 D v Y e d R 4 + f 7 B 8 c H j 2 9 N C r X l I 2 p E k p f x 8 Q w w V M 2 B g 6 C X W e a E R k L d h X P z + v 6 1 Y J p w 1 X 6 E Y q M R Z L c p H z G K Q F L T Y 6 c 3 n E v C G N Z L q q J H / X w 3 2 R Y J 2 E y V W B a 7 F v L B i G w W 2 h W l 5 p N q x I 0 4 S m 2 H 7 E k R R U N O p 2 W Z r 8 G t 9 X L N e 0 2 u b 2 j V f 2 v X e G i U W h g 8 g c 2 g / G s P F f v s 6 q V b w q + Y 0 t M B T G G m</formula><formula xml:id="formula_5">V 0 = " &gt; A A A D N n i c b V J N b 9 N A E F 2 b r x K g H 3 D k s i K t F C 6 W n U N a c S r q h Q v Q S k 1 b y b a i 9 X r T r L L r t X b H S S P L / 4 Q r / A b + C h d u i C s / g b U T I E 4 Y y d a b N z N v d p 8 2 y Q U 3 4 P v f H P f e / Q c P H + 0 8 7 j x 5 + m x 3 b / / g + Z V R h a Z s S J V Q + i Y h h g m e s S F w E O w m 1 4 z I R L D r Z H p W 1 6 9 n T B u u s k t Y 5 C y W 5 D b j Y 0 4 J W G p 0 4 O w d h l E i y 1 k 1 C u J D / D f p 1 0 k 0 S R W Y N f a 9 Z c M 5 z 1 I 8 J l r G X q d z 9 G + 8 V 4 O 7 6 n V L Z p 3 c l l u r h h G w O 2 g u V G q W V i V o w j N s f 2 J O F t V y V z R r F B o 4 + Q O b w W R c n q m P e b W W b w p + Y H N M B T G G m a r p Y z K f l G 8 p L T S h i z f 4 e O C d D K r R f t f 3 / C b w N g h W o I t W c W 4 t 3 I 1 S R Q v J M m j k w 8 D P I S 6 J B k 4 F q z p R Y V h O 6 J T c s t D C j E h m 4 r I 5 W I W P L G P d V N p + G e C G X Z 8 o i T R m I R P b K Q l M z G a t J v 9 X C w s Y n 8 Q l z / I C W E a X i 8 a F w K B w / Q 5 w y j W j I B Y W E K q 5 P S u m E 2 K d A P t a W l s S 2 b p D K Q s B X K</formula><formula xml:id="formula_6">v T Q q 1 5 Q N q R J K X y f E M M F T N g Q O g l 1 n m h G Z C H a V z M 7 r + t W c a c N V + g G K j M W S 3 K R 8 w i k B S 4 0 O n M O j M E p k O a 9 G Q X y E / y b 9 O o m m Y w V m j X 1 j 2 R A 0 4 S m 2 P 7 E g R d z z v O N / E t 0 a 3 F Y v W l L r 5 L b k W j W M g N 1 C c 6 l S s 3 F V t l Z V y 1 3 R v F F o 4 P Q P b A a T S X m u 3 m X V W r 4 p + J Y t M B X E G G a q p o / J b F q + o j T X h B Y v 8 c m g d z q o R v s d v + c 3 g b d B s A I d t I o L a + N u N F Y 0 l y y F R j 4 M / A z i k m j g V L D K i 3 L D M k J n 5 I a F F q Z E M h O X z c E q f G y Z M Z 4 o b b 8 U c M O u T 5 R E G l P I x H Z K A l O z W a v J / 9 X C H C a n c c n T L A e W 0 u W i S S 4 w K F y / B T z m m l E Q h Q W E a m 7 P i u m U W C f A v p j W l k S 2 7 l D K X A D X a t F m E 6 V m Q B J T e d b B Y N O v b X D Z 7 w W D n v + +</formula><formula xml:id="formula_7">v v G s i F o w j N s J 7 E i Z e x 1 O v 8 U h g 2 4 q l 9 s K W 2 S u 4 o b 1 X D F s x T P i J a N 6 q A f L d v m F s 7 / w A j Y F S S z 6 k S 9 y + u N v P 2 2 f l S a p X X 1 l q 0 w F c Q Y Z u q 2 j 8 l 8 X r 2 i t N C E l i / x 4 d g 7 G t f T b s / 3 / D b w L g j W o I f W c T o 9 c P a j V N F C s g x a + T D w c 4 g r o o F T w e p O V B i W E 7 o g l y y 0 M C O S m b h q D 1 b j g W X s D Z W 2 I w P c s p s r K i K N K W V i O y W B u b l Z a 8 j / 1 c I C Z k d x x b O 8 A J b R 6 4 1 m h c C g c P P q O O W a U R C l B Y R q b s + K 6 Z x Y J 8 D + G 1 u 7 J H L r D p U s B H C t V t t s o t Q C S G L q j n U w u O n X L j g b e c H Y 8 9 + P e s f D t Z d 7 6 B l 6 j o Y o Q I f o G L 1 G p 2 i C q J M 6 H 5 1 P z m f 3 i / v V / e Z + v 2 5 1 n f W a p 2 g r 3 J + / A Z G r / F E = &lt; / l a t e x i t &gt; [v1(x)] [v2(x)] . . . [vM (x)] [wind farm].</formula><formula xml:id="formula_8">= " &gt; A A A D K X i c b V L L j t M w F H X C a w g w L 5 Z s L N p K Z V M l X X R G r G Y 0 G z b A I N G Z k Z q o c l x 3 a t W O I / u m n S j K N 7 C F b + B r 2 A F b f g Q n U y C Z c q U 4 5 5 5 7 7 7 F 9 k j g V 3 I D v / 3 D c e / c f P H y 0 8 9 h 7 8 v T Z 7 t 7 + w e G F U Z m m b E y V U P o q J o Y J n r A x c B D s K t W M y F i w y 3 h 5 V t U v V 0 w b r p K P k K c s k u Q 6 4 X N O C V h q e u C 4 v e 4 k j G W x K q d B 1 M V / k 2 G V h I u Z A t N g 3 1 p 2 A p r w B N t F r E k e D T z v n 0 K / A j f l q 5 Z S k 9 x W b F S 3 l X v d c F U P 1 H D x B 4 b A b i C e F 2 f q f V o 2 8 v p d m 1 J o N i u L d 2 y N q S D G M F P W f U y m i + K U 0 k w T m r / G R 6 P B 8 a i c 7 n f 8 g V 8 H 3 g b B B n T Q J s 6 t a 7 v h T N F M s g R q + U n g p x A V R A O n g p V e m B m W E r o k 1 2 x i Y U I k M 1 F R H 6 z E P c v M 8 F x p + y S A a 7 Y 5 U R B p T C 5 j 2 y k J L M z d W k X + r z b J Y H 4 c F T x J M 2 A J v d 1 o n g k M C l e f H s + 4 Z h R E b g G h m t u z Y r o g 1 g m w P 0 h r l 1 i 2 7 l D I T A D X a t 1 m Y 6 W W Q G J T e t b B 4 K 5 f 2 + B i O A h G A / / D</formula><formula xml:id="formula_9">t Q W d + d b C 3 p Y q V R j I n 2 d w U i R 7 C O 4 = " &gt; A A A D Q H i c b V J N b 9 Q w E H X C V 1 m g t H B C X C y 2 l Z Z L l O x h W 3 E q 6 o U L U C S 2 r b S J V o</formula><formula xml:id="formula_10">T p O r J n b Y a J o I V k G j c g k 8 H O I K q K B U 8 H q X l g Y l h M 6 J x d s Y m F G J D N R 1 R y 3 x v u W S f B M a f t l g B u 2 P V E R a U w p Y 9 s p C a T m Z m 1 F 3 l a b F D A 7 j C q e 5 Q W w j F 5 t N C s E B o V X L w I n X D M K o r S A U M 3 t W T F N i f U H 7 L v p 7 B L L z h 0 q W Q j g W i 2 7 b K z U H E</formula><formula xml:id="formula_11">R k B Z M g h r k x b W Y t X o T L 4 C T y Y A v L U = " &gt; A A A D N X i c b V L L b t N A F B 2 b V z H Q B y z Z j E g r h U 1 k Z 5 F W r I q 6 Y Q M U i T S V b C s a T y b N K D M e a + Y 6 q W X 5 S 9 j C N / A t L N g h t v w C Y z c g u + m V b</formula><formula xml:id="formula_12">V n V J v c H t l S 7 x o d r Z q G B i 7 + w Q j Y N S T z 8 k x 9 z K p W 3 n w b V 0 r N Z l X 5 g a 0 x F c Q Y Z q q m j s l s U b 6 l N N e E F m / w 8 W h w M r L C h K c z P C d a T v d 7 / s B v A m + D Y A N 6 a B P n t Y P R T N F c s h S a T W H g Z x C X R A O n g l V e l B u W E b o k V y y 0 M C W S m b h s z l j h I 8 v Y x U r b J w X c s O 2 O k k h j C p n Y S k l g Y W 5 r N X m X F u Y w P 4 l L n m Y 5 s J T e L J r n A o P C 9 T X A M 6 4 Z B V F Y Q K j m 9 q y Y L o g 1 B e x l 6 W x J Z O c f S p k L 4 F q t u 2 y i 1 B J I Y i r P O h j c 9 m s b X A w H w W j g f x r 2 T v s b L 3 f Q S / Q K 9 V G A j t E p e o f O</formula><formula xml:id="formula_13">F 2 S g D U K U 2 C w j F 7 V 4 l n n V u Q D S f M = " &gt; A A A D O X i c b V L L b t N A F B 2 b V z H Q F 0 s 2 I 9 J K Y R P Z W a Q V q 6 J u 2 A B F a t p K t h W N J 5 N m l B m P N X O d 1 L L 8 L W z h G / g S l u w Q W 3 6 A s R u Q 3 f R K t</formula><formula xml:id="formula_14">U 0 V z y V J o t o W B n 0 F c E g 2 c C l Z 5 U W 5 Y R u i C X L P Q w p R I Z u K y O W e F D y 0 z x T O l 7 Z M C b t h 2 R 0 m k M Y V M b K U k M D d 3 t Z q 8 T w t z m B 3 H J U + z H F h K b x f N c o F B 4 f o q 4 C n X j I I o L C B U c 3 t W T O f E G g P 2 w n S 2 J L L z D 6 X M B X C t V l 0 2 U W o B J D G V Z x 0 M 7 v q 1 C S 6 G g 2 A 0 8 D 8 P e y f 9 t Z d b 6 B V 6 j f o o Q E f o B L 1 H Z 2 i M q F M 4 X 5 y v z j f 3 u / v T / e X + v</formula><p>In our study, we identify a critical problem of CoOp: the learned context is not generalizable to wider unseen classes within the same task. <ref type="figure" target="#fig_16">Figure 1</ref> illustrates the problem: the context learned by CoOp works well in distinguishing the base classes like "arrival gate" and "cathedral" but suffers a significant drop in accuracy when it is transferred to the new (unseen) classes, such as "wind farm" and "train railway"even though the task's nature remains the same, i.e., recognizing scenes. The results suggest that the learned context overfits the base classes, thus failing to capture more generalizable elements that are vital for broader scene recognition. We argue that such a problem is caused by CoOp's static design: the context, which is fixed once learned, is optimized only for a specific set of (training) classes. On the contrary, the manually-designed prompts adopted by the zero-shot method are relatively generalizable.</p><p>To address the weak generalizability problem, we introduce a novel concept: conditional prompt learning. The key idea is to make a prompt conditioned on each input instance (image) rather than fixed once learned. To make the model parameter-efficient, we introduce a simple yet effective implementation of conditional prompt learning. Specifically, we extend CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector), which is combined with the learnable context vectors. We call our approach Conditional Context Optimization (CoCoOp). <ref type="bibr" target="#b2">2</ref> An overview is shown in <ref type="figure" target="#fig_18">Figure 2</ref>. Interestingly, the paradigm of CoCoOp is analogous <ref type="bibr" target="#b2">2</ref> Pronounced as /k@Uku:p/. to image captioning <ref type="bibr" target="#b49">[49]</ref>, which explains why instanceconditional prompts are more generalizable: they are optimized to characterize each instance (more robust to class shift) rather than to serve only for some specific classes.</p><p>We present comprehensive experiments on 11 datasets covering a diverse set of visual recognition tasks. Specifically, we design a base-to-new generalization setting where a model is first learned using base classes and then tested on completely new classes. Compared with the zero-shot method <ref type="bibr" target="#b40">[40]</ref> and CoOp <ref type="bibr" target="#b63">[63]</ref>, our approach achieves the best overall performance <ref type="table" target="#tab_0">(Table 1)</ref>. Importantly, CoCoOp gains significant improvements over CoOp in unseen classes <ref type="figure" target="#fig_20">(Figure 3(a)</ref>), allowing the gap between manual and learningbased prompts to be substantially reduced.</p><p>In a more challenging scenario where the context learned for one task is directly transferred to other tasks with drastically different classes, CoCoOp still beats CoOp with a clear margin ( <ref type="table">Table 2</ref>), suggesting that instance-conditional prompts are more transferable and have the potential to succeed at larger scale. CoCoOp also obtains stronger domain generalization performance than CoOp <ref type="table">(Table 3)</ref>, further justifying the strengths of dynamic prompts.</p><p>In summary, our research provides timely insights into the generalizability problem in prompt learning, and crucially, demonstrates the effectiveness of a simple idea in various problem scenarios. We hope our approach and the findings presented in this work can pave the way for future research in generalizable-and transferable-prompt learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Vision-Language Models We mainly review studies focused on aligning images and texts to learn a joint embedding space <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b59">59]</ref>. The idea of cross-modality align-ment is certainly not new and has been investigated since nearly a decade ago-though with dramatically different technologies than today.</p><p>A typical vision-language model consists of three key elements: two for image and text encoding while the third is related to the design of loss functions. In early days, models for processing images and texts are often designed and also learned independently, with their outputs connected by extra modules (losses) for alignment. Images are often encoded using hand-crafted descriptors <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b45">45]</ref> or neural networks <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b29">29]</ref>, while texts are encoded using, for instance, pre-trained word vectors <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b45">45]</ref> or the frequency-based TF-IDF features <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b29">29]</ref>. In terms of cross-modality alignment, common approaches include metric learning <ref type="bibr" target="#b12">[12]</ref>, multi-label classification <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b26">26]</ref>, and n-gram language learning <ref type="bibr" target="#b31">[31]</ref>. Recently, a study suggests that training the vision part with an image captioning loss can make the visual representation more transferable <ref type="bibr" target="#b7">[7]</ref>.</p><p>Recent vision-language models <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b40">40]</ref> bridge the two modalities by learning two encoders jointly. Also, the models are now built with much larger neural networks. As discussed in Zhou et al. <ref type="bibr" target="#b63">[63]</ref>, recent successes in visionlanguage models are mainly attributed to the developments in i) Transformers <ref type="bibr" target="#b48">[48]</ref>, ii) contrastive representation learning <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b20">20]</ref>, and iii) web-scale training datasets <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b40">40]</ref>. A representative approach is CLIP <ref type="bibr" target="#b40">[40]</ref>, which trains two neural network-based encoders using a contrastive loss to match pairs of images and texts. After consuming 400 million data pairs, the CLIP model demonstrates a remarkable zero-shot image recognition capability. Similar to CoOp <ref type="bibr" target="#b63">[63]</ref>, our approach is orthogonal to the research of CLIP-like models <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b40">40]</ref>, aiming to offer an efficient solution for adapting pre-trained vision-language models to downstream applications.</p><p>Prompt Learning This topic originates from the NLP domain. The motivation was to view pre-trained language models, such as BERT <ref type="bibr" target="#b8">[8]</ref> or GPT <ref type="bibr" target="#b41">[41]</ref>, as knowledge bases from which information useful to downstream tasks is elicited <ref type="bibr" target="#b39">[39]</ref>. Concretely, given a pre-trained language model, the task is often formulated as a "fill-in-the-blank" cloze test, such as asking the model to predict the masked token in "No reason to watch. It was [MASK]" as either "positive" or "negative" for sentiment classification. The key lies in how to design the underlined part, known as prompt (template), in such a format familiar to the model.</p><p>Instead of manually designing a prompt, research in prompt learning aims to automate the process with the help of affordable-sized labeled data. Jiang et al. <ref type="bibr" target="#b25">[25]</ref> use text mining and paraphrasing to generate a group of candidate prompts, within which the optimal ones are chosen to have the highest training accuracy. Shin et al. <ref type="bibr" target="#b44">[44]</ref> propose Au-toPrompt, a gradient-based approach that selects from a vocabulary the best tokens that cause the greatest changes in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Net</head><p>. <ref type="bibr">[</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLASS]</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i 4 z p m u R m 7 l 6 o l i 4 l X w k T 2 O Q 8 J s k = " &gt; A A A C F X i c b V C 7 T g J B F J 3 1 i f h C L W 0 2 g g k V 2 a V Q S x I b S 0 z k k c C G z M 7 O w o R 5 b G b u a s i G n 7 C x 0 F + x M 7 b W / o m l A 2 w h 4 E l u c n L O v b n 3 n j D h z I D n f T s b m 1 v b O 7 u F v e L + w e H R c e n k t G 1 U q g l t E c W V 7 o b Y U M 4 k b Q E D T r u J p l i E n H b C 8 e 3 M 7 z x S b Z i S D z B J a C D w U L K Y E Q x W 6 l b 6 o 0 i B q Q x K Z a / m z e G u E z 8 n Z Z S j O S j 9 9 C N F U k E l E I 6 N 6 f l e A k G G N T D C 6 b T Y T w 1 N M B n j I e 1 Z K r G g J s j m 9 0 7 d S 6 t E b q y 0 L Q n u X P 0 7 k W F h z E S E t l N g G J l V b y b + 5 / V S i G + C j M k k B S r J Y l G c c h e U O 3 v e j Z i m B P j E E k w 0 s 7 e 6 Z I Q 1 J m A j W t o S i q U f M p F y Y F o 9 T Y s 2 K n 8 1 m H X S r t f 8 q 5 p 3 X y 8 3 q n l o B X S O L l A V + e g a N d A d a q I W I o i j Z / S K 3 p w X 5 9 3 5 c D 4 X r R t O P n O G l u B 8 / Q I m m Z + d &lt; / l a t e x i t &gt;</head><p>. . . gradients based on the label likelihood. Our research is most related to continuous prompt learning methods <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b60">60]</ref>, where the main idea is to turn a prompt into a set of continuous vectors that can be end-to-end optimized with respect to an objective function. See Liu et al. <ref type="bibr" target="#b34">[34]</ref> for a more comprehensive survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>context tokens &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H O G t H 0 e X P 5 R 5 + E E u 2 f Q H Z w u L 4 x w = " &gt; A A A C F 3 i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I 5 h Y k T s K t S S x s c R E P i J c y N 6 y w I b d v c v u H I Z c 7 l / Y W O h f s T O 2 l v 4 T S x e 4 Q s C X T P L y 3 k x m 5 g W R 4 A Z c 9 9 v J b W x u b e / k d w t 7 + w e H R 8 X j k 6 Y J Y 0 1 Z g 4 Y i 1 O 2 A G C a 4 Y g 3 g I F g 7 0 o z I Q L B W M L 6 d + a 0 J 0 4 a H 6 g G m E f M l G S o + 4 J S A l R 7 L 3 U A m k 7 T n l X v F k l t x 5 8 D r x M t I C W W o 9 4 o / 3 X 5 I Y 8 k U U E G M 6 X h u B H 5 C N H A q W F r o x o Z F h I 7 J k H U s V U Q y 4 y f z i 1 N 8 Y Z U + H o T a l g I 8 V / 9 O J E Q a M 5 W B 7 Z Q E R m b V m 4 n / e Z 0 Y B j d + w l U U A 1 N 0 s W g Q C w w h n r 2 P + 1 w z C m J q C a G a 2 1 s x H R F N K N i Q l r Y E c u m H R M Y C u A 6 f 0 o K N y l s N Z p 0 0 q x X v q u L e V 0 u 1 a h Z a H p 2 h c 3 S J P H S N a u g O 1 V E D U a T Q M 3 p F b 8 6 L 8 + 5 8 O J + L 1 p y T z Z y i J T h f v 5 c L o G Y = &lt; / l a t e x i t &gt; v1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / O b T 7 n t J t f R d w D 1 X 3 O g K D x B Q K Y 0 = " &gt; A A A C F 3 i c b V C 7 T g J B F J 3 F F + I L t b S Z C C Z W Z H c L t S S x s c R E H h E 2 Z H a Y h Q n z 2 M z M Y s h m / 8 L G Q n / F z t h a + i e W D r C F g C e 5 y c k 5 9 + b e e 8 K Y U W 1 c 9 9 s p b G x u b e 8 U d 0 t 7 + w e H R + X j k 5 a W i c K k i S W T q h M i T R g V p G m o Y a Q T K 4 J 4 y E g 7 H N / O / P a E K E 2 l e D D T m A Q c D Q W N K E b G S o / V X s j T S d b 3 q / 1 y x a 2 5 c 8 B 1 4 u W k A n I 0 + u W f 3 k D i h B N h M E N a d z 0 3 N k G K l K G Y k a z U S z S J E R 6 j I e l a K h A n O k j n F 2 f w w i o D G E l l S x g 4 V / 9 O p I h r P e W h 7 e T I j P S q N x P / 8 7 q J i W 6 C l I o 4 M U T g x a I o Y d B I O H s f D q g i 2 L C p J Q g r a m + F e I Q U w s a G t L Q l 5 E s / p D x h h i r 5 l J V s V N 5 q M O u k 5 d e 8 q 5 p 7 7 1 f q f h 5 a E Z y B c 3 A J P H A N 6 u A O N E A T Y C D A M 3 g F b 8 6 L 8 + 5 8 O J + L 1 o K T z 5 y C J T h f v 5 i 2 o G c = &lt; / l a t e x i t &gt; v2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t 4 h s q U X 1 T f 4 u b / K 3 H N B J T i d A Y C 8 = " &gt; A A A C F 3 i c b V A 9 T w J B E J 3 z E / E L t b T Z C C Z W 5 I 5 C L U l s b E w w k Y 8 I F 7 K 3 7 M G G 3 b 3 L 7 h 6 G X P g X N h b 6 V + y M r a X / x N I F r h D w J Z O 8 v D e T m X l B z J k 2 r v v t r K 1 v b G 5 t 5 3 b y u 3 v 7 B 4 e F o + O G j h J F a J 1 E P F K t A G v K m a R 1 w w y n r V h R L A J O m 8 H w Z u o 3 R 1 R p F s k H M 4 6 p L 3 B f s p A R b K z 0 W O o E I h 1 N u n e l b q H o l t 0 Z 0 C r x M l K E D L V u 4 a f T i 0 g i q D S E Y 6 3 b n h s b P 8 X K M M L p J N 9 J N I 0 x G e I + b V s q s a D a T 2 c X T 9 C 5 V X o o j J Q t a d B M / T u R Y q H 1 W A S 2 U 2 A z 0 M v e V P z P a y c m v P Z T J u P E U E n m i 8 K E I x O h 6 f u o x x Q l h o 8 t w U Q x e y s i A 6 w w M T a k h S 2 B W P g h F Q k 3 T E V P k 7 y N y l s O Z p U 0 K m X v s u z e V 4 r V S h Z a D k 7 h D C 7 A g y u o w i 3 U o A 4 E</head><p>In computer vision, prompt learning is a nascent research direction that has only been explored very recently <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b63">63]</ref>. Our research is built on top of CoOp <ref type="bibr" target="#b63">[63]</ref>, which is the earliest work to bring continuous prompt learning to the vision domain for adaptation of pre-trained visionlanguage models. Crucially, our approach solves the weak generalizability problem of CoOp <ref type="bibr" target="#b63">[63]</ref>, based on a simple idea of conditional prompt learning-which to our knowledge is also novel in the context of NLP and thus could be of interest to the NLP community as well.</p><p>Zero-Shot Learning (ZSL) is another relevant research area where the goal is similar to ours, i.e., to recognize novel classes by training only on base classes <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b57">57]</ref>. Moreover, the generalization problem where a model trained on base classes often fails on novel classes is also linked to the "seen-class bias" issue raised in the ZSL literature <ref type="bibr" target="#b54">[54]</ref>. The most common approach to ZSL is to learn a semantic space based on auxiliary information such as attributes <ref type="bibr" target="#b23">[23]</ref> or word embeddings <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b52">52]</ref>. Different from existing ZSL methods, our work addresses the emerging problem of adapting large vision-language models and uses drastically different techniques based on prompting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>An overview of our approach is shown in <ref type="figure" target="#fig_18">Figure 2</ref>. Below we first provide brief reviews on CLIP <ref type="bibr" target="#b40">[40]</ref>, which is the base model used in this paper, and CoOp <ref type="bibr" target="#b63">[63]</ref>. Then, we present the technical details of our approach as well as the rationale behind the design. Same as CoOp, our approach is applicable to broader CLIP-like vision-language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Reviews of CLIP and CoOp</head><p>Contrastive Language-Image Pre-training known as CLIP <ref type="bibr" target="#b41">[41]</ref>, has well demonstrated the potential of learning open-set visual concepts. CLIP is built using two encoders, one for image and the other for text, as shown in <ref type="figure" target="#fig_18">Figure 2</ref>. The image encoder can be either a ResNet <ref type="bibr" target="#b18">[18]</ref> or a ViT <ref type="bibr" target="#b9">[9]</ref>, which is used to transform an image into a feature vector. The text encoder is a Transformer <ref type="bibr" target="#b48">[48]</ref>, which takes as input a sequence of word tokens and again produces a vectorized representation.</p><p>During training, CLIP adopts a contrastive loss to learn a joint embedding space for the two modalities. Specifically, for a mini-batch of image-text pairs, CLIP maximizes for each image the cosine similarity with the matched text while minimizes the cosine similarities with all other unmatched texts, and the loss is computed in a similar fashion for each text too. After training, CLIP can be used for zeroshot image recognition. Let x be image features generated by the image encoder and {w i } K i=1 a set of weight vectors produced by the text encoder, each representing a category (suppose there are K categories in total). In particular, each w i is derived from a prompt, such as "a photo of a {class}" where the "{class}" token is filled with the i-th class name. The prediction probability is then</p><formula xml:id="formula_15">p(y|x) = exp(sim(x, w y )/? ) K i=1 exp(sim(x, w i )/? ) ,<label>(1)</label></formula><p>where sim(?, ?) denotes cosine similarity and ? is a learned temperature parameter.</p><p>Context Optimization (CoOp) aims to overcome the inefficiency problem in prompt engineering for better adapting pre-trained vision-language models to downstream applications <ref type="bibr" target="#b63">[63]</ref>. The key idea in CoOp is to model each context token using a continuous vector that can be end-to-end learned from data. Concretely, instead of using "a photo of a" as the context, CoOp introduces M learnable context vectors, {v 1 , v 2 , . . . , v M }, each having the same dimension with the word embeddings. The prompt for the i-th class, denoted by t i , now becomes</p><formula xml:id="formula_16">t i = {v 1 , v 2 , . . . , v M , c i }</formula><p>where c i is the word embedding(s) for the class name. The context vectors are shared among all classes. <ref type="bibr" target="#b3">3</ref> Let g(?) denote the text encoder, the prediction probability is then</p><formula xml:id="formula_17">p(y|x) = exp(sim(x, g(t y ))/? ) K i=1 exp(sim(x, g(t i )/? ) .<label>(2)</label></formula><p>To adapt CLIP to a downstream image recognition dataset, a cross-entropy loss can be used as the learning objective. Since the text encoder g(?) is differentiable, gradi-ents can be propagated all the way back to update the context vectors. Note that the base model of CLIP is frozen in the entire training process (ours too).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CoCoOp: Conditional Context Optimization</head><p>CoOp is a data-efficient approach allowing the context vectors to be trained with only a few labeled images in a downstream dataset. However, as discussed CoOp is not generalizable to wider unseen classes within the same task. We argue that instance-conditional context can generalize better because it shifts the focus away from a specific set of classes-for reducing overfitting-to each input instance, and hence to the entire task.</p><p>A straightforward way to implement CoCoOp is to build M neural networks to get M context tokens. However, such a design would require M ? the size of a neural network, which is much larger than having M context vectors as in CoOp. Here we propose a parameter-efficient design that works very well in practice. Specifically, on top of the M context vectors, we further learn a lightweight neural network, called Meta-Net, to generate for each input a conditional token (vector), which is then combined with the context vectors. See <ref type="figure" target="#fig_18">Figure 2</ref> for a sketch of the architecture.</p><p>Let h ? (?) denote the Meta-Net parameterized by ?, each context token is now obtained by v m (x) = v m + ? where ? = h ? (x) and m ? {1, 2, ..., M }. The prompt for the i-th class is thus conditioned on the input, i.e., t i (x) = {v 1 (x), v 2 (x), . . . , v M (x), c i }. The prediction probability is computed as</p><formula xml:id="formula_18">p(y|x) = exp(sim(x, g(t y (x)))/? ) K i=1 exp(sim(x, g(t i (x))/? ) .<label>(3)</label></formula><p>During training, we update the context vectors {v m } M m=1 together with the Meta-Net's parameters ?. In this work, the Meta-Net is built with a two-layer bottleneck structure (Linear-ReLU-Linear), with the hidden layer reducing the input dimension by 16?. The input to the Meta-Net is simply the output features produced by the image encoder. We leave exploration of more advanced designs for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our approach is mainly evaluated in the following three problem settings: 1) generalization from base to new classes within a dataset (Section 4.1); 2) cross-dataset transfer (Section 4.2); 3) domain generalization (Section 4.3). All models used in our experiments are based on the open-source CLIP <ref type="bibr" target="#b40">[40]</ref>. <ref type="bibr" target="#b4">4</ref> Before discussing the results, we provide the details of the experimental setup below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>For the first two settings, i.e., base-to-new generalization and cross-dataset transfer, we use the 11 image recognition datasets as in Zhou et al. <ref type="bibr" target="#b63">[63]</ref>, which cover a diverse set of recognition tasks. Specifically, the benchmark includes ImageNet <ref type="bibr" target="#b6">[6]</ref> and Caltech101 <ref type="bibr" target="#b11">[11]</ref> for classification on generic objects; OxfordPets <ref type="bibr" target="#b38">[38]</ref>, StanfordCars <ref type="bibr" target="#b28">[28]</ref>, Flowers102 [36], Food101 <ref type="bibr" target="#b2">[2]</ref> and FGVCAircraft <ref type="bibr" target="#b35">[35]</ref> for fine-grained classification; SUN397 <ref type="bibr" target="#b55">[55]</ref> for scene recognition; UCF101 <ref type="bibr" target="#b46">[46]</ref> for action recognition; DTD <ref type="bibr" target="#b5">[5]</ref> for texture classification; and finally EuroSAT <ref type="bibr" target="#b19">[19]</ref> for satellite imagery recognition. For domain generalization experiments, we use ImageNet as the source dataset and four other variants of ImageNet that contain different types of domain shift as the target datasets, namely ImageNetV2 <ref type="bibr" target="#b43">[43]</ref>, ImageNet-Sketch <ref type="bibr" target="#b50">[50]</ref>, ImageNet-A <ref type="bibr" target="#b22">[22]</ref> and ImageNet-R <ref type="bibr" target="#b21">[21]</ref>.</p><p>Following Zhou et al. <ref type="bibr" target="#b63">[63]</ref>, we randomly sample for each dataset a few-shot training set while using the original test set for testing. We only evaluate the highest shot number studied in Zhou et al. <ref type="bibr" target="#b63">[63]</ref>, i.e., 16 shots, which is sufficient to justify our approach. For learning-based models, the results are averaged over three runs.</p><p>Baselines The direct rival to our approach is CoOp <ref type="bibr" target="#b63">[63]</ref>, which essentially learns static prompts (in comparison to our dynamic prompts). The zero-shot method, i.e., CLIP <ref type="bibr" target="#b40">[40]</ref> is also compared, which is based on manual prompts. It is worth mentioning that the manual prompt for each dataset was intensively tuned using all classes in the test data <ref type="bibr" target="#b40">[40]</ref>.</p><p>Training Details Our implementation is based on CoOp's code. <ref type="bibr" target="#b5">5</ref> Throughout the experiments, we use the best available vision backbone in CLIP, i.e., ViT-B/16. Zhou et al. <ref type="bibr" target="#b63">[63]</ref> have suggested that a shorter context length and a good initialization can lead to better performance and stronger robustness to domain shift. Therefore, we fix the context length to 4 and initialize the context vectors using the pre-trained word embeddings of "a photo of a" for both CoOp and CoCoOp. Due to the instance-conditional design, our approach is slow to train and consumes much more GPU memory than CoOp. Therefore, to ensure the model can fit into a GPU and meanwhile reduce the training time, we train CoCoOp with batch size of 1 for 10 epochs. Such a limitation is discussed in more detail in Section 5.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Generalization From Base to New Classes</head><p>Solving the weak generalizability problem of CoOp is the main focus in this research. On each of the 11 datasets, we split the classes equally into two groups, one as base classes and the other as new classes. Learning-based models, i.e., CoOp and CoCoOp, are trained using only the base classes while evaluation is conducted on the base and new classes separately to test generalizability. The detailed results are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failures of CoOp in Unseen Classes</head><p>The split does not guarantee that the two class groups are equally difficult, as evidenced in CLIP's bumpy results: the base and new accuracy numbers are dramatically different. <ref type="bibr" target="#b6">6</ref> Nonetheless, CoOp's new accuracy is consistently much weaker than the base accuracy on nearly all datasets, leaving a huge gap of almost 20% on average (82.69% vs 63.22%). Despite maintaining an advantage over CLIP in terms of average performance, CoOp's gains in the base classes are nearly zeroed out by the catastrophic failures in the new classes, highlighting the need to improve generalizability for learning-based prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CoCoOp Significantly Narrows Generalization Gap</head><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>(a), CoCoOp improves the accuracy in unseen classes from 63.22% to 71.69%, which largely reduces the gap with manual prompts. The results confirm that instance-conditional prompts are more generalizable. A more detailed breakdown of per-dataset improvement is visualized in <ref type="figure" target="#fig_20">Figure 3</ref>(a) where we observe more than 10% increases in accuracy on 5 out of 11 datasets. Notably, on the challenging ImageNet dataset, CoCoOp's surge from 67.88% to 70.43% represents a non-trivial progress (the 70.43% accuracy even surpasses CLIP's 68.14%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CoCoOp's Gains in Generalization Far Outweigh</head><p>Losses in Base Accuracy In comparison to CoOp, performance drops in the base classes occur for CoCoOp on most datasets (see <ref type="figure" target="#fig_20">Figure 3(b)</ref>). This is reasonable because CoOp optimizes specifically for base classes whereas Co-CoOp optimizes for each instance in order to gain more generalization over an entire task. But it is worth noting that on the 9 datasets where CoCoOp's base accuracy drops below CoOp's, most losses are under 3% (precisely on 6 out of 9 datasets), which are far outweighed by the gains in unseen classes shown in <ref type="figure" target="#fig_20">Figure 3</ref>(a); even for those where CoCoOp suffers the biggest losses, the boosts in generalization are mostly significant enough to turn the averages into positives, e.g., StanfordCars sees the worst base accuracy drop of -7.63% but has the third-highest accuracy gain of +13.19% in the new classes, which together bring a 5.56% positive improvement for CoCoOp.</p><p>CoCoOp Is More Compelling Than CLIP When taking into account both the base and new classes, CoCoOp shows a gain of more than 4% over CLIP (75.83% vs 71.70), suggesting that instance-conditional prompts have a better potential in capturing more generalizable elements that are relevant for a recognition task. Theoretically, learning-based prompts have a much higher risk of overfitting base classes than manual prompts. Therefore, CLIP is a strong competitor to beat in unseen classes. Different from CoOp, we obtain promising results for CoCoOp: the new accuracy is even better than CLIP's on 4 out of 11 datasets (i.e., ImageNet, OxfordPets, Food101 and SUN397) and not too far away from CLIP's on the rest except FGVCAircraft where the gap between manual and learning-based prompts is generally large. In the ablation study on context length, we find that FGVCAircraft benefits from longer context, which is aligned with the findings in Zhou et al. <ref type="bibr" target="#b63">[63]</ref>.</p><p>To close or even overturn the gaps between manual and learning-based prompts in unseen classes, more efforts are required and we hope the insights presented in this research can help the community tackle the generalizability issue in prompt learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Cross-Dataset Transfer</head><p>Having demonstrated CoCoOp's generalizability within a dataset, we further show that CoCoOp has the potential to transfer beyond a single dataset, which is a much more challenging problem because the fundamentals can be totally changed across different datasets (e.g., from object recognition to texture classification). We only consider prompt learning methods in this setting.</p><p>We compare CoCoOp with CoOp by transferring context learned from ImageNet, with all 1,000 classes used, to each of the other 10 datasets. The results are detailed in <ref type="table">Table 2</ref>. On the source dataset, the two models perform similarly. Whereas on the target datasets, CoCoOp mostly outperforms CoOp by a clear margin. Since the ImageNet classes mainly contain objects, as well as a fair amount of dog breeds, it is reasonable to see high accuracy for both models on the relevant target datasets including Caltech101 and OxfordPets.</p><p>By comparison, the performance on other datasets with distant-and more fine-grained or specialized-categories is much lower, such as FGVCAircraft and DTD (containing various textures) where the accuracy numbers are well below 50%. Nonetheless, CoCoOp exhibits much stronger transferability than CoOp on the two mentioned datasets as well as on most other fine-grained or specialized datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Domain Generalization</head><p>Generalization to out-of-distribution data is a capability essential for machine learning models to succeed in practical applications <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b62">62]</ref>. Zhou et al. <ref type="bibr" target="#b63">[63]</ref> have revealed that their learnable prompts are more robust than manual prompts to domain shift. We are also interested to know if instance-conditional prompts still maintain the advantages as in previous experiments.</p><p>Following Zhou et al. <ref type="bibr" target="#b63">[63]</ref>, we evaluate CoCoOp's domain generalization performance by transferring the context learned from ImageNet to the four specially designed benchmarks. We also include the comparison with CLIP. <ref type="table">Table 3</ref> shows the results. Both prompt learning methods clearly beat CLIP on all target datasets. Compared to CoOp, CoCoOp performs slightly worse on ImageNetV2 but better on the other three. The results confirm that instanceconditional prompts are more domain-generalizable.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Further Analysis</head><p>Class-Incremental Test We consider a practical problem scenario where the recognition targets originally composed of base classes are expanded to include completely new classes. This problem is relevant to the existing continual learning literature <ref type="bibr" target="#b37">[37]</ref> but different in that the model here does not have access to any training data from new classes and needs to perform zero-shot recognition on them. We compare CLIP, CoOp and CoCoOp using the 11 datasets. The average results are reported in <ref type="table" target="#tab_2">Table 4</ref>. Clearly, CoOp loses competitiveness against CLIP as their performance is similar but the former needs training data. Again, CoCoOp beats the two competitors with a significant margin.</p><p>Initialization To understand the impact of initialization, we conduct an ablation study by comparing word embeddings-based initialization and random initialization while keeping all other parameters identical. For random initialization, we follow Zhou et al. <ref type="bibr" target="#b63">[63]</ref> to sample from a zero-mean Gaussian distribution with 0.02 standard deviation. <ref type="figure" target="#fig_22">Figure 4(a)</ref> shows the base-to-new generalization results averaged over the 11 datasets, which suggest that a proper initialization is more beneficial to both the base and new classes. Note that the findings from <ref type="figure" target="#fig_22">Figure 4</ref> only represent the overall trend while each individual dataset might have a different result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Length</head><p>The ablation study on context length is also carried out in the base-to-new generalization setting. Following Zhou et al. <ref type="bibr" target="#b63">[63]</ref>, we study 4, 8 and 16 context tokens. For fair comparison, we use random initialization for all context tokens. <ref type="figure" target="#fig_22">Figure 4(b)</ref> summarizes the results on the 11 datasets. The differences in the base classes are fairly small whereas in the new classes the models with a longer context length clearly perform better. From <ref type="figure" target="#fig_22">Figure 4</ref>(a) and (b) we observe that using 8 randomly initialized context tokens is marginally better than using 4 properly initialized context tokens, suggesting that a further boost might be possible if we initialize 8 context tokens with word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CoCoOp vs a Bigger CoOp</head><p>Since CoCoOp introduces more parameters than CoOp, namely the Meta-Net, one might question if the improvements simply come from an increased learning capacity. To clear the doubt, we remove the Meta-Net part and increase the number of context tokens in CoOp to the maximum such that CoOp's and CoCoOp's sizes are similar. The results in <ref type="table" target="#tab_3">Table 5</ref> show that increasing the parameter size is not the key.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations</head><p>The first limitation is about training efficiency: CoCoOp is slow to train and would consume a significant amount of GPU memory if the batch size is set larger than one. The reason is because CoCoOp is based on an instanceconditional design that requires for each image an independent forward pass of instance-specific prompts through the text encoder. This is much less efficient than CoOp that only needs a single forward pass of prompts through the text encoder for an entire mini-batch of any size.</p><p>The second limitation is that on 7 out of the 11 datasets (see <ref type="table" target="#tab_0">Table 1</ref>), CoCoOp's performance in unseen classes still lags behind CLIP's, indicating that more efforts are needed from the community to fully close or overturn the gaps between manual and learning-based prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Conclusion</head><p>Our research addresses an important issue that arises with the availability of large pre-trained AI models, i.e., how to adapt them to downstream applications. These models, also called foundation models <ref type="bibr" target="#b1">[1]</ref>, have received increasing attention from academia and industry in both the vision and NLP communities because they are so powerful in terms of their capabilities for diverse downstream tasks. However, foundation models are costly to pre-train in terms of data scale and compute resources; and typically contain an enormous number of parameters in order to develop sufficient capacity. For instance, the CLIP model <ref type="bibr" target="#b40">[40]</ref> based on ViT-B/16 used in our experiments has a whopping 150M parameter size. These factors together highlight the need for research of efficient adaptation methods for democratizing foundation models.</p><p>Our studies, which follow the line of parameter-efficient prompt learning <ref type="bibr" target="#b63">[63]</ref>, provide timely insights into the generalizability issue of static prompts, and more importantly, demonstrate that a simple design based on conditional prompt learning performs superbly in a variety of problem scenarios, including generalization from base to new classes, cross-dataset prompt transfer, and domain generalization.</p><p>In terms of future work, one direction is to further develop conditional prompt learning with potentially a more efficient implementation that can accelerate the training, as well as enhance generalizability. The cross-dataset transfer experiments indicate that instance-conditional prompts are more transferable-compared to static prompts-across tasks of varying natures. Therefore, it would be interesting to see if such an idea can scale to, e.g., bigger model size for the Meta-Net, larger-scale training images, and even heterogeneous training data mixed with different datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>&lt;</head><label></label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " w 2 u a w p O y n x e I A r 2 p + Z z 3 w z 9 w Q g Q = " &gt; A A A D N n i c b V J N b 9 N A E F 2 b r 2 K g H 3 D k s i K t F C 6 W n U N a c S r q h Q v Q S k 1 b y b a i 9 X r S r L L r t X b X S S P L / 4 Q r / A b + C h d u i C s / g Y 0 T I E 4 Y y d a b N / N m d p 8 2 L T j T J g i + O e 6 9 + w 8 e P t p 5 7 D 1 5 + m x 3 b / / g + Z W W p a I w o J J L d Z M S D Z z l M D D M c L g p F B C R c r h O J 2 e L + v U U l G Y y v z T z A h J B b n M 2 Y p Q Y S w 0 P n L 3 D K E 5 F N a 2 H Y X K I / y a 9 R R K P M 2 n 0 G v v e s p G V j i F T h C e + 5 x 3 9 k 3 c X 4 K 5 + 3 R q z T m 6 P W 6 t G s Y E 7 0 1 y o U p D V l V G E 5 d j + + I z M 6 + W u e N p M a O D 4 D 2 y E 6 a g 6 k x + L e i 3 f H P g B Z p h y o j X o u u k D U Y y r t 5 S W i t D 5 G 3 z c 9 0 / 6 9 X C / E / h B E 3 g b h C v Q Q a s 4 t x b u x p m k p Y D c N O O j M C h M U h F l G O V Q e 3 G p o S B 0 Q m 4 h s j A n A n R S N Q e r 8 Z F l M j y S y n 6 5 w Q 2 7 r q i I 0 H o u U t s p r O 9 6 s 7 Y g / 1 e L S j M 6 S S q W F 6 W B n C 4 X j U q O j c S L d 4 A z p o A a P r e A U M X s W T E d E + u E s a + l t S U V r T t U o u S G K T l r s 6 m U E 0 N S X X v W w X D T r 2 1 w 1 f P D v h 9 c 9 D q n 3 Z W X O + g l e o W 6 K E T H 6 B S 9 Q + d o g K g z d T 4 5 n 5 0 v 7 l f 3 u / v D / b l s d Z 2 V 5 g V q h f v r N 5 l Q A 7 A = &lt; / l a t e x i t &gt; [v1] [v2] . . . [vM ] [cathedral]. &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f 1 S U B f 8 c V k B 8 d E / 1 Q 5 l x w I U 6 D y 4 = " &gt; A A A D O X i c b V J N b 9 N A E F 2 b r 2 K g t O X I Z U V a K V w i O 4 e 0 4 l T U C x e g S K S t Z F v R e r 1 J V t n 1 W r v j p J b l 3 8 I V f g O / h C M 3 x J U / w M Y J E C e M Z O v N m 5 k 3 u 0 + b 5 I I b 8 P 1 v j n v n 7 r 3 7 D / Y e e o 8 e P 9 l / e n B 4 d G V U o S k b U i W U v k m I Y Y J n b A g c B L v J N S M y E e w 6 m V 0 s 6 9 d z p g 1 X 2 U c o c x Z L M s n 4 m F M C l h o d O k f H Y Z T I a l 6 P g v g Y / 0 3 6 y y S a p g r M B v v W s i H R m s + J w B M C L O 5 5 3 s k / h e 4 S 3 N Y v W 0 q b 5 K 7 i R j W M g N 1 C c 6 d K s 7 S u Q B O e Y f s T C 1 L W q 1 3 R v F F o 4 P Q P b A a T c X W h 3 u f 1 R r 4 t + I 4 t M B X E G G b q p o / J f F q 9 p r T Q h J a v 8 O m g d z a o R w c d v + c 3 g X d B s A Y d t I 5 L 6 + J + l C p a S J Z B I x 8 G f g 5 x R TR w K l j t R Y V h O a E z M m G h h R m R z M R V c 7 A a n 1 g m x W O l 7 Z c B b t j N i Y p I Y 0 q Z 2 E 5 J Y G q 2 a 0 v y f 7 W w g P F Z X P E s L 4 B l d L V o X A g M C i + f A k 6 5 Z h R E a Q G h m t u z Y j o l 1 g m w D 6 a 1 J Z G t O 1 S y E M C 1 W r T Z R K k Z k M T Un n U w 2 P Z r F 1 z 1 e 8 G g 5 3 / o d 8 6 7 a y / 3 0 H P 0 A n V R g E 7 R O X q D L t E Q U a d 0 P j m f n S / u V / e 7 + 8 P 9 u W p 1 n f X M M 9 Q K 9 9 d v + R A E 2 A = = &lt; / l a t e x i t &gt; [v1] [v2] . . . [vM ] [arrival gate].&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z X 3 n n 3 S q n + w / q D Q Y j s 0 d 4 x d W 0 s Q = " &gt; A A A D K H i c b V L L j t M w F H X C a w g w D 1 i y s W g r l U 2 V d N E Z s R o 0 G z b A I N G Z k Z K o c h y 3 s W r H k X 3 T T h X l F 9 j C N / A 1 7 N B s + R K c T I F 2 y p W S n H v u v c f O s Z N C cA O + f + O 4 9 + 4 / e P h o 7 7 H 3 5 O m z / Y P D o + c X R p W a s j F V Q u m r h B g m e M 7 G w E G w q 0 I z I h P B L p P 5 W V O / X D B t u M o / w 6 p g s S S z n E 8 5 J W C p y Z H j 9 L p h l M h q U U + C u I v / J s M m i b J U g d l g 3 1 s 2 B E 1 4 j u 1 L L M k q H n j e P 4 V + A 6 7 r 1 1 t K m + S u 4 k Y 1 J F r z B R F 4 R o A 1 w r 1 u t G j 7 W 5 j 9 g R G w a 0 i m 1 Z n 6 W N Q b e f t t P a k 0 S + v q A 1 t i K o g x z N R t H 5 N F V r 2 l t N S E r t 7 g 4 9 H g Z F R P D j v + w G 8 D 7 4 J g D T p o H e f W t P 0 o V b S U L I d W P g z 8 A u K K a O B U s N q L S s M K Q u d k x k I L c y K Z i a t 2 Y z X u W S b F U 6 X t k w N u 2 c 2 J i k h j V j K x n Z J A Z u 7 W G v J / t b C E 6 U l c 8 b w o g e X 0 d q F p K T A o 3 J w 8 T r l m F M T K A k I 1 t 3 v F N C P W C b D 3 Y 2 u V R G 7 9 Q y V L A V y r 5 T a b K D U H k p j a s w 4 G d / 3 a B R f D Q T A a + J + G n d P + 2 s s 9 9 B K 9 Q n 0 U o G N 0 i t 6 h c z R G 1 M m c L 8 5 X 5 5 v 7 3 f 3 h / n R v b l t d Z z 3 z A m 2 F + + s 3 r / P 8 t A = = &lt; / l a t e x i t &gt; [v1(x)] [v2(x)] . . . [vM (x)] [arrival gate]. &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S d s 3 Q J M I K g V / H N c J + p 5 g I s 1 s 9 3 4 = " &gt; A A A D J H i c b V J N j 9 M w E H X C 1 1 J g 6 c K R i 0 V b q V y q p I f u i t O i v X A B F o n u r p R E l e N M N l b t O L K d d q s o f 4 A r / A Z + D T f E g Q s / B e F k C 7 R b R o r z 5 s 3 4 e f y S u O B M G 8 / 7 4 b i 3 b t + 5 e 2 / v f u f B w 0 f 7 j 7 s H T 8 6 0 L B W F K Z V c q o u Y a O A s h 6 l h h s N F o Y C I m M N 5 P D 9 p 6 u c L U J r J / I N Z F R A J c p m z l F F i L D X r / h r 0 g z A W 1 a K e + V E f / 0 3 G T R J m i T R 6 g 3 1 j 2 c A o w n J s F 7 4 k q 2 j U 6 f x T G D b g q n 6 x p b R J 7i p u V A M 7 V A a J I r x R H f T D R d v c w u w P D A 1 c m T i t T u S 7 o t 7 I 2 3 f r R 6 U g q a u 3 s M S U E 6 1 B 1 2 0 f i C K r X l F a K k J X L / H h Z H Q 0 q W f d n j f y 2 s C 7 w F + D H l r H 6 e z A 2 Q 8 T S U s B u W n l A 9 8 r T F Q R Z R j l U H f C U k N B 6 J x c Q m B h T g T o q G o H q / H A M g l O p b J P b n D L b u 6 o i N B 6 J W L b K a w X + m a t I f 9 X C 0 q T H k U V y 4 v S Q E 6 v D 0 p L j o 3 E z V f H C V N A D V 9 Z Q K h i d l Z M M 2 K d M P b f 2 D o l F l t 3 q E T J D VN y u c 3 G U s 4 N i X X d s Q 7 6 N / 3 a B W f j k T 8 Z e e / H v e P h 2 s s 9 9 A w 9 R 0 P k o 0 N 0 j F 6 j U z R F 1 E m c j 8 4 n 5 7 P 7 x f 3 q f n O / X 7 e 6 z n r P U 7 Q V 7 s / f + g r 8 g Q = = &lt; / l a t e x i t &gt; [v1(x)] [v2(x)] . . . [vM (x)] [cathedral]. &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 i v C j / Q E d X Y H n n I 7 r Z s y E x e Y + n E = " &gt; A A A D Q X i c b V L P b 9 M w F H Y y f o w C Y x s 3 u F h 0 k 8 o l S n r o J k 6 D X b g A Q 6 L b p C S q H M d Z r d p x Z L + 0 q 6 I c + G u 4 w t / A X 8 G f w A 1 x 5 Y K b d V O 6 7 k l x v v e 9 9 z 7 b n 5 w U g h v w / V + O u 3 H v / o O H m 4 8 6 j 5 8 8 3 X q 2 v b N 7 a l S p K R t S J Z Q + T 4 h h g u d s C B w E O y 8 0 I z I R 7 C y Z H C / q Z 1 O m D V f 5 F 5 g X L J b k I u c Z p w Q s N d p x X u z v h V E i q 2 k 9 C u I 9 f J P 0 F 0 k 0 T h W Y F v v B s i F o w n N s F z E j 8 9 j r d F o S v Q W 4 r F + v S L X J d c l W 9 S 7 p a N o M N H B 8 D S N g l 5 B k 1 b H 6 V N S t v P k 3 r l S a p X X 1 k c 0 w F c Q Y Z u q m j 8 l i X L 2 l t N S E z t / g g 4 F 3 O L C F 6 / l 3 1 s q b g d F 2 1 / f 8 J v A 6 C J a g i 5 Z x Y t 3 c i l J F S 8 l y a E T C w C 8 g r o g G T g W r O 1 F p W E H o h F y w 0 M K c S G b i q j l v j f c t k + J M a f v l g B u 2 P V E R a c x c J r Z T E h i b 2 7 U F e V c t L C E 7 j C u e F y W w n F 5 t l J U C g 8 K L J 4 F T r h k F M b e A U M 3 t W T E d E 2 s Q 2 I e z s k s i V + 5 Q y V I A 1 2 q 2 y i Z K T Y A k p u 5 Y B 4 P b f q 2 D 0 7 4 X D D z / c 7 9 7 1 F t 6 u Y l e o l e o h w J 0 g I 7 Q e 3 S C h o g 6 X 5 1 v z n f n h / v T / e 3 + c f 9 e t b r O c u Y 5 W g n 3 3 3 9 d N A b l &lt; / l a t e x i t &gt; Base classes &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y r d l o P p T X X e t A Z 2 q l W j / z 7 d T 0 l Q = " &gt; A A A D O H i c b V L L b t N A F B 2 b V w l Q G l i y G Z F W C p v I z i K t W L X q h g 1 Q J N J W i q 1 o P L l J R p n x W D P X e c j y r 7 C F b + B P 2 L F D b P k C J m 5 A T t M r 2 T 7 3 n P s Y H 0 2 S S W E x C H 5 4 / r 3 7 D x 4 + 2 n v c e P L 0 2 f 7 z g + a L S 6 t z w 6 H P t d T m O m E W p E i h j w I l X G c G m E o k X C W z 8 7 V + N Q d j h U 4 / 4 y q D W L F J K s a C M 3 T U s O k 1 j w 4 H U a K K e T k M 4 0 P 6 P + m u k 2 g 6 0 m h r 7 H v H D t A w k V L 3 k g u 2 i j u N R m 1 E e w 2 W 5 Z u t U X V y d 2 R N v W t 0 N K 8 a K j j 9 B y O E J S b j 4 l x / z M p a X n 0 r V w o D o 7 L 4 A A v K J b M W b F n V g c q m x R n n u W F 8 9 Z Y e 9 z o n P S e c G S P m T N I J Q x g e t I J O U A X d B e E G t M g m L p y J + 9 F I 8 1 x B i t W y Q R h k G B f M o O A S y k a U W 8 g Y n 7 E J D B x M m Q I b F 9 U x S 3 r k m B E d a + O e F G n F 1 j s K p q x d q c R V K o Z T e 1 t b k 3 d p g x z H J 3 E h 0 i x H S P n N o n E u K W q 6 v g l 0 J A x w l C s H G D f C n Z X y K X O + o L s v W 1 s S t f U P h c o l C q M X 2 2 y i 9 Q x Z Y s u G c z C 8 7 d c u u O x 2 w l 4 n + N R t n b Y 3 X u 6 R V + Q 1 a Z O Q H J N T 8 o 5 c k D 7 h 3 t L 7 4 n 3 1 v v n f / Z / + L / / 3 T a n v b X p e k q 3 w / / w F 8 B A C s w = = &lt; / l a t e x i t &gt; Arrival gate &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 9 D U C o r V W p o V U M 8 / d / 6 D B 0 Z w k h w = " &gt; A A A D N X i c b V J N b 9 M w G H b C 1 w i w D z h y s e g m l U u U 9 N B N n I Z 6 4 Q I M i W 6 T k q h y n L e r V T u O b K d d F e W X c I X f w G / h w A 1 x 5 S / g Z A W l 6 1 4 p z v M + 7 6 c f O S 0 4 0 y Y I f j j u v f s P H j 7 a e e w 9 e f p s d 2 / / 4 P m 5 l q W i M K a S S 3 W Z E g 2 c 5 T A 2 z H C 4 L B Q Q k X K 4 S O e j J n 6 x A K W Z z D + b V Q G J I F c 5 m z J K j K U m B 8 7 u 0 W E U p 6 J a 1 J M w O c T / n U H j x L N M G t 1 h 3 1 s 2 M o q w H N u D L 8 k q 8 T 2 v 0 6 L f g O v 6 9 U a r L r n d s h O 9 q 3 W 8 a A t a O P s H Y w P X J p 1 W I / m x q D t + + 2 9 V q R R k d f U B l p h y o j X o u s 0 D U c y q t 5 S W i t D V G 3 w 8 9 E + G N j A i Z g a Z I n y y 3 w v 8 o D W 8 D c I 1 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>d a 7 q C X 6 B X q o x A d o 1 P 0 D p 2 h M a J O 6 X x x v j r f 3 O / u T / e X + / s m 1 X X W N S / Q h r l / / g K V 5 A G L &lt; / l a t e x i t &gt; Cathedral &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 T W 2 Q 2 k v o I w A f S i d p P z 6 R 0 O 1 N 8 s = " &gt; A A A D A H i c j V F N j 9 M w E H X C 1 x J g 6 c K R i 0 W 7 U r l U S Q / A s V I v X B C L R H d X a q L K d p y t V T u O 7 E l 3 q y g X f g 0 3 x J V / A r 8 G J 1 u h Z M u B k W y 9 e f P x x m N a S G E h D H 9 5 / r 3 7 D x 4 + O n o c P H n 6 7 P j 5 4 O T F u d W l Y X z B t N T m k h L L p c j 5 A g R I f l k Y T h S V / I J u 5 k 3 8 Y s u N F T r / A r u C J 4 p c 5 S I T j I C j V o P f p 6 N l T F W 1 r V d R M s J / n W n j x O t U g + 2 w H x 2 7 j I H f Q K t c G Z 7 W F R g i c u w u e U 1 2 d T I J g k 7 P c Q N u 6 j e 9 3 l 3 y U K M T / Q + t U b x t G z j V N p l m 1 V z P 9 a e i X g 2 G 4 S R s D R + C a A + G a G 9 n q x P v O E 4 1 K x X P g U l i 7 T I K C 0 g q Y k A w y e s g L i 0 v C N u Q K 7 5 0 M C e K 2 6 R q x 6 v x q W N S n G n j T g 6 4 Z b s V F V H W 7 h R 1 m Y r A 2 t 6 N N e S / Y s s S s v d J J f K i B J 6 z W 6 G s l B g 0 b n 4 U p 8 J w B n L n A G F G u F k x W x N D G L h / 7 6 l Q 1 X t D p U o J w u j r P k u 1 3 g C h t g 7 c B q O 7 + z o E 5 9 N J 9 H Y S f p 4 O Z + P 9 L o / Q K / Q a j V G E 3 q E Z + o D O 0 A I x b + Z l n v Y K / 6 v / z f / u / 7 h N 9 b 1 9 z U v U M / / n H w / 5 8 a 8 = &lt; / l a t e x i t &gt; . . . &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I J t C a r a V V L l W 6 H H a g g h v M Z y B O H c = " &gt; A A A C O H i c b V A 9 T w J B E N 3 D L 8 Q v x N L m I j G x k d x R q C W J j a U m o k Q g Z H e Z k w 2 7 t 5 f d O Y V c + C u 2 + h v 8 J 3 Z 2 x t Z f 4 A J X i P q S S V 7 e m 8 n M P J Z I Y T E I 3 r z C 0 v L K 6 l p x v b S x u b W 9 U 9 6 t 3 F i d G g 5 N r q U 2 L U Y t S B F D E w V K a C U G q G I S b t n w f O r f P o C x Q s f X O E 6 g q + h 9 L C L B K T q p V 6 5 0 E E b I o u w O j D 6 2 A 4 2 T X r k a 1 I I Z / L 8 k z E m V 5 L j s 7 X r b n b 7 m q Y I Y u a T W t s M g w W 5 G D Q o u Y V L q p B Y S y o f 0 H t q O x l S B 7 W a z 4 y f + o V P 6 f q S N q x j 9 m f p z I q P K 2 r F i r l N R H N j f 3 l T 8 z 2 u n G J 1 1 M x E n K U L M 5 4 u i V P q o / W k S f l 8 Y 4 C j H j l B u h L v V 5 w N q K E e X 1 8 I W p h Z + y F Q q U R j 9 u K g y r Y d I m Z 2 U X I L h 7 7 z + k p t 6 L T y p B V f 1 a u M o z 7 J I 9 s k B O S I h O S U N c k E u S Z N w M i J P 5 J m 8 e K / e u / f h f c 5 b C 1 4 + s 0 c W 4 H 1 9 A z V 1 r S 8 = &lt; / l a t e x i t &gt; Zero-shot &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + x V R X t B c 7 t 5 3 w w w 2 R 1 a k e G F / K 1 k = " &gt; A A A C S n i c b V C 7 T g M x E P S F V w i v A C W N R U C i i u 4 o g B K J h j J I J C A d p 2 j P 8 S V W 7 P P J 3 g u K T v k B v o Y W v o E f 4 D f o E A 3 O o y A J I 9 k e z e x q v R N n U l j 0 / U + v t L K 6 t r 5 R 3 q x s b e / s 7 l X 3 D 1 p W 5 4 b x J t N S m 8 c Y L J c i 5 U 0 U K P l j Z j i o W P K H u H 8 z 9 h 8 G 3 F i h 0 3 s c Z j x S 0 E 1 F I h i g k 9 r V k x A i G m Y 9 j d q 9 O n H X W A B j x A A k 7 Q L y q N 6 u 1 v y 6 P w F d J s G M 1 M g M j f a + t / v U 0 S x X P E U m w d o w 8 D O M C j A o m O S j y l N u e Q a s D 1 0 e O p q C 4 j Y q J u u M 6 K l T O j T R x p 0 U 6 U T 9 2 1 G A s n a o Y l e p A H t 2 0 R u L / 3 l h j s l V V I g 0 y 5 G n b D o o y S V F T c f Z 0 I 4 w n K E c O g L M C P d X y n p g g K F L c G 5 K r O Z 2 K F Q u U R j 9 P K / G W v c R Y j u q u A S D x b y W S e u 8 H l z U / b v z 2 v X Z L M s y O S L H 5 I w E 5 J J c k 1 v S I E 3 C y A t 5 J W / k 3 f v w v r x v 7 2 d a W v J m P Y d k D q X V X 8 L t s j E = &lt; / l a t e x i t &gt; [a] [photo] [of] [a] [arrival gate]. &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o y F S k V R 5 p V K + B a F e H p U R W 7 S E k E o = " &gt; A A A C R 3 i c b V B N T w I x E O 3 i F + I X 6 N F L l Z h w I r s c 1 C O J F 4 + Y y E e y b E i 3 d K G h 3 W 7 a W Q 3 Z c P b X e N X f 4 E / w V 3 g z H u 0 C B w E n a e f l v Z n M z A s T w Q 2 4 7 q d T 2 N r e 2 d 0 r 7 p c O D o + O T 8 q V 0 4 5 R q a a s T Z V Q u h c S w w S P W R s 4 C N Z L N C M y F K w b T u 5 y v f v E t O E q f o R p w g J J R j G P O C V g q U H 5 w i c B 9 p O x A m W z i u y X E 1 Y e s 6 E m I q g P y l W 3 7 s 4 D b w J v C a p o G a 1 B x T n u D x V N J Y u B C m K M 7 7 k J B B n R w K l g s 1 I / N S w h d E J G z L c w J p K Z I J v f M s N X l h n i S G n 7 Y s B z 9 m 9 H R q Q x U x n a S m m X N O t a T v 6 n + S l E t 0 H G 4 y Q F F t P F o C g V G B T O j c F D r h k F M b W A U M 3 t r p i O i S Y U r H 0 r U 0 K 5 c k M m U w F c q + d V N l R q A i Q 0 s 5 J 1 0 F v 3 a x N 0 G n X v u u 4 + N K r N 2 t L L I j p H l 6 i G P H S D m u g e t V A b U f S C X t E b e n c + n C / n 2 / l Z l B a c Z c 8 Z W o m C 8 w t b S 7 E J &lt; / l a t e x i t &gt; [a] [photo] [of] [a] [cathedral]. &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 f R 6 H m A B M l e N c U o W a 6 o y K g / n / 6 s = " &gt; A A A C P H i c b Z D L T g I x F I Y 7 e E O 8 g S Z u 3 D Q S E 1 Z k B h O 8 r D B u X G I i l w Q I 6 Z Q C D e 1 0 0 p 7 R k J G X c a v P 4 H u 4 d 2 f c u r b A L A T 8 k y Z / / n N O T s / n h 4 I b c N 0 P J 7 W 2 v r G 5 l d 7 O 7 O z u 7 R 9 k c 4 d 1 o y J N W Y 0 q o X T T J 4 Y J H r A a c B C s G W p G p C 9 Y w x / d T u u N R 6 Y N V 8 E D j E P W k W Q Q 8 D 6 n B G z U z R 6 3 m Q y H 8 Q 2 l k S Z 0 f I 3 L V 8 X z 8 q S b z b t F d y a 8 a r z E 5 F G i a j f n 7 L d 7 i k a S B U A F M a b l u S F 0 Y q K B U 8 E m m X Z k W E j o i A x Y y 9 q A S G Y 6 8 e y A C T 6 z S Q / 3 l b Y v A D x L / 0 7 E R B o z l r 7 t l A S G Z r k 2 D f + r t S L o X 3 Z i H o Q R s I D O F / U j g U H h K Q 3 c 4 5 p R E G N r C N X c / h X T I b E k w D J b 2 O L L h R t i G Q n g W j 0 t p r 5 S I y C + m W Q s Q W + Z 1 6 q p l 4 p e u e j e l / K V Q s I y j U 7 Q K S o g D 1 2 g C r p D V V R D F D 2 j F / S K 3 p x 3 5 9 P 5 c r 7 n r S k n m T l C C 3 J + f g E W T a 1 0 &lt; / l a t e x i t &gt; Accuracy: 69.36 (a) Both CoOp and CoCoOp work well on the base classes observed during training and beat manual prompts by a significant margin. &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 T W 2 Q 2 k v o I w A f S i d p P z 6 R 0 O 1 N 8 s = " &gt; A A A D A H i c j V F N j 9 M w E H X C 1 x J g 6 c K R i 0 W 7 U r l U S Q / A s V I v X B C L R H d X a q L K d p y t V T u O 7 E l 3 q y g X f g 0 3 x J V / A r 8 G J 1 u h Z M u B k W y 9 e f P x x m N a S G E h D H 9 5 / r 3 7 D x 4 + O n o c P H n 6 7 P j 5 4 O T F u d W l Y X z B t N T m k h L L p c j 5 A g R I f l k Y T h S V / I J u 5 k 3 8 Y s u N F T r / A r u C J 4 p c 5 S I T j I C j V o P f p 6 N l T F W 1 r V d R M s J / n W n j x O t U g + 2 w H x 2 7 j I H f Q K t c G Z 7 W F R g i c u w u e U 1 2 d T I J g k 7 P c Q N u 6 j e 9 3 l 3 y U K M T / Q + t U b x t G z j V N p l m 1 V z P 9 a e i X g 2 G 4 S R s D R + C a A + G a G 9 n q x P v O E 4 1 K x X P g U l i 7 T I K C 0 g q Y k A w y e s g L i 0 v C N u Q K 7 5 0 M C e K 2 6 R q x 6 v x q W N S n G n j T g 6 4 Z b s V F V H W 7 h R 1 m Y r A 2 t 6 N N e S / Y s s S s v d J J f K i B J 6 z W 6 G s l B g 0 b n 4 U p 8 J w B n L n A G F G u F k x W x N D G L h / 7 6 l Q 1 X t D p U o J w u j r P k u 1 3 g C h t g 7 c B q O 7 + z o E 5 9 N J 9 H Y S f p 4 O Z + P 9 L o / Q K / Q a j V G E 3 q E Z + o D O 0 A I x b + Z l n v Y K / 6 v / z f / u / 7 h N 9 b 1 9 z U v U M / / n H w / 5 8 a 8 = &lt; / l a t e x i t &gt; . . . &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 T W 2 Q 2 k v o I w A f S i d p P z 6 R 0 O 1 N 8 s = " &gt; A A A D A H i c j V F N j 9 M w E H X C 1 x J g 6 c K R i 0 W 7 U r l U S Q / A s V I v X B C L R H d X a q L K d p y t V T u O 7 E l 3 q y g X f g 0 3 x J V / A r 8 G J 1 u h Z M u B k W y 9 e f P x x m N a S G E h D H 9 5 / r 3 7 D x 4 + O n o c P H n 6 7 P j 5 4 O T F u d W l Y X z B t N T m k h L L p c j 5 A g R I f l k Y T h S V / I J u 5 k 3 8 Y s u N F T r / A r u C J 4 p c 5 S I T j I C j V o P f p 6 N l T F W 1 r V d R M s J / n W n j x O t U g + 2 w H x 2 7 j I H f Q K t c G Z 7 W F R g i c u w u e U 1 2 d T I J g k 7 P c Q N u 6 j e 9 3 l 3 y U K M T / Q + t U b x t G z j V N p l m 1 V z P 9 a e i X g 2 G 4 S R s D R + C a A + G a G 9 n q x P v O E 4 1 K x X P g U l i 7 T I K C 0 g q Y k A w y e s g L i 0 v C N u Q K 7 5 0 M C e K 2 6 R q x 6 v x q W N S n G n j T g 6 4 Z b s V F V H W 7 h R 1 m Y r A 2 t 6 N N e S / Y s s S s v d J J f K i B J 6 z W 6 G s l B g 0 b n 4 U p 8 J w B n L n A G F G u F k x W x N D G L h / 7 6 l Q 1 X t D p U o J w u j r P k u 1 3 g C h t g 7 c B q O 7 + z o E 5 9 N J 9 H Y S f p 4 O Z + P 9 L o / Q K / Q a j V G E 3 q E Z + o D O 0 A I x b + Z l n v Y K / 6 v / z f / u / 7 h N 9 b 1 9 z U v U M / / n H w / 5 8 a 8 = &lt; / l a t e x i t &gt; . . . &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C r D U i d h b V + g Y x 4 d W g 1 z M 4 w w F C G M = " &gt; A A A D N n i c j V L L b t Q w F H X C q w T o A 5 Z s L K a V h s 0 o m U V h W T E b N o g i M W 2 l J B r Z j t O x x o k j + 2 b a U e Q / Y Q v f w K + w Y Y f Y 8 g k 4 6 Y A y H R Z c y d H x u b 7 n O E e m l R Q G w v C b 5 9 + 5 e + / + g 5 2 H w a P H T 3 b 3 9 g + e n h l V a 8 a n T E m l L y g x X I q S T 0 G A 5 B e V 5 q S g k p / T x a T t n y + 5 N k K V H 2 F V 8 b Q g l 6 X I B S P g q N m B t 3 d 0 G C e 0 a J Z 2 F q W H + O 9 m 3 G 6 S e a b A 9 N h 3 j o 0 T 4 N f Q W T e a Z 7 Y B T U S J 3 U d e k Z V N R 0 H Q 0 x y 2 4 N q + 3 N D u k 9 s e v e 5 / e S X L T i E I / m g 5 s p u j e T N R E / W + s j 2 m p 0 h l z W 3 z x s W H m S T G c G P t b H 8 Q j s K u 8 D a I 1 m C A 1 n X q I t x N M s X q g p f Q q c R R W E H a E A 2 C S W 6 D p D a 8 I m x B L n n s Y E k K b t K m u 4 H F R 4 7 J c K 6 0 W y X g j u 1 P N K Q w Z l V Q d 7 I g M D e 3 e y 3 5 r 1 5 c Q / 4 6 b U R Z 1 c B L d m O U 1 x K D w u 0 7 w J n Q n I F c O U C Y F u 6 u m M 2 J J g z c a 9 l w o c X G P z R F L U F o d b X J U q U W Q K i x g U s w u p 3 X N j g b j 6 L j U f h h P D g Z r r P c Q c / R C z R E E X q F T t B b d I q m i H l L 7 5 P 3 2 f v i f / W / + z / 8 n z d H f W 8 9 8 w x t l P / r N 1 0 6 B l U = &lt; / l a t e x i t &gt; . . . &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 1 4 3 c V d c e 6 S s J F q V + h M 9 g R z s I 3 Q = " &gt; A A A D N H i c j V J N b 9 M w G H b C 1 w h s b H D k Y t F N K p c q 6 W F w n N Q L F 2 B I d J u U R J X j u K t V J 4 7 s N + 2 q y H + E K / w G / g s S N 8 S V 3 4 C T B Z S 0 H H i l W I + f x + / z x K + c F I J r 8 P 1 v j n v n 7 r 3 7 D / Y e e o 8 e 7 x 8 8 O T x 6 e q F l q S i b U i m k u k q I Z o L n b A o c B L s q F C N Z I t h l s p z U + u W K K c 1 l / h E 2 B Y s z c p 3 z O a c E L D U 7 c v Z P j s M o y a q V m Q X x M f 6 7 G d e b a J F K 0 B 3 2 r W X D C N g N N N G V Y q m p Q B G e Y 7 u I N d m Y e O R 5 H c 9 h D W 7 M y 5 5 3 l 9 z N 6 K j / l R W t G o c G L l r Y 9 C X z a i I n 8 n 1 h r P a H 2 X Z 8 x 9 a Y C q I 1 0 8 b M D g f + y G 8 K 7 4 K g B Q P U 1 r k d 4 E G U S l p m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>M 7 B B b P g I n H V A y w 4 I r J T r 3 H N 9 z k i s n h e A G f P + b 4 9 6 7 / + D h o 5 3 H 3 u 6 T p 3 v P 9 g + e X x h V a s o m V A m l r x J i m O A 5 m w A H w a 4 K z Y h M B L t M 5 q e N f r l g 2 n C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>n s / P F / e p + d 3 + 4 P + + O u s 5 6 5 g X q l f v r N 8 M d B L 0 = &lt; / l a t e x i t &gt; CoOp &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 s r z 8 p V J r n h 6 c 3 f D 9 A d u K 7 G k 5 D 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>m 5 s 3 6 a a N c c A O e 9 9 1 x t + 5 s 3 7 2 3 c 7 / z 4 O G j 3 c d 7 + 0 8 u j C o 0 Z S O q h N J X E T F M 8 I y N g I N g V 7 l m R E a C X U b T s 7 p + O W P a c J V 9 h E X O Q k m u M 5 5 w S s B S k 3 2 n d 9 A b B 5 E s Z 9 X E D 3 v 4 b z K s k y C N F Z g W + 9 a y 4 w D Y D T S r S 8 3 i q g R N e I b t R 8 z J o g o H n U 5 L s 1 + D m + r F i n a b 3 N z R q v 7 X r m D W K D Q w / Q O b w S g p z 9 T 7 v G r l 6 4 L v 2 B x T Q Y x h p r J 9 A Z N 5 W r 6 m t N C E L k 7 w 4 c v B 0 X E 1 2 e t 6 A 6 8 J v A n 8 J e i i Z Z x b X 3 e D W N F C s g w a 9 b H v 5 R C W R A O n g l W d o D A s J 3 R K r t n Y w o x I Z s K y u V e F D y w T 4 0 R p e z L A D d u e K I k 0 Z i E j 2 y k J p G a 9 V p P / q o 0 L S I 7 C k m d 5 A S y j t 4 u S Q m B Q u H 4 c O O a a U R A L C w j V 3 N 4 V 0 5 R Y J 8 A + o Z U t k V z 5 h 1 I W A r h W 8 1 U 2 U m o K J D J V x z r o r / u 1 C S 6 G A / 9 w 4 H 0 Y d k / 7 S y 9 3 0 D P 0 H P W R j 1 6 h U / Q G n a M R o s 4 n 5 7 P z x f n q f n N / u D / d X 7 e t r r O c e Y p W Y m v 7 N 7 9 6 D H 4 = &lt; / l a t e x i t &gt; Accuracy: 65.89 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o K J T G r x y + 9 Q o H d 9 n S F U v s 7 0 Y T H k = " &gt; A A A D S 3 i c j V L N b t N A E F 6 7 F E q A / s C R y 4 q k U r h E d g 5 p x a m o F y 5 A k U h b y b a i 9 W Z T r 7 L r t X b H S S 3 L T 8 D T c I V n 4 A F 4 D m 6 I A 2 s 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>c r 2 h U x m S f m a 0 l w T W r z C J 6 P B 6 a i a H H a 9 g d c E 3 g b + C n T R K i 6 s r / v h V N F c s h Q a 9 c D 3 M o h K o o F T w a p O m B u W E T o n N y y w M C W S m a h s 7 l X h Y 8 t M 8 U x p e 1 L A D d u e K I k 0 p p C x 7 Z Q E E r N Z q 8 l / 1 Y I c Z q d R y d M s B 5 b S u 0 W z X G B Q u H 4 c e M o 1 o y A K C w j V 3 N 4 V 0 4 R Y J 8 A + o b U t s V z 7 h 1 L m A r h W y 3 U 2 V m o O J D Z V x z r o b / q 1 D S 6 H A 3 8 0 8 D 4 M u 2 f 9 l Z d 7 6 D l 6 g f r I R y f o D L 1 B F 2 i M q P P J + e x 8 c b 6 6 3 9 w f 7 k / 3 1 1 2 r 6 6 x m n q G 1 2 N n 9 D b 2 / D H 0 = &lt; / l a t e x i t &gt; Accuracy: 76.86 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b Y y m b d s 6 3 R O I j n Q F B 2 X B S Z y p 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>t 5 m 0 2 U m g J J T N W x D g a b f m 2 D q 7 4 X D D z / o t 8 9 7 a 2 8 3 E E v 0 S v U Q w E 6 R q f o H T p H Q 0 S d m f P J + e x 8 c b + 6 3 9 0 f 7 s 9 l q + u s Z l 6 g V r i / f g M c s Q O A &lt; / l a t e x i t &gt; [v1] [v2] . . . [vM ] [wind farm]. &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C X y Z v Q b z q G g R K R 8 u T 6 + Q Z O y C W j s = " &gt; A A A D O n i c b V J N j 9 M w E H X C 1 x J g P 9 g j F 4 v u S u V S J T 1 0 V 5 w W 7 Y U L s E h 0 d 6 U k q h z X 3 V q 1 4 8 i e t B u i / B e u 8 B v 4 I 1 y 5 I a 7 8 A J y 0 Q N M y U q I 3 b 2 b e 2 E 9 O M s E N + P 4 3 x 7 1 z 9 9 7 9 B z s P v U e P n + z u 7 R 8 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>3 z n r r r z c Q c / Q c 9 R F A T p B Z + g 1 u k B D R J 2 P z i f n s / P F / e p + d 3 + 4 P 5 e t r r O a O U S t c H / 9 B o w y B W c = &lt; / l a t e x i t &gt; [v1] [v2] . . . [vM ] [train railway]. &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k n M 3 e r q h s h 6 b C 2 D E t w w f 3 5 p 1 u Z E = " &gt; A A A D J H i c b V J N b 9 M w G H b C 1 y g w O j h y s W g r l U u U 9 N B N n I Z 2 4 Q I M i W 6 T k q h y H H e 1 a s e R / a Z d F O U P c I X f w K / h h j h w 4 a c g n K x A u / J K d p 7 3 e V 8 / t h 8 n y Q U 3 4 P s / H P f W 7 T t 3 7 + 3 d 7 z x 4 + G j / c f f g y Z l R h a Z s Q p V Q + i I h h g m e s Q l w E O w i 1 4 z I R L D z Z H H S 1 M + X T B u u s g 9 Q 5 i y W 5 D L j M 0 4 J W G r a / T X o h 1 E i q 2 U 9 D e I + / p u M m i S a p w r M B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w 8 K D L M X s 6 S a Y y w g a z G I U X W r + c 4 c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>s H P S 3 3 i 5 g 1 6 g l 6 i P A n S E T t A b d I 7 G i D r c + e R 8 d r 6 4 X 9 1 v 7 n f 3 5 2 2 r 6 2 x m n q N W u L 9 + A y n P / U M = &lt; / l a t e x i t &gt; [v1(x)] [v2(x)] . . . [vM (x)] [train railway]. &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>7 j b a y 1 4 8 i e 7 D a K I v F r u M J v 4 F / w D 7 g h r p z w p l u U d D t S n D d v Z p 7 t J 8 e 5 4 A Z 8 / 6 f j 3 r l 7 7 / 6 D r Y e 9 R 4 + f b D / d 2 X 1 2 a l S h K R t T J Z Q + j 4 l h g m d s D B w E O 8 8 1 I z I W 7 C y e H 6 / q Z w u m D V f Z Z y h z F k l y k f E Z p w Q s N d 1 1 X u z v T c J Y V o t 6 G k R 7 + H 8 y X C V h m i g w L f a 9 Z S e g C c + w X c S S l J H X 6 7 U k B i t w W b / u S L X J T c l W 9 T b p c N E M N D C 9 h i G w S 4 h n 1 b H 6 m N e t v P k 3 r l S a J X X 1 g S 0 x F c Q Y Z u q m j 8 k 8 r d 5 S W m h C y z f 4 Y O Q d j m z h e r 7 d P 9 3 p + 5 7 f B N 4 E w R r 0 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>h s 6 p 5 1 M L j p 1 y Y 4 H X r B y P M / D f t H g 7 W X W + g l e o U G K E A H 6 A i 9 Q y d o j K j z x f n q f H O + u z / c X + 5 v 9 8 9 V q + u s Z 5 6 j T r h / / w F J J g a K &lt; / l a t e x i t &gt; New classes &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>J 9 7 z n 2 M j y b J B D f g + z 8 c 9 9 7 9 B w 8 f 7 T z 2 n j x 9 t r u 3 f / D 8 w q h c U z a m S i h 9 m R D D B E / Z G D g I d p l p R m Q i 2 C R Z n t X 6 Z M W 0 4 S r 9 D E X G Y k m u U j 7 n l I C l p g f O 7 t F h G C W y X F X T I D 7 E / 5 N h n U S L m Q L T Y t 9 b N g R N e I r t S 6 x J E Q 8 8 r z W i X 4 P r 6 n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>0 R h R J 3 e + O F + d b + 5 3 9 6 f 7 y / 1 9 U + o 6 m 5 4 X q B P u n 7 9 B N Q F b &lt; / l a t e x i t &gt; Wind farm &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>s 8 9 5 z 7 G R 5 N k g h v w / R + O + + D h o 8 d P t p 5 6 z 5 6 / 2 N 7 Z 3 d u / M C r X l I 2 p E k p f J c Q w w V M 2 B g 6 C X W W a E Z k I d p k s T m v 9 c s m 0 4 S o 9 h y J j s S T X K Z 9 x S s B S k z 1 n / / A g j B J Z L q t J E B / g / 8 m w T q L 5 V I F p s R 8 s G 4 I m P M X 2 J V a k i A e e 1 x r R r 8 F N 9 a Y z q k 1 u j m y p 9 4 2 O l k 1 D A + f / Y A T s B p J Z e a o + Z V U r b 7 6 N K 6 V m 0 6 r 8 y F a Y C m I M M 1 V T x 2 Q 2 L 9 9 R m m t C i 7 f 4 a D Q 4 H l n h v L 1 3 s t v z B 3 4 T e B M E a 9 B D 6 z i z L m 5 H</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>i 1 1 n X X P S 9 Q J 9 8 9 f X H M D Q g = = &lt; / l a t e x i t &gt; Train railway &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 T W 2 Q 2 k v o I w A f S i d p P z 6 R 0 O 1 N 8 s = " &gt; A A A D A H i c j V F N j 9 M w E H X C 1 x J g 6 c K R i 0 W 7 U r l U S Q / A s V I v X B C L R H d X a q L K d p y t V T u O 7 E l 3 q y g X f g 0 3 x J V / A r 8 G J 1 u h Z M u B k W y 9 e f P x x m N a S G E h D H 9 5 / r 3 7 D x 4 + O n o c P H n 6 7 P j 5 4 O T F u d W l Y X z B t N T m k h L L p c j 5 A g R I f l k Y T h S V / I J u 5 k 3 8 Y s u N F T r / A r u C J 4 p c 5 S I T j I C j V o P f p 6 N l T F W 1 r V d R M s J / n W n j x O t U g + 2 w H x 2 7 j I H f Q K t c G Z 7 W F R g i c u w u e U 1 2 d T I J g k 7 P c Q N u 6 j e 9 3 l 3 y U K M T / Q + t U b x t G z j V N p l m 1 V z P 9 a e i X g 2 G 4 S R s D R + C a A + G a G 9 n q x P v O E 4 1 K x X P g U l i 7 T I K C 0 g q Y k A w y e s g L i 0 v C N u Q K 7 5 0 M C e K 2 6 R q x 6 v x q W N S n G n j T g 6 4 Z b s V F V H W 7 h R 1 m Y r A 2 t 6 N N e S / Y s s S s v d J J f K i B J 6 z W 6 G s l B g 0 b n 4 U p 8 J w B n L n A G F G u F k x W x N D G L h / 7 6 l Q 1 X t D p U o J w u j r P k u 1 3 g C h t g 7 c B q O 7 + z o E 5 9 N J 9 H Y S f p 4 O Z + P 9 L o / Q K / Q a j V G E 3 q E Z + o D O 0 A I x b + Z l n v Y K / 6 v / z f / u / 7 h N 9 b 1 9 z U v U M / / n H w / 5 8 a 8 = &lt; / l a t e x i t &gt; . . . &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I J t C a r a V V L l W 6 H H a g g h v M Z y B O H c= " &gt; A A A C O H i c b V A 9 T w J B E N 3 D L 8 Q v x N L m I j G x k d x R q C W J j a U m o k Q g Z H e Z k w 2 7 t 5 f d O Y V c + C u 2 + h v 8 J 3 Z 2 x t Z f 4 A J X i P q S S V 7 e m 8 n M P J Z I Y T E I 3 r z C 0 v L K 6 l p x v b S x u b W 9 U 9 6 t 3 F i d G g 5 N r q U 2 L U Y t S B F D E w V K a C U G q G I S b t n w f O r f P o C x Q s f X O E 6 g q + h 9 L C L B K T q p V 6 5 0 E E b I o u w O j D 6 2 A 4 2 T X r k a 1 I I Z / L 8 k z E m V 5 L j s 7 X r b n b 7 m q Y I Y u a T W t s M g w W 5 G D Q o u Y V L q p B Y S y o f 0 H t q O x l S B 7 Wa z 4 y f + o V P 6 f q S N q x j 9 m f p z I q P K 2 r F i r l N R H N j f 3 l T 8 z 2 u n G J 1 1 M x E n K U L M 5 4 u i V P q o / W k S f l 8 Y 4 C j H j l B u h L v V 5 w N q K E e X 1 8 I W p h Z + y F Q q U R j 9 u K g y r Y d I m Z 2 U X I L h 7 7 z + k p t 6 L T y p B V f 1 a u M o z 7 J I 9 s k B O S I h O S U N c k E u S Z N w M i J P 5 J m 8 e K / e u / f h f c 5 b C 1 4 + s 0 c W 4 H 1 9 A z V 1 r S 8 = &lt; / l a t e x i t &gt; Zero-shot &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 z / 5 H E K i m h 9 e Q z 1 k L t m R + H p i K R A = " &gt; A A A C R 3 i c b V C 7 T g M x E P S F d 3 g l U N I Y I i S q 6 I 4 C K J F o K I N E I N J x i v Y c H 7 H i x 8 n e A 0 W n 1 H w N L X w D n 8 B X 0 C F K n J C C E E a y P Z r d 1 X g n z a V w G I b v Q W V h c W l 5 Z X W t u r 6 x u b V d q + / c O F N Y x t v M S G M 7 K T g u h e Z t F C h 5 J 7 c c V C r 5 b T q 4 G N d v H 7 h 1 w u h r H O Y 8 U X C v R S Y Y o J e 6 t f 0 Y E h r n f Y P G v y b z 1 1 h 4 F L p H M 7 A q a X Z r j b A Z T k D n S T Q l D T J F q 1 s P t u 5 6 h h W K a 2 Q S n I u j M M e k B I u C S T 6 q 3 h W O 5 8 A G c M 9 j T z U o 7 p J y s s u I H n r F W x v r j 0 Y 6 U X 9 P l K C c G 6 r U d y r A v v t b G 4 v / 1 e I C s 7 O k F D o v k G v 2 Y 5 Q V k q K h 4 2 B o T 1 j O U A 4 9 A W a F / y t l f b D A 0 M c 3 4 5 K q m R 1 K V U g U 1 j z O q q k x A 4 T U j a o + w e h v X v P k 5 r g Z n T T D q + P G + d E 0 y 1 W y R w 7 I E Y n I K T k n l 6 R F 2 o S R J / J M X s h r 8 B Z 8 B J / B 1 0 9 r J Z j O 7 J I Z V I J v B j y w 2 Q = = &lt; / l a t e x i t &gt; [a] [photo] [of] [a] [wind farm]. &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J 6 U H T D M J 1 G O s D 1 8 m 3 z f v p k S 9 P 0 s = " &gt; A A A C S 3 i c b V B L S g N B E O 2 J / / h L d O m m M Q i u w o w L d R l w 4 1 L B m M A 4 h J p O j 2 n S n 6 G 7 x h C G n M D T u N U z e A D P 4 U 5 c 2 I l Z G P V B V z 1 e V V F d L 8 2 l c B i G b 0 F l a X l l d W 1 9 o 7 q 5 t b 2 z W 6 v v 3 T p T W M b b z E h j u y k 4 L o X m b R Q o e T e 3 H F Q q e S c d X k z r n Q d u n T D 6 B s c 5 T x T c a 5 E J B u i l X u 0 o h o T G + c C g 8 d l k P k w F t C A 0 9 U G O Y J w 0 e 7 V G 2 A x n o H 9 J N C c N M s d V r x 7 s 3 P U N K x T X y C Q 4 F 0 d h j k k J F g W T f F K 9 K x z P g Q 3 h n s e e a l D c J e X s n g k 9 8 k q f Z s b 6 p 5 H O 1 J 8 T J S j n x i r 1 n Q p w 4 H 7 X p u J / t b j A 7 D w p h c 4 L 5 J p 9 L 8 o K S d H Q q T m 0 L y x n K M e e A L P C / 5 W y A V h g 6 C 1 c 2 J K q h R t K V U g U 1 o w W 1 d S Y I U L q J l X v Y P T b r 7 / k 9 q Q Z n T b D 6 5 N G 6 3 j u 5 T o 5 I I f k m E T k j L T I J b k i b c L I I 3 k i z + Q l e A 3 e g 4 / g 8 7 u 1 E s x n 9 s k C K i t f 3 s a y w A = = &lt; / l a t e x i t &gt; [a] [photo] [of] [a] [train railway]. &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s p E v 4 e j 9 m g 3 L J 3 0 A X I G K z o l R d c Y = " &gt; A A A C P H i c b Z D L T g I x F I Y 7 e E N U B E 3 c u G k k J q z I D A Y x r j B u X G I i l w Q I 6 Z Q C D e 1 0 0 p 7 R k J G X c a v P 4 H u 4 d 2 f c u r Z c F g L + S Z M / / z k n p + f z Q 8 E N u O 6 H k 9 j Y 3 N r e S e 6 m 9 v Y P 0 o e Z 7 F H d q E h T V q N K K N 3 0 i W G C B 6 w G H A R r h p o R 6 Q v W 8 E e 3 0 3 r j k W n D V f A A 4 5 B 1 J B k E v M 8 p A R t 1 M y d t J s N h f E N p p A k d X + N y q X B R m n Q z O b f g z o T X j b c w O b R Q t Z t 1 0 u 2 e o p F k A V B B j G l 5 b g i d m G j g V L B J q h 0 Z F h I 6 I g P W s j Y g k p l O P D t g g s 9 t 0 s N 9 p e 0 L A M / S v x M x k c a M p W 8 7 J Y G h W a 1 N w / 9 q r Q j 6 V 5 2 Y B 2 E E L K D z R f 1 I Y F B 4 S g P 3 u G Y U x N g a Q j W 3 f 8 V 0 S C w J s M y W t v h y 6 Y Z Y R g K 4 V k / L q a / U C I h v J i l L 0 F v l t W 7 q x Y J 3 W X D v i 7 l K f s E y i U 7 R G c o j D 5 V R B d 2 h K q o h i p 7 R C 3 p F b 8 6 7 8 + l 8 O d / z 1 o S z m D l G S 3 J + f g E P P a 1 w &lt; / l a t e x i t &gt; Accuracy: 75.35 (b) The instance-conditional prompts learned by CoCoOp are much more generalizable than CoOp to the unseen classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 1 .</head><label>1</label><figDesc>Motivation of our research: to learn generalizable prompts. The images are randomly selected from SUN397<ref type="bibr" target="#b55">[55]</ref>, which is a widely-used scene recognition dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " i 4 z p m u R m 7 l 6 o l i 4 l X w k T 2 O Q 8 J s k = " &gt; A A A C F X i c b V C 7 T g J B F J 3 1 i f h C L W 0 2 g g k V 2 a V Q S x I b S 0 z k k c C G z M 7 O w o R 5 b G b u a s i G n 7 C x 0 F + x M 7 b W / o m l A 2 w h 4 E l u c n L O v b n 3 n j D h z I D n f T s b m 1 v b O 7 u F v e L + w e H R c e n k t G 1 U q g l t E c W V 7 o b Y U M 4 k b Q E D T r u J p l i E n H b C 8 e 3 M 7 z x S b Z i S D z B J a C D w U L K Y E Q x W 6 l b 6 o 0 i B q Q x K Z a / m z e G u E z 8 n Z Z S j O S j 9 9 C N F U k E l E I 6 N 6 f l e A k G G N T D C 6 b T Y T w 1 N M B n j I e 1 Z K r G g J s j m 9 0 7 d S 6 t E b q y 0 L Q n u X P 0 7 k W F h z E S E t l N g G J l V b y b + 5 / V S i G + C j M k k B S r J Y l G c c h e U O 3 v e j Z i m B P j E E k w 0 s 7 e 6 Z I Q 1 J m A j W t o S i q U f M p F y Y F o 9 T Y s 2 K n 8 1 m H X S r t f 8 q 5 p 3 X y 8 3 q n l o B X S O L l A V + e g a N d A d a q I W I o i j Z / S K 3 p w X 5 9 3 5 c D 4 X r R t O P n O G l u B 8 / Q I m m Z + d &lt; / l a t e x i t &gt; . . . &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i 4 z p m u R m 7 l 6 o l i 4 l X w k T 2 O Q 8 J s k = " &gt; A A A C F X i c b V C 7 T g J B F J 3 1 i f h C L W 0 2 g g k V 2 a V Q S x I b S 0 z k k c C G z M 7 O w o R 5 b G b u a s i G n 7 C x 0 F + x M 7 b W / o m l A 2 w h 4 E l u c n L O v b n 3 n j D h z I D n f T s b m 1 v b O 7 u F v e L + w e H R c e n k t G 1 U q g l t E c W V 7 o b Y U M 4 k b Q E D T r u J p l i E n H b C 8 e 3 M 7 z x S b Z i S D z B J a C D w U L K Y E Q x W 6 l b 6 o 0 i B q Q x K Z a / m z e G u E z 8 n Z Z S j O S j 9 9 C N F U k E l E I 6 N 6 f l e A k G G N T D C 6 b T Y T w 1 N M B n j I e 1 Z K r G g J s j m 9 0 7 d S 6 t E b q y 0 L Q n u X P 0 7 k W F h z E S E t l N g G J l V b y b + 5 / V S i G + C j M k k B S r J Y l G c c h e U O 3 v e j Z i m B P j E E k w 0 s 7 e 6 Z I Q 1 J m A j W t o S i q U f M p F y Y F o 9 T Y s 2 K n 8 1 m H X S r t f 8 q 5 p 3 X y 8 3 q n l o B X S O L l A V + e g a N d A d a q I W I o i j Z / S K 3 p w X 5 9 3 5 c D 4 X r R t O P n O G l u B 8 / Q I m m Z + d &lt; / l a t e x i t &gt; . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 2 .</head><label>2</label><figDesc>J D z D K 7 w 5 L 8 6 7 8 + F 8 z l v X n G z m B B b g f P 0 C x b + g g g = = &lt; / l a t e x i t &gt; vM &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i 4 z p m u R m 7 l 6 o l i 4 l X w k T 2 O Q 8 J s k = " &gt; A A A C F X i c b V C 7 T g J B F J 3 1 i f h C L W 0 2 g g k V 2 a V Q S x I b S 0 z k k c C G z M 7 O w o R 5 b G b u a s i G n 7 C x 0 F + x M 7 b W / o m l A 2 w h 4 E l u c n L O v b n 3 n j D h z I D n f T s b m 1 v b O 7 u F v e L + w e H R c e n k t G 1 U q g l t E c W V 7 o b Y U M 4 k b Q E D T r u J p l i E n H b C 8 e 3 M 7 z x S b Z i S D z B J a C D w U L K Y E Q x W 6 l b 6 o 0 i B q Q x K Z a / m z e G u E z 8 n Z Z S j O S j 9 9 C N F U k E l E I 6 N 6 f l e A k G G N T D C 6 b T Y T w 1 N M B n j I e 1 Z K r G g J s j m 9 0 7 d S 6 t E b q y 0 L Q n u X P 0 7 k W F h z E S E t l N g G J l V b y b + 5 / V S i G + C j M k k B S r J Y l G c c h e U O 3 v e j Z i m B P j E E k w 0 s 7 e 6 Z I Q 1 J m A j W t o S i q U f M p F y Y F o 9 T Y s 2 K n 8 1 m H X S r t f 8 q 5 p 3 X y 8 3 q n l o B X S O L l A V + e g a N d A d a q I W I o i j Z / S K 3 p w X 5 9 3 5 c D 4 X r R t O P n O G l u B 8 / Q I m m Z + d &lt; / l a t e x i t &gt; . . . &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M d Q Q k H P E T j y e 9 i f J O X G R j 5 e F 6 M 4 = " &gt; A A A C F 3 i c b V C 7 S g N B F J 3 1 G e M r a m m z G A R B C L s p 1 D J g Y x n B P D B Z w u x k N h k y j 2 X m r h q W / Q s b C / 0 V O 7 G 1 9 E 8 s n S R b m M Q D F w 7 n 3 M u 9 9 4 Q x Z w Y 8 7 9 t Z W V 1 b 3 9 g s b B W 3 d 3 b 3 9 k s H h 0 2 j E k 1 o g y i u d D v E h n I m a Q M Y c N q O N c U i 5 L Q V j q 4 n f u u B a s O U v I N x T A O B B 5 J F j G C w 0 n 0 X 6 B O E U X q e 9 U p l r + J N 4 S 4 T P y d l l K P e K / 1 0 + 4 o k g k o g H B v T 8 b 0 Y g h R r Y I T T r N h N D I 0 x G e E B 7 V g q s a A m S K c X Z + 6 p V f p u p L Q t C e 5 U / T u R Y m H M W I S 2 U 2 A Y m k V v I v 7 n d R K I r o K U y T g B K s l s U Z R w F 5 Q 7 e d / t M 0 0 J 8 L E l m G h m b 3 X J E G t M w I Y 0 t y U U c z + k I u H A t H r M i j Y q f z G Y Z d K s V v y L i n d b L d e q e W g F d I x O 0 B n y 0 S W q o R t U R w 1 E k E T P 6 B W 9 O S / O u / P h f M 5 a V 5 x 8 5 g j N w f n 6 B Z u R o Q E = &lt; / l a t e x i t &gt; + &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M d Q Q k H P E T j y e 9 i f J O X G R j 5 e F 6 M 4 = " &gt; A A A C F 3 i c b V C 7 S g N B F J 3 1 G e M r a m m z G A R B C L s p 1 D J g Y x n B P D B Z w u x k N h k y j 2 X m r h q W / Q s b C / 0 V O 7 G 1 9 E 8 s n S R b m M Q D F w 7 n 3 M u 9 9 4 Q x Z w Y 8 7 9 t Z W V 1 b 3 9 g s b B W 3 d 3 b 3 9 k s H h 0 2 j E k 1 o g y i u d D v E h n I m a Q M Y c N q O N c U i 5 L Q V j q 4 n f u u B a s O U v I N x T A O B B 5 J F j G C w 0 n 0 X 6 B O E U X q e 9 U p l r + J N 4 S 4 T P y d l l K P e K / 1 0 + 4 o k g k o g H B v T 8 b 0 Y g h R r Y I T T r N h N D I 0 x G e E B 7 V g q s a A m S K c X Z + 6 p V f p u p L Q t C e 5 U / T u R Y m H M W I S 2 U 2 A Y m k V v I v 7 n d R K I r o K U y T g B K s l s U Z R w F 5 Q 7 e d / t M 0 0 J 8 L E l m G h m b 3 X J E G t M w I Y 0 t y U U c z + k I u H A t H r M i j Y q f z G Y Z d K s V v y L i n d b L d e q e W g F d I x O 0 B n y 0 S W q o R t U R w 1 E k E T P 6 B W 9 O S / O u / P h f M 5 a V 5 x 8 5 g j N w f n 6 B Z u R o Q E = &lt; / l a t e x i t &gt; + &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M d Q Q k H P E T j y e 9 i f J O X G R j 5 e F 6 M 4 = " &gt; A A A C F 3 i c b V C 7 S g N B F J 3 1 G e M r a m m z G A R B C L s p 1 D J g Y x n B P D B Z w u x k N h k y j 2 X m r h q W / Q s b C / 0 V O 7 G 1 9 E 8 s n S R b m M Q D F w 7 n 3 M u 9 9 4 Q x Z w Y 8 7 9 t Z W V 1 b 3 9 g s b B W 3 d 3 b 3 9 k s H h 0 2 j E k 1 o g y i u d D v E h n I m a Q M Y c N q O N c U i 5 L Q V j q 4 n f u u B a s O U v I N x T A O B B 5 J F j G C w 0 n 0 X 6 B O E U X q e 9 U p l r + J N 4 S 4 T P y d l l K P e K / 1 0 + 4 o k g k o g H B v T 8 b 0 Y g h R r Y I T T r N h N D I 0 x G e E B 7 V g q s a A m S K c X Z + 6 p V f p u p L Q t C e 5 U / T u R Y m H M W I S 2 U 2 A Y m k V v I v 7 n d R K I r o K U y T g B K s l s U Z R w F 5 Q 7 e d / t M 0 0 J 8 L E l m G h m b 3 X J E G t M w I Y 0 t y U U c z + k I u H A t H r M i j Y q f z G Y Z d K s V v y L i n d b L d e q e W g F d I x O 0 B n y 0 S W q o R t U R w 1 E k E T P 6 B W 9 O S / O u / P h f M 5 a V 5 x 8 5 g j N w f n 6 B Z u R o Q E = &lt; / l a t e x i t &gt; + &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i 4 z p m u R m 7 l 6 o l i 4 l X w k T 2 O Q 8 J s k = " &gt; A A A C F X i c b V C 7 T g J B F J 3 1 i f h C L W 0 2 g g k V 2 a V Q S x I b S 0 z k k c C G z M 7 O w o R 5 b G b u a s i G n 7 C x 0 F + x M 7 b W / o m l A 2 w h 4 E l u c n L O v b n 3 n j D h z I D n f T s b m 1 v b O 7 u F v e L + w e H R c e n k t G 1 U q g l t E c W V 7 o b Y U M 4 k b Q E D T r u J p l i E n H b C 8 e 3 M 7 z x S b Z i S D z B J a C D w U L K Y E Q x W 6 l b 6 o 0 i B q Q x K Z a / m z e G u E z 8 n Z Z S j O S j 9 9 C N F U k E l E I 6 N 6 f l e A k G G N T D C 6 b T Y T w 1 N M B n j I e 1 Z K r G g J s j m 9 0 7 d S 6 t E b q y 0 L Q n u X P 0 7 k W F h z E S E t l N g G J l V b y b + 5 / V S i G + C j M k k B S r J Y l G c c h e U O 3 v e j Z i m B P j E E k w 0 s 7 e 6 Z I Q 1 J m A j W t o S i q U f M p F y Y F o 9 T Y s 2 K n 8 1 m H X S r t f 8 q 5 p 3 X y 8 3 q n l o B X S O L l A V + e g a N d A d a q I W I o i j Z / S K 3 p w X 5 9 3 5 c D 4 X r R t O P n O G l u B 8 / Q I m m Z + d &lt; / l a t e x i t &gt; . . . &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i 4 z p m u R m 7 l 6 o l i 4 l X w k T 2 O Q 8 J s k = " &gt; A A A C F X i c b V C 7 T g J B F J 3 1 i f h C L W 0 2 g g k V 2 a V Q S x I b S 0 z k k c C G z M 7 O w o R 5 b G b u a s i G n 7 C x 0 F + x M 7 b W / o m l A 2 w h 4 E l u c n L O v b n 3 n j D h z I D n f T s b m 1 v b O 7 u F v e L + w e H R c e n k t G 1 U q g l t E c W V 7 o b Y U M 4 k b Q E D T r u J p l i E n H b C 8 e 3 M 7 z x S b Z i S D z B J a C D w U L K Y E Q x W 6 l b 6 o 0 i B q Q x K Z a / m z e G u E z 8 n Z Z S j O S j 9 9 C N F U k E l E I 6 N 6 f l e A k G G N T D C 6 b T Y T w 1 N M B n j I e 1 Z K r G g J s j m 9 0 7 d S 6 t E b q y 0 L Q n u X P 0 7 k W F h z E S E t l N g G J l V b y b + 5 / V S i G + C j M k k B S r J Y l G c c h e U O 3 v e j Z i m B P j E E k w 0 s 7 e 6 Z I Q 1 J m A j W t o S i q U f M p F y Y F o 9 T Y s 2 K n 8 1 m H X S r t f 8 q 5 p 3 X y 8 3 q n l o B X S O L l A V + e g a N d A d a q I W I o i j Z / S K 3 p w X 5 9 3 5 c D 4 X r R t O P n O G l u B 8 / Q I m m Z + d &lt; / l a t e x i t &gt; . . . meta token &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 K z Q H U + 3 d 8 F 0 b p L X i R H f 1 + G M P S k = " &gt; A A A C F 3 i c b V A 9 T 8 M w E L 3 w W c p X g Z H F o k V i q p I O w F i J h b F I 9 E M 0 U e W 4 T m v V d i L b A V V R / w U L A / w V N s T K y D 9 h x G 0 z 0 J Y n n f T 0 3 p 3 u 7 o U J Z 9 q 4 7 r e z t r 6 x u b V d 2 C n u 7 u 0 f H J a O j l s 6 T h W h T R L z W H V C r C l n k j Y N M 5 x 2 E k W x C D l t h 6 O b q d 9 + p E q z W N 6 b c U I D g Q e S R Y x g Y 6 W H i h + K z E / Y p N I r l d 2 q O w N a J V 5 O y p C j 0 S v 9 + P 2 Y p I J K Q z j W u u u 5 i Q k y r A w j n E 6 K f q p p g s k I D 2 j X U o k F 1 U E 2 u 3 i C z q 3 S R 1 G s b E m D Z u r f i Q w L r c c i t J 0 C m 6 F e 9 q b i f 1 4 3 N d F 1 k D G Z p I Z K M l 8 U p R y Z G E 3 f R 3 2 m K D F 8 b A k m i t l b E R l i h Y m x I S 1 s C c X C D 5 l I u W E q f p o U b V T e c j C r p F W r e p d V 9 6 5 W r t f y 0 A p w C m d w A R 5 c Q R 1 u o Q F N I C D h G V 7 h z X l x 3 p 0 P 5 3 P e u u b k M y e w A O f r F + U S o J U = &lt; / l a t e x i t &gt; ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 K z Q H U + 3 d 8 F 0 b p L X i R H f 1 + G M P S k = " &gt; A A A C F 3 i c b V A 9 T 8 M w E L 3 w W c p X g Z H F o k V i q p I O w F i J h b F I 9 E M 0 U e W 4 T m v V d i L b A V V R / w U L A / w V N s T K y D 9 h x G 0 z 0 J Y n n f T 0 3 p 3 u 7 o U J Z 9 q 4 7 r e z t r 6 x u b V d 2 C n u 7 u 0 f H J a O j l s 6 T h W h T R L z W H V C r C l n k j Y N M 5 x 2 E k W x C D l t h 6 O b q d 9 + p E q z W N 6 b c U I D g Q e S R Y x g Y 6 W H i h + K z E / Y p N I r l d 2 q O w N a J V 5 O y p C j 0 S v 9 + P 2 Y p I J K Q z j W u u u 5 i Q k y r A w j n E 6 K f q p p g s k I D 2 j X U o k F 1 U E 2 u 3 i C z q 3 S R 1 G s b E m D Z u r f i Q w L r c c i t J 0 C m 6 F e 9 q b i f 1 4 3 N d F 1 k D G Z p I Z K M l 8 U p R y Z G E 3 f R 3 2 m K D F 8 b A k m i t l b E R l i h Y m x I S 1 s C c X C D 5 l I u W E q f p o U b V T e c j C r p F W r e p d V 9 6 5 W r t f y 0 A p w C m d w A R 5 c Q R 1 u o Q F N I C D h G V 7 h z X l x 3 p 0 P 5 3 P e u u b k M y e w A O f r F + U S o J U = &lt; / l a t e x i t &gt; ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 K z Q H U + 3 d 8 F 0 b p L X i R H f 1 + G M P S k = " &gt; A A A C F 3 i c b V A 9 T 8 M w E L 3 w W c p X g Z H F o k V i q p I O w F i J h b F I 9 E M 0 U e W 4 T m v V d i L b A V V R / w U L A / w V N s T K y D 9 h x G 0 z 0 J Y n n f T 0 3 p 3 u 7 o U J Z 9 q 4 7 r e z t r 6 x u b V d 2 C n u 7 u 0 f H J a O j l s 6 T h W h T R L z W H V C r C l n k j Y N M 5 x 2 E k W x C D l t h 6 O b q d 9 + p E q z W N 6 b c U I D g Q e S R Y x g Y 6 W H i h + K z E / Y p N I r l d 2 q O w N a J V 5 O y p C j 0 S v 9 + P 2 Y p I J K Q z j W u u u 5 i Q k y r A w j n E 6 K f q p p g s k I D 2 j X U o k F 1 U E 2 u 3 i C z q 3 S R 1 G s b E m D Z u r f i Q w L r c c i t J 0 C m 6 F e 9 q b i f 1 4 3 N d F 1 k D G Z p I Z K M l 8 U p R y Z G E 3 f R 3 2 m K D F 8 b A k m i t l b E R l i h Y m x I S 1 s C c X C D 5 l I u W E q f p o U b V T e c j C r p F W r e p d V 9 6 5 W r t f y 0 A p w C m d w A R 5 c Q R 1 u o Q F N I C D h G V 7 h z X l x 3 p 0 P 5 3 P e u u b k M y e w A O f r F + U S o J U = &lt; / l a t e x i t &gt; ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 K z Q H U + 3 d 8 F 0 b p L X i R H f 1 + G M P S k = " &gt; A A A C F 3 i c b V A 9 T 8 M w E L 3 w W c p X g Z H F o k V i q p I O w F i J h b F I 9 E M 0 U e W 4 T m v V d i L b A V V R / w U L A / w V N s T K y D 9 h x G 0 z 0 J Y n n f T 0 3 p 3 u 7 o U J Z 9 q 4 7 r e z t r 6 x u b V d 2 C n u 7 u 0 f H J a O j l s 6 T h W h T R L z W H V C r C l n k j Y N M 5 x 2 E k W x C D l t h 6 O b q d 9 + p E q z W N 6 b c U I D g Q e S R Y x g Y 6 W H i h + K z E / Y p N I r l d 2 q O w N a J V 5 O y p C j 0 S v 9 + P 2 Y p I J K Q z j W u u u 5 i Q k y r A w j n E 6 K f q p p g s k I D 2 j X U o k F 1 U E 2 u 3 i C z q 3 S R 1 G s b E m D Z u r f i Q w L r c c i t J 0 C m 6 F e 9 q b i f 1 4 3 N d F 1 k D G Z p I Z K M l 8 U p R y Z G E 3 f R 3 2 m K D F 8 b A k m i t l b E R l i h Y m x I S 1 s C c X C D 5 l I u W E q f p o U b V T e c j C r p F W r e p d V 9 6 5 W r t f y 0 A p w C m d w A R 5 c Q R 1 u o Q F N I C D h G V 7 h z X l x 3 p 0 P 5 3 P e u u b k M y e w A O f r F + U S o J U = &lt; / l a t e x i t &gt; ? Conditional Context Optimization (CoCoOp) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i 4 z p m u R m 7 l 6 o l i 4 l X w k T 2 O Q 8 J s k = " &gt; A A A C F X i c b V C 7 T g J B F J 3 1 i f h C L W 0 2 g g k V 2 a V Q S x I b S 0 z k k c C G z M 7 O w o R 5 b G b u a s i G n 7 C x 0 F + x M 7 b W / o m l A 2 w h 4 E l u c n L O v b n 3 n j D h z I D n f T s b m 1 v b O 7 u F v e L + w e H R c e n k t G 1 U q g l t E c W V 7 o b Y U M 4 k b Q E D T r u J p l i E n H b C 8 e 3 M 7 z x S b Z i S D z B J a C D w U L K Y E Q x W 6 l b 6 o 0 i B q Q x K Z a / m z e G u E z 8 n Z Z S j O S j 9 9 C N F U k E l E I 6 N 6 f l e A k G G N T D C 6 b T Y T w 1 N M B n j I e 1 Z K r G g J s j m 9 0 7 d S 6 t E b q y 0 L Q n u X P 0 7 k W F h z E S E t l N g G J l V b y b + 5 / V S i G + C j M k k B S r J Y l G c c h e U O 3 v e j Z i m B P j E E k w 0 s 7 e 6 Z I Q 1 J m A j W t o S i q U f M p F y Y F o 9 T Y s 2 K n 8 1 m H X S r t f 8 q 5 p 3 X y 8 3 q n l o B X S O L l A V + e g a N d A d a q I W I o i j Z / S K 3 p w X 5 9 3 5 c D 4 X r R t O P n O G l u B 8 / Q I m m Z + d &lt; / l a t e x i t &gt; Our approach, Conditional Context Optimization (Co-CoOp), consists of two learnable components: a set of context vectors and a lightweight neural network (Meta-Net) that generates for each image an input-conditional token.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 3 .</head><label>3</label><figDesc>Comprehensive comparisons of CoCoOp and CoOp in the base-to-new generalization setting. (a) CoCoOp is able to gain consistent improvements over CoOp in unseen classes on all datasets. (b) CoCoOp's declines in base accuracy are mostly under 3%, which are far outweighed by the gains in generalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>(a) Ablation on initialization.(b) Ablation on context length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 4 .</head><label>4</label><figDesc>Ablation studies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of CLIP, CoOp and CoCoOp in the base-to-new generalization setting. For learning-based methods (CoOp and CoCoOp), their prompts are learned from the base classes (16 shots). The results strongly justify the strong generalizability of conditional prompt learning. H: Harmonic mean (to highlight the generalization trade-off [54]). (a) Average over 11 datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) ImageNet.</cell><cell></cell><cell></cell><cell>(c) Caltech101.</cell></row><row><cell></cell><cell>Base</cell><cell>New</cell><cell>H</cell><cell></cell><cell>Base</cell><cell>New</cell><cell>H</cell><cell></cell><cell>Base</cell><cell>New</cell><cell>H</cell></row><row><cell>CLIP</cell><cell cols="3">69.34 74.22 71.70</cell><cell>CLIP</cell><cell cols="3">72.43 68.14 70.22</cell><cell>CLIP</cell><cell>96.84 94.00 95.40</cell></row><row><cell>CoOp</cell><cell cols="3">82.69 63.22 71.66</cell><cell>CoOp</cell><cell cols="3">76.47 67.88 71.92</cell><cell>CoOp</cell><cell>98.00 89.81 93.73</cell></row><row><cell cols="4">CoCoOp 80.47 71.69 75.83</cell><cell cols="4">CoCoOp 75.98 70.43 73.10</cell><cell cols="2">CoCoOp 97.96 93.81 95.84</cell></row><row><cell></cell><cell cols="2">(d) OxfordPets.</cell><cell></cell><cell></cell><cell cols="2">(e) StanfordCars.</cell><cell></cell><cell></cell><cell>(f) Flowers102.</cell></row><row><cell></cell><cell>Base</cell><cell>New</cell><cell>H</cell><cell></cell><cell>Base</cell><cell>New</cell><cell>H</cell><cell></cell><cell>Base</cell><cell>New</cell><cell>H</cell></row><row><cell>CLIP</cell><cell cols="3">91.17 97.26 94.12</cell><cell>CLIP</cell><cell cols="3">63.37 74.89 68.65</cell><cell>CLIP</cell><cell>72.08 77.80 74.83</cell></row><row><cell>CoOp</cell><cell cols="3">93.67 95.29 94.47</cell><cell>CoOp</cell><cell cols="3">78.12 60.40 68.13</cell><cell>CoOp</cell><cell>97.60 59.67 74.06</cell></row><row><cell cols="4">CoCoOp 95.20 97.69 96.43</cell><cell cols="4">CoCoOp 70.49 73.59 72.01</cell><cell cols="2">CoCoOp 94.87 71.75 81.71</cell></row><row><cell></cell><cell cols="2">(g) Food101.</cell><cell></cell><cell></cell><cell cols="2">(h) FGVCAircraft.</cell><cell></cell><cell></cell><cell>(i) SUN397.</cell></row><row><cell></cell><cell>Base</cell><cell>New</cell><cell>H</cell><cell></cell><cell>Base</cell><cell>New</cell><cell>H</cell><cell></cell><cell>Base</cell><cell>New</cell><cell>H</cell></row><row><cell>CLIP</cell><cell cols="3">90.10 91.22 90.66</cell><cell>CLIP</cell><cell cols="3">27.19 36.29 31.09</cell><cell>CLIP</cell><cell>69.36 75.35 72.23</cell></row><row><cell>CoOp</cell><cell cols="3">88.33 82.26 85.19</cell><cell>CoOp</cell><cell cols="3">40.44 22.30 28.75</cell><cell>CoOp</cell><cell>80.60 65.89 72.51</cell></row><row><cell cols="4">CoCoOp 90.70 91.29 90.99</cell><cell cols="4">CoCoOp 33.41 23.71 27.74</cell><cell cols="2">CoCoOp 79.74 76.86 78.27</cell></row><row><cell></cell><cell cols="2">(j) DTD.</cell><cell></cell><cell></cell><cell cols="2">(k) EuroSAT.</cell><cell></cell><cell></cell><cell>(l) UCF101.</cell></row><row><cell></cell><cell>Base</cell><cell>New</cell><cell>H</cell><cell></cell><cell>Base</cell><cell>New</cell><cell>H</cell><cell></cell><cell>Base</cell><cell>New</cell><cell>H</cell></row><row><cell>CLIP</cell><cell cols="3">53.24 59.90 56.37</cell><cell>CLIP</cell><cell cols="3">56.48 64.05 60.03</cell><cell>CLIP</cell><cell>70.53 77.50 73.85</cell></row><row><cell>CoOp</cell><cell cols="3">79.44 41.18 54.24</cell><cell>CoOp</cell><cell cols="3">92.19 54.74 68.69</cell><cell>CoOp</cell><cell>84.69 56.05 67.46</cell></row><row><cell cols="4">CoCoOp 77.01 56.00 64.85</cell><cell cols="4">CoCoOp 87.49 60.04 71.21</cell><cell cols="2">CoCoOp 82.33 73.45 77.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Comparison of prompt learning methods in the cross-dataset transfer setting. Prompts applied to the 10 target datasets are learned from ImageNet (16 images per class). Clearly, CoCoOp demonstrates better transferability than CoOp. ? denotes CoCoOp's gain over CoOp. 71.51 93.70 89.14 64.51 68.71 85.30 18.47 64.15 41.92 46.39 66.55 63.88 CoCoOp 71.02 94.43 90.14 65.32 71.88 86.06 22.94 67.36 45.73 45.37 68.21 65.74 ? -0.49 +0.73 +1.00 +0.81 +3.17 +0.76 +4.47 +3.21 +3.81 -1.02 +1.66 +1.86 Comparison of manual and learning-based prompts in domain generalization. CoOp and CoCoOp use as training data 16 images from each of the 1,000 classes on ImageNet. In general, CoCoOp is more domain-generalizable than CoOp.</figDesc><table><row><cell>Source</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Target</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ImageNet</cell><cell>Caltech101</cell><cell>OxfordPets</cell><cell>StanfordCars</cell><cell>Flowers102</cell><cell>Food101</cell><cell>FGVCAircraft</cell><cell>SUN397</cell><cell>DTD</cell><cell>EuroSAT</cell><cell>UCF101</cell><cell>Average</cell></row><row><cell cols="3">CoOp [63] Source</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Target</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Learnable?</cell><cell>ImageNet</cell><cell></cell><cell>ImageNetV2</cell><cell></cell><cell cols="2">ImageNet-Sketch</cell><cell cols="2">ImageNet-A</cell><cell cols="2">ImageNet-R</cell></row><row><cell>CLIP [40]</cell><cell></cell><cell>66.73</cell><cell></cell><cell>60.83</cell><cell></cell><cell>46.15</cell><cell></cell><cell>47.77</cell><cell></cell><cell>73.96</cell><cell></cell></row><row><cell>CoOp [63]</cell><cell></cell><cell>71.51</cell><cell></cell><cell>64.20</cell><cell></cell><cell>47.99</cell><cell></cell><cell>49.71</cell><cell></cell><cell>75.21</cell><cell></cell></row><row><cell>CoCoOp</cell><cell></cell><cell>71.02</cell><cell></cell><cell>64.07</cell><cell></cell><cell>48.75</cell><cell></cell><cell>50.63</cell><cell></cell><cell>76.18</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Recognition accuracy (average over 11 datasets) on a combination of base and new classes. The learnable models only have access to training data from base classes.</figDesc><table><row><cell>Learnable?</cell><cell>Accuracy</cell></row><row><cell>CLIP [40]</cell><cell>65.22</cell></row><row><cell>CoOp [63]</cell><cell>65.55</cell></row><row><cell>CoCoOp</cell><cell>69.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>CoCoOp (last row) vs a bigger CoOp on ImageNet. 76.47 67.88 71.92 CoOp (ctx=60) 30,720 76.16 65.34 70.34 CoOp (ctx=4) + Meta-Net 34,816 75.98 70.43 73.10</figDesc><table><row><cell>Model</cell><cell># params Base New</cell><cell>H</cell></row><row><cell>CoOp (ctx=4)</cell><cell>2,048</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We follow existing studies<ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b40">40]</ref> to refer to CLIP-like models as vision-language models.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">CoOp has an alternative version that learns class-specific context, which is not considered here because it is not straightforward to transfer class-specific context to unseen classes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/openai/CLIP.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/KaiyangZhou/CoOp.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">For convenience, we refer to base accuracy as the performance in base classes; and similarly for new accuracy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work is supported by NTU NAP, and under the RIE2020 Industry Alignment Fund -Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). CoOp's average performance is higher. In summary, the results suggest that efficient adaptation methods like CoOp and CoCoOp have great potential in tackling transfer learning problems.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A. Results on DOSCO-2k  The DOSCO (DOmain Shift in COntext) benchmark <ref type="bibr" target="#b64">[64]</ref> contains 7 image recognition datasets, which cover a wide range of classification problems, such as generic object recognition, fine-grained recognition on aircraft models, and action recognition. Unlike existing domain generalization datasets where the domain labels are manually defined and often limited to image style variations, DOSCO-2k focuses on broader contextual domain shift, which is automatically detected by a neural network pre-trained on the Places dataset <ref type="bibr" target="#b61">[61]</ref>. Following Zhou et al. <ref type="bibr" target="#b64">[64]</ref>, we use the 2k version where the training and validation splits in each dataset have 2,000 images in total (1,600 for training and 400 for validation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We study three methods' domain generalization performance on DOSCO-2k: CLIP, CoOp and CoCoOp. All models are trained on the training set and the checkpoints with the best validation performance are used for final test in unseen domains. <ref type="table">Table 6</ref> shows the results of four different architectures. It is clear that the two learnable methods outperform the zero-shot method with a large margin, despite having only a small number of parameters to tune. CoCoOp beats CoOp on 4 out of 7 datasets but</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Among the three approaches, CoOp and its follow-up, CoCoOp, contain learnable components while CLIP here denotes the zero-shot model. Both CoOp and CoCoOp use four learnable context tokens initialized with the word embeddings of</title>
	</analytic>
	<monogr>
		<title level="m">Bold denotes the best performance on each dataset for a specific architecture. P-Air P-Cars P-Ctech P-Ins P-Mam P-Pets P-UCF Avg</title>
		<imprint/>
	</monogr>
	<note>Table 6. Domain generalization results on DOSCO-2k, a recently proposed benchmark focusing on broader contextual domain shift</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simran</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<title level="m">On the opportunities and risks of foundation models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical study and analysis of generalized zeroshot learning for object recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Virtex: Learning visual representations from textual annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno>ICLR, 2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Write a classifier: Zero-shot learning using purely textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR-W</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>F?rst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeth</forename><surname>Rumetshofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kreil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?nter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Bitto-Nemling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.11316</idno>
		<title level="m">Modern hopfield networks with infoloob outperform clip</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongyao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04544</idno>
		<title level="m">Clip-adapter: Better vision-language models with feature adapters</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Making pretrained language models better few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15723</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-supervised learning of visual features through embedding images into text topic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mar?al</forename><surname>Rusi?ol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ali Eslami, and A?ron van den Oord. Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Doersch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<idno>2021. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Jacob Steinhardt, and Dawn Song. Natural adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fine-grained generalized zero-shot learning via dense attribute-based attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Elhamifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Jun Araki, and Graham Neubig. How can we know what language models know? ACL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning visual features from large weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Prompting visual-language models for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04478</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV-W</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Predicting deep zero-shot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning visual n-grams from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05208</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13586</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICVGIP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Continual lifelong learning with neural networks: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>German</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Part</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners. OpenAI blog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Denseclip: Language-guided dense prediction with contextaware prompting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Robert L Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamsa</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osbert</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Measuring robustness to natural distribution shifts in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A survey of zero-shot learning: Settings, methods, and applications. TIST</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via semantic embeddings and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Robust fine-tuning of zero-shot models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01903</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Zero-shot learning-the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Cpt: Colorful prompt tuning for pre-trained vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.11797</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Exploring hierarchical graph representation for large-scale zero-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhao</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.01386</idno>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xupeng</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.02413</idno>
		<title level="m">Pointclip: Point cloud understanding by clip</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langlotz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00747</idno>
		<title level="m">Contrastive learning of medical visual representations from paired images and text</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Factual probing is [mask]: Learning vs. learning to recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02503</idno>
		<title level="m">Domain generalization: A survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Learning to prompt for vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01134</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.07521</idno>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">On-device domain generalization. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
