<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanoona</forename><surname>Rasheed</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Maaz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Uzair</forename><surname>Khattak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of AI</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Link?ping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing open-vocabulary object detectors typically enlarge their vocabulary sizes by leveraging different forms of weak supervision. This helps generalize to novel objects at inference. Two popular forms of weak-supervision used in openvocabulary detection (OVD) include pretrained CLIP model and image-level supervision. We note that both these modes of supervision are not optimally aligned for the detection task: CLIP is trained with image-text pairs and lacks precise localization of objects while the image-level supervision has been used with heuristics that do not accurately specify local object regions. In this work, we propose to address this problem by performing object-centric alignment of the language embeddings from the CLIP model. Furthermore, we visually ground the objects with only imagelevel supervision using a pseudo-labeling process that provides high-quality object proposals and helps expand the vocabulary during training. We establish a bridge between the above two object-alignment strategies via a novel weight transfer function that aggregates their complimentary strengths. In essence, the proposed model seeks to minimize the gap between object and image-centric representations in the OVD setting. On the COCO benchmark, our proposed approach achieves 36.6 AP 50 on novel classes, an absolute 8.2 gain over the previous best performance. For LVIS, we surpass the state-of-the-art ViLD model by 5.0 mask AP for rare categories and 3.4 overall. Code: https://github.com/hanoonaR/object-centric-ovd.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Open-vocabulary detection (OVD) aims to generalize beyond the limited number of base classes labeled during the training phase. The goal is to detect novel classes defined by an unbounded (open) vocabulary at inference. Owing to the challenging nature of the OVD task, different forms of weak-supervision for novel categories are typically used, e.g., extra image-caption pairs to enlarge the vocabulary <ref type="bibr" target="#b0">[1]</ref>, image-level labels on classification datasets <ref type="bibr" target="#b1">[2]</ref> and pretrained open-vocabulary classification models like CLIP <ref type="bibr" target="#b2">[3]</ref>. The use of weak-supervision to enlarge the vocabulary is intuitive as the cost of annotating large-category detection datasets is monumental while the image-text/label pairs are readily available via large classification datasets <ref type="bibr" target="#b3">[4]</ref> or internet sources <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>One of the major challenges with enlarging vocabulary via image-level supervision (ILS) or pretrained models learned using ILS is the inherent mis-match between region and image-level cues. For instance, pretrained CLIP embeddings used in the existing OVD models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b1">2]</ref> do not perform well in locating object regions <ref type="bibr" target="#b6">[7]</ref> since the CLIP model is trained with full scale images. Similarly, weak supervision on images using caption descriptions or image-level labels does not convey the precise object-centric information. For label grounding in images, the recent literature explores expensive pretraining with auxiliary objectives <ref type="bibr" target="#b0">[1]</ref> or use heuristics such as, the max-score or max-size boxes <ref type="bibr" target="#b1">[2]</ref>.</p><p>In this paper, we set out to bridge the gap between object and image-centric representations within the OVD pipeline. To this end, we propose to utilize high-quality class-agnostic and class-specific object proposals via the pretrained multi-modal vision transformer (ViT) <ref type="bibr" target="#b7">[8]</ref>. The class-agnostic object proposals are then used to distill region-specific information in the CLIP visual embeddings, making them suitable for local objects. Furthermore, the class-specific proposal set allows us to visually ground a larger vocabulary, thereby aiding in generalization to novel categories. Next, the final and important question is how to make visual-language (VL) mapping amenable to local object-centric information. For this purpose, we introduce a region-conditioned weight transfer process which closely ties together image and region VL mapping. In a nut-shell, the proposed approach connects the image, region and language representations to generalize better to novel open-vocabulary objects.</p><p>The major contributions of this work include:</p><p>? We propose region-based knowledge distillation to adapt image-centric CLIP embeddings for local regions, thereby improving alignment between region and language embeddings. We show that the resulting well-aligned representations aid in improving the overall performance of our text driven OVD pipeline. ? In order to visually ground weak image labels, our approach performs pseudo-labeling using the high-quality object proposals from pretrained multi-modal ViTs. This helps in enlarging the class vocabulary and therefore generalizes better to new object classes. ? The above contributions mainly target the visual domain. In order to preserve the benefits of object-centric alignment in the language domain, we also propose to explicitly condition the (pseudo-labeled) image-level VL mapping on the region-level VL mapping via a novel weight transfer function. In this manner, we are the first to simultaneously integrate objectcentric visual and language alignment within a single architecture for OVD. ? Our extensive experiments demonstrate the improved OVD capability of the proposed approach. On COCO and LVIS benchmarks, our method achieves absolute gains of 11.9 and 5.0 AP on novel and rare classes over the current SOTA methods. Further generalizability is demonstrated by our cross-dataset evaluations performed on COCO, OpenImages and Objects365, leading to consistent improvements compared to existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Zero-shot Object Detection (ZSD): This setting involves detecting novel class objects at inference, for which no visual examples are available during training. Zhu et al. <ref type="bibr" target="#b8">[9]</ref> use semantic information with visual features to get proposals for both seen and unseen classes. Bensal et al. <ref type="bibr" target="#b9">[10]</ref> show that learning a good separation between background and foreground is critical in ZSD and propose to use multiple latent classes for modeling background during training. Rahman et al. <ref type="bibr" target="#b10">[11]</ref> propose a polarity loss to solve the ambiguity between background and unseen classes. DELO <ref type="bibr" target="#b11">[12]</ref> focuses on generating good proposals for unseen classes by synthesizing visual features for unseen objects using a generative model. Gupta et al. <ref type="bibr" target="#b12">[13]</ref> benefits from the contemporary cues in semantic and visual space ensuring better class separation for ZSD. Other works use additional learning signals, including unlabeled images from target domain <ref type="bibr" target="#b13">[14]</ref> and raw textual descriptions from the internet <ref type="bibr" target="#b14">[15]</ref>. Although significant progress has been made on this topic <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13]</ref>, the inherent complexity of the task makes it challenging for the ZSD models to generalize well to unseen object classes.</p><p>Weakly-supervised Object Detection (WSOD): In this setting, only image-level labels are used to approach object detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, or are used alongside the detection dataset to enlarge the detector vocabulary <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Bilen et al. <ref type="bibr" target="#b23">[24]</ref> proposed a weakly-supervised deep detection network (WSDNN) that uses off-the-shelf region proposals <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> and computes objectness and recognition scores for each proposal using separate subnetworks. Cap2Det <ref type="bibr" target="#b26">[27]</ref> operates in a similar setting and uses raw text captions to generate pseudo-labels to guide image-level supervision. Li et al. <ref type="bibr" target="#b27">[28]</ref> uses segmentation-detection collaborative network (SDCN) for accurate detection under weakly-supervised setting using only image labels. PCL <ref type="bibr" target="#b28">[29]</ref> proposes to cluster the spatially adjacent proposals and then assign image labels to each cluster. CASD <ref type="bibr" target="#b29">[30]</ref> argues that the detectors trained only with image-level labels are prone to detect boxes around salient objects and propose feature attention along with self-distillation to address the issue. YOLO9000 <ref type="bibr" target="#b30">[31]</ref> and DLWL <ref type="bibr" target="#b31">[32]</ref> augments the detection training by assigning image-level labels to the max-score proposal. Detic <ref type="bibr" target="#b1">[2]</ref> shows that using max-size proposal is an optimal choice for assigning image-level labels as it does not rely on the predictions of the network being optimized and provides better signals for the novel classes.</p><p>ii RoI Head Region-based KD <ref type="figure">Figure 1</ref>: An overview of our proposed object-centric framework for OVD. We pair a two-stage object detector with fixed language embeddings from a pretrained visual-language (VL) model, CLIP <ref type="bibr" target="#b2">[3]</ref> . Our proposed pseudo-labeling strategy Qpseudo uses pretrained multi-modal ViTs to obtain high-quality class-agnostic and class-specific proposals. The overall pipeline follows a stage-wise learning strategy. First, we introduce region-based knowledge distillation (RKD) to adapt image-centric CLIP embeddings for local regions. Using the pretrained VL image encoder as a teacher model, we train the detector to induce point-wise and interembedding relationship alignment with our region embeddings using class-agnostic proposals from Qpseudo. Next, we utilize a weakly-supervised learning framework by combining instance-level labels from detection dataset and image-level labels from classification dataset which are visually grounded using Qpseudo. This weaksupervision helps in enlarging the class vocabulary and generalizes the detector to novel classes. To preserve the benefits of object-centric alignment in the language domain learned via RKD, we explicitly condition the image-level VL mapping WP , on the learned region-level VL mapping WD via a novel weight transfer function.</p><p>We also operate in a similar WSOD setting and use high-quality object proposals from pretrained multi-modal ViT <ref type="bibr" target="#b7">[8]</ref> to enlarge detector vocabulary and generalize towards novel object categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open-vocabulary Object Detection (OVD):</head><p>In OVD, the objective is to detect target class objects not present in the training/base class vocabulary. A typical solution of the problem is to replace the classifier weights with text embeddings of the target vocabulary (e.g., GloVe <ref type="bibr" target="#b32">[33]</ref>, BERT <ref type="bibr" target="#b33">[34]</ref>, CLIP <ref type="bibr" target="#b2">[3]</ref>). OVR-RCNN <ref type="bibr" target="#b0">[1]</ref> uses BERT embeddings as classifier weights and proposes to use openvocabulary captions to learn the vision-to-language mapping. It surpasses the ZSD approaches by a large margin. ViLD <ref type="bibr" target="#b5">[6]</ref> uses pretrained CLIP <ref type="bibr" target="#b2">[3]</ref> to distill knowledge into a two-stage object detector <ref type="bibr" target="#b34">[35]</ref> and replaces the classifier weights with CLIP text embeddings obtained by ensembling multiple text prompts (e.g., a {category}, a photo of a {category}). Gao et al. <ref type="bibr" target="#b35">[36]</ref> generate pseudo bounding-box labels using pretrained VL models for training open-vocabulary detector. All these methods use carefully designed manual prompts for generating text embeddings. DetPro <ref type="bibr" target="#b36">[37]</ref> and PromptDet <ref type="bibr" target="#b37">[38]</ref> replace these manual prompts with learnable tokens and achieve competitive results on novel/rare categories. However, in our work, we use fixed manual prompts and instead focus on improving the object-centric representations for open-vocabulary object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Object-centric Open-Vocabulary Detection</head><p>Here, we first present a brief overview of the proposed open-vocabulary detection (OVD) framework. As discussed earlier, existing OVD methods use different forms of weak supervision that employ image-centric representations, making them less suited for the end detection task. Our proposed method aims to bridge the gap between image and object-centric visual-language (VL) representations. We summarize the architectural overview of our method in <ref type="figure">Fig. 1</ref>. The proposed design has three main elements. 1) Our region-based knowledge distillation (refer Sec. 3.2) adapts image-centric language representations to be object-centric. A VL mapping learns to align the local region representations of the detector to the language representations by distilling the detector's region representations with region representations from a VL model (CLIP). 2) Given weak image-level supervision, we use pseudo-labeling from pretrained multi-modal ViTs (refer Sec. 3.3) to improve generalization of the detector to novel classes. 3) For an efficient combination of the above two proposed components, we condition the VL mapping learned during the weak supervision on the VL mapping learned with region-based distillation via a novel weight transfer function (refer Sec. 3.4). Specifically, we follow a stage-wise learning strategy to first align the region and language embeddings using RKD, and use this distilled VL mapping for object-centric visual and language alignment in the subsequent stage.</p><p>iii</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Detection Pipeline: Preliminaries</head><p>In the open-vocabulary detection problem, we have access to an object detection dataset where the training set, D det , comprises samples from the set of base object categories, C B . The images of D det are exhaustively annotated with bounding-box labels and corresponding class labels y r ? C B , for the different objects in the image. Given an image I ? R H?W ?3 , we design an open-vocabulary object detector to solve two subsequent problems: (1) effectively localize all objects in the image, (2) classify the detected region into one of the class label of C test , which is provided by the user at test time. The categories during test time also include novel categories C N beyond the closed set of base categories seen during the training phase, i.e., C test = C B ? C N .</p><p>We convert a generic two-stage object detector <ref type="bibr" target="#b34">[35]</ref> to an open-vocabulary detector by replacing the learnable classifier head with fixed language embeddings, T corresponding to the category names of C test , that are obtained using a large-scale pretrained VL model. Following <ref type="bibr" target="#b5">[6]</ref>, we use the text embeddings from CLIP text encoder <ref type="bibr" target="#b2">[3]</ref> for classification, where only the embeddings of C B categories, T CB are used during training. Specifically, we generate the text embeddings offline, by processing the prompts corresponding to each category with a template of 'a photo of {category}' through the CLIP text encoder. The RoI <ref type="bibr" target="#b34">[35]</ref> head computes pooled feature representations ?(r) of the proposals r generated by the region proposal network (RPN). These feature embeddings are projected to a common feature space shared by the text embedding T using a linear layer f (?), which we represent as region embeddings, R = f (?(r)) ? R D . For classification, we compute the cosine similarity between the region embeddings and text embeddings to find the matching pairs. During training, the regions that do not match with any of the ground-truths are assigned to the background category represented by a fixed all zero embedding. We compute the cosine similarity by comparing each region to each base class,</p><formula xml:id="formula_0">V = sim(r, b) = cos R(r), T b ? b ? C B .</formula><p>The classification loss is a softmax cross-entropy (CE) where the logits are the cosine similarity scores,</p><formula xml:id="formula_1">L cls = 1 N r L CE softmax V ? , y r , y r ? C B .</formula><p>where ? is the temperature, N is the total number of proposals per image, and r represents a single proposal with the ground-truth label y r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Region-based Knowledge Distillation</head><p>In the OVD setting, we assume that f (?) learns a VL mapping and aligns the output region embeddings of the detector with the corresponding CLIP text embeddings. However, the performance on novel categories is not comparable to what CLIP encoded embeddings would provide (refer Appendix B for details). We hypothesize that this performance gap is mainly due to two reasons, i) the data that has been used for training CLIP model consist of scene-centric images, making it less suitable for region classification, e.g., in our case where object-centric tightly bounded proposals are used, ii) the zero-shot generalization ability of the pair-wise trained CLIP image and text embeddings cannot be fully utilized due to the mismatch between regions representations from CLIP image encoder and our detector. Based on these insights, we propose a region-based knowledge distillation (RKD).</p><p>The proposed RKD uses distillation in the detection pipeline by distilling region embeddings from high-quality class-agnostic proposals (r) obtained from a pretrained multi-modal ViT (MViT) <ref type="bibr" target="#b7">[8]</ref>.</p><p>Note that we obtain both class-agnostic (used in RKD) and class-specific (refer Sec. 3.3) object proposals using this pseudo-labeling process, which we refer to as Q pseudo . This is possible via using intuitive text queries to interact with the MViT model that can locate generic objects and provides the corresponding set of candidate proposals. The queries can be generic or targeted, based on the task, e.g., 'all objects' to generate class-agnostic proposals, or 'every dog' for a specific class.</p><p>For RKD, we compute class agnostic proposals on D det using simple text query, 'all objects' and select top-K proposals <ref type="figure">(Fig. 3b</ref>). CLIP embeddings I(r) are then computed offline using the CLIP image encoder I(?). With the detector region embeddings and the corresponding CLIP region representations, we propose to use two types of distillation losses to improve the alignment.</p><p>(1) Point-wise embedding matching loss: The L 1 loss matches the individual region embeddings R = f (?(r)) with the CLIP region representations I(r), Using this criteria, our visual encoder, along with the VL projection layer f (?), approximates the CLIP image encoder and consequently aligns our region embeddings with the CLIP text embeddings.</p><formula xml:id="formula_2">L 1 = 1 K r R ? I(r) 1 .<label>(1)</label></formula><p>(2) Inter-embedding relationship matching loss (IRM): It is a knowledge distillation based loss L irm that instills inter-embedding relationships within our region representations to be consistent to the CLIP region representations <ref type="bibr" target="#b38">[39]</ref>. Instilling such inter-embedding relations would be beneficial as we know that the teacher model I(?), and the student model (our detector), are different in nature with respect to their training methods <ref type="figure" target="#fig_0">(Fig. 2)</ref>. The IRM loss is defined on pairwise similarity matrices of the two different sets of embeddings. Specifically, with the top-K proposals computed from Q pseudo , we compose K ? K similarity matrices for I(r) andR denoted by S I and S R respectively. Notably, these matrices are normalized by L2 norm applied row-wise. The IRM loss is a Frobenius norm ? F , over the mean element-wise squared difference between S I and S R ,</p><formula xml:id="formula_3">S R =R ?R T R ?R T 2 , S I = I(r) ? I(r) T I(r) ? I(r) T 2 , L irm = 1 K 2 S R ? S I 2 F .<label>(2)</label></formula><p>We weight the L 1 and L irm losses by factors ? 1 and ? 2 , respectively. Together with the standard twostage detector losses; RPN loss (L rpn ), regression loss (L reg ) and classification loss (L cls ) <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref>; the overall training objective with RKD can be expressed as,</p><formula xml:id="formula_4">L RKD = L rpn + L reg + L cls + ? 1 L 1 + ? 2 L irm .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image-level Supervision with Pseudo Box Labels</head><p>In the open-vocabulary setting, a fundamental challenge is to generalize the detector to novel classes. However, due to the daunting task of densely locating all objects in natural scenes, the existing detection datasets are of relatively smaller magnitude compared to the classification datasets, which are easier to annotate. To this end, Zhou et al. <ref type="bibr" target="#b1">[2]</ref> proposed to take advantage of a large-scale image classification dataset during training to expand the detector's vocabulary. However, an important question is how to effectively associate the region proposals of novel objects with the corresponding labels. We note that the existing approach uses heuristics such as selecting the whole image as a single box, or just the maximum sized box from the RPN, which can ignore potential objects ( <ref type="figure">Fig. 3a</ref>).</p><p>We propose a weakly-supervised method to generalize the detector to novel categories by using pseudo-box labels from pretrained MViT <ref type="bibr" target="#b7">[8]</ref>. We follow <ref type="bibr" target="#b1">[2]</ref> to train the detector with a combination v <ref type="figure">Figure 3</ref>: (a) Class-specific Proposals: A visual comparison of heuristic methods (left) used for visual grounding in image-level supervision <ref type="bibr" target="#b1">[2]</ref> with our proposed method (right). Using heuristic based approaches like selecting maximum sized box from the RPN can ignore local objects in the scene. In our method, we design class-specific text queries with known class labels for pseudo-labeling potential objects. (b) Class-agnostic Proposals: In region-based knowledge distillation (RKD), we induce better region-level alignment with fewer high-quality proposals from a generalized class-agnostic proposal generator <ref type="bibr" target="#b7">[8]</ref>. We compare top-K RPN proposals (left) with top-K multi-modal ViTs proposals used in a class-agnostic manner (right).</p><p>of detection and classification dataset. A batch of data is prepared by combining data from the detection dataset D det that are exhaustively annotated with bounding-box and class labels, with data from a classification dataset D cls that only contains image-level labels. With Q pseudo , we obtain the pseudo-box labels on this classification dataset, which we use for image-level supervision (ILS).</p><p>Specifically, consider a sample image I ? D cls , which has a total of N ground-truth class labels, we generate object proposals offline with the use of MViT corresponding to these weak labels. Specifically, we construct N class-specific text queries {t n } N n=1 with template 'every {category}', and obtain K proposals {r k } K k=1 and corresponding confidence scores {s k } K k=1 for each query,</p><formula xml:id="formula_5">[(r 1 ,s 1 ), (r 2 ,s 2 ), ? ? ? (r K ,s K )] = Q pseudo (I, t n ); I ? D cls , n ? N.</formula><p>We select the top-1 proposal with the highest confidence score, as the pseudo-box label for a particular category. This gives us N high-quality pseudo-box labels for each image, corresponding to its N image-level category labels ( <ref type="figure">Fig. 3a)</ref>. We compute the region embeddingsR for proposalsr as,</p><formula xml:id="formula_6">R n = f (?(rk)),k = argmax k (s k ).</formula><p>In the case of D det , the training follows the standard two-stage RCNN training recipe. However, for D cls , only the classification loss is updated. We call this pseudo-max score, L pms loss.</p><formula xml:id="formula_7">L pms = 1 N n BCE(V, yr), where V = cos R n , T .<label>(4)</label></formula><p>We weight L pms by a factor ? and the overall training objective with our ILS can be expressed as,</p><formula xml:id="formula_8">L ILS = L rpn + L reg + L cls , if I ? D det ? L pms , if I ? D cls .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Weight Transfer Function</head><p>To combine the alignment from region-based distillation (Sec. 3.2) with the benefits from weak supervision with pseudo-box labels (Sec. 3.3), a naive approach would be to train the detector with a combination of losses: L 1 (1), L irm (2) and L pms (4). However, we demonstrate that a simple combination of the two approaches does not lead to complimentary benefits, instead they compete with each other ( <ref type="table" target="#tab_4">Table 2</ref>). The additional supervision from pseudo-labels improves the generalization of the detector, while the region-based distillation works towards object-centric alignment in the language domain, thereby improving the overall performance of the detector. We aim to incorporate the benefits from the two approaches and preserve the object-centric alignment in the language domain. To this end, we use a weight transfer mechanism <ref type="bibr" target="#b40">[41]</ref> from VL projection used in regionbased distillation to the weak supervision by learning a weight transfer function, W T (?). In other words, the VL projection function f (?) used during the weak image-level supervision is explicitly conditioned on the mapping function used for alignment in the distillation process. This way, both the transformations are tied together to reinforce mutual representation capability and avoid any conflict in the learned function mapping. Let the weights of the projection layer in RKD and weak vi image-level supervision be represented as W D and W P respectively. The weight transfer operation is given by,</p><formula xml:id="formula_9">W P = W T (W D ) = W ?2 ?(W ?1 W D ) ; W T : W D ? W P .</formula><p>Here, W D is kept frozen and we design W T as a 2-layer MLP, W ?1 followed by W ?2 a with LeakyReLU (?) activation with a negative slope of 0.1. Further, we use a skip connection across W P by projecting the original representations using a separate 2-layer MLP <ref type="figure">(Fig. 1)</ref>. The total loss here is a combination of L RKD (Eq. 3) and L ILS (Eq. 5) loss, given by,  We conduct our experiments on COCO <ref type="bibr" target="#b41">[42]</ref> and LVIS v1.0 <ref type="bibr" target="#b42">[43]</ref> under OVD setting. For evaluation, we use the generalized ZSD setting where the classifier contains both base and novel categories. <ref type="table" target="#tab_2">Table 1</ref> summarizes all the datasets used in our work. Following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>, we use a subset of ImageNet-21K having 997 overlapping LVIS categories and COCO captions dataset for ILS in LVIS and COCO experiments respectively (refer Appendix. A for more details). For the pseudo-labeling process Q pseudo , we use the MViT pretrained on a Large-scale Modulated Detection (LMDet) dataset <ref type="bibr" target="#b7">[8]</ref>. We ensure that MViT pretraining dataset has no overlap with any of the evaluation datasets in our work. Additionally, in all our experiments we use a pretrained MViT that we train using the author's provided code on filtered LMDet ( ?LMDet) dataset by entirely restricting any exposure to the novel/rare classes in evaluation.</p><formula xml:id="formula_10">L = L rpn + L reg + L cls + ? 1 L 1 + ? 2 L irm + ? L pms .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COCO OVD:</head><p>We use COCO-2017 dataset for training and validation. We follow the ZS splits proposed in <ref type="bibr" target="#b9">[10]</ref>, in which 48 categories are selected as base and 17 are selected as novel classes.</p><p>LVIS OVD: LVIS contains 1203 categories which are further split into frequent, common and rare categories. Inline with <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b1">2]</ref>, we combine the frequent and common categories to form base classes and keep all rare classes as novel, resulting in 866 base and 337 rare classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-transfer Datasets:</head><p>To validate the adaptability of our method, we evaluate and compare results of our LVIS trained model on OpenImages <ref type="bibr" target="#b43">[44]</ref> and Objects365 <ref type="bibr" target="#b44">[45]</ref> and COCO <ref type="bibr" target="#b41">[42]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>We conduct COCO experiments using Faster R-CNN <ref type="bibr" target="#b34">[35]</ref> with ResNet-50 backbone. We train the supervised-base model on 48 base classes (C B ) for 1x schedule (?12 COCO epochs) and report box AP 50 . For RKD, we finetune this model for another 1x schedule using box labels from C B and class-agnostic proposals from the pretrained MViT <ref type="bibr" target="#b7">[8]</ref>. This model is further finetuned for 1x schedule with ILS and the associated weight transfer function using class labels from COCO captions and corresponding class-specific proposals from MViT. This sums to an overall 3x training schedule.</p><p>For LVIS experiments, we use Mask R-CNN <ref type="bibr" target="#b39">[40]</ref> with federated loss <ref type="bibr" target="#b45">[46]</ref> and sigmoid cross-entropy, and report mask AP. For RKD and weight transfer, we use the same training schedules as of COCO and report the average over three runs. For comparison with Detic <ref type="bibr" target="#b1">[2]</ref>, we apply our proposed method on their strong CenterNetV2 <ref type="bibr" target="#b45">[46]</ref> baseline under the same settings. It uses ImangeNet21K pretrained backbone with 4x schedule using large scale jittering (LSJ) <ref type="bibr" target="#b46">[47]</ref> augmentations. All of our models are trained using 8 A100 GPUs with an approximate training time of 9 and 6 hours for 1x schedule of COCO and LVIS respectively.</p><p>In our experiments, we use SGD optimizer with a weight decay of 1e ?4 and a momentum of 0.9. We train for 1x schedule with batch size of 16 and an initial learning rate of 0.02 which drops by a factor of 10 at the 8 th and 11 th epoch. We set temperature ? to 50. Our longer schedules experiments use 100-1280 LSJ <ref type="bibr" target="#b46">[47]</ref>. We use ? of 0.1 to weight L pms . For computing CLIP embeddings we use the vii CLIP model ViT-B/32 <ref type="bibr" target="#b2">[3]</ref>, with input size of 224?224. We use the query 'a photo of a {category}' for to compute the text embeddings for the classifier. For distillation, we use top 5 proposals from the pretrained MViT <ref type="bibr" target="#b7">[8]</ref> evaluated with generic query, 'all objects', generating class-agnostic proposals. We refer to Appendix D for additional details on the approach we use to generate class-agnostic and class-specific proposals from MViT. In COCO experiments, we set weights ? 1 and ? 2 to 0.15. In LVIS, we set ? 1 to 0.15 and ? 2 to 0.25. We choose these values using a randomized hyper-parameter search on the corresponding held-out datasets. The 2-layer MLP in our weight transfer function has a hidden dim of 512, and a hidden dim of 1024 is used in the MLP skip connection across W P in <ref type="figure">Fig. 1</ref> (refer to Appendix C for more details). <ref type="table" target="#tab_4">Table 2</ref> shows the contribution of individual components in our proposed approach. Building on top of the supervised-base model, our region-based knowledge distillation (RKD) shows an absolute gain of 19.5 and 1.5 AP for COCO novel and base classes respectively, indicating the adaptability of image-centric CLIP embeddings for local regions. With pseudo-box labeled weak image-level supervision (PIS), novel class AP improves by 28.7, demonstrating generalization to novel classes and thus enlarging the detector's vocabulary. Naively combining the two approaches shows improvement, but struggles to maintain the gains from the individual components. In contrast, our weight transfer method suitably combines the complimentary benefits of both components <ref type="figure" target="#fig_0">(Fig. 2</ref>  Open-vocabulary Detection -COCO: We compare our OVD results with previously established methods in <ref type="table">Table 3</ref>. OVR-CNN learns a vision-to-language mapping with expensive pretraining. Detic uses ILS to improve detection on novel classes. We use a novel weight transfer function to perform object-centric VL alignment and achieve 54.0 AP on the base classes, surpassing OVR-CNN and Detic by 8.0 AP and 0.2 AP respectively. On novel classes our method achieves 36.6 AP, the highest novel AP achieved over all methods. In comparison with ViLD, which trains for 8x schedule (? 96 epochs), our method with the same schedule provides 56.6 base AP, lagging by 2.9.  <ref type="table">Table 3</ref>: OVD results on COCO. Here C B and C N represents the base and novel classes respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Our Approach: Main results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>?The results quoted from <ref type="bibr" target="#b0">[1]</ref>. ?ViLD and our methods are trained for longer 8x schedule (shown in gray). ?We train detic for another 1x for a fair comparison with our method. For ViLD, we use their unified model that trains ViLD-text and ViLD-Image together. For Detic, we report their best model. viii On novel classes, we achieve 36.9 AP surpassing ViLD by a gain of 9.3. In contrast to ViLD design, our weight transfer function allows both RKD and ILS to provide complimentary gains without any negative competition among the two methods <ref type="bibr" target="#b5">[6]</ref>.  <ref type="figure" target="#fig_1">384 epochs)</ref>, already surpassing the rare AP by 1.0 while having slightly lower performance on frequent classes. Extending our model to 8x schedule fills the gap, surpassing ViLD by 0.8 in frequent and 5.0 AP in rare classes respectively. In <ref type="table" target="#tab_6">Table 4</ref> (right), we compare our method with Detic by using their strong LVIS baseline that uses CenterNetV2 network. Following similar settings, we finetune their box-supervised model using our weight transfer method and show improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open-vocabulary Detection -LVIS:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Epochs APr APc AP f AP    <ref type="table">Table 6</ref>: Cross-dataset evaluation. ?The results evaluated using official implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-dataset evaluation performance:</head><p>We provide cross-dataset evaluation of our model in <ref type="table">Table 6</ref> and compare with prior OVD works. ViLD-text <ref type="bibr" target="#b5">[6]</ref> and Detic-base <ref type="bibr" target="#b1">[2]</ref> are box-supervised baseline models for ViLD and Detic respectively. Our method builds on top of Detic-base and shows favourable results when directly transferred to cross-datasets without any dataset-specific finetuning. We use our method trained on LVIS and report AP 50 on COCO <ref type="bibr" target="#b41">[42]</ref>, OpenImages <ref type="bibr" target="#b43">[44]</ref> and Objects365 <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of RKD and ILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Region-based Knowledge Distillation (RKD):</head><p>We ablate the effect of L 1 (Eq. 1) and L irm (Eq. 2) RKD approach on COCO ( <ref type="table" target="#tab_11">Table 7)</ref>. The results show the importance of both loss functions, where using L 1 loss over base model with top-5 proposals from MViT <ref type="bibr" target="#b7">[8]</ref> improves the base and novel class by 1.9 and 15.0 AP (row-1 vs 3). Using L irm in row-4 further improves the overall and novel class AP. To show the importance of using quality proposals in RKD, we compare the model trained with L 1 loss using top-5 RPN vs MViT proposals (row-2 vs 3). All the models in rows 2-4 are finetuned on the base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Weak Image-level Supervision (ILS):</head><p>We compare different choices of ILS in <ref type="table">Table 8</ref>. Our L pms loss (Eq. 4) is compared with previously adopted ILS approaches <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b1">2]</ref>    <ref type="table">Table 8</ref>: Analysis on our weak IL supervision.</p><p>row-4, we generate class-agnostic object proposals using 'all objects' text query with multi-modal ViTs (MViTs) <ref type="bibr" target="#b7">[8]</ref> and select max-size proposal for ILS. In row-5, our proposed ILS approach uses target specific 'every {category}' text query with MViT and selects top-1 proposal for each ILS category. Our method (row-5) shows better performance compared to other alternatives. Additionally, we present all ablations on LVIS dataset in Appendix C.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper develops a novel framework to leverage the representation and generalization capability of pre-trained multi-modal models towards improved open-vocabulary detection (OVD). Specifically, we note that the existing OVD methods use weak supervision modes that are more image-centric, rather than object-centric for the end detection task. We proposed a novel knowledge distillation approach together with object-level pseudo-labeling to promote region-wise alignment between visual and language representations. Our weight transfer module provide an integration mechanism to combine the benefits of knowledge distillation and object-level pseudo-labeling. We demonstrate encouraging results on four popular OVD benchmarks, demonstrating sound generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>In this section, we provide additional information regarding, </p><formula xml:id="formula_11">?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>We provide additional implementation details for our approach and datasets used in this work. We use standard Faster R-CNN <ref type="bibr" target="#b34">[35]</ref> with ResNet-50 C4 backbone and Mask R-CNN <ref type="bibr" target="#b39">[40]</ref> with ResNet-50 FPN backbone for COCO and LVIS experiments respectively. We use L2 normalization on the region and text embeddings before computing the RKD loss and final classification scores. We note that this normalization is helpful to stabilize the training. For ILS, we sample images from detection and classification datasets with a ratio of 1:4. Specifically, we use a batch size of 16 and 64 for detection and classification datasets, respectively. We will release our codes and pretrained models publicly to ensure reproducibility of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets for weak Image-level Supervision (ILS):</head><p>We use COCO captions and ImageNet-21k <ref type="bibr" target="#b3">[4]</ref> datasets for our proposed Image Level supervision (ILS) on COCO and LVIS datasets respectively. COCO captions dataset uses images from COCO detection dataset and provides five captions for each image. The words in a caption are compared heuristically, with every category name in the list of categories in COCO (base + novel). Using this method, we generate a list of positive categories for each image which is used as labels for ILS. We use ImageNet-21k <ref type="bibr" target="#b47">[48]</ref> for LVIS experiments which is a large scale classification dataset containing approximately 14M images and 21K classes. We use categories from ImageNet-21k which overlaps with LVIS categories, resulting in a subset containing 997 categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-dataset evaluation:</head><p>We provide cross-dataset evaluation of our LVIS trained model in <ref type="table">Table 6</ref>. Following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>, we use validation sets of OpenImages V5 containing ?41K images and Objects365 V2 containing ?80K images for evaluation. We report AP 50 for cross-data evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Zero-shot Region Classification</head><p>We compare the zero-shot classification performance of open-vocabulary detector with pretrained CLIP [3] model on COCO validation dataset. <ref type="table">Table 9</ref> shows the results where the top-1 classification accuracy is evaluated using the ground-truth object bounding boxes from COCO. The CLIP pretrained model shows better results for novel classes as compared to supervised-base model, indicating the strong generalization of the CLIP (row-1 vs 2). However the base class accuracy is higher for the supervised-base model as it is trained using COCO base classes. Further, using our region-based knowledge distillation (RKD) and novel weight transfer function improves the base and novel class performance, indicating the object-centric alignment in latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Ablation Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Ablation Experiments on LVIS</head><p>Effect of individual components: <ref type="table" target="#tab_2">Table 10</ref> shows the contribution of individual components in our proposed approach on LVIS dataset. The baseline Mask-RCNN model (row-1) is trained on LVIS frequent and common classes using only the box-level supervision along with the zero-shot CLIP <ref type="bibr" target="#b2">[3]</ref> classifier. The results indicate the effectiveness of our region-based distillation (RKD)  <ref type="table">Table 9</ref>: Classification results on novel and base classes with boxes cropped from COCO validation dataset using ground truth annotations. The pretrained CLIP shows competitive novel class accuracy. Our proposed RKD and weight transfer approach further improve the performance.</p><p>which explicitly aligns the image-centric CLIP embeddings to the object-centric region embeddings. Our image-level supervision (ILS) which uses class-specific pseudo-labels from the pretrained multimodal ViT <ref type="bibr" target="#b7">[8]</ref>, effectively enlarges the detector's vocabulary indicated by an increase of 4.8 AP over the base model for rare categories. Further, our proposed weight transfer scheme combines the strengths of the two methods and achieves better results on the common and frequent categories, while performing on par for the rare classes compared to naively combining the two approaches (row-4 vs 5).  <ref type="table" target="#tab_2">Table 10</ref>: Effect of individual components in our method on LVIS dataset. Using RKD provides improvements over the baseline in all metrics (row-1 vs 2). Using ILS mainly helps in improving rare class performance (row-1 vs 3). Simply combining two methods shows improvements over the baseline but struggles to retain the individual performances especially for common and frequent categories (row-4). Our weight transfer approach provides complimentary gains from RKD and ILS, achieving good results as compared to simply adding both components (row-4 vs 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Effect of Region-based Knowledge Distillation (RKD): <ref type="table" target="#tab_2">Table 11</ref> shows the effect of different loss functions (L 1 and L irm in Eq. 1 and Eq. 2 respectively) used in our region-based knowledge distillation (RKD) on LVIS dataset. It shows the effectiveness of using proposals from multi-modal ViT (MViT) <ref type="bibr" target="#b7">[8]</ref> as compared to RPN for region-level alignment (row-2 vs 3). Using high-quality</p><p>MViT proposals provides significant gains compared to using RPN proposals. Further, using our inter-embedding relationship matching (IRM) loss along with L 1 loss provides an overall good trade-off between rare, common and frequent class AP.   <ref type="table" target="#tab_2">Table 12</ref>: Analysis on our weak ILS on LVIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Initialization for RKD Training</head><p>We note that it is important to properly initialize the RKD training to gain its full advantages.   <ref type="table" target="#tab_2">Table 14</ref> shows the ablation on using a MLP skip connection across W P in <ref type="figure">Fig. 1</ref>. We add this skip connection to form a direct path for region classification using CLIP in ILS. This allows the weight transfer function to specifically focus on the residual signal in the ILS pathway. It improves the convergence and helps to attain better results in most cases on LVIS/COCO datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Additional Ablation Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Pseudo Labeling using Multi-modal ViTs</head><p>In this section, we describe the process of generating class-agnostic and class-specific proposals using multi-modal ViTs (MViTs) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b49">50]</ref>. We name this process as pseudo labeling Q pseudo . The MViT model is trained using aligned image text pairs and is capable of locating novel and base class objects using relevant human-intuitive text queries. For example, targeted text queries such as 'every person' and 'every elephant' can be used to locate all persons and all elephants in an image respectively <ref type="figure" target="#fig_4">(Fig. 6b</ref>). Maaz et al. <ref type="bibr" target="#b7">[8]</ref> show that the MViTs encode the object-centric concepts using aligned image-caption pairs and are excellent class-agnostic object detectors. The authors designed text queries such as 'all objects' and 'all entities' and demonstrated state-of-the-art class-agnostic object detection results on multiple datasets across different domains. We use these MViTs to generate class-agnostic and class-specific object proposals for region-based knowledge distillation (RKD) and weak image-level supervision (ILS), respectively.</p><p>Class-agnostic proposals for RKD: We generate class-agnostic object proposals from the MViT <ref type="bibr" target="#b7">[8]</ref> using 'all objects' text query. The generated proposals are ranked using predicted objectness scores xvii and the top 5 proposals per image are selected for RKD as shown in <ref type="figure" target="#fig_4">Fig. 6a</ref>. Next, the CLIP <ref type="bibr" target="#b2">[3]</ref> imageencoder and our OVD detector is used to generate embeddings corresponding to these proposals which are then used for calculating the RKD loss in Eq. 3. To save the computation load and increase the training efficiency, we compute the class-agnostic proposals and the corresponding CLIP region embeddings offline and load them during training. Further for LVIS experiments, we use images from a subset of ImageNet-21K (consisting of 997 overlapping LVIS categories) for RKD as well.</p><p>Class-specific proposals for ILS: We generate class-specific proposals from the MViT <ref type="bibr" target="#b7">[8]</ref> using 'every &lt;category name&gt;' text query. Given the N category names present in an image, we use N queries of format 'every &lt;category name&gt;' to generate class-specific proposals followed by selecting top 1 proposal for each category. This provides us N high-quality box proposals per image corresponding to N categories present in the image. These proposals are used to effectively enhance the detector's vocabulary using ILS during training. Further, to maintain the training efficiency of our experiments, we compute these class-specific proposals offline and load them during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Limitations</head><p>Our proposed OVD method encourages object centric visual-language (VL) alignment using a novel weight transfer method which combines benefits from RKD and ILS. Irrespective of the state-ofthe-art results on novel/rare classes, there is still a significant gap between base and novel class performances (e.g. 56.7 and 40.5 AP for COCO base and novel categories in <ref type="table">Table 3</ref>, 29.1 and 21.1 Mask AP for LVIS frequent and rare categories in <ref type="table" target="#tab_6">Table 4</ref>). Further, the open-vocabulary capabilities of our model largely depends or are limited to the vocabulary of the pretrained CLIP <ref type="bibr" target="#b2">[3]</ref> model, which is used as a teacher in our RKD pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Potential Negative Social Impacts</head><p>The results of cross-dataset transfer evaluations show that the vocabulary of our detector is highly flexible and can be expanded to any number of categories, based on the downstream tasks and datasets. This poses a risk on how our OVD detector with a large vocabulary can be used in inappropriate ways in the community such as for large scale illegal video surveillance. Furthermore, OVD capabilities can be modulated for targeted detections instead of generic detections by tuning the classifier weights using specialized prompts. This could add biases in the detector and can lead to unfair predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Ethical Considerations</head><p>The OVD response to recognize object categories strongly depends on the image-text pretraining datasets used for the training of VL model (CLIP in our case). Thus, the source of these datasets can xviii pose ethical issues. For example, datasets extracted from internet can contain racial and unethical bias and can modulate the ethical behaviour of the detector as well. Thus, before applying our OVD detector in a practical scenario, such biases of the pretraining/training datasets should be removed to have fairness and ethically correct results of the detector. Moreover, the detector vocabulary is flexible and it can be tuned to show racial biasness while detecting humans. For example, weights of the zero-shot classifier generated with specialized biased prompts could lead to biased and unethically targeted human detections (e.g., black vs white) which must be taken into consideration.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H License Details</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>ivFigure 2 :</head><label>2</label><figDesc>Top-row: Similarity matrices computed on the CLIP (SI ) and detector (SR) region embeddings for COCO novel classes. A subset of 100 randomly selected samples per category form a batch represented by a column are grouped together. Our region-based distillation enforces the similarity patterns in the RKD model to be closer to the teacher model, CLIP, indicated by the bright colors along diagonals. Bottom-row: t-SNE plots of CLIP and detector region embeddings on novel COCO categories. The CLIP aligned RKD and weight transfer detector embeddings shows improved separability among novel class features as compared to the supervised detector region embeddings (figure best viewed in-zoom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results on (a) COCO and (b) LVIS images. For COCO, base and novel categories are shown in purple and green colors respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results of cross-dataset transfer of our LVIS OVD model on (a) Objects365 and (b) OpenImages. Without any finetuning, our method provides high-quality detections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Implementation details (Appendix A) ? Qualitative Results (Appendix 5) ? Zero-shot Region Classification (Appendix B) ? Additional Ablation Experiments (Appendix C) ? Pseudo-labeling using Multi-modal ViTs (Appendix D) ? Limitations (Appendix E) ? Potential Negative Social Impacts (Appendix F) ? Ethical Considerations (Appendix G) ? Datasets License Details (Appendix H)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>(a) Class-agnostic Proposals: The figure shows the top 5 class-agnostic proposals obtained from the MViT [8] using 'all objects' text query. As illustrated, these high-quality tightly bound object proposals provide rich local-semantics for RKD in our proposed pipeline. (b) Class-specific Proposals: The figure shows the class-specific proposals obtained from the MViT using 'every &lt;category name&gt;' text queries. The left image in each pair shows all proposals while the corresponding right image shows the selected top 1 proposal per category for ILS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Summary of the datasets used in our experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Effect of individual</cell></row><row><cell>components in our method. Our</cell></row><row><cell>weight transfer method provides</cell></row><row><cell>complimentary gains from RKD</cell></row><row><cell>and ILS, achieving superior re-</cell></row><row><cell>sults as compared to naively</cell></row><row><cell>adding both components.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 (</head><label>4</label><figDesc>left) compares our results with ViLD [6] on LVIS benchmark. With 3x training schedule (? 36 epochs) we perform reasonably well compared to ViLD 32x schedule (?</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>OVD results on LVIS. (Left): Comparison with prior work ViLD, using their unified model (ViLD-text + ViLD-Image), show improvement across novel and base categories. (Right): We show the comparison with Detic, by building on their strong LVIS baseline using CenterNetV2 detector [2]Method Epochs APr APc AP f AP ViLD [6] 384 16.1 20.0 28.3 22.5 Ours 36 16.0 20.2 26.3 21.8</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Performance on LVIS benchmark using a strict OVD setting. Specifically, we expand the vocabulary to five times its size in<ref type="bibr" target="#b1">[2]</ref>, by applying ILS from randomly sampled 5K categories from ImageNet-21k, in addition to the LVIS base classes.Table 5compares our strict OVD setting results with ViLD where our performance slightly degrades showing sensitivity to ILS. However, we expect a gain with longer training as inTable 4. In addition to above two settings, we train our LVIS model under stricter OVD conditions in a non weakly-supervised setting by only using LVIS base categories for ILS. We achieve an overall 21.71 AP which is close to the model trained using ILS from 997 categories (22.75 AP).</figDesc><table><row><cell>Method</cell><cell cols="3">COCO OpenImages Objects365</cell></row><row><cell>ViLD-text</cell><cell>43.4</cell><cell>-</cell><cell>11.1</cell></row><row><cell cols="2">Detic-base ? 55.3</cell><cell>37.4</cell><cell>19.2</cell></row><row><cell>ViLD</cell><cell>55.6</cell><cell>-</cell><cell>18.2</cell></row><row><cell>Detic ?</cell><cell>56.3</cell><cell>42.2</cell><cell>21.7</cell></row><row><cell>Ours</cell><cell>56.6</cell><cell>42.9</cell><cell>22.3</cell></row></table><note>Strict Open-vocabulary Setting: Inspired from Detic, we define our work under the weakly-supervised open- vocabulary setting as it uses image-level labels for expand- ing the detector's vocabulary. However in this setting, the complete target vocabulary set is unknown, i.e., only a selected number of novel and base categories are used for ILS from ImageNet-21K in LVIS. To evaluate our model in an extensive open-vocabulary setting, we modify our ILS by considering a larger vocabulary.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Analysis on our region-based KD.</figDesc><table><row><cell>Method</cell><cell cols="2">APnovel APbase AP</cell></row><row><cell>1: Supervised (Base)</cell><cell>1.7</cell><cell>53.2 39.6</cell></row><row><cell>2: Max-Score loss on RPN</cell><cell>15.9</cell><cell>48.2 39.7</cell></row><row><cell>3: Max-Size loss on RPN</cell><cell>25.9</cell><cell>51.1 44.5</cell></row><row><cell>4: Max-Size of MViT</cell><cell>28.9</cell><cell>50.7 45.0</cell></row><row><cell>5: Pseudo-box on MViT</cell><cell>30.4</cell><cell>52.6 46.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Analysis on our RKD method on LVIS. Score loss on RPN 12.8 18.6 24.7 20.0 3: Max-Size loss on RPN 14.9 21.3 26.1 22.1 4: Pseudo-box on MViT 17.0 21.2 26.1 22.4</figDesc><table><row><cell>for rare</cell></row></table><note>Effect of Weak Image-level Supervision (ILS): Table 12 compares the different heuristics based approaches opted for image-level supervision (ILS) versus our method that utilizes class-specific proposals from the pretrained MViT on LVIS dataset. Selecting top-1 proposal from MViT using target specific specific queries such as 'every {category}' provides optimal performance</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13</head><label>13</label><figDesc>shows that training RKD from scratch (row-2) results in lower base class AP. However, initializing the RKD training from the Supervised base model recovers this loss and provides improvements over the base model. This indicates that region-based alignment is sensitive to the distribution of the features and requires mature features for effectively distilling knowledge from pretrained CLIP model. This observation is same as in<ref type="bibr" target="#b48">[49]</ref> where the contrastive clustering is enabled only on the mature features after a few training epochs for open-world object detection.</figDesc><table><row><cell>Method</cell><cell cols="3">AP novel AP base AP</cell></row><row><cell>1: Supervised (Base)</cell><cell>1.7</cell><cell>53.2</cell><cell>39.6</cell></row><row><cell>2: RKD from scratch</cell><cell>21.3</cell><cell>50.9</cell><cell>43.1</cell></row><row><cell>3: Base + RKD</cell><cell>21.2</cell><cell>54.7</cell><cell>45.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 13 :</head><label>13</label><figDesc>Effect of initialization for RKD training on COCO dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 14 :</head><label>14</label><figDesc>The ablation on using MLP skip connection inFig. 1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 15 :</head><label>15</label><figDesc>Summary of licenses for datasets used in our experiments.Here we provide license details of the datasets used in our work, summarized inTable 15. COCO is available for non-commercial use under the Creative CommonsAttribution 4.0 (CC BY 4.0) license. LVIS is based on the COCO dataset, and it is licensed under both CC BY 4.0 and the COCO license. ImageNet-21k is a publically available dataset available for research and non-commercial use. It is licensed under Creative Commons (CC), and its type is "CC BY-NC". We use a pretrained MViT model for proposal generation, which is trained on LMDet (Large scale Modulated Detection dataset). It uses Flicker30k, Visual Genome, and GQA datasets. The license type of Flicker30k is CC BY-NC. Visual Genome and GQA both have the same license type CC BY 4.0. For crossdatasets evaluation, Objects365 and OpenImages are used, which are licensed under Creative Commons Attribution 4.0. Annotations of OpenImages are licensed by Google LLC under Creative Commons Attribution 2.0.</figDesc><table><row><cell>xix</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>The computations were performed in the Berzelius resource provided by the Knut and Alice Wallenberg Foundation at the National Supercomputer Centre.</p><p>x</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We provide our code for reproducing main experiments of our work in the supplemental material. (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] We use publically available datasets for our experiments. We have not explicitly discussed such consent in the main paper, but we have checked and made sure that all used datasets are allowed to be used for research. (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? <ref type="bibr">[Yes]</ref> We discuss this in the supplemental material. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Open-vocabulary object detection using captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">Dela</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">Hao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Detecting twentythousand classes using image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02605</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Open-vocabulary object detection via vision and language knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian</forename><forename type="middle">Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09106</idno>
		<title level="m">Region-based language-image pretraining</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multi-modal transformers excel at class-agnostic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Maaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanoona</forename><surname>Rasheed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11430</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Zero shot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengkai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved visual-semantic alignment for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Don&apos;t even look once: Synthesizing features for zero-shot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengkai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A multi-space approach to zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dikshant</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Anantharaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nehal</forename><surname>Mamgain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transductive learning for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Zero-shot object detection with textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salil</forename><surname>Kanhere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enabling deep residual networks for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cyclic guidance for weakly supervised joint detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liujuan</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Jianbin Jiao, and Qixiang Ye. C-mil: Continuation multiple instance learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards precise end-to-end weakly supervised object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Boosting weakly supervised object detection with progressive knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Weakly-and semi-supervised object detection with expectation-maximization algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weishen</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08740</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Boosting weakly supervised object detection via learning bounding box adjusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuelin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Wssod: A new pipeline for weakly-and semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11293</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Edge Boxes: Locating Object Proposals from Edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cap2det: Learning to amplify weak caption supervision for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keren</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Berent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with segmentation collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pcl: Proposal cluster learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Comprehensive attention self-distillation for weakly-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dlwl: Improving detection for lowshot classes with weakly labelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Towards open vocabulary object detection without human-provided bounding boxes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09452</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning to prompt for open-vocabulary object detection with vision-language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaojing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.14940</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Promptdet: Expand your detector vocabulary with uncurated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjian</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.16513</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">Girshick</forename><surname>Mask R-Cnn</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to segment every thing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The open images dataset v4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1956" to="1981" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07461</idno>
		<title level="m">Probabilistic two-stage detection</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards open world object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Kj Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
