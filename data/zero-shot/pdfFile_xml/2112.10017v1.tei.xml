<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
							<email>liub@uic.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchang</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing research on continual learning of a sequence of tasks focused on dealing with catastrophic forgetting, where the tasks are assumed to be dissimilar and have little shared knowledge. Some work has also been done to transfer previously learned knowledge to the new task when the tasks are similar and have shared knowledge. To the best of our knowledge, no technique has been proposed to learn a sequence of mixed similar and dissimilar tasks that can deal with forgetting and also transfer knowledge forward and backward. This paper proposes such a technique to learn both types of tasks in the same network. For dissimilar tasks, the algorithm focuses on dealing with forgetting, and for similar tasks, the algorithm focuses on selectively transferring the knowledge learned from some similar previous tasks to improve the new task learning. Additionally, the algorithm automatically detects whether a new task is similar to any previous tasks. Empirical evaluation using sequences of mixed tasks demonstrates the effectiveness of the proposed model. 2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In many applications, the system needs to incrementally or continually learn a sequence of tasks. This learning paradigm is called continual learning (CL) or lifelong learning <ref type="bibr" target="#b6">(Chen and Liu, 2018)</ref>. Ideally, in learning each new task t, the learner should (1) not forget what it has learned from previous tasks in order to achieve knowledge accumulation, (2) transfer the knowledge learned in the past forward to help learn the new task t if t is similar to some previous tasks and has shared knowledge with those previous tasks, (3) transfer knowledge backward to improve the models of similar previous tasks, and (4) learn a mixed sequence of dissimilar and similar tasks and achieve (1), (2) and (3) at the same time. To our knowledge, no existing CL technique has all these four capabilities. This paper makes an attempt to achieve all these objectives in the task continual learning (TCL) setting (also known as task incremental learning), where each task is a separate or distinct classification problem. This work generalizes the existing works on TCL. Note, there is also the class continual learning (CCL) setting (or class incremental learning), which learns a sequence of classes to build one overall multi-class classifier for all the classes seen so far.</p><p>As AI agents such as chatbots, intelligent personal assistants and physical robots are increasingly made to learn many skills or tasks, this work is becoming more and more important. In practice, when an agent faces a new task t, naturally some previous tasks are similar and some are dissimilar to t. The agent should learn the new task without forgetting knowledge learned from previous tasks while also improving its learning by transferring the shared knowledge from those similar tasks.</p><p>Most existing CL models focus on (1), i.e., dealing with catastrophic forgetting or simply forgetting <ref type="bibr" target="#b6">(Chen and Liu, 2018;</ref><ref type="bibr" target="#b31">Parisi et al., 2019;</ref><ref type="bibr" target="#b24">Li and Hoiem, 2016;</ref><ref type="bibr" target="#b41">Seff et al., 2017;</ref><ref type="bibr" target="#b43">Shin et al., 2017;</ref><ref type="bibr" target="#b34">Rebuffi et al., 2017;</ref><ref type="bibr" target="#b55">Yoon et al., 2018;</ref><ref type="bibr" target="#b10">He and Jaeger, 2018;</ref><ref type="bibr" target="#b55">Yoon et al., 2018;</ref><ref type="bibr" target="#b27">Masse et al., 2018;</ref><ref type="bibr" target="#b40">Schwarz et al., 2018;</ref><ref type="bibr" target="#b30">Nguyen et al., 2018;</ref><ref type="bibr" target="#b11">Hu et al., 2019)</ref>. In learning a new task, the learner has to update the network parameters but this update can cause the models for previous tasks to degrade or to be forgotten <ref type="bibr" target="#b28">(McCloskey and Cohen, 1989)</ref>. Existing works dealing with forgetting typically try to make the update of the network toward less harmful directions to protect the previously learned knowledge. Forgetting mainly affects the learning of a sequence of dissimilar tasks. When a sequence of similar tasks is learned, there is little forgetting as we will see in Section 4. There are also existing methods for knowledge transfer <ref type="bibr" target="#b39">(Ruvolo and Eaton, 2013;</ref><ref type="bibr" target="#b5">Chen and Liu, 2014;</ref><ref type="bibr" target="#b7">Chen et al., 2015;</ref><ref type="bibr" target="#b51">Wang et al., 2019;</ref><ref type="bibr" target="#b15">Ke et al., 2020)</ref> when all tasks are similar. This paper proposes a novel TCL model called CAT (Continual learning with forgetting Avoidance and knowledge Transfer) that can effectively learn a mixed sequence of similar and dissimilar tasks and achieve all the aforementioned objectives. CAT uses a knowledge base (KB) to keep the knowledge learned from all tasks so far and is shared by all tasks. Before learning each new task t, the learner first automatically identifies the previous tasks T sim that are similar to t. The rest of the tasks are dissimilar to t and denoted by T dis . In learning t, the learner uses the task masks (TM) learned from the previous tasks to protect the knowledge learned for those dissimilar tasks in T dis so that their important parameters are not affected (no forgetting). A set of masks (one for each layer) is also learned for t in the process to be used in the future to protect its knowledge. For the set of similar tasks T sim , the learner learns a knowledge transfer attention (KTA) to selectively transfer useful knowledge from the tasks in T sim to the new task to improve the new task learning (forward knowledge transfer). During training the new task t, CAT also allows the past knowledge to be updated so that some tasks in T sim may be improved as well (backward knowledge transfer). Our empirical evaluation shows that CAT outperforms the state of the art existing baseline models that can be applied to the proposed problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Work on continual learning (CL) started in 1990s (see a review in <ref type="bibr" target="#b24">(Li and Hoiem, 2016)</ref>). Most existing papers focus on dealing with catastrophic forgetting in neural networks. <ref type="bibr" target="#b24">Li and Hoiem (2016)</ref> proposed the technique LwF to deal with forgetting using knowledge distillation.  proposed EWC to quantify the importance of network weights to previous tasks, and update weights that are not important for previous tasks. Similar methods are also used in <ref type="bibr" target="#b1">(Aljundi et al., 2018;</ref><ref type="bibr" target="#b40">Schwarz et al., 2018;</ref><ref type="bibr" target="#b57">Zenke et al., 2017)</ref>. Some methods memorize a small set of training examples in each task and use them in learning a new task to deal with forgetting (called replay) <ref type="bibr" target="#b34">(Rebuffi et al., 2017;</ref><ref type="bibr" target="#b25">Lopez-Paz and Ranzato, 2017;</ref><ref type="bibr" target="#b4">Chaudhry et al., 2019;</ref><ref type="bibr" target="#b16">Kemker and Kanan, 2018)</ref>. Some works built generators for previous tasks so that learning is done using a mixed set of real data of the new task and generated data of previous tasks (called pseudo-replay) <ref type="bibr" target="#b43">(Shin et al., 2017;</ref><ref type="bibr" target="#b13">Kamra et al., 2017;</ref><ref type="bibr" target="#b37">Rostami et al., 2019;</ref><ref type="bibr" target="#b11">Hu et al., 2019)</ref>.</p><p>Additionally, <ref type="bibr" target="#b36">Rosenfeld and Tsotsos (2018)</ref> proposed to optimize loss on the new task with representations learned from old tasks. <ref type="bibr" target="#b11">Hu et al. (2019)</ref> proposed PGMA, which deals with forgetting by adapting a shared model through parameter generation. <ref type="bibr" target="#b56">Zeng et al. (2019)</ref> tried to learn the new task by revising the weights in the orthogonal direction of the old task data. <ref type="bibr" target="#b8">Dhar et al. (2019)</ref> combined three loss functions to encourage the model resulted from the new task to be similar to the previous model. Other related works include Phantom Sampling <ref type="bibr" target="#b49">(Venkatesan et al., 2017)</ref>, Conceptor-Aided Backprop <ref type="bibr" target="#b10">(He and Jaeger, 2018)</ref>, Gating Networks <ref type="bibr" target="#b27">(Masse et al., 2018;</ref><ref type="bibr" target="#b42">Serr? et al., 2018)</ref>, PackNet <ref type="bibr" target="#b26">(Mallya and Lazebnik, 2018)</ref>, Diffusion-based Neuromodulation <ref type="bibr" target="#b48">(Velez and Clune, 2017)</ref>, IMM , Expandable Networks <ref type="bibr" target="#b55">(Yoon et al., 2018;</ref><ref type="bibr" target="#b23">Li et al., 2019)</ref>, RPSNet <ref type="bibr" target="#b32">(Rajasegaran et al., 2019)</ref>, reinforcement learning <ref type="bibr" target="#b14">(Kaplanis et al., 2019;</ref><ref type="bibr" target="#b35">Rolnick et al., 2019)</ref>, and meta-learning <ref type="bibr" target="#b12">(Javed and White, 2019)</ref>. See the surveys in <ref type="bibr" target="#b6">(Chen and Liu, 2018;</ref><ref type="bibr" target="#b31">Parisi et al., 2019)</ref>.</p><p>Most CL works focus on class continual learning (CCL). This paper focuses on task continual learning (TCL) <ref type="bibr" target="#b9">(Fernando et al., 2017;</ref><ref type="bibr" target="#b42">Serr? et al., 2018)</ref>. For example, GEM <ref type="bibr" target="#b25">(Lopez-Paz and Ranzato, 2017)</ref> takes task id in addition to the training data of the specific task as input. A-GEM <ref type="bibr" target="#b4">(Chaudhry et al., 2019)</ref> improves GEM's efficiency. HAT <ref type="bibr" target="#b42">(Serr? et al., 2018)</ref> takes the same inputs and use hard attention to learn binary masks to protect old models in the TCL setting. But unlike CAT, HAT does not have mechanisms for knowledge transfer. Note that the controller in iTAML <ref type="bibr">(Rajasegaran et al., 2020)</ref> behaves similarly to HAT's annealing strategy in training. But the controller is for balancing between plasticity and stability, while HAT trains binary masks. UCL <ref type="bibr" target="#b0">(Ahn et al., 2019</ref>) is a latest work on TCL. However, none of these methods can deal with forgetting and perform knowledge transfer to improve the new task learning at the same time. Progressive Network  tries to perform forward knowledge transfer. It first builds one model for each task and then connects them. However, it cannot do backward transfer and its whole network size grows quadratically in the number of tasks, which makes it difficult to handle a large number of tasks. It also does not deal with a mixed sequence of tasks.</p><p>Earlier work on lifelong learning has focused on forward knowledge transfer to help learn the new task better <ref type="bibr" target="#b45">(Thrun, 1998;</ref><ref type="bibr" target="#b39">Ruvolo and Eaton, 2013;</ref><ref type="bibr" target="#b44">Silver et al., 2013;</ref><ref type="bibr" target="#b7">Chen et al., 2015;</ref><ref type="bibr" target="#b29">Mitchell et al., 2015)</ref>. <ref type="bibr" target="#b15">Ke et al. (2020)</ref> and <ref type="bibr" target="#b51">Wang et al. (2019)</ref> also did backward transfer. However, these lifelong learning works mainly use the traditional learning methods such as regression <ref type="bibr" target="#b39">(Ruvolo and Eaton, 2013)</ref>, naive Bayes <ref type="bibr" target="#b7">(Chen et al., 2015;</ref><ref type="bibr" target="#b51">Wang et al., 2019)</ref>, and <ref type="bibr">KNN (Thrun, 1998)</ref> to build a model for each task independently and hence there is no forgetting problem. Although there are also works using neural networks <ref type="bibr" target="#b15">(Ke et al., 2020;</ref><ref type="bibr" target="#b52">Wang et al., 2018;</ref><ref type="bibr" target="#b45">Thrun, 1998;</ref><ref type="bibr" target="#b44">Silver et al., 2013)</ref>, all of them (including those based on traditional learning methods) work on tasks that are very similar and thus there is almost no forgetting. Earlier research on lifelong reinforcement learning worked on cross-domain lifelong reinforcement learning, where each domain has a sequence of similar tasks <ref type="bibr" target="#b53">(Wilson et al., 2007;</ref><ref type="bibr" target="#b2">Bou Ammar et al., 2015)</ref>. But like others, they don't deal with forgetting. To the best of our knowledge, no existing work has been done to learn a sequence of mixed similar and dissimilar tasks that deal with forgetting and improve learning at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed CAT Model</head><p>The CAT model is depicted in <ref type="figure" target="#fig_0">Figure 1(A)</ref>. At the bottom, it is the input data and the task ID t. Above it, we have the knowledge base (KB), which can be any differentiable layers (CAT has been experimented with a 2-layer fully connected network and a CNN architecture). The task ID t is used to generate three different task ID embeddings. Two of them are element-wise multiplied (?) with the outputs of the corresponding layers in the KB while the last one is the input to the knowledge transfer module (top right). The output of the KB can go to two branches (blue and red parallelogram). It will always go to the blue branch, which learns a classification model f mask for task t and at the same time, learns a binary mask (called the task mask (TM)) for each layer in the KB indicating the units that are important/useful for the task in the layer. The mask is used to protect the learned knowledge for task t in learning future tasks. If task t is similar to some previous tasks, the output of the KB also goes to the right branch for selective knowledge transfer, which is achieved through knowledge transfer attention (KTA). On top of KTA, another classifier f KT A is built for task t, which leverages the transferred knowledge to learn t. This classifier should be used in testing rather than f mask for the task. f mask is only used in testing when the task t has no similar previous task. Note that in task continual learning (TCL), task ID is needed because in testing each test instance will be tested by the classifier/model of its corresponding task (van de Ven and Tolias, 2019). 3 Note that <ref type="figure" target="#fig_0">Figure 1</ref>(A) does not show the network for detecting whether task t is similar to some previous tasks, which we will discuss in Section 3.3. For now, we can assume that a set of similar previous tasks to t is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preventing Forgetting for Dissimilar Tasks: Task Masks</head><p>Let the set of tasks learned so far be T (before learning a new task t). Let T sim ? T be a set of similar tasks to t and T dis = T ? T sim be the set of dissimilar tasks to t. We will discuss how to compute T sim in Section 3.3. In learning t, we overcome forgetting for the dissimilar tasks in T dis by identifying the units in the KB that are used by the tasks and blocking the gradient flow through the units (i.e., setting their gradients to 0). To achieve this goal, a task mask (a binary mask) m (t) l is trained for each task t at each layer l of the KB during training for task t's classifier/model, indicating which units are important for the task in the layer. Here we borrow the hard attention idea in <ref type="bibr" target="#b42">(Serr? et al., 2018)</ref> and leverage the task ID embedding to train the mask. In the matrix before training, those cells with 0's are the units to be protected (masked), and with 1's are units to be shared and whose tasks are similar to the current task. Those cells without a number are free units (not used). In the matrix after training, those cells with 1's show those units that are important for the current task, which are used as a mask for the future. The rest of the cells or units are not important for the task. Those 0 cells without a color are not used by any task.</p><p>From Task ID Embedding to Task Mask. For a task ID t, its embedding e (t) l consists of differentiable deterministic parameters that can be learned together with other parts of the network. The subscript l indicates the layer number. A separate task ID embedding is trained for each layer of the KB. To generate the task mask m</p><formula xml:id="formula_0">(t) l from e (t)</formula><p>l , Sigmoid is used as a pseudo-gate function and a positive scaling hyper-parameter s is applied to help training. m (t) l is computed as follows:</p><formula xml:id="formula_1">m (t) l = ?(se (t) l )<label>(1)</label></formula><p>Given the output of each layer in the KB, h</p><formula xml:id="formula_2">(t) l , we element-wise multiply h (t) l ? = m (t) l . The masked output of the last layer h (t)</formula><p>mask is fed to the f mask classification head to train the task classifier. After learning task t, the final m (t) l is saved and added to the set {m (t) l }. Block Gradients Flow through Used Units for Dissimilar Tasks. For each previous dissimilar task i dis in T dis of the current task t, its mask m</p><formula xml:id="formula_3">(i dis ) l</formula><p>indicates which units in the KB are used by task i dis . In learning task t, m</p><formula xml:id="formula_4">(i dis ) l is used to set the gradient g (t)</formula><p>l on all used units of the layer l to 0. Before modifying the gradient, we first accumulate all used units by all previous dissimilar tasks making use of their masks. Since m (i dis ) l is binary, we can use element-wise maximum to achieve the accumulation: m</p><formula xml:id="formula_5">(tac) l = ElementM ax({m (i dis ) l }) (2) m (tac) l</formula><p>is applied to the gradient:</p><formula xml:id="formula_6">g (t) l ? = (1 ? m (tac) l )<label>(3)</label></formula><p>Those gradients corresponding to the 1 entries in m (tac) l are set to 0 while the others remain unchanged. Note that we expand (copy) the vector m (tac) l to match the dimensions of g Training Tricks. Though the idea is intuitive, e (t) l is not easy to train. To make the learning of e (t) l easier, an annealing strategy is applied <ref type="bibr" target="#b42">(Serr? et al., 2018)</ref>. That is, s is annealed during training, inducing a gradient flow and set s = s max during testing. Eq. 1 approximates a unit step function as the mask, with m </p><formula xml:id="formula_7">s = 1 s max + (s max ? 1 s max ) b ? 1 B ? 1 ,<label>(4)</label></formula><p>where b is the batch index and B is the total number of batches in an epoch. The task masks are trained together with f mask by minimizing (using cross entropy):</p><formula xml:id="formula_8">1 N t Nt i=1 L(f mask (x (t) i ; ? mask ), y (t) i )<label>(5)</label></formula><p>Illustration. In <ref type="figure" target="#fig_0">Figure 1(B)</ref>. Task 0 is the first task. After learning it, we obtain its useful units marked in orange with a 1 in each unit, which serves as a mask for future tasks. In learning task 1, we found that task 1 is not similar to task 0. Those useful units for task 0 is masked (with 0 in those orange units or cells in the matrix on the left). The process also learns the useful units for task 2 marked in green with 1's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Knowledge Transfer from Similar Tasks: Knowledge Transfer Attention</head><p>If there are similar tasks (T sim = ?) to the new task t, we want to learn t better, by encouraging knowledge transfer from T sim . Due to the fact that every task may have its domain specific knowledge that is not applicable to other tasks, knowledge transfer has to be selective. We propose a knowledge transfer attention (KTA) for the purpose. The idea is to give different importance to different previous tasks by an attention mechanism so that their knowledge can be selectively transferred to the new task. Those transferred units are also made updateable to achieve backward knowledge transfer automatically so that the previous task models may be improved in training the new task t.</p><p>Knowledge Base Output for Previous Tasks. Recall we know which units are for which task j by reading {m (j) l }. For each previous similar task i sim in T sim of the current task t, we can compute its masked KB output h Knowledge Transfer Attention. We learn another task ID embedding (separate from the task ID embedding used in the KB) e </p><formula xml:id="formula_9">a (isim) = sof tmax( (e (t) KT A ? q )({h (isim) mask }? k ) ? d k )<label>(6)</label></formula><p>where d k is the number of previous similar tasks (|T sim |). ? q and ? k are parameters matrices for projections in self-attention <ref type="bibr" target="#b47">(Vaswani et al. (2017)</ref>). The a (isim) indicates the importance of each previous task in {h <ref type="bibr">(isim)</ref> mask }. We then compute the weighted sum among {h (isim) mask } to get the output of similar tasks: h</p><formula xml:id="formula_10">(t) KT A = i a (isim) ({h (isim) mask }? v )<label>(7)</label></formula><p>where ? v is the parameter matrix for projection in self-attention. h</p><p>KT A is then fed to the f KT A classification head to learn the classifier.</p><p>Loss function for training f mask and f KT A . Both terms below use cross entropy.</p><formula xml:id="formula_12">1 N t Nt j=1 L(f mask (x (t) j ; ? mask ), y (t) j ) + 1 N t Nt j=1 L(f KT A (x (t) j ; ? KT A ), y (t) j ),<label>(8)</label></formula><p>where N t is the number of training examples in task t, ? mask is the set of parameters of f mask , and ? KT A is the set of parameters of f KT A .</p><p>Illustration. In <ref type="figure" target="#fig_0">Figure 1(B)</ref>, when task 2 arrives, the system found that task 1 is similar to task 2, but task 0 is not. Then, task 0's important units are blocked, i.e., its mask entries are set to 0 (orange units in the left matrix). Task 1's important units are left open with its mask entries set to 1 (green units in the left matrix). After learning with knowledge transfer, task 2 and task 1 have some shared units that are important to both of them, i.e., those units marked in both red and green.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Task Similarity Detection</head><p>In the above discussion, we assume that we know the set of similar previous tasks T sim of the new task t. We now present how to find similar tasks for a given task t. We use a binary vector TSV (task similarity vector) to indicate whether each previous task k ? T is similar to the new task t.</p><p>We define task similarity by determining whether there is a positive knowledge transfer from a previous task k to the current task t. A transfer model f k?t is used to transfer knowledge from task k to task t. A single task model f ? , called the reference model, is used to learn t independently. If the following statistical risk holds, which indicates a positive knowledge transfer, we say that task k is similar to task t; otherwise task k is dissimilar to task t.</p><formula xml:id="formula_13">E (X (t) ,Y (t ) [L(f k?t (X (t) ; ? k?t ), Y t )] &gt; E (X (t) ,Y (t) ) [L(f ? (X (t) ; ? ? ), Y (t) )]<label>(9)</label></formula><p>We use a validation set to check whether Eq. 9 holds. Specifically, if the transfer model f k?t classifies the validation data of task t better than the reference model f ? , then we say k contains shareable prior knowledge that can help t learn a better model than without the knowledge, f ? , indicating positive knowledge transfer. We set T SV (t) [k] = 1 indicating that k is similar to t; otherwise T SV <ref type="bibr">(t)</ref> [k] = 0 indicating that k is dissimilar to t.</p><p>Transfer model. The transfer model f k?t trains a small readout function (1 layer fully-connected network on top of the KB) for the previous task k given the representation/features for x (t) produced by the task k model in the KB. In training the transfer network, the KB is frozen or not updated.</p><p>Recall the model of task k is specified by its mask {m (k) l }, which is saved after training task k. The transfer network is trained by minimizing the following empirical risk using the cross entropy loss:</p><formula xml:id="formula_14">1 N t Nt i=1 L(f k?t (x (t) i ; ? k?t ), y (t) i )<label>(10)</label></formula><p>Reference model. The reference model f ? is a separate network for building a model for task t alone from scratch with random initialization. It uses the same architecture as f k?t without applying any task masks. However, the size of the network is smaller, 50% of f k?t in our experiments. The network is trained by the following using the cross entropy loss:</p><formula xml:id="formula_15">1 N t Nt i=1 L(f ? (x (t) i ; ? ? ), y (t) i )<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We now evaluate CAT following the standard continual learning evaluation method in <ref type="bibr" target="#b20">(Lange et al., 2019)</ref>. We present CAT a sequence of tasks for it to learn. Once a task is learned, its training data is discarded. After all tasks are learned, we test all task models using their respective test data. In training each task, we use its validation set to decide when to stop training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Datasets</head><p>Since CAT not only aims to deal with forgetting for dissimilar tasks but also to perform knowledge transfer for similar tasks, we consider two types of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similar Task Datasets:</head><p>We adopt two similar-task datasets from federated learning. Federated learning is an emerging machine learning paradigm with its emphasis on data privacy. The idea is to train through model aggregation rather than the conventional data aggregation and keep local data staying on the local device. These datasets naturally consist of similar tasks. We randomly choose 10 tasks  from two publicly available federated learning datasets <ref type="bibr" target="#b3">(Caldas et al., 2018)</ref> to form (1) F-EMNIST -each of the 10 tasks contains one writer's written digits/characters (62 classes in total), and (2) F-CelebA -each of the 10 tasks contains images of a celebrity labeled by whether he/she is smiling or not. Note that the training and testing sets are already provided in <ref type="bibr" target="#b3">(Caldas et al., 2018)</ref>. We further split about 10% of the original training data as the validate data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dissimilar Task Datasets:</head><p>We use 2 benchmark image classification datasets: <ref type="bibr">EMNIST (LeCun et al., 1998)</ref> and CIFAR100 <ref type="bibr" target="#b18">(Krizhevsky et al., 2009)</ref>. We consider two splitting scenarios. For each dataset, we prepare two sets of tasks with a different number of classes in each task. For EMNIST, which has 47 classes in total, the first set has 10 tasks and each task has 5 classes (the last task has 2 classes). The second set has 20 tasks and each task has 2 classes (the last task has 9 classes). For CIFAR100, which has 100 classes, the first set has 10 tasks and each task has 10 classes. The second set has 20 tasks and each task has 5 classes. For EMNIST, for efficiency reasons we randomly sampled a subset of the original dataset and also make the validation set the same as the training set following <ref type="bibr" target="#b42">(Serr? et al., 2018)</ref>. For CIFAR100, we split 10% of the training set and keep it for validation purposes. Statistics of the datasets are given in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Mixed Sequence Datasets for Experimentation: To experiment with learning a mixed sequence of tasks, we constructed four mixed sequence datasets from the above similar and dissimilar tasks datasets: M(EMNIST-10, F-EMNIST), M(EMNIST-20, F-EMNIST), M(CIFAR100-10, F-CelebA), and M(CIFAR100-20, F-CelebA). The first mixed sequence dataset M(EMNIST-10, F-EMNIST) consists of 5 random sequences of tasks from EMNIST (10 dissimilar tasks) and all 10 similar F-EMNIST task. M(EMNIST-20, F-EMNIST) consists of 5 random sequences of tasks from EMNIST (20 dissimilar tasks) and all 10 similar F-EMNIST tasks. Note that EMNIST and F-EMNIST datasets are paired together because they contain images of the same size. The other two mixed sequence datasets involving CIFAR100 and F-CelebA are prepared similarly. CIFAR100 and F-CelebA are paired together also because their images are of the same size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compared Baselines</head><p>We consider ten task continual learning (TCL) baselines. EWC ) -a popular regularization-based class continual learning (CCL) method. We adopt its TCL variant implemented by <ref type="bibr" target="#b42">Serr? et al. (2018)</ref>. We also adopt its TCL variant implemented by <ref type="bibr" target="#b42">Serr? et al. (2018)</ref>. RPSNet <ref type="bibr" target="#b32">(Rajasegaran et al., 2019)</ref> -an improvement over PathNet which encourages knowledge sharing and reuse. As RPSNet is a CCL method, we adapted it to a TCL method. Specifically, we only train on the corresponding head of the specific task ID during training and only consider the corresponding head's prediction during testing. NCL (naive continual learning) -greedily training a sequence of tasks incrementally without dealing with forgetting. It uses the same network as the next baseline. ONE (one task learning) -building a model for each task independently using a separate neural network, which clearly has no knowledge transfer and no forgetting involved.  <ref type="table">Table 2</ref>: Accuracy results of different models on the four mixed sequence datasets (average over 5 random sequences) using a 2-layer fully connected network. The number in bold in each row is the best result of the row.  <ref type="table">Table 3</ref>: Accuracy results of different models on the mixed sequences of F-CelebA and CIFAR100 (average over 5 random sequences) using an AlexNet-like architecture. The number in bold in each row is the best result of the row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Network and Training Details</head><p>Unless otherwise stated, for NCL, ONE, and the KB in CAT, we employ a 2-layer fully connected network for all our datasets. For F-CelebA and CIFAR100, we further experiment CAT using a CNN based AlexNet-like architecture <ref type="bibr" target="#b19">(Krizhevsky et al., 2012)</ref>. We also employ the embedding with 2000 dimensions as the final and hidden layer of the KB. The task ID embeddings have 2000 dimensions. A fully connected layer with softmax output is used as the f mask and f KT A classification heads, together with the categorical cross-entropy loss. We use 140 for s max in s, dropout of 0.5 between fully connected layers. For the knowledge transfer attention (KTA), we apply layer normalization and dropout following the setting in <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref>. We also employ multiple attention heads. We set the number of attention heads to 5 (grid search from the candidates set {1, 5, 10, 15, 20} on the validation set). We train all models using SGD with the learning rate of 0.05. We stop training when there is no improvement in the validation accuracy for 5 consecutive epochs (i.e., early stopping with patience = 5). The batch size is set to 64. For all the other baselines, we use the code provided by their authors and adopt their original parameters. <ref type="table">Table 2</ref> gives the accuracy results of all systems on the four mixed sequence datasets. For NCL, ONE, HAT, and CAT, we use a 2-layer fully connected network. The first part of the table contains the average results for the 5 random sequences of the 20 tasks M(EMNIST-10, F-EMNIST) dataset. The first row shows the average accuracy of all 20 tasks for each system. The second row shows the average accuracy results of only the 10 dissimilar tasks of EMNIST-10 among all 20 tasks of the compared systems. The third row shows the average accuracy results of only the 10 similar tasks of F-EMNIST among all 20 tasks of the compared systems. Other parts of the table contain the corresponding results of the 20/30 tasks mixture sequence datasets. The detailed results of different sequences are given in Supplementary Materials. <ref type="table">Table 3</ref> gives the results of the two mixed sequence datasets involving CIFAR100 and F-CelebA using the CNN based AlexNet-like architecture.    Overall Performance. The overall accuracy results of all tasks for four mixed sequence datasets in <ref type="table">Tables 2 and 3</ref> show that CAT outperforms all baselines. In <ref type="table">Table 2</ref>, although EWC, HAT, and UCL perform better than NCL due to their mechanisms for avoiding forgetting, they are all significantly worse than CAT as they don't have methods to encourage knowledge transfer. HYP, HYP-R, PathNet and RPSNet fail to outperform NCL, indicating their failure in learning mixed sequence of tasks. Even though PRO, by construction, never forget, it still exhibits difficulties in dealing with mixed sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and Analysis</head><p>Performance on Dissimilar Tasks. We can see from <ref type="table">Tables 2 and 3</ref> that CAT performs better than most of the baselines on dissimilar tasks. It is not surprising that ONE is the best overall except for EMNIST-20 as it builds separate classifiers independently. CAT performs similarly to HAT, which has little forgetting. This indicates that CAT deals with forgetting reasonably well. In <ref type="table">Table 2</ref>, we see that PathNet and RPSNet work well on EMNIST-20 datasets but extremely poorly on F-EMNIST, indicating they are not able to deal with a mixed sequence well as CAT does.</p><p>Performance on Similar Tasks. For the results of similar tasks, In both Tables 2 and 3, we can see that CAT markedly outperforms HAT and all other baselines as CAT can leverage the shared knowledge among similar tasks while the other CL approaches, including HAT, only tries to avoid interference with the important units of previous tasks to deal with forgetting.</p><p>Effectiveness of Knowledge Transfer. Here, we look at only the similar tasks from F-EMNIST and F-CelebA in the four mixed sequence experiments. For forward transfer, we use the test accuracy of each similar task in F-EMNIST or F-CelebA when it was first learned. For backward transfer, we use the final result after all tasks are learned. The average results are given in <ref type="table" target="#tab_2">Table 4</ref>. <ref type="table" target="#tab_2">Table  4</ref> clearly shows that forward knowledge transfer is highly effective. For backward transfer, CAT slightly improves the forward performance for F-MNIST and markedly improves the performance for F-CelebA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Experiments</head><p>We now show the results of ablation experiments in <ref type="table" target="#tab_3">Table 5</ref>. "-KTA" means without deploying KTA (knowledge transfer attention). "-TSV" means without detecting task similarity, i.e., no TSV (task similarity vector), which has two cases, treating all previous tasks as dissimilar <ref type="bibr">(all-dis)</ref> or treating all previous tasks as similar (all-sim). From <ref type="table" target="#tab_3">Table 5</ref>, we can see that the full CAT system always gives the best overall accuracy and every component contributes to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper first described four desired capabilities of a continual learning system: no forgetting, forward knowledge transfer, backward knowledge transfer, and learning a mixed sequence of similar and dissimilar tasks. To our knowledge, no existing continual learning method has all these capabilities. This paper proposed a novel architecture CAT to achieve all these goals. Experimental results showed that CAT outperforms strong baselines. Our future work will focus on improving the accuracy of learning similar tasks (e.g., by considering task similarity computation as a regression problem rather than a binary classification problem), and improving the efficiency (e.g., by removing explicit similarity computation). We also plan to explore ways to use fewer labeled data in training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(A) CAT architecture, and (B) illustration of task masking and knowledge transfer. Some notes about (B) are:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, 1} when s ? ?. A training epoch starts with all units being equally active, which are progressively polarized within the epoch. Specifically, s is annealed as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>stack all the outputs of the last layer in the KB h (isim) mask into a collection {h (isim) mask }. We then compute the attention weight for each h (isim) mask in the collection in the right branch of Figure 1(A) by:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>HAT<ref type="bibr" target="#b42">(Serr? et al., 2018</ref>) -one of the best TCL methods with almost no forgetting. UCL<ref type="bibr" target="#b0">(Ahn et al., 2019</ref>) -a latest TCL method using a Bayesian online learning framework. HYP (von Oswald et al., 2020) -a latest TCL method addressing forgetting by generating the weights of the target model based on the task ID. HYP-R (von Oswald et al., 2020) -a latest replay-based method. PRO (Progressive Network)) -another popular continual learning method which focuses on forward transfer. PathNet<ref type="bibr" target="#b9">(Fernando et al., 2017</ref>) -a classical continual learning method selectively masks out irrelevant model parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>-20,F-CelebA): CIFAR100-20 0.6669 0.7870 0.7419 0.7339 M(CIFAR100-20,F-CelebA): F-CelebA 0.7455 0.6545 0.6000 0.7727</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>-10, F-EMNIST): F-EMNIST 0.5238 0.6134 0.6104 M(EMNIST-20, F-EMNIST): F-EMNIST 0.5310 0.6187 0.6081 M(CIFAR100-10, F-CelebA): F-CelebA 0.6073 0.6909 0.6873 M(CIFAR100-20, F-CelebA): F-CelebA 0.6273 0.7164 0.6782</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets, which contain the total number of classes and the total numbers of training, validation and testing instances for each dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Effect of forward and backward knowledge transfer in CAT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Ablation experiment results.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For example, one task is to classify fish and non-fish, and another task is to classify different kinds of fishes. Without knowing the user's specific task (task ID) at hand, the system will not know which is the right answer.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by two grants from National Science Foundation: IIS-1910424 and IIS-1838770, a DARPA Contract HR001120C0023, and a research gift from Northrop Grumman.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>An intelligent agent typically needs to learn many skills or tasks. Some of the tasks are similar to each other and some are distinct. It is desirable that the agent can learn these tasks without interference with each other and also improve its learning when there is shared/transferable knowledge learned in the past. As more and more chatbots, intelligent personal assistants and physical robots appear in our lives, we believe that this research will become more and more important. We could not see anyone will be put at disadvantage by this research. The consequence of failure of the system is that the system makes some incorrect classifications. Our task and method do not leverage biases in the data.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Uncertainty-based Continual Learning with Adaptive Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Memory Aware Synapses: Learning What (not) to Forget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autonomous Cross-Domain Knowledge Transfer in Lifelong Policy Gradient Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Haitham Bou Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">Marcio</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Luna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ruvolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">LEAF: A Benchmark for Federated Settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Caldas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Konecn?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient Lifelong Learning with A-GEM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Topic Modeling using Topics from Many Domains, Lifelong Learning and Big Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lifelong machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="207" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lifelong learning for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianzu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="750" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning without Memorizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat Vikram</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chuan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">PathNet: Evolution Channels Gradient Descent in Super Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yori</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Overcoming Catastrophic Forgetting for Continual Learning via Model Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengwei</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Meta-Learning Representations for Continual Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS-2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep Generative Dual Memory Network for Continual Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Kamra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umang</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Policy Consolidation for Continual Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Kaplanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Clopath</surname></persName>
		</author>
		<idno>ICML-2019</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Continual Learning with Knowledge Transfer for Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML-PKDD</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FearNet: Brain-Inspired Model for Incremental Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Overcoming catastrophic forgetting in neural networks</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>In NIPS</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Continual learning: A comparative study on how to defy forgetting in classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting by incremental moment matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning Without Forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient Episodic Memory for Continual Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">Y</forename><surname>Masse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U.S.A</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Never ending learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Estevam R Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Variational Continual Learning. In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Continual lifelong learning with neural networks: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><forename type="middle">Ignacio</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">L</forename><surname>Part</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Random Path Selection for Incremental Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jathushan</forename><surname>Rajasegaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khan Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">2020. iTAML: An Incremental Task-Agnostic Meta-learning Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jathushan</forename><surname>Rajasegaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="13588" to="13597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">iCaRL: Incremental Classifier and Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Experience Replay for Continual Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS-2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Incremental learning through deep adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John K Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Complementary Learning for Overcoming Catastrophic Forgetting Using Experience Replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rostami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><forename type="middle">K</forename><surname>Pilly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progressive Neural Networks. CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ELLA: An Efficient Lifelong Learning Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ruvolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Eaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="507" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Progress &amp; Compress: A scalable framework for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelena</forename><surname>Luketina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Beatson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Continual Learning in Generative Adversarial Nets. CoRR abs/1705</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">8395</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Overcoming Catastrophic Forgetting with Hard Attention to the Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didac</forename><surname>Suris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Continual Learning with Deep Generative Replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Jung Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Lifelong Machine Learning Systems: Beyond Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">L</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianghao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lifelong Machine Learning, Papers from the 2013 AAAI Spring Symposium</title>
		<meeting><address><addrLine>Palo Alto, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-03-25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Lifelong learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebastian Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to Learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="181" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Three scenarios for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Van De Ven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tolias</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1904.07734" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Diffusion-based neuromodulation can eliminate catastrophic forgetting in simple neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roby</forename><surname>Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A Strategy for an Uncompromising Incremental Learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ragav</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxin</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Continual learning with hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Johannes Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Henning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Sacramento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benjamin F Grewe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Forward and Backward Knowledge Transfer for Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianzu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Lifelong Learning Memory Networks for Aspect Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Lva Nd Sahisnu Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geli</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE BigData</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-task reinforcement learning: a hierarchical Bayesian approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1015" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Memory replay GANs: Learning to generate new categories without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenshen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Raducanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Lifelong Learning with Dynamically Expandable Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongtae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Continuous Learning of Context-dependent Processing in Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanxiong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedemann</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
