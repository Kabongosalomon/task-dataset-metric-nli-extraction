<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Progressive End-to-End Object Detection in Crowded Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anlin</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Jiao</orgName>
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
							<email>xjqi@eee.hku.hk</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>sunjian@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Progressive End-to-End Object Detection in Crowded Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a new query-based detection framework for crowd detection. Previous query-based detectors suffer from two drawbacks: first, multiple predictions will be inferred for a single object, typically in crowded scenes; second, the performance saturates as the depth of the decoding stage increases. Benefiting from the nature of the one-to-one label assignment rule, we propose a progressive predicting method to address the above issues. Specifically, we first select accepted queries prone to generate true positive predictions, then refine the rest noisy queries according to the previously accepted predictions. Experiments show that our method can significantly boost the performance of query-based detectors in crowded scenes. Equipped with our approach, Sparse RCNN achieves 92.0% AP, 41.4% MR ?2 and 83.2% JI on the challenging CrowdHuman [37] dataset, outperforming the box-based method MIP [8] that specifies in handling crowded scenarios. Moreover, the proposed method, robust to crowdedness, can still obtain consistent improvements on moderately and slightly crowded datasets like CityPersons [49] and COCO <ref type="bibr" target="#b26">[27]</ref>. Code will be made publicly available at https://github.com/megvii-model/Iter-E2EDET.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Crowded object detection is a practical yet challenging research field in computer vision. Many research efforts have been made and achieved impressive progress <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref> in the last few decades. However, most of them <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b13">14]</ref> require handcraft components, e.g. anchor settings and post-processing, resulted in sub-optimal performance in handling scenes.</p><p>Recently, Carion et al. <ref type="bibr" target="#b2">[3]</ref> proposed a fully end-toend object detection framework DETR, which introduces learnable queries to represent objects and achieves competitive performance without any post-processing. It can be categorized as a query-based approach to differentiate * Equal contribution.   <ref type="figure">Figure 2</ref>: 2a. The bottom histogram describes the prediction distribution of Sparse RCNN <ref type="bibr" target="#b38">[39]</ref> under different confidence scores, while the top one reflects the absolute improvements achieved by our approach compared with Sparse RCNN <ref type="bibr" target="#b38">[39]</ref>. 2b. The FP-TP curve when computing Average Precision (AP).</p><p>from the box-based <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b50">50]</ref> and point-based <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b46">46]</ref> methods. Following DETR <ref type="bibr" target="#b2">[3]</ref>, Sparse RCNN <ref type="bibr" target="#b38">[39]</ref> ensures object queries interact with local feature of Region of Interest (RoI), while deformable DETR <ref type="bibr" target="#b54">[54]</ref> proposes attention modules that only attend to a small set of key sampling points. They further improve the detection accuracy and mitigate several issues occurred in DETR: slow conver-gence and high computational overhead. The above success inspires us to study query-based object detection methods in crowded scenes, aiming at designing a more sophisticated end-to-end detection framework. Although these query-based approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b54">54]</ref> can obtain significant results on the slightly crowded datasets like COCO <ref type="bibr" target="#b26">[27]</ref>, our initial studies show they suffer from several unresolved challenges in crowded scenes: <ref type="bibr" target="#b0">(1)</ref>. the query-based detector tends to infer multiple predictions for a single object, with false positives introduced. <ref type="figure" target="#fig_1">Figure. 1a</ref> shows a common failure case; <ref type="bibr" target="#b1">(2)</ref>. The performance of a query-based detector becomes saturated or even worse as the depth of decoding stage increases, which is depicted in the Appendix. Our motivations. Further investigations on the querybased method, Sparse RCNN <ref type="bibr" target="#b38">[39]</ref>, yield the following intriguing findings in crowd scenes. As described in <ref type="bibr">Figure</ref>. 2a, a large percentage of target objects can be accurately predicted by those predictions with high confidence scores (e.g. higher than a threshold of 0.7), while containing few false positives. These predictions are more likely to be true positives that can be taken as accepted predictions. While the rest, where a considerable number of true positives and false positives exist, can be regarded as noisy predictions. Naturally, if an object is detected by one accepted prediction, there is no need for noisy predictions to detect it again. Hence, Why not strengthen the discrimination of those noisy predictions given the context of the accepted predictions? To this end, the noisy queries can 'perceive' whether their targets have been detected or not. If so, their confidence scores will be reduced and then filtered out. Our contributions Motivated by this, we propose a progressive prediction method equipped with a prediction selector, relation information extractor, query updater, and label assignment to improve the performance of query-based object detectors in handling crowded scenes.</p><p>First, we develop a prediction selector to select queries associated with high confidence scores as accepted queries, leaving the rest as noisy queries. Then, to let the noisy queries 'perceive' whether their targets have been detected or not, we design a relation extractor for relation modeling between noisy queries and their accepted neighbors. Further, a query updater is developed by performing a new local self-attention attending to spatially-related neighbors only. Finally, a new one-to-one label assignment rule is introduced to assign samples among the accepted and refined noisy queries step by step. With the proposed method, the above problems can be well addressed: <ref type="bibr" target="#b0">(1)</ref>. Each object can be detected only once, which greatly decreases the number of false positives while increasing the number of true positives, as described in <ref type="figure">Figure.</ref> 1b; <ref type="bibr" target="#b1">(2)</ref>. As depicted in <ref type="figure">Figure</ref>. 2b, the performance is consistently improved compared with its counterparts <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b54">54]</ref> that have the same depth of decoding stage.</p><p>Our method is generic and can be incorporated into multiple architectures <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b54">54]</ref>, and delivers significant performance improvements of query-based detectors. Equipped with our approach, Sparse RCNN <ref type="bibr" target="#b38">[39]</ref> with ResNet-50 <ref type="bibr" target="#b15">[16]</ref> backbone obtains 92.0% AP, 41.4% MR ?2 and 83.2% JI on the challenging dataset CrowdHuman <ref type="bibr" target="#b36">[37]</ref>, outperforming the box-based method MIP <ref type="bibr" target="#b7">[8]</ref>. Besides, deformable DETR <ref type="bibr" target="#b54">[54]</ref>, equipped with our approach, also achieves 92.1% AP and 84.0% JI. Moreover, our approach works reasonably well for less crowded scenes, e.g. the Sparse RCNN with our approach can still obtain 1.0% MR ?2 and 1.1% AP gains on moderately and slightly crowded datasets Citypersons <ref type="bibr" target="#b49">[49]</ref> and COCO <ref type="bibr" target="#b26">[27]</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>End-to-end object detection. RelationNet <ref type="bibr" target="#b19">[20]</ref> is one of the pioneering works trying to predict results directly, achieving promising performance compared to their counterparts on several famous benchmarks. DETR <ref type="bibr" target="#b2">[3]</ref> introduces learnable queries to represent objects and perform single prediction for each instance directly. Subsequently, deformable-DETR <ref type="bibr" target="#b54">[54]</ref> limits the attention field of each query to a local area around the reference points to accelerate the convergence and improve detection performance. Meanwhile, Sparse R-CNN <ref type="bibr" target="#b38">[39]</ref> utilizes a fixed set of learnable queries to formulate objects instead of a number of proxy representation, e.g. anchors. Analogous to deformable DETR, RoIAlign <ref type="bibr" target="#b14">[15]</ref> is applied to limit the attention field in a local region. Adaptive Clustering Transformer <ref type="bibr" target="#b53">[53]</ref> proposes to improve the attention distribution in DETR's encoder by LSH approximate clustering for convergence acceleration. UP-DETR <ref type="bibr" target="#b8">[9]</ref> designs a new selfsupervised method to improve the convergence speed of DETR, while TSP <ref type="bibr" target="#b39">[40]</ref> analyzes the main factors contributing to slow convergence in DETR. SMCA <ref type="bibr" target="#b12">[13]</ref> explores a better information interaction mechanism to further accelerate convergence and improve the performance of DETR. Object detection in crowded scenes. Research community has poured much interest in exploring occlusion problems on pedestrian detection. Specific methods have been proposed to mitigate this problem, including detecting by parts <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b51">51]</ref> and improving hand-crafted rules in training target design. Recently, CNN-based methods have dominated the crowded object detection and achieved considerable gains. Several works propose new loss functions to address problems of crowded detection <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b51">51]</ref>. Besides, the effectiveness of NMS is based on the assumption that multiple instances rarely occur at the same location in an image, which is not true in crowded scenes. But designing duplicate removal for crowded scenes is non-trivial. Soft-NMS <ref type="bibr" target="#b0">[1]</ref> and Softer-NMS <ref type="bibr" target="#b16">[17]</ref> replace hard removal of nearby proposals with score decay. Several works propose to use a neural network to simulate the function of NMS for duplicates removal <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34]</ref>. Others explore NMS-aware training, including NMS with adaptive threshold <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref>, feature embedding <ref type="bibr" target="#b35">[36]</ref> and multiple prediction with set suppression <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>, to tackle problem of object detection in crowded scenes.</p><p>Recently, PEDR <ref type="bibr" target="#b23">[24]</ref> proposes several techniques to improve the performance of query-based detectors in coping with crowded detection, which is orthogonal to ours. Their techniques are also applicable to our work.</p><p>Relation modeling for object detection. As discussed in <ref type="bibr" target="#b19">[20]</ref>, early works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b31">32]</ref> use object relations as a post-processing step. The detected objects are re-scored by considering object relationships. For example, co-occurrence, which indicates how likely two object classes can exist in the same image, is used by DPM <ref type="bibr" target="#b10">[11]</ref> to refine object scores. The subsequent approaches <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b6">7]</ref> try more complex relation models, by taking additional positions and size into account. These methods achieve moderate success in the pre-deep learning era but do not prove the effectiveness in CNNs. Several recent works perform spatial reasoning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18]</ref> to model object relations. Among them, GossipNet <ref type="bibr" target="#b17">[18]</ref> and RelationNet <ref type="bibr" target="#b19">[20]</ref> are the representative methods. Both share the same spirit of modeling relations among boxes. However, the network of Gos-sipNet <ref type="bibr" target="#b17">[18]</ref> is complex (depth&gt;80) and its computation cost is demanding. Although it allows end-to-end learning in principle, no experimental evidence approves. Relation-Net <ref type="bibr" target="#b19">[20]</ref> utilized the self-attention for feature interaction and obtained a promising improvement in general object detection. Nevertheless, it doesn't show a promising performance in dealing with crowd scenes <ref type="bibr" target="#b36">[37]</ref>.</p><p>Recent works related to ours are PS-RCNN <ref type="bibr" target="#b13">[14]</ref> and Iter-Det <ref type="bibr" target="#b34">[35]</ref>. They proposed to detect objects according to the previous predicted ones. They need to mask the feature <ref type="bibr" target="#b13">[14]</ref> or produce a history map <ref type="bibr" target="#b34">[35]</ref> to memorize the previous detections, introducing noise while limiting performance improvement <ref type="bibr" target="#b13">[14]</ref> or incur heavy computation <ref type="bibr" target="#b34">[35]</ref>. Even so, both of them need a post-processing method to remove duplicates in every iteration.</p><p>Recent query-based object detectors <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b12">13]</ref> utilized learnable queries to represent objects, and take advantage of the self/cross-attention to model the relations among queries, detecting objects in an end-to-end manner. Our work inherits the methodology and boosts their performance in heavily, moderately, and slightly crowded scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we first revisit the query-based object detector, e.g. Sparse RCNN <ref type="bibr" target="#b38">[39]</ref> briefly. Next, we illustrate our approach primarily deployed on Sparse RCNN explicitly. Finally, the main differences of detector design will be discussed as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Query Based Object Detector</head><p>Our approach can be deployed on most query-based object detectors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b54">54]</ref>. To illustrate the proposed method, we choose Sparse RCNN <ref type="bibr" target="#b38">[39]</ref> as our default instantiation. <ref type="figure">Figure.</ref> 4a depicts its object detection pipeline, which can also be formulated as:</p><formula xml:id="formula_0">x t?1 ? P box (x F P N , b t?1 ), q ? t?1 ? MSA t?1 (q t?1 ), q t ? DynConv t?1 (q ? t?1 , x t?1 ), b t ? B t?1 (q t ),<label>(1)</label></formula><p>where q ? R N ?d denotes the learnable object query. N and d denote the number and dimension of query q, respectively. At stage t, an RoIAlign [15] P box extracts RoI features from FPN features x F P N , under the guidance of bounding box b t?1 predicted by the previous stage. Meanwhile, a multihead self-attention module MSA t?1 is applied to the input query q t?1 to get the transformed query q ? t?1 . Then, a dynamic convolution module DynConv t?1 takes both x t?1 and q ? t?1 as inputs and performs dynamic convolution to generates q t for the next stage. Simultaneously, q t is fed into the box prediction branch B t?1 for current bounding box prediction b t , which is the input of the next stage t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Our Method</head><p>As illustrated in <ref type="figure">Figure.</ref> 3, the proposed progressive predicting method consists of several components: prediction selector, relation information extractor, query updater, and label assignment, which will be introduced in detail next. Prediction selector. For the findings described in Sec. 1, a prediction selector is developed to select those queries prone to generating predictions with high confidence scores as accepted queries, while leaving the rest as noisy ones that need to be further refined. This procedure can be formulated in Equ. <ref type="bibr" target="#b1">(2)</ref>.</p><formula xml:id="formula_1">D h t?1 ? {b i |s i ? s ? b i ? D t?1 }, D l t?1 ? D t?1 ? D h t?1 .<label>(2)</label></formula><p>where t is the stage number. D t?1 denote the whole predictions produced by the whole queries in the previous t ? 1 stage. D h t?1 and D l t?1 indicate the accepted predictions and noisy predictions generated from the accepted and noisy queries, respectively. b i and s i denote the predicted box and its confidence score, respectively. s is the confidence score threshold. Relation information extractor. As mentioned in Sec. 1, a large percentage of target objects can be accurately predicted by the accepted queries. Therefore, if an object is detected by one accepted prediction, there is no need for  <ref type="figure">Figure 3</ref>: The diagram of the proposed progressive end-to-end object detection framework. First, the prediction selector select queries associated with high confidence scores as accepted queries, leaving the rest as noisy queries. Then Relation information extractor models the relations between noisy queries and their neighbors from accepted predictions. Next, the queries are fed into the queries updater to be further refined by performing a new local self-attention. noisy predictions to detect it again. In order to equip these noisy queries with the capability of perceiving whether their targets have been detected or not, we develop a relation information extractor to model the relation between the noisy predictions and their accepted neighbors.</p><formula xml:id="formula_2">q t?1 b t?1 bt qt DyConv t?1 x FPN P box MSA t?1 (a) Sparse R-CNN [39]. q t?1 b t?1 bt qt DyConv t?1 x FPN P box QU t?1 S R (b) Our approach</formula><p>The detailed design of the relation information extractor is illustrated in <ref type="figure">Figure.</ref>  </p><formula xml:id="formula_3">b i ) in D h t?1 , constructing the spatially-related pairs (b i , N (b i )).</formula><p>Then, the encoded pairs together with the intersection-over-union (IoU) between them are fed to a compact network to obtain the geometry relation features H(b i ). Since the number of accepted neighbors corresponding to each noisy prediction is uncertain. An aggregation function is employed to reduce H(b i ) to the same feature dimension, while maintaining the permutation-invariance property. In our approach, we use max pooling by default. Besides, the pooled geometry features, fused with the transformed query features, are further activated by a non-linear function.</p><formula xml:id="formula_4">N (b i ) ? {b j |O(b i , b j ) ? ?}, b i ? D l t?1 , b j ? D h t?1 , H(b i ) ? U(E(b i , N (b i ))), b i ? D l t?1 , R(b i ) ? T (MaxPool(H(b i )) + F(q i )).<label>(3)</label></formula><p>where N (?) represents a function that finds neighbors for a box b i based on the IoU O(?, ?) with a threshold ?. Here, we use it to find the accepted neighbors in D h t?1 for the noisy predictions in D l t?1 . E(?, ?) refers to the sine and cosine spatial positional encoding function which is the same as that <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b45">45]</ref>. Also, U(?, ?) denotes a function used to generate geometry relation features H(b i ) from the encoded inputs. The noisy query q i corresponds to noisy prediction b i in D l t?1 , transformed by the function F(?). The pooled geometry features and transformed query features F(q i ) are fused through element-wise summation, followed by a function T to produce the desired relation features R(b i ).</p><p>As depicted in <ref type="figure">Figure.</ref> 5, U(?, ?) consists of two consecutive fc layers with ReLU <ref type="bibr" target="#b30">[31]</ref> activation to increase the non-linearity. Note that F(?) and U(?) share the same architecture, but are weight-independent. Here, the gradients of q i are stopped from back-propagating to the previous stages. Query updater. To further refine the features of noisy queries, a query updater is developed, which is formulated in Equ. <ref type="bibr" target="#b3">(4)</ref>. Since the data distribution of D l t?1 and D h t?1 is different from that of D t?1 , a new set of learnable queries is first introduced to complement the relation features through element-wise summation. Then the set of complemented noisy queries is taken as the input query q t?1 to perform a new local self-attention LMSA t?1 and the subsequent dynamic convolution given in Equ. <ref type="bibr" target="#b0">(1)</ref>.</p><formula xml:id="formula_5">q t?1 ? { q i | q i = R(b i ) + e i }, b i ? D l t?1 , e i ? E q ? t?1 ? LMSA t?1 (q t?1 ).<label>(4)</label></formula><formula xml:id="formula_6">Algorithm 1 Label Assignment for D l t . Input: D l t , D h t , G; 1: D l t : results of D l t?1 in Equ.(2) from stage t; 2: D h t : results of D h t?1 in Equ.</formula><p>(2) from stage t; 3: G: target boxes. Output: The matched predictions M l D and corresponding targets M l G after assignment. <ref type="bibr">4:</ref> Compute matching costs C h t between D h t and G;</p><formula xml:id="formula_7">5: M h G , M h D = HungarianMatch(D h t , G, C h t ); 6: G l t = G ? M h G ; 7: Compute matching costs C l t between D l t and G l t ; 8: M l G , M l D = HungarianMatch(D l t , G l t , C l t ); 9: return M l G , M l D ;</formula><p>Since object detection mainly focuses on the local region in an image. We design a new local self-attention module LMSA t to update the noisy query q t?1 . It ensures each query only interacts with local neighbors instead of the whole queries over the full image. The local self-attention first finds those neighbors of each query based on the boxes' IoUs whose values are greater than 0. Then it performs the 'qkv' mechanism in the same way as MSA. To this end, we perform self-attention locally instead of globally.</p><p>Different from the neighbor finding rule in <ref type="bibr" target="#b23">[24]</ref>, we adopt the function N (?) to select neighbors from D t?1 that are spatially related to q t?1 in terms of IoU. Note that, the new local self-attention LMSA t?1 is used to replace the MSA t?1 in Equ.(1) for feature interaction. Label assignment Since accepted queries tend to generate true positive predictions, while the noisy ones involve a considerable number of true positives and false positives. Towards end-to-end object detection, we introduce a new one-to-one label assignment rule to assign samples step by step. Specifically, we first match the accepted predictions D h t?1 with the ground truth set of objects. Then remove those targets that have been matched, and mainly consider the bipartite matching between noisy predictions D l t?1 and the remaining ground truth set of objects. This matching process is described in Algorithm 1 1 , where the matching cost computation is slightly different from the original version <ref type="bibr" target="#b38">[39]</ref>. A spatial prior is adopted to compute the matching cost C, that is, the center of bounding box b t needs to fall in the corresponding target box. Except for it, the formulation of the matching cost function is identical to the original work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Difference of Detector Design</head><p>Generally, our approach can be deployed on most querybased object detectors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b54">54]</ref>. To illustrate the proposed method, we choose Sparse RCNN <ref type="bibr" target="#b38">[39]</ref> as our default instantiation. It consists t (t = 6 by default) decoding stages, each of which performs prediction according to Equ. <ref type="bibr" target="#b0">(1)</ref>. As described in <ref type="figure">Figure.</ref> 3, we keep the first t ? 1 decoding stages unchanged and only equip the last stage with the proposed method. Therefore, main differences lie in the last decoding stage, which will be described in the following. Architecture of the last stage. As depicted in <ref type="figure">Figure.</ref> 4b, the last decoding stage t first employs a prediction selector S to split queries into accepted queries and noisy queries according to the confidence scores of their associated predictions. Then they are input to the relation information extractor R to extract the relation feature between the noisy predictions and their accepted neighbors. Finally, queries are fed into the query updater QU to be further refined for recognition and localization. Box prediction Like <ref type="bibr" target="#b38">[39]</ref>, a box regression branch is used for box prediction in the first t ? 1 stages. Differently, for the box prediction in the last stage, we directly use the identity mapping results from the previous t ? 1 stage both in the training and testing phase. This is because, at the latter layers, the predicted bounding boxes are less likely to fluctuate, which is observed in <ref type="bibr" target="#b23">[24]</ref>. Meanwhile, the recognition branch remains the same as that of previous stages. Training loss We adopt the set prediction loss adopted in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b54">54]</ref> for training. For the samples to train stage t, we remove those samples from accepted predictions D h t?1 . to mitigate the class-imbalance issue, we follow the negative sample filtering mechanism <ref type="bibr" target="#b52">[52]</ref> to early reject those well-classified negative queries whose confidence scores are lower than 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate our approach on heavily, moderately, and slightly crowded datasets <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b26">27]</ref> to demonstrate the generality of the proposed method in diverse scenarios. Datasets. We adopt three datasets -CrowdHuman <ref type="bibr" target="#b36">[37]</ref>, CityPersons <ref type="bibr" target="#b49">[49]</ref> and COCO [27] -for comprehensive evaluations on heavily, moderately and slightly crowded situations, respectively. <ref type="table" target="#tab_1">Table 1</ref> lists the "instance density" of each dataset. Since our proposed approach mainly aims to improve crowded detections, we perform most of the comparisons and ablation experiments on CrowdHuman. The evaluation experiments on Citypersons <ref type="bibr" target="#b49">[49]</ref> and COCO <ref type="bibr" target="#b26">[27]</ref> are also conducted to suggest the proposed method is robust to crowdedness.</p><p>Evaluation metrics. Following <ref type="bibr" target="#b7">[8]</ref>, we mainly take three criteria: AP, MR ?2 and JI as evaluation metrics. Generally, a larger AP, larger JI and smaller MR ?2 indicates a better performance.</p><p>Implementation details. Unless otherwise specified, we take Sparse RCNN <ref type="bibr" target="#b38">[39]</ref> as our default instantiation, using Dataset # objects/img # overlaps/img CrowdHuman <ref type="bibr" target="#b36">[37]</ref> 22.64 2.40 CityPersons <ref type="bibr" target="#b49">[49]</ref> 6.47 0.32 COCO * <ref type="bibr" target="#b26">[27]</ref> 9.34 0.015 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments on CrowdHuman</head><p>CrowdHuman <ref type="bibr" target="#b36">[37]</ref> contains 15,000, 4,370 and 5,000 images for training, validation and test, respectively. For a fair comparison, we re-implement most of the involved models <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>. Results are evaluated on the validation set, using the full-body annotations in the dataset.</p><p>Main results. We compare with mainstream object detectors, including box-based: one-stage <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b50">50]</ref> , twostages <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref>, and point-based <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b46">46]</ref> as well as query-based <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>As shown in <ref type="table">Table.</ref> 2, our approach outperforms these well-established detectors, achieving significant performance improvements over the box-based, point-based, and query-based counterparts, illustrating the effectiveness of our approach in handling crowded scenes. Specifically, our method achieves 1.8% AP and 0.9% JI gains over the stateof-the-art box-based approach MIP <ref type="bibr" target="#b7">[8]</ref>, which specializes in coping with crowded scenes.</p><p>The query-based method Sparse RCNN <ref type="bibr" target="#b38">[39]</ref>, equipped with the proposed method and 500 queries, can achieve 92.0% AP, 41.4% MR ?2 and 83.2% JI on the challenging CrowdHuman dataset <ref type="bibr" target="#b36">[37]</ref>, which is 1.3%, 3.3% and 1.8% better than its counterpart -original Sparse RCNN <ref type="bibr" target="#b38">[39]</ref>. When increasing the number of queries to 750, our approach can still obtain a better performance of 92.5% AP and 83.3% JI. This is because more queries can cover more patterns of objects in the image, such as scale, size, position, and other characteristics. Additionally, equipped with our approach, deformable DETR [54] 2 can also obtain 2.2% MR ?2 improvements over the original deformable DETR <ref type="bibr" target="#b54">[54]</ref>. Moreover, It also achieves 1.4% AP and 1.6% JI gains over the box-based method MIP <ref type="bibr" target="#b7">[8]</ref>, demonstrating the effectiveness of our approach.  Ablation study of hyper-parameter s. To analyze the effect of the confidence score threshold s, we first formulate the relation between detection boxes and target boxes in an image as a bipartite graph G = (V, E). It consists of a set V = D G and nodes E. D represents a set of predicted boxes whose scores are higher than the pre-defined score threshold, while G denotes the target boxes. An edge in E is defined as overlapping when the IoU value, between a box in D and the other one from G, is higher than 0.5 by default <ref type="bibr" target="#b2">3</ref> . Hence, the matching results can be acquired after applying the Hungarian Algorithm. As shown in <ref type="figure">Figure.</ref> 2, as the confidence score increases, the number of true positives shows a clean upward trend while the number of false positives decreases rapidly. Also, <ref type="figure">Figure.</ref> 7a depicts the performance our method can achieve under different values of s, where the performance increases slightly as s increases. Thus, if not specific, we set s to 0.7 by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Ablation study of hyper-parameter ?. Here, we analyze the effect of the hyper-parameter intersection-over-union threshold ?. As discussed in Sec. 1, making sure a box candidate can 'perceive' its neighbors helps a noisy query decide to decrease its confidence score or not, which is also the prerequisite for our method to work effectively. Different settings of intersection-over-union (IoU) threshold ? may affect the performance of the whole detector. We perform experiments on the CrowdHuman dataset <ref type="bibr" target="#b36">[37]</ref> with s frozen as 0.7 while changing the value of ? linearly. From <ref type="figure">Figure.</ref> 7b, we found our approach is robust to the change of IoU threshold. This success may attribute to the good approximating feature of the newly designed components. Comparison with previous relation modeling works.</p><p>To differentiate the previous works and ours, we evaluate several representative relation modeling methods: Relation-Net <ref type="bibr" target="#b19">[20]</ref>, GossipNet <ref type="bibr" target="#b17">[18]</ref>, IterDet <ref type="bibr" target="#b34">[35]</ref>. RelationNet <ref type="bibr" target="#b19">[20]</ref> utilized self-attention modules to model the relations among different predictions. Meanwhile, GossipNet <ref type="bibr" target="#b17">[18]</ref> uses several hand-designed relation blocks to explore the relationships among the predicted boxes, while IterDet <ref type="bibr" target="#b34">[35]</ref> iteratively infers predictions based on a historical map produced from the previous iteration. We re-implement its re-scoring version for RelationNet <ref type="bibr" target="#b19">[20]</ref>. For GossipNet 4 and IterDet 5 , we use their open-source implementations for evaluation. All models use FPN <ref type="bibr" target="#b24">[25]</ref> with ResNet-50 <ref type="bibr" target="#b15">[16]</ref> as backbone, following the same training setting in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>As shown in <ref type="table">Table.</ref> 3b, our approach shows better performance when compared with previous relation modeling works. Surprisingly, both RelationNet <ref type="bibr" target="#b19">[20]</ref> and Gossip-Net <ref type="bibr" target="#b17">[18]</ref> suffer from a significant drop in AP and MR ?2 . It could attribute to the sub-optimal label assignment rule. Since both of them choose the prediction with the highest confidence score around one target as the correct box and take the rest as negatives. The predicted coordinates are not involved in computing loss, which might lead to the performance degradation in crowded scenes. Analysis on false positives. To understand the factors contributing to the performance improvement, we conduct an error analysis on our method. We adopt the recently proposed TIDE <ref type="bibr" target="#b1">[2]</ref> to compare our approach with the counterpart Sparse RCNN <ref type="bibr" target="#b38">[39]</ref>. We analyzed the composite error at Recall=0.9 for all methods. As illustrated in <ref type="figure">Figure.</ref> 6, our method performs better at removing duplication, providing more accurate localization, and reducing mistaken recognition. Since part of queries can perceive whether their targets are detected or not through the relation information extractor. Also, the local self-attention module ensures queries only interact with their neighbors rather than the whole. To this end, the duplicates could be eliminated efficiently. Besides, with identity mapping plugged in the last regression branch for box prediction, the number of training samples in the previous decoding stage increases, making the optimization much easier. Additionally, benefiting from the new learnable embeddings for data distribution approximation, the representation ability of object queries are further enhanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on Citypersons</head><p>CityPersons <ref type="bibr" target="#b49">[49]</ref> is one of the widely used benchmarks for pedestrian detection. It contains 5, 000 images (2, 975 for training, 500 for validation, and 1, 525 for testing, respectively). Each image has a size of 1024 ? 2048. To improve the overall performance, we proposed to pre-train all models on the CrowdHuman dataset and fine-tune them on CityPersons (reasonable) training subset, then tested on the (reasonable) validation subset. For those box-based methods, we train and evaluate them with the image resolution enlarged by 1.3? compared to the original one for better accuracy. The query-based approaches are trained and evaluated at the original image size with 500 queries. The other settings remain the same as those of Sparse RCNN <ref type="bibr" target="#b38">[39]</ref> and deformable DETR <ref type="bibr" target="#b54">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on COCO.</head><p>According to <ref type="table" target="#tab_1">Table 1</ref>, the crowdness of COCO <ref type="bibr" target="#b26">[27]</ref> is very low, which is beyond our design purpose. Nevertheless, we still conduct an experiment on this dataset to verify: 1) whether our method generalizes well to multi-class detection; 2) whether our approach can still handle slightly crowded scenarios, especially with isolated instances.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of errors (#errors)</head><p>Sparse R-CNN ours <ref type="figure">Figure 6</ref>: Error analysis on Sparse RCNN <ref type="bibr" target="#b38">[39]</ref> and our approach, with ResNet-50 <ref type="bibr" target="#b15">[16]</ref> as backbone. The bar plots show different error types that contribute to the false positives.   Following the common practice of Sparse RCNN <ref type="bibr" target="#b38">[39]</ref> with 300 queries, we use a subset of 5000 images in the original validation set (named minival) for validation while using the remaining images in the training and validation set for training. Except for the proposed modules and label assignment rule in the last stage, other settings remain the same as the original methods <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b38">39]</ref>. <ref type="table" target="#tab_8">Table. 4</ref> shows the performance comparisons with deformable DETR <ref type="bibr" target="#b54">[54]</ref> and Sparse RCNN <ref type="bibr" target="#b38">[39]</ref>. Moderate improvements are obtained, e.g. 0.9% AP higher than the deformable DETR [54] and 1.1% AP higher than the Sparse RCNN <ref type="bibr" target="#b38">[39]</ref>. The experimental results reflect the effectiveness of our progressive predicting approach in slightly crowded scenarios, proving the proposed method can also solve the performance saturation problem of query-based detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a progressive prediction method to boost the performance of query-based object detectors in handling crowded scenes. Equipped with our approach, two representatives query-based methods, Sparse RCNN <ref type="bibr" target="#b38">[39]</ref> and deformable DETR <ref type="bibr" target="#b54">[54]</ref> achieve consistent improvements over the heavily, moderately, as well as slightly crowded datasets <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b26">27]</ref>  our approach is robust to crowdedness. Since Sparse RCNN <ref type="bibr" target="#b38">[39]</ref> and deformable DETR <ref type="bibr" target="#b54">[54]</ref> require large computing resources, making it difficult for our method to be deployed on devices with limited computing capacity. How to develop a computation-efficacy end-to-end detector is still under exploration. Besides, we found the decision boundary for the noisy queries is unclear. We believe that the performance can be further improved if a better feature engineering method or loss function is adopted. However, it is beyond the purpose of this work.</p><p>A. Implementation of Deformable DETR with progressive predicting method.</p><p>We also deploy our progressive predicting approach on the deformable DETR <ref type="bibr" target="#b54">[54]</ref> to demonstrate the generality of our method. Similar to Sparse RCNN <ref type="bibr" target="#b38">[39]</ref>, the decoder in deformable DETR consists of 6 stages, which is depicted in <ref type="figure">Figure.</ref> 8a. As described in the paper, we integrate our designed components into the last decoding stage. <ref type="figure">Figure.</ref> 8b also depicts its detail architecture. For the hyper-parameters setting, i.e confidence score threshold s, are identical to those adopted in Sparse RCNN <ref type="bibr" target="#b38">[39]</ref>.</p><p>We choose the deformable DETR with iterative bounding box refinement. Following deformable DETR <ref type="bibr" target="#b54">[54]</ref>, we use ResNet-50 <ref type="bibr" target="#b15">[16]</ref> as backbone. The whole detector is trained with an Adam optimizer <ref type="bibr" target="#b21">[22]</ref> and a weight decay of 0.0001. The total training duration is 50 epochs on 8 GPUs with 1 image per GPU. The initial learning rate is 0.0002 and dropped by a factor of 0.1 after 40 epochs. The parameters initialization in the newly added components and losses weights are identical to the original work <ref type="bibr" target="#b54">[54]</ref>. The default number of queries and stages is 500 and 6, respectively. The hyper-parameters s and ? are also 0.7 and 0.4, respectively. The gradients are detached at proposal boxes from the second stage to stabilize training. We stop gradient back-propagation from the last stage to the previous ones. Besides, those negative samples that overlap with ignore region with an intersection-over-area(IoA) greater than 0.7 are not involved in training.</p><p>B. Performance change of a query-based decoder when handling crowded scenes.</p><p>The performance of a query-based detector would not be improved but will degrade as the depth of a decoder increases when handling crowded scenes. Experiments are conducted on CrowdHuman dataset, taking Sparse RCNN based on ResNet-50 as base detector. It equips with 500 queries. We adjust the depth of its decoder while keeping the others unchanged. As is described in    <ref type="figure">Figure 8</ref>: 8a is the architecture of decoding stage in deformable DETR <ref type="bibr" target="#b54">[54]</ref>; 8b describes the decoding stage structure equipped with our designed components for progressive predicting schema.</p><p>C. Performance of query detector with large model in crowded scenes.</p><p>To explore the detection upper bound of a query-based detector in tackling crowded scenes, we replace the ResNet-50 with a large backbone, Swin-Large <ref type="bibr" target="#b28">[29]</ref>. Experiments are conducted on CrowdHuman <ref type="bibr" target="#b36">[37]</ref> and CityPersons <ref type="bibr" target="#b49">[49]</ref> datasets, with the same training strategy described in the paper. As depicted in <ref type="table">Table.</ref> 6, our method can significantly boost the performance of a query-based detector, which achieves a state-of-the-arts results on both Crowd-Human and CityPersons validating datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Dataset #Queries AP MR ?2 JI S-RCNN 500 93.1 39.  RelationNet <ref type="bibr" target="#b19">[20]</ref> IterDet <ref type="bibr" target="#b34">[35]</ref> Sparse R-CNN <ref type="bibr" target="#b38">[39]</ref> D-DETR <ref type="bibr" target="#b2">[3]</ref> Sparse RCNN+Ours D-DETR+Ours <ref type="figure">Figure 9</ref>: Results visualization of RelationNet <ref type="bibr" target="#b19">[20]</ref>, IterDet <ref type="bibr" target="#b34">[35]</ref>, Sparse RCNN <ref type="bibr" target="#b38">[39]</ref>, deformable DETR <ref type="bibr" target="#b54">[54]</ref> and our approach based on them <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b54">54]</ref>. Blue boxes are true positive detections, light yellow boxes are missed instances and orange boxes are false positives. Green boxes represent progressively refined detections in our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>1a. Sparse RCNN<ref type="bibr" target="#b38">[39]</ref> introduces false positives in crowded scenes. 1b. Our approach can remove those false positives and ensure each object can be detected only once. Green boxes indicate true positives while red ones represent false positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The diagram of decoding stage. P-RoIAlignPool<ref type="bibr" target="#b14">[15]</ref>, DynConv -Dynamic Convolution, MSA -Multi-head Self-Attention, S -Prediction Selector, R -Relation Information Extractor, QU -Query Updater.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>5, with the procedure formulated in Equ.(3) as well. For each noisy prediction b i , we first find their accepted neighbors N (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Relation information extractor R. E -sine and cosine spatial positional encoding function<ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b45">45]</ref>, linear fc layer, r -ReLU, Tfc layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>.</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Performance of the proposed method with different configurations of hyper-parameter s and ? on CrowdHuman [37] dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Decoder in deformable DETR<ref type="bibr" target="#b54">[54]</ref>. MCA -multi-head crossattention, MSA -multi-head self-attention. Decoder in SR-Deformable DETR (Ours). S -Prediction Selector, R -Relation information extractor, LMSA -local multihead self-attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>the final decoding stage equipped with the progressive prediction method previous decoding stages query updater</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>queries &amp;</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>objects</cell><cell></cell><cell></cell></row><row><cell>feature</cell><cell></cell><cell></cell><cell>accepted boxes noisy queries</cell><cell>relation</cell></row><row><cell>pyramid network</cell><cell>stage-0</cell><cell>stage-! ? #</cell><cell>prediction selector</cell><cell>information extractor</cell></row><row><cell>/transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>accepted queries</cell><cell></cell></row><row><cell></cell><cell>queries</cell><cell></cell><cell>stop gradient back-propagation</cell><cell>new queries</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Instance density of each dataset. The threshold for overlap statistics is IoU &gt; 0.5. *Averaged by the number of classes.</figDesc><table><row><cell>standard ResNet-50 [16] pre-trained on ImageNet as back-</cell></row><row><cell>bone. We train our model with the Adam optimizer with</cell></row><row><cell>a momentum of 0.9 and weight decay of 0.0001. Models</cell></row><row><cell>are trained for 50, 000 iterations. The initial learning rate is</cell></row><row><cell>0.00005 and reduced by a factor of 0.1 at iteration 37,500.</cell></row><row><cell>The last stage joints the optimization after 5,000 iterations</cell></row><row><cell>of training. ?</cell></row></table><note>cls = 2, ? L1 = 5, ? giou = 2. The default number of proposal boxes, proposal features, and stages are set to 500, 500, and 6, respectively. Additionally, The di- mension of intermediate features in relationship extractor R is 256. The gradients are detached at proposal boxes from the second stage to stabilize training. Besides, those neg- ative samples, whose intersection-over-area (IoA) between any ignore region is higher than a threshold of 0.7, are not involved in training. Further, the hyper-parameters s and ? are 0.7 and 0.4 by default in different query-based detec- tors [39, 54].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of different methods on CrowdHuman validation set, +MIP represents multiple instance prediction with set NMS as post-processing. ? indicates the approach is implemented by PBM<ref type="bibr" target="#b20">[21]</ref>. S-RCNN -Sparse RCNN<ref type="bibr" target="#b38">[39]</ref>. D-DETR -deformable DETR<ref type="bibr" target="#b54">[54]</ref>. Ablation study of different modules. To explore the effectiveness of the proposed modules in Sec. 3.2, we conduct extensive ablation study of the relation information extractor R, local self-attention module LMSA and the newly initialized embedding E. All experiments are conducted on Sparse RCNN<ref type="bibr" target="#b38">[39]</ref> with 500 queries, ResNet-50<ref type="bibr" target="#b15">[16]</ref> backbone and evaluated on CrowdHuman dataset.Table.3a shows that the relation information extractor R can obtain an improvement of 0.8% AP, 1.7% MR ?2 and 1.6% JI. It indicates its effectiveness in reducing false positives and recalling false negatives. Moreover, when equipped with the new local self-attention LMSA, the performance on three evaluation metrics is further boosted, since the local self-attention can reduce duplicates effectively. Further, the newly initialized embeddings, aiming to approximate the new data distribution of noise predictions, can slightly improve MR ?2 .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>3a. Ablation study of different modules proposed in our approach, taking Sparse RCNN<ref type="bibr" target="#b38">[39]</ref> with 500 queries as our default instantiation. 3b. Comparisons of different relation modeling appraoches. All the experiments are conducted on CrowdHuman<ref type="bibr" target="#b36">[37]</ref> dataset. 3c Performance comparisons of different methods on CityPersons<ref type="bibr" target="#b49">[49]</ref>. Both box-based<ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref> and query-based approaches<ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b54">54]</ref> are evaluated.</figDesc><table><row><cell></cell><cell>?10 4</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.780</cell><cell>1.467</cell><cell>0.352</cell><cell>0.995</cell></row><row><cell></cell><cell>0.902</cell><cell>1.140</cell><cell>0.261</cell><cell>0.995</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell cols="4">Duplicate Localization Background Missing</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>, which suggests Method AP AP 50 AP 75 AP S AP M AP L</figDesc><table><row><cell>S-RCNN [39]</cell><cell>45.0 64.2 49.1 27.6 47.5 59.1</cell></row><row><cell>D-DETR [54]</cell><cell>45.8 64.5 49.4 28.2 49.0 61.7</cell></row><row><cell>S-RCNN+Ours</cell><cell>46.1 65.3 50.6 29.2 48.7 59.9</cell></row><row><cell>D-DETR+Ours</cell><cell>46.7 65.3 50.3 28.6 49.8 61.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Performance comparisons of different methods on COCO 2017 [27] minival set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Table. 5, the performance degrades as the depth of the decoder increases.</figDesc><table><row><cell cols="4">#Depth #Queries AP MR ?2</cell><cell>JI</cell></row><row><cell>6</cell><cell></cell><cell>90.7</cell><cell>44.7</cell><cell>81.4</cell></row><row><cell>7</cell><cell></cell><cell>90.6</cell><cell>45.7</cell><cell>81.0</cell></row><row><cell>8</cell><cell>500</cell><cell>90.4</cell><cell>45.9</cell><cell>80.3</cell></row><row><cell>9</cell><cell></cell><cell>90.7</cell><cell>44.4</cell><cell>80.9</cell></row><row><cell>10</cell><cell></cell><cell>90.2</cell><cell>46.6</cell><cell>80.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Experiment analysis as the depth of a decoder increases, which performs on CrowdHuman dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Experiment on CHuman(CrowdHuman) and CPer- sons(CityPersons) with Swin-L [29]. S-RCNN -Sparse RCNN [39], D-DETR -Deformable DETR [54]</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The HungarianMatch operation in Algorithm 1 is a combinatorial optimization approach that solves the assignment problem, which is commonly used in<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b46">46]</ref> for one-to-one label assignment.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The detail implementation of deformable DETR with the proposed schema is illustrated in the Appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Here, we follow the procedure to compute evaluation metric JI.<ref type="bibr" target="#b3">4</ref> GossipNet:https://github.com/hosang/gossipnet 5 IterDet:https://github.com/saic-vul/iterdet</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Soft-nms -improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04503</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tide: A general toolbox for identifying object detection errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Spatial memory for context reasoning in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno>abs/1704.04224</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pedhunter: Occlusion robust pedestrian detector in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2020 : The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Relational learning for joint head and human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2020 : The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A tree-based context model for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung</forename><forename type="middle">Jin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="240" to="252" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Detection in crowded scenes: One proposal, multiple predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuangeng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anlin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12214" to="12223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Up-detr: Unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="1601" to="1610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An empirical study of context in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Santosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1271" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object categorization using co-occurrence, location and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fast convergence of DETR with spatially modulated co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/2101.07448</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">PS-RCNN: detecting secondary human instances in a crowd via primary object suppression. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osamu</forename><surname>Yoshie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn. computer vision and pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Softer-nms: Rethinking bounding box regression for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning nonmaximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A convnet for non-maximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">38th German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="192" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nms by representative region: Towards crowded pedestrian detection by proposal pairing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osamu</forename><surname>Yoshie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10750" to="10759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attentive contexts for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="944" to="954" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Detr for crowd pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06785</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptive nms: Refining pedestrian detection in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6459" to="6468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Semantic head enhanced pedestrian detection in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11985</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. icml</title>
		<meeting>icml</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>page 3. Citeseer</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequential context encoding for duplicate removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2049" to="2058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Iterdet: Iterative scheme for objectdetection in crowded environments. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danila</forename><surname>Rukhovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danil</forename><surname>Galeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Konushin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Featurenms: Non-maximum suppression by learning feature embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niels</forename><forename type="middle">Ole</forename><surname>Salscheider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7848" to="7854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2325" to="2333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12450</idno>
		<title level="m">Sparse r-cnn: End-to-end object detection with learnable proposals</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Rethinking transformer-based set prediction for object detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcao</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9626" to="9635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">FCOS: A simple and strong anchor-free object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Context-based vision system for place and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murphy</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freeman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rubin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Ninth IEEE International Conference on Computer Vision</title>
		<meeting>Ninth IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Auto-context and its application to high-level vision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">End-to-end object detection with fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03544</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Repulsion loss: Detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07752</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Double anchor r-cnn for human detection in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09998</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Citypersons: A diverse dataset for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4457" to="4465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02424</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Occlusion-aware r-cnn: Detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="637" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">End-to-end object detection with adaptive clustering transformer. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2021: The Ninth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
