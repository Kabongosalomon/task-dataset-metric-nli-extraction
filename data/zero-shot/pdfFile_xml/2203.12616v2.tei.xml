<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Pre-Training on Patient Population Graphs for Patient-Level Predictions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chantal</forename><surname>Pellegrini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anees</forename><surname>Kazi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Pre-Training on Patient Population Graphs for Patient-Level Predictions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Outcome/Disease Prediction ? Population Graphs ? Pre- Training</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-training has shown success in different areas of machine learning, such as Computer Vision (CV), Natural Language Processing (NLP) and medical imaging. However, it has not been fully explored for clinical data analysis. Even though an immense amount of Electronic Health Record (EHR) data is recorded, data and labels can be scarce if the data is collected in small hospitals or deals with rare diseases. In such scenarios, pre-training on a larger set of EHR data could improve the model performance. In this paper, we apply unsupervised pre-training to heterogeneous, multi-modal EHR data for patient outcome prediction. To model this data, we leverage graph deep learning over population graphs. We first design a network architecture based on graph transformer designed to handle various input feature types occurring in EHR data, like continuous, discrete, and time-series features, allowing better multi-modal data fusion. Further, we design pre-training methods based on masked imputation to pre-train our network before fine-tuning on different end tasks. Pre-training is done in a fully unsupervised fashion, which lays the groundwork for pre-training on large public datasets with different tasks and similar modalities in the future. We test our method on two medical datasets of patient records, TAD-POLE and MIMIC-III, including imaging and non-imaging features and different prediction tasks. We find that our proposed graph based pretraining method helps in modeling the data at a population level and further improves performance on the fine tuning tasks in terms of AUC on average by 4.15% for MIMIC and 7.64% for TADPOLE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Enormous amounts of data are collected on a daily basis in hospitals. Nevertheless, labeled data can be scarce, as labeling can be tedious, time-consuming, and expensive. Further, in small hospitals and for rare diseases, only little data is accumulated <ref type="bibr" target="#b14">[15]</ref>. The ability to leverage the large body of unlabeled medical data to improve in prediction tasks over such small labeled datasets could increase the confidence of AI models for clinical outcome prediction. Unsupervised pre-training was shown to be useful to exploit unlabeled data in NLP <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b3">4]</ref>, arXiv:2203.12616v2 <ref type="bibr">[cs.</ref>LG] 9 Jun 2022 CV <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b0">1]</ref> and medical imaging <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b1">2]</ref>. However, for more complex clinical data, it is not explored enough. Some works study how to pre-train BERT <ref type="bibr" target="#b3">[4]</ref> over EHR data of medical diagnostic codes. They pre-train with modified masked language modeling and, in one case, a supervised prediction task. The targeted downstream tasks lie in disease and medication code prediction <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21]</ref>. Mc-Dermott et al. <ref type="bibr" target="#b13">[14]</ref> propose a pre-training approach over heterogeneous EHR data, including e.g. continuous lab results. They create a benchmark with several downstream tasks over the eICU <ref type="bibr" target="#b18">[19]</ref> and MIMIC-III <ref type="bibr" target="#b6">[7]</ref> datasets and present two baseline pre-training methods. Pre-training and fine-tuning are performed over single EHRs from ICU stays with a Gated Recurrent Unit.</p><p>On the other hand, population graphs have been leveraged in the recent literature to help analyze patient data using the relationships among patients, leading to clinically semantic modeling of the data. During unsupervised pre-training, graphs allow learning representations based on feature similarities between the patients, which can then help to improve patient-level predictions. Several works successfully apply pre-training to graph data in different domains like molecular graphs and on common graph benchmarks. Proposed pre-training strategies include node level tasks, like attribute reconstruction, graph level tasks like property prediction, or generative tasks such as edge prediction <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref>. To the best of our knowledge, no previous work applied pre-training to population graphs.</p><p>In this paper, we propose a model capable of pre-training for understanding patient population data. We choose two medical applications in brain imaging and EHR data analysis on the public datasets TADPOLE <ref type="bibr" target="#b12">[13]</ref> and MIMIC-III <ref type="bibr" target="#b6">[7]</ref> for Alzheimer's disease prediction <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3]</ref> and Length-of-Stay prediction <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b13">14]</ref>. The code is available at https://github.com/ChantalMP/Unsupervised-Pre-Training-on-Patient-Population-Graphs-for-Patient-Level-Predictions. Contribution: We develop an unsupervised pre-training method to learn a general understanding of patient population data modeled as graph, providing a solution to limited labeled data. We show significant performance gains through pre-training when fine-tuning with as little as 1% and up to 100% labels. Further, we propose a (graph) transformer based model suitable for multi-modal data fusion. It is designed to handle various EHR input types, taking static and time-series data and continuous as well as discrete numerical features into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Let D be a dataset composed of the EHR data of N patients. The i th record is represented by r i ? [d i , c i , t i ] with static discrete features d i ? N D , continuous features c i ? R C , and time-series features t i ? R S?? , where ? denotes the length of the time-series. For every downstream task T , labels Y ? N L are given for L classes. The task is to predict the classes for the test set patients given all features. Towards this task, we propose to use patient population graphs. Unlike in non-graph-based methods, the model can exploit similarities between patients to better understand the EHR data at patient and population level. Further, unlike conventional graph neural networks, graph transformers allow flexible attention to all nodes, learning which patients are relevant for a task. This would be most apt for learning over population EHR data. Our pipeline consists of two steps. 1) Unsupervised pre-training and 2) Fine-tuning. Unsupervised pre-training enables understanding of general EHR data and disorder progression by training the model for masked imputation task. This understanding can help learn downstream tasks better despite limited labeled data. Graph Construction For each node pair with records r i and r j , we calculate a similarity score S(r i , r j ) between the node features. We use L2 distance for continuous and absolute matching for discrete features. As graph construction is not our focus, we choose the most conventional method using k-NN selection rule. We set k=5 to avoid having many disconnected components and very densely connected regions (see supplementary material). A detailed description of the graph construction per dataset follows in the experiment section. Model Architecture Our model consists of an encoder and decoder. The encoder comprises a data embedding module and a graph transformer module explained later. We design the encoder to handle various input data types. The decoder is a simple linear layer capable of capturing the essence of features inclined towards a node-level classification task. <ref type="figure" target="#fig_0">Figure 1</ref> shows an overview of our model architecture. Data embedding module: Following the conventional Graphormer <ref type="bibr" target="#b24">[25]</ref>, we pro- cess discrete input features by an embedding layer, followed by a summation over the feature dimension, resulting in embedded features d i ? R D , where D is the output feature dimension. While Graphormer is limited to static, discrete input features only, we improve upon Graphormer to support also static, continuous input features, which are processed by a linear layer resulting in the embedding vector c i ? R C . The third branch of our data embedding module handles timeseries input features t i ? R S?? with a linear layer, followed by two transformer layers to deal with variable sequence lengths and allow the model to incorporate temporal context. The output is given by t i,h ? R E per time-step h. The mean of these embeddings forms the final time-series embeddings t i ? R S . The feature vectors d i , c i and t i are concatenated to form the final node embeddings</p><formula xml:id="formula_0">n i ? R F , where F = F k ?[D ,C ,S ] F k , for each of the N nodes. Graphormer Module:</formula><p>The backbone of our model comprises multiple graph transformer layers <ref type="bibr" target="#b24">[25]</ref>. Graphormer uses attention between all nodes in the graph.</p><p>To incorporate the graph structure, structural encodings are used, which encode in and out degrees of the nodes, the distance between nodes, and edge features.</p><p>Pre-training and Fine-Tuning: We propose an unsupervised pre-training technique on the same input features as for downstream tasks, but without using labels Y. Instead, we randomly mask a fixed percentage of feature values for every record r i and optimize the model to predict these values. For all methods masking is performed by replacing certain feature values with a fixed value called 'masked token' for discrete features and with zero for continuous features. For time-series features, we further add a binary column per feature to the input vector, that encodes which hours in the time-series are masked. We optimize the model using (binary) cross entropy loss for discrete and mean squared error loss for continuous features. A model for fine-tuning is initialized using the encoder weights learned during pre-training and random weights for the decoder. Then the model is fine-tuned for the task T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We use two publicly available medical data sets: TADPOLE <ref type="bibr" target="#b12">[13]</ref> and MIMIC-III <ref type="bibr" target="#b6">[7]</ref>. They differ in size, the targeted prediction task, and the type of input features, allowing comprehensive testing and evaluation of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets description:</head><p>TADPOLE <ref type="bibr" target="#b12">[13]</ref> contains 564 patients from the Alzheimer's Disease Neuroimaging Initiative (ADNI). We use twelve features, which the TADPOLE challenge claims are informative. They include discrete cognitive test results, demographics, and continuous features extracted from MR and PET imaging, normalized between zero and one. The task is to classify the patients into the groups Cognitive Normal (CN), Mild Cognitive Impairment (MCI), or Alzheimer's Disease (AD). We only use data from patients' first visits to avoid leakage of information.</p><p>Graph Construction: We construct a k-NN graph with k=5, dependent on the mean similarity (S) between the features. For the demographics, age, gender and </p><formula xml:id="formula_1">apoe4, S dem (r i , r j ) = 1 if f i = f j else 0 1 if |age i ? age j | ? 2 else 0 ? 3 where, f =(apoe4, gender</formula><formula xml:id="formula_2">S cog (r i , r j ) = f ?d i ||fr i ?fr j || max(d i ) and S img (r i , r j ) = sig( f ?c i ||f ri ? f rj ||).</formula><p>The overall similarity S(r i , r j ) is then given as mean of S dem , S cog and S img . Pre-Training Configuration: During pre-training on TADPOLE, we randomly mask 30% of the medical features (APOE4, cognitive tests, and imaging features) in each sample. The masking ratio of 30% was chosen experimentally. MIMIC-III <ref type="bibr" target="#b6">[7]</ref> is a large EHR dataset of patient records with various static and time-series data collected over the patient's stay. We use the pre-processed dataset published by McDermott et al. <ref type="bibr" target="#b13">[14]</ref>. It includes 19.7 K patients that are at least 15 years old and stayed 24 hours or more in the ICU. The features include demographics, measurements from bed-side monitoring and lab tests in hourly granularity (continuous), and binary features stating if different treatments were applied in each hour. In total we have 76 features. We use linear interpolation to impute missing measurements. Fine-tuning is evaluated on Length-of-Stay (LOS) prediction as defined in <ref type="bibr" target="#b13">[14]</ref>. The input encompasses the first 24 hours of each patient's stay, and the goal is to predict if a patient will stay longer than three days or not. Graph Construction: It is computationally infeasible to process a graph containing all patients. Thus, we create sub-graphs with 500 patients each, which fit into memory, each containing train, validation and test patients. We split randomly as we do not want to make assumptions on which types of patients the model should see, but learn this via the attention in the graph transformer. Block-wise Masking (BM): Instead of the full features, we mask a random block of 6 hours within the 24-hour time-series in 100% of the features. Here, the model can access past and future values to make a prediction. Thus, it can learn to understand temporal context during pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Setup</head><p>Given a pre-trained model, we compare the results of fine-tuning it, with training the same but randomly initialized model from scratch. We manually tuned hyper-parameters per dataset separately for pre-training, from scratch training, and fine-tuning. To simulate scenarios with limited labeled data, we measure the model performance at different label ratios, meaning different amounts of labels (1%, 5%, 10%, 50%, 100%) are used for training or fine-tuning. For pre-training always the full training data is used. Implementation Details All experiments are implemented in PyTorch, performed on a TITAN Xp GPU with 12GB VRAM, and optimized with the Adam optimizer <ref type="bibr" target="#b9">[10]</ref>. For cross-validation, pre-training is performed separately per fold. The model comprises four Graphormer layers for TADPOLE and eight for MIMIC-III. For TADPOLE, we pre-train for 6000 epochs with a LR of 1e-5. We train task prediction for 1200 epochs with a polynomial decaying LR (1e-5 to 5e-6) to train from scratch and a LR of 5e-6 for fine-tuning. When fine-tuning with 1% labels, we reduce the epochs to 200. All results are computed with 10-fold cross-validation. For MIMIC-III, we pre-train for 3000 epochs with a polynomial decaying LR (1e-3 to 1e-4). We train for 1100 epochs with a LR of 1e-4 from scratch, or fine-tune for 600 epochs with a LR of 1e-5. For a fair comparison with the state of the art, results are averaged over six folds, each with an 80-10-10 split into train, validation and test data. The models are selected based on the validation sets, and performance is computed over the test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Comparative methods: <ref type="table" target="#tab_1">Table 1</ref> We compare our model to related work without any pre-training. On TADPOLE, we compare to a latent graph learning paper proposed by Cosmo et al. <ref type="bibr" target="#b2">[3]</ref>, which proposes to learn an optimal population graph for the given task. Besides, one recent arxiv paper <ref type="bibr" target="#b7">[8]</ref> further improves performance on TADPOLE by learning input feature importance. However it is out of context for this work. We achieve comparable accuracy to DGM and outperform in terms of AUC, which is an important metric for imbalanced datasets. For MIMIC-III, we compare our method to the EHR pre-training benchmark of McDermott et al. <ref type="bibr" target="#b13">[14]</ref>, which uses the same LOS definition and dataset. We significantly outperform the benchmark model. The results show that the proposed architecture is a good fit for the task at hand. Effect of pre-training: <ref type="table" target="#tab_2">Table 2</ref> The motivation of this experiment is to investigate the smallest amount of labels required during the fine-tuning of the downstream task. The results emphasize the benefits of our unsupervised pretraining with limited labels. On TADPOLE the main benefit of pre-training can be seen for settings with limited labels (1%, 5%, 10%), where performance improves significantly. Moreover, AUC continues to improve for all ratios. For LOS on MIMIC-III, both metrics significantly improve for all label ratios compared to from scratch training. Further, for MIMIC-III we compare two types of masking (BM, FM). We see that feature masking consistently outperforms blockwise masking. The performance improvements achieved through pre-training on MIMIC-III are significantly higher than in the benchmark <ref type="bibr" target="#b13">[14]</ref>. Moreover, we see improvements until the full dataset size and not only for limited labels. Further the pre-trained models have a lower standard deviation, indicating higher stability. Ablation experiments: <ref type="table" target="#tab_3">Table 3</ref> We perform several ablation studies to eval-  <ref type="figure">)</ref>). We see a clear benefit from using Graphormer compared to the linear model and GCN. For TADPOLE, the linear model reaches slightly better performance in terms of AUC as TADPOLE is a relatively small and easy dataset. The effect of the node level attention mechanism to all nodes given by Graphormer is clearly visible when compared to GCN. Further, we perform pre-training followed by fine-tuning for the linear model ( <ref type="table" target="#tab_3">Table 3 b)</ref>). Our proposed unsupervised pretraining method proves to be beneficial also for the linear model, but the effects are less as for our proposed architecture.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we present an unsupervised pre-training method based on masked imputation, significantly improving prediction results. We propose a graph transformer based architecture for learning on population graphs built from heterogeneous EHR data. We show the superiority of our pipeline in both pre-training and various prediction tasks for two datasets, TADPOLE and MIMIC-III. Pretraining helps for all dataset sizes but especially in scenarios where only a limited amount of labeled data is used for fine-tuning. Our pre-training method is unsupervised and therefore independent from the end task, and further it is well suited for transfer learning. This work opens the path for the community to deals with small dataset specially with limited labels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of the proposed architecture. All input features are combined into one node embedding, applying transformer layers to enhance the time-series features. The upscale layer for time-series features is a linear layer for continuous and an embedding layer followed by a summation over the feature dimension for discrete features. The resulting graph is processed by several Graphormer layers and a linear task layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Given the time-series of the measurement features f, we form feature descriptors f d = (mean(f ), std(f ), min(f ), max(f )) per patient and feature, where d equals the 56 measurement features. We then compute the average similarity over all features f d between two patients r i and r j : Sim(r i , r j ) = f ?f d ||fr i ?fr j || |f d | and build a k-NN graph with k=5. Pre-Training Configuration: On MIMIC-III, we perform masking on the timeseries features from measurement and treatment data. Pre-training is performed over data from the first 24 hours of the patient's stay. We compute the loss only over measured values, not over interpolated ones. Masking ratios are chosen experimentally. We compare two types of masking: Feature Masking (FM): We randomly select 30% of the features per patient and mask the full 24 hours of the time-series. The model can not see past or future values, only other features and patients, aiming to force an understanding of relations between features and patients to infer masked features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). For the cognitive test results d i (ordinal features), and c i (continuous imaging features), we calculate the respective normalized L2 distances:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Accuracy and AUC of the proposed method compared with DGM on TAD-POLE and McDermott et al. [14] on MIMIC-III.</figDesc><table><row><cell></cell><cell>TADPOLE</cell><cell></cell><cell></cell><cell>MIMIC-III</cell><cell></cell></row><row><cell>Model</cell><cell>ACC</cell><cell>AUC</cell><cell>Model</cell><cell>ACC</cell><cell>AUC</cell></row><row><cell cols="4">Cosmo [3] 92.91 ? 02.50 94.49 ? 03.70 McDermott [14]</cell><cell>-</cell><cell>71.00 ? 1.00</cell></row><row><cell cols="3">Proposed 92.59 ? 3.64 96.96 ? 2.32</cell><cell>Proposed</cell><cell cols="2">70.29 ? 1.10 76.17 ? 1.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance of the proposed model in accuracy and AUC trained from scratch (SC) or fine-tuned after pre-training (FT) for different label ratios. For MIMIC-III we additionally compare the block-wise (BM) and feature masking (FM) to each other. 100% ACC 92.59 ? 3.64 92.24 ? 3.47 70.29 ? 1.10 70.73 ? 0.70 71.44 ? 1.25 AUC 96.96 ? 2.23 97.52 ? 1.67 76.17 ? 1.02 76.20 ? 0.54 77.78 ? 1.31uate different parts of our proposed model on pre-and task training. Effect of Graphormer: We replace the Graphormer module with a simple linear or GCN layer and train the model from scratch on the full dataset(Table 3 a</figDesc><table><row><cell></cell><cell>TADPOLE</cell><cell></cell><cell></cell><cell>MIMIC-III</cell><cell></cell></row><row><cell>Size Metric</cell><cell>SC</cell><cell>FT</cell><cell>SC</cell><cell>FT: BM</cell><cell>FT: FM</cell></row><row><cell cols="6">1% ACC 59.42 ? 8.40 78.89 ? 2.45 59.86 ? 2.11 63.22 ? 2.39 65.25 ? 1.09</cell></row><row><cell cols="6">AUC 68.72 ? 12.74 93.49 ? 2.07 62.98 ? 2.55 68.07 ? 1.80 69.90 ? 1.26</cell></row><row><cell cols="6">5% ACC 78.23 ? 6.83 83.37 ? 6.29 64.79 ? 1.16 66.82 ? 0.89 68.66 ? 0.73</cell></row><row><cell cols="6">AUC 87.23 ? 4.91 94.99 ? 2.55 68.85 ? 1.53 72.27 ? 1.19 73.97 ? 1.28</cell></row><row><cell cols="6">10% ACC 87.00 ? 4.86 87.71 ? 4.65 64.72 ? 0.45 67.71 ? 0.69 69.42 ? 1.23</cell></row><row><cell cols="6">AUC 92.03 ? 3.39 95.96 ? 2.51 68.97 ? 0.66 73.55 ? 0.60 75.09 ? 1.29</cell></row><row><cell cols="6">50% ACC 92.41 ? 3.69 91.52 ? 3.76 67.41 ? 1.31 69.98 ? 0.69 70.85 ? 0.92</cell></row><row><cell cols="6">AUC 96.06 ? 2.48 97.23 ? 1.94 72.53 ? 1.08 76.02 ? 0.87 76.86 ? 1.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 c</head><label>3</label><figDesc>) shows masked imputation performance during pre-training, measured by RMSE for continuous (imaging/ measurements) and accuracy or F1 for discrete features (apoe4+cognitive tests/treatments). Here the proposed model outperforms the linear model, explaining why pre-training has a greater effect for it. In summary we see a positive effect of using Graphormer over the linear model for solving the pre-training task and improving fine-tuning performance. Effect of Transformer: For MIMIC-III, Transformer is inserted in the encoder to deal with time series data. To test the transformer layers, we remove this component and train the model from scratch on the full dataset, resulting in a reduction of accuracy from 70.29 to 69.39% and AUC from 76.17 to 75.03%. This shows that the transformer layers are helpful for processing time-series inputs. The model needs to predict time-dependent outputs for pre-training on MIMIC-III, for which the transformer layers are important, as they can understand the temporal context. To investigate the effect of transformer during pre-training, we remove the transformer layer and replace the Graphormer module with an linear layer. We observe a reduction in the performance by 0.45% for ACC and 3.03% in AUC through pre-training. Accordingly, removing the transformer layer results in a 0.049 larger RMSE and a 3.9% lower F1 score in pre-training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablations to test Graphormer module by replacing it with a linear/GCN layer, a) downstream task performance trained from scratch b) results of fine-tuning (FT) on limited labels (TADPOLE 1%, MIMIC-III 10%), compared to training from scratch (SC) c) pre-training task performance, multi-class accuracy for cognitive tests uses feature-dependent error margins in which predictions are considered correct. The small number of imaging features might cause the low std of 0.006/0.008. ? 02.62 97.77 ? 01.59 67.<ref type="bibr" target="#b24">25</ref> ? 01.11 72.69 ? 00.97 a GCN 74.27 ? 06.41 89.89 ? 04.12 68.74 ? 01.50 72.64 ? 01.10 Proposed 92.59 ? 03.64 96.96 ? 02.23 70.29 ? 01.10 76.17 ? 01.02 Linear SC 54.20 ? 08.74 70.41 ? 11.41 63.78 ? 00.74 67.72 ? 00.68 b Linear FT 71.27 ? 09.76 89.25 ? 06.53 64.71 ? 00.84 67.94 ? 01.20 Proposed SC 59.42 ? 08.40 68.72 ? 12.74 64.72 ? 00.45 68.97 ? 00.66 Proposed FT 78.89 ? 02.45 93.49 ? 02.07 69.42 ? 01.23 75.09 ? 01.29 .008 62.58 ? 04.87 00.79 ? 0.023 81.49 ? 00.35 Proposed 0.14 ? 0.006 63.23 ? 04.25 0.78 ? 0.011 81.58 ? 00.41</figDesc><table><row><cell></cell><cell></cell><cell>TADPOLE</cell><cell></cell><cell cols="2">MIMIC-III</cell></row><row><cell></cell><cell>Model</cell><cell>ACC</cell><cell>AUC</cell><cell>ACC</cell><cell>AUC</cell></row><row><cell></cell><cell>Linear</cell><cell>91.14 RMSE</cell><cell>ACC</cell><cell>RMSE</cell><cell>F1</cell></row><row><cell>c</cell><cell>Linear</cell><cell>00.15 ? 0</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Selfsupervised learning for medical image analysis using image context restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101539</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent-graph learning for disease prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cosmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (1)</title>
		<editor>Burstein, J., Doran, C., Solorio, T.</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<title level="m">Strategies for pre-training graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gpt-gnn: Generative pretraining of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mimic-iii, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W H</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Anthony Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Ia-gcn: Interpretable attention based graph convolutional network for disease prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farghadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15587</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Selfattention equipped graph convolutions for disease prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shekarforoush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kortuem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Symposium on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Behrt: transformer for electronic health records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R A</forename><surname>Solares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hassaine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Canoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salimi-Khorshidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning to pre-train graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Marinescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Oxtoby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Bron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Toga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Weiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Barkhof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Alexander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03909</idno>
		<title level="m">Tadpole challenge: prediction of longitudinal evolution in alzheimer&apos;s disease</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comprehensive ehr timeseries pre-training benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nestor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Health, Inference, and Learning</title>
		<meeting>the Conference on Health, Inference, and Learning</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Small data challenges of studying rare diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Mitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haneuse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA network open</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervision with superpixels: Training few-shot medical image segmentation without annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Biffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="762" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Disease prediction using graph convolutional networks: application to autism spectrum disorder and alzheimer&apos;s disease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Ktena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The eicu collaborative research database, a freely available multi-center database for critical care research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Raffa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Badawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Med-bert: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rasmy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Grover: Selfsupervised message passing transformer on large-scale molecular data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pre-training of graph augmented transformers for medication recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00346</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mimic-extract: A data extraction, preprocessing, and representation pipeline for mimic-iii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM conference on health, inference, and learning</title>
		<meeting>the ACM conference on health, inference, and learning</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A deep learning approach for length of stay prediction in clinical settings from medical records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zebin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rezvy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Chaussalet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology</title>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05140</idno>
		<title level="m">Graph-bert: Only attention is needed for learning graph representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
