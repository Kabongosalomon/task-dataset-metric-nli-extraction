<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Multi-dimensional Edge Feature-based AU Relation Graph for Facial Action Unit Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Luo</surname></persName>
							<email>luocheng2020@email.szu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Institute</orgName>
								<orgName type="institution">Shenzhen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institute of Artificial Intelligence and Robotics for Society</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Guangdong Key Laboratory of Intelligent Information Processing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyang</forename><surname>Song</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Institute</orgName>
								<orgName type="institution">Shenzhen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institute of Artificial Intelligence and Robotics for Society</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Guangdong Key Laboratory of Intelligent Information Processing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Shen</surname></persName>
							<email>llshen@szu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Institute</orgName>
								<orgName type="institution">Shenzhen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institute of Artificial Intelligence and Robotics for Society</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Guangdong Key Laboratory of Intelligent Information Processing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hatice</forename><surname>Gunes</surname></persName>
							<email>hatice.gunes@cl.cam.ac.uk</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Multi-dimensional Edge Feature-based AU Relation Graph for Facial Action Unit Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The activations of Facial Action Units (AUs) mutually influence one another. While the relationship between a pair of AUs can be complex and unique, existing approaches fail to specifically and explicitly represent such cues for each pair of AUs in each facial display. This paper proposes an AU relationship modelling approach that deep learns a unique graph to explicitly describe the relationship between each pair of AUs of the target facial display. Our approach first encodes each AU's activation status and its association with other AUs into a node feature. Then, it learns a pair of multi-dimensional edge features to describe multiple task-specific relationship cues between each pair of AUs. During both node and edge feature learning, our approach also considers the influence of the unique facial display on AUs' relationship by taking the full face representation as an input. Experimental results on BP4D and DISFA datasets show that both node and edge feature learning modules provide large performance improvements for CNN and transformer-based backbones, with our best systems achieving the state-of-the-art AU recognition results. Our approach not only has a strong capability in modelling relationship cues for AU recognition but also can be easily incorporated into various backbones. Our PyTorch code is made available at https://github.com/CVI-SZU/ME-GraphAU. * Equal contribution. ? Corresponding author. k AU j AU i AU [1.6, 0.8, , 4.3] ? k AU j AU i AU [2.1, 6.6, , 0.7] ? 1.0 k AU j AU i AU 1.0 k AU j AU i AU 0.6 k AU j AU i AU Facial display 3 Facial display 1 Facial display 2 Pre-defined AU graph (a) Facial display-specific AU graph (b) Our approach (c) 1.0 k AU j AU i AU 0.2 k AU j AU i AU 0.8 k AU j AU i AU k AU j AU i AU [2.8, 0.9, , 1.1] ?</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Facial Action Coding System <ref type="bibr" target="#b1">[Friesen and Ekman, 1978]</ref> represents human face by a set of facial muscle movements called Action Units (AUs). Compared with the emotionbased categorical facial expression model, AUs describe human facial expressions in a more comprehensive and objective manner <ref type="bibr">[Martinez et al., 2017]</ref>. Facial AU recognition is a multi-label classification problem as multiple AUs can be Figure 1: Comparison between our approach with existing AU graph-based approaches: (a) pre-defined AU graphs that use a single topology to define AU association for all facial displays; (b) Facial display-specific AU graphs that assign a unique topology to define AU association for each facial display. Both (a) and (b) use a single value as an edge feature; (c) Our approach encodes a unique AU association pattern for each facial display in node features, and additionally describes the relationship between each pair of AUs using a pair of multi-dimensional edge features. activated simultaneously. While previous studies found that underlying relationships among AUs' activation <ref type="bibr">[Corneanu et al., 2018;</ref><ref type="bibr">Song et al., 2021c;</ref> are crucial for their recognition, how to properly model such relationships is still an open research question in the field.</p><p>A popular strategy applies various traditional machine learning models (e.g., conditional models <ref type="bibr" target="#b1">[Eleftheriadis et al., 2015]</ref>) or neural network-based operations (e.g., convolution <ref type="bibr" target="#b8">[Zhao et al., 2016]</ref>, Long-Short-Term-Memory networks <ref type="bibr" target="#b3">[Niu et al., 2019]</ref> or attention ), to encode all AU descriptors as a single representation which reflects the underlying relationship among all AUs. A key drawback of such solutions is that they fail to individually model the relationship between each pair of AUs, which may contain crucial cues for their recognition (Problem 1). Some studies represent all AUs of the target face as a graph, where each AU is represented as a node, and each pair of AUs relationship is specifically described by an edge that contains a binary value or a single weight to describe their connectivity or association <ref type="bibr">[Song et al., 2021b;</ref><ref type="bibr">Song et al., 2021c]</ref>. However, a single value may not be enough to represent the complex underlying relationship between a pair of AUs (Problem 2). In particular, some studies <ref type="bibr" target="#b2">[Li et al., 2019a;</ref><ref type="bibr" target="#b2">Liu et al., 2020]</ref> even manually define a single graph topology for all face images based on prior knowledge (e.g., AUs co-occurrence pattern), which fails to consider the influences of the unique facial display on AU relationships (Problem 3).</p><p>In this paper, we propose a novel AUs relationship modelling approach to address the problems described above, which can be easily incorporated with various deep learning backbones. It takes a full face representation produced by the backbone as the input, and outputs an AUs relation graph that explicitly describes the relationship between each pair of AUs (addressing problem 1). Specifically, our approach consists of two modules: (i) the AUs relationship-aware node feature learning (ANFL) module first individually learns a representation for each AU from the input full face representation (Sec. 2.1), which encodes not only the AU's activation status but also its association with other AUs; and then (ii) the multi-dimensional edge feature learning (MEFL) module learns multiple task-specific relationship cues as the edge representation for each pair of AUs (Sec. 2.2) (addressing problem 2). Since both node and edge feature learning take the full face representation as the input, the influence of the unique facial display on AUs relationship is considered when generating its AUs relation graph (addressing problem 3).</p><p>In summary, the main contributions of our AU relationship modelling approach are that it represents AU relationships as a unique graph for each facial display, which (i) encodes both the activation status of the AU and its association with other AUs into each node feature; and (ii) learns a multi-dimensional edge feature to explicitly capture the taskspecific relationship cues between each pair of AUs. Our multi-dimensional edge encodes unique and multiple relationships between each pair of AUs, rather than a single relationship (e.g., spatial adjacency, co-occurrence patterns, etc.) that the single value-edge encoded, which would theoretically generalizes better in modeling complex relationships between vertices <ref type="bibr">[Gong and Cheng, 2019;</ref><ref type="bibr" target="#b6">Song et al., 2021a;</ref><ref type="bibr">Shao et al., 2021b]</ref>. The main novelty of the proposed approach in comparison to pre-defined AU graphs <ref type="bibr" target="#b2">[Li et al., 2019a;</ref><ref type="bibr" target="#b2">Liu et al., 2020]</ref> and deep learned facial displayspecific graphs <ref type="bibr">[Song et al., 2021b;</ref><ref type="bibr">Song et al., 2021c]</ref> are illustrated in <ref type="figure">Figure 1</ref>. To the best of our knowledge, this is the first CNN-GCN approach that conducts end-to-end multidimensional edge feature learning for face image processing tasks. The pipeline of the proposed approach is illustrated in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposed Approach</head><p>Our AU relationship modelling approach deep learns a unique AU relation graph from the representation of the target face, which explicitly captures recognition-related relationship cues among AUs based on the end-to-end learned relationship modelling modules. The learned AU relation graph represents the i th AU as the node v i ? V in the graph, which contains a vector describing the activation status of the i th AU as well as its association with other AUs in the target facial display. Besides, the task-specific relationship cues between nodes (AUs) v i and v j are also explicitly described by two directed edges e i,j , e j,i ? E that are represented by two deep learned vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">AUs Relationship-aware Node Feature Learning</head><p>As illustrated in <ref type="figure">Figure 2</ref>, the ANFL module consists of two blocks: an AU-specific Feature Generator (AFG) and a Facial Graph Generator (FGG). The AFG individually generates a representation for each AU, based on which the FGG automatically designs an optimal graph for each facial display, aiming to accurately recognize all target AUs. To achieve this, the FGG would enforce the AFG to encode task-specific associations among AUs into their AU-specific representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AU-specific Feature Generator</head><p>The AFG is made up of N AU-specific feature extractors, each of which contains a fully connected layer (FC) and a global average pooling (GAP) layer. It takes the full face representation X ? R D?C (C channels with D dimensions) as the input, which can be produced by any standard machine learning backbone. The FC layer of i th AU-specific feature extractor first projects the X to an AU-specific feature map U i ? R D?C , which is then fed to a GAP layer, yielding a vector containing C values as the i th AU's representation v i . Consequently, N AU representations can be learned from the full face representation X, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Facial Graph Generator</head><p>Our hypothesis is that the relationship cues among AUs are unique for each facial display. As a result, directly utilizing relationship cues defined in the training set (e.g., cooccurrence pattern) may not generalise well at the inference stage. As a result, we propose to represent AU relationships in each facial display as a unique graph which considers the influence of the target facial display on AUs relationship. For a face image, the FGG block treats N target AUs' feature vectors V = {v 1 , v 2 , ? ? ? , v N } as N node features and defines the connectivity (edge presence) between a pair of nodes v i and v j by their features' similarity (Sim(i, j) = v T i v j ). Specifically, we choose the K nearest neighbours of each node as its neighbours, and thus the graph topology is defined by the learned node features. Then, a GCN layer is employed to jointly update all AUs activation status from the produced graph, where the i th AU's activation representation v FGG i is generated by v i and its connected nodes as:  <ref type="figure">Figure 2</ref>: The pipeline of the proposed AU relationship modelling approach. It takes the full face representation X as the input, and the AFG block that is jointly trained with the FGG block, firstly provides a vector as a node feature to describe each AU's activation as well as its association with other AUs (Sec. 2.1). Then, the MEFL module learns a pair of vectors as multi-dimensional edge features to describe taskspecific relationship cues between each pair of AUs (Sec. 2.2). The AU relation graph produced by our approach is then fed to a GatedGCN for AU recognition. Only the modules and blocks contained within the blue dashed lines are used at the inference stage.</p><formula xml:id="formula_0">v FGG i = ?[v i + g(v i , N j=1 r(v j , a i,j ))],<label>(1)</label></formula><p>where ?[] is the non-linear activation; g and r denote differentiable functions of the GCN layer, and a i,j ? {0, 1} represents the connectivity between v i and v j .</p><p>To provide a prediction for the i th AU, we propose a similarity calculating (SC) strategy which learns a trainable vector s i that has the same dimension as the v FGG i , and then generates the i th AU's occurrence probability by computing the cosine similarity between v FGG i and s i as:</p><formula xml:id="formula_1">p FGG i = ReLU(v FGG i ) T ReLU(s i ) ReLU(v FGG i ) 2 ReLU(s i ) 2 ,<label>(2)</label></formula><p>where ReLU denotes a non-linearity activation. As a result, a pair of AUs that have a strong association (high similarity) would have connected nodes. In other words, the FGG block enforces the AFG block to encode node (AU) features that contain task-specific relationship cues among AUs of the target facial display, in order to produce an optimal graph for their recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-dimensional Edge Feature Learning</head><p>In addition to relationship cues encoded in node features, we also propose a Multi-dimensional Edge Feature Learning (MEFL) module to deep learn a pair of multi-dimensional edge features, aiming to explicitly describe task-specific relationship cues between each pair of AUs. Importantly, we learn edge features for both connected and un-connected node pairs defined in Sec. 2.1. Even when a pair of nodes have low similarity, their relationship may still contain crucial cues for AU recognition, which are ignored during the node feature learning. Since an AU's activation may also influence other AUs' status, the relationship between a pair of AUs can be reflected by not only their features but also AUs defined by other facial regions. Thus, the MEFL module consists of two blocks: a Facial display-specific AU representation modelling (FAM) block that first locates activation cues of each AU from the full face representation, and an AU relationship modelling (ARM) block that further extracts features from these located cues, which relate to both AUs activation. This is also illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. FAM. As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>, for a pair of AUs, the FAM takes their AU-specific feature maps U i , U j , and the full face representation X as the input. It first conducts cross attention between U i and X as well as U j and X, respectively, where AU-specific feature maps U i and U j are individually used as queries, while the full face representation X is treated as the key and value. This process can be formulated as:</p><formula xml:id="formula_2">F AS i,x , F AS j,x = FAM(U i , X), FAM(U j , X),<label>(3)</label></formula><p>with the cross attention operation in FAM defined as:</p><formula xml:id="formula_3">FAM(A, B) = softmax( AW q (BW k ) T ? d k )BW v ,<label>(4)</label></formula><p>where W q , W k and W v are learnable weights, and d k is a scaling factor equalling to the number of the 'key's' channels. As a result, the produced F AS i,x and F AS j,x extract and highlight the most important facial cues from all facial regions of the target facial display for AU i and AU j's recognition, respectively, which consider the influence of the unique facial display on AUs relationships. ARM. After encoding task-specific facial cues for each AU's recognition independently, the ARM block further extracts the facial cues related to both AUs' recognition. It also conducts the cross-attention (has the same form as Eq. 4 but independent weights) between F AS i,x and F AS j,x , and produces features F AR i,j,x and F AR j,i,x , where F AR i,j,x is generated by using F AS j,x as the query and F AS i,x as the key and value, while F AR j,i,x is generated by using F AS i,x as the query and F AS j,x as the key and value. As a result, the F AR i,j,x summarizes F AS j,x -related cues in the F AS i,x , and F AR j,i,x summarizes F AS i,x -related cues in the F AS j,x . Finally, we feed F AR i,j,x and F AR j,i,x to a GAP layer to obtain multi-dimensional edge feature vectors e i,j and e j,i , respectively. Mathematically speaking, this process can be represented as</p><formula xml:id="formula_4">e i,j , e j,i = GAP(ARM(F AS j,x , F AS i,x ), ARM(F AS i,x , F AS j,x )).</formula><p>(5) In short, the features encoded in edge features e i,j and e j,i summarize multiple facial cues that relate to both i th and j th AUs' recognition, from all facial regions of the target face.</p><p>Once the AUs relation graph G 0 = (V 0 , E 0 ) that consists of N node features and N ? N multi-dimensional directed edge features is learned, we feed it to a GCN model to jointly recognize all target AUs. In this paper, we use a model that only consists of L gated graph convolution layers (Gat-edGCN) <ref type="bibr" target="#b0">[Bresson and Laurent, 2017]</ref>, and thus the output</p><formula xml:id="formula_5">G L = (V L , E L )</formula><p>is also a graph that has the same topology as G 0 , where the i th node v L i represents the i th AU's activation status (L = 2 in this paper). We finally re-employ the SC module proposed in the FGG block to predict N AUs' activation from the node features of G L . During the inference stage, only the well-trained AFG and MEFL are used to process the input full face representation and generate the AU relation graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Strategy</head><p>In this paper, we propose a two-stage training method to jointly optimize the proposed ANFL and MEFL modules with the backbone and classifier in an end-to-end manner.</p><p>In the first stage, we train the backbone with the ANFL module, aiming to learn an AFG block that produces node features containing both AU activation status and their associations for each facial display. A priori, we notice that existing AU datasets usually have imbalanced labels, where some AUs occurred less frequently than others, and most AUs are inactivated for the majority of face images. To alleviate such issues, we propose a weighted asymmetric loss to compute the loss value between the ground-truth and predictions generated by the FGG block. It is inspired by the asymmetric loss <ref type="bibr">[Ridnik et al., 2021]</ref>, but has a unique weight for each subtask (each AU's recognition) as well as fewer hyperparameters. The proposed weighted asymmetric loss is formulated as:</p><formula xml:id="formula_6">L WA = ? 1 N N i=1 w i [y i log(p i ) + (1 ? y i )p i log(1 ? p i )],<label>(6)</label></formula><p>where p i , y i and w i are the prediction (occurrence probability), ground truth and weight of the i th AU, respectively. Here, the w i = N (1/r i )/? N j=1 (1/r j ) is defined by the i th AU's occurrence rate r i computed from the training set. It allows loss values to account less for AUs that have higher occurrence rates in the training set, leading loss values caused by less frequently occurring AUs to have higher weights during the training. Additionally, the term 'p i ' in the center of (1 ? y i )p i log(1 ? p i ) down weights loss values caused by inactivated AUs that are easy to be recognized, whose predicted occurrence probabilities are close to zero (p i 0.5), enforcing the training process to focus on activated AUs and inactivated AUs that are hard to be correctly recognized.</p><p>The second stage trains the MEFL module and classifier (GatedGCN) with the pre-trained backbone and AFG block. Here, we again employ the proposed weighted asymmetric loss (Eq. 6) to compute the loss value L WA between the outputs of the classifier and ground truth. Additionally, we also leverage the AUs co-occurrence patterns to supervise the training process. We feed multi-dimensional edge features e L i,j and e L j,i generated from the last GatedGCN layer to a shared FC layer, in order to predict the co-occurrence pattern of the i th and j th AUs of the target face. We define this task as a four-class classification problem, i.e., for a pair of nodes v i and v j : (1) both v i and v j are inactivated; (2) v i is inactivated and v j is activated; (3) v i is activated and v j is inactivated; or (4) both v i and v j are activated. As a result,  <ref type="bibr" target="#b1">[Li et al., 2018]</ref> 39.0 35.2 48.6 76.1 72.9 81.9 86.2 58.8 37.5 59.1 35.9 35.8 55.9 JAA-Net <ref type="bibr" target="#b4">[Shao et al., 2018]</ref> 47.2 44.0 54.9 77.5 74.6 84.0 86.9 61.9 43.6 60.3 42.7 41.9 60.0 LP-Net <ref type="bibr" target="#b3">[Niu et al., 2019]</ref> 43.4 38.0 54.  the categorical cross-entropy loss is introduced as:</p><formula xml:id="formula_7">L E = ? 1 |E| |E| i=1 N E j=1 y e i,j log( e p e i,j k e p e i,k ),<label>(7)</label></formula><p>where |E| denotes the number of edges in the facial graph; N E is the number of co-occurrence patterns; p e i,j is the cooccurrence prediction output from the shared FC layer. Consequently, The overall training loss of the second stage is formulated as the weighted combination of the two losses:</p><formula xml:id="formula_8">L = L WA + ?L E ,<label>(8)</label></formula><p>where ? decides the relative importance of the two losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Datasets. We evaluate the performance of our approach on two widely-used benchmark datasets: BP4D <ref type="bibr" target="#b8">[Zhang et al., 2014]</ref> and DISFA <ref type="bibr" target="#b2">[Mavadati et al., 2013]</ref>. BP4D recorded 328 videos (about 140,000 facial frames) from 41 young adults (23 females and 18 males) who were asked to respond to 8 emotion elicitation tasks. DISFA recorded 130, 815 frames from 27 subjects (12 females and 15 males) who were watching Youtube videos. Each frame in BP4D and DISFA is annotated with occurrence labels of multiple AUs.</p><p>Implementation Details. For both datasets, we use MTCNN [Yin and Liu, 2017] to perform face detection and alignment for each frame and crop it to 224 ? 224 as the input for backbones. We then follow the same protocol as previous studies <ref type="bibr" target="#b8">[Zhao et al., 2016;</ref><ref type="bibr" target="#b1">Li et al., 2018;</ref><ref type="bibr">Song et al., 2021c]</ref> to conduct subject-independent three folds cross-validation for each dataset, and report the average results over 3 folds. During the training, we employ an AdamW optimizer with ? 1 = 0.9, ? 2 = 0.999 and weight decay of 5e ?4 . The number K for choosing nearest neighbors in the FGG is set to 3 and 4 for BP4D and DISFA, respectively. For the hyperparameter ? in Eq. 8, we set it to 0.05 and 0.01 for models based on ResNet and Swin Transformer, respectively. We totally train the proposed model for 40 epochs, including 20 epochs for the first stage (the initial learning rate of 1e ?4 ) and 20 epochs for the second stage (the initial learning rate <ref type="figure">Figure 4</ref>: Visualization of association cues encoded in node features (only systems of the last two columns encode such cues). We connect each node to its K nearest neighbours, where nodes of activated AUs usually have more connections than nodes of inactivated AUs. Systems used such relationship cues have enhanced AU recognition results (predictions of the column 3 is better than the column 2).</p><p>of 1e ?6 ), with a batch size of 64. The cosine decay learning rate scheduler is also used. Both backbones are pre-trained on ImageNet <ref type="bibr" target="#b1">[Deng et al., 2009]</ref>.</p><p>Evaluation Metric. We follow previous AU occurrence recognition studies <ref type="bibr" target="#b0">Churamani et al., 2021;</ref><ref type="bibr" target="#b2">Li et al., 2019b;</ref><ref type="bibr">Song et al., 2021c]</ref> using a common metric: frame-based F1 score, to evaluate the performance of our approach, which is denoted as F 1 = 2 P ?R P +R . It takes the recognition precision P and recall rate R into consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Discussion</head><p>Comparison to State-of-the-art Methods. This section compares our best systems of two backbones with several state-of-the-art methods on both datasets. <ref type="table">Table 1</ref> reports the occurrence recognition results of 12 AUs on BP4D. We additionally provide the AUC results in Appendix A. It can be observed that the proposed AU relationship modelling approach allows both backbones (ResNet-50 and Swin Transformer-Base (Swin-B)) to achieve superior overall F1 scores than all other listed approaches, with 0.5% and 1.3% average improvements over the state-of-the-art <ref type="bibr">[Jacob and Stenger, 2021]</ref>. Specifically, our approach allows both backbones to achieve the top three performances for 9 out of 12 AUs' recognition (e.g., AU 4, AU 6, AU 7, AU 10, AU 12, AU 14, AU 15, AU 17, and AU 23) among all listed approaches. Similar results were also achieved on DISFA. According to <ref type="table" target="#tab_2">Table 2</ref>, our approach helps both backbones to achieve the state-of-the-art average F1 scores over 8 AUs, which outperform the current state-of-the-art with 1.6% and 0.9% improvements, respectively. For fair comparisons, we only compare our approach with static face-based methods that did not remove any frame from the datasets.</p><p>According to both tables, our ResNet-50-based system also clearly outperforms other graph-based AU recognition approaches which also use CNNs (ResNet <ref type="figure">(UGN-B</ref>, HMP-PS) or VGG (SRERL)) as backbones. Since SRERL only uses a pre-computed adjacent matrix to describe the relationship between AUs for all faces, our system shows a large advan-  <ref type="table">Table 3</ref>: Average AU recognition results (F1 scores (in %)) achieved by various settings using two backbones on the BP4D. The systems of the first two rows are trained with widely-used weighted binary cross-entropy loss.</p><p>tage over it, with 1.8% and 7.2% F1 score improvements for the average results on BP4D and DISFA, respectively. Although UGN-B and HMP-PS assigned each facial display a unique adjacent matrix and achieved better performance than SRERL, they still use a single value to describe the relationship between each pair of AUs, without considering multiple relationship cues. Thus, our deep-learned task-specific multi-dimensional edge features lead our system to achieve more than 1.3% and 2.1% average F1 score improvements over UGN-B and HMP-PS on both datasets.</p><p>Ablation Studies. <ref type="table">Table 3</ref> evaluates the influence of each component of our pipeline on the average AU recognition results. It can be observed that simply using the AFG block to specifically learn a representation for each AU enhanced the performance for both backbones, indicating that the relationship between each AU's activation and the full face representation is unique. In particular, when a facial AU is activated, its movement usually affects other facial regions (i.e., the activation of other AUs) while inactivated AUs would not have such an effect. As visualized in <ref type="figure">Figure 4</ref>, our FGG simulates this phenomenon by connecting activated AUs to all other AUs (including activated and inactivated AUs). Building on the backbone-AFG system, we also found that individually adding the FGG block or MEFL module further increased the recognition performance for both backbones. These results suggest that (i) the FGG block allows the AFG block to encode additional AU recognition-related cues into node features, i.e., we hypothesize that the FGG can help the AFG to learn AUs' relationship cues for their recognition; and (ii) the multi-dimensional edge features learned by the MEFL module provide more task-specific AU relationship cues to improve the recognition performance, which further validates our hypothesis that a single value is not enough to carry all useful relationship cues between a pair of AUs. In short, the proposed approach can provide valuable relationship cues for AU recognition during both node and edge feature learning. More importantly, jointly using FGG and MEFL with our weighted asymmetric loss largely boosted both backbones' recognition capabilities, i.e., 5.6% and 2.9% F1 score improvements over the original backbones, as well as 1.7% and 0.9% improvements over the backbone-AFG systems. Besides the proposed relationship modelling approaches, we show that the two loss functions also positively improved the recognition performance. The weighted asymmetric loss clearly enhanced the performance over the widely-used weighted binary cross-entropy loss, illustrating its superiority in alleviating data imbalance issue. Meanwhile, the proposed AU co-occurrence supervision also slightly enhanced recognition results for both backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper proposes to deep learn a graph that explicitly represents relationship cues between each pair of AUs for each facial display. These relationship cues are encoded in both node features and multi-dimensional edge features of the graph. The results demonstrate that the proposed node and edge feature learning methods extracted reliable task-specific relationship cues for AU recognition, i.e., both CNN and transformer-based backbones have been largely enhanced, and achieved state-of-the-art results on two widely used datasets. Since our graph-based relationship modelling approach can be easily incorporated with standard CNN/transformer backbones, it can be directly applied to enhance the performance of multi-label tasks or tasks whose data contains multiple objects, by explicitly exploring the task-specific relationship cues among labels or objects.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Experimental Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the MEFL module. The FAM first independently locates activation cues related to i th and j th AU-specific feature maps Ui and Uj in the full face representation X (activated face areas are depicted in red and yellow). Then, the ARM further extracts cues related to both Ui and Uj (depicted in white), based on which multi-dimensional edge features ei,j and ej,i are produced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Zhao et al., 2016] 36.4 41.8 43.0 55.0 67.0 66.3 65.8 54.1 33.2 48.0 31.7 30.0 48.3 EAC-Net</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Avg.</cell></row><row><cell>1</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>7</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>15</cell><cell>17</cell><cell>23</cell><cell>24</cell></row><row><cell>DRML [</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2 77.1 76.7 83.8 87.2 63.3 45.3 60.5 48.1 54.2 61.0 ARL [Shao et al., 2019] 45.8 39.8 55.1 75.7 77.2 82.3 86.6 58.8 47.6 62.1 47.4 [55.4] 61.1 SEV-Net [Yang et al.F1 scores (in %) achieved for 12 AUs on BP4D dataset, where the three methods (SRERL,UGN-Band HMP-PS) listed in the middle of the table are also built with graphs. The best, second best, and third best results of each column are indicated with brackets and bold font, brackets alone, and underline, respectively.</figDesc><table><row><cell>, 2021]</cell><cell cols="8">[58.2] [50.4] 58.3 [81.9] 73.9 [87.8] 87.5 61.6 [52.6] 62.2 44.6 47.6 63.9</cell></row><row><cell cols="9">FAUDT [Jacob and Stenger, 2021] 51.7 [49.3] [61.0] 77.8 79.5 82.9 86.3 [67.6] 51.9 63.0 43.7 [56.3] 64.2</cell></row><row><cell>SRERL [Li et al., 2019a]</cell><cell cols="8">46.9 45.3 55.6 77.1 78.4 83.5 87.6 63.9 52.2 [63.9] 47.1 53.3 62.9</cell></row><row><cell>UGN-B [Song et al., 2021b]</cell><cell cols="8">[54.2] 46.4 56.8 76.2 76.7 82.4 86.1 64.7 51.2 63.1 48.5 53.6 63.3</cell></row><row><cell>HMP-PS [Song et al., 2021c]</cell><cell cols="8">53.1 46.1 56.0 76.5 76.9 82.1 86.4 64.8 51.5 63.0 [49.9] 54.5 63.4</cell></row><row><cell>Ours (ResNet-50)</cell><cell cols="8">53.7 46.9 59.0 78.5 [80.0] 84.4 [87.8] 67.3 52.5 63.2 50.6 52.4 [64.7]</cell></row><row><cell>Ours (Swin Transformer-Base)</cell><cell cols="8">52.7 44.3 [60.9] [79.9] [80.1] [85.3] [89.2] [69.4] [55.4] [64.4] 49.8 55.1 [65.5]</cell></row><row><cell>Table 1: Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AU</cell><cell></cell><cell></cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>9</cell><cell>12</cell><cell>25</cell><cell>26</cell></row><row><cell cols="2">DRML [Zhao et al., 2016]</cell><cell cols="7">17.3 17.7 37.4 29.0 10.7 37.7 38.5 20.1 26.7</cell></row><row><cell cols="2">EAC-Net [Li et al., 2018]</cell><cell cols="7">41.5 26.4 66.4 50.7 [80.5] [89.3] 88.9 15.6 48.5</cell></row><row><cell cols="2">JAA-Net [Shao et al., 2018]</cell><cell cols="7">43.7 46.2 56.0 41.4 44.7 69.6 88.3 58.4 56.0</cell></row><row><cell cols="2">LP-Net [Niu et al., 2019]</cell><cell cols="7">29.9 24.7 72.7 46.8 49.6 72.9 93.8 65.0 56.9</cell></row><row><cell cols="2">ARL [Shao et al., 2019]</cell><cell cols="7">43.9 42.1 63.6 41.8 40.0 76.2 [95.2] [66.8] 58.7</cell></row><row><cell cols="2">SEV-Net [Yang et al., 2021]</cell><cell cols="7">[55.3] [53.1] 61.5 53.6 38.2 71.6 [95.7] 41.5 58.8</cell></row><row><cell cols="9">FAUDT [Jacob and Stenger, 2021] 46.1 [48.6] 72.8 [56.7] 50.0 72.1 90.8 55.4 61.5</cell></row><row><cell cols="2">SRERL [Li et al., 2019a]</cell><cell cols="7">45.7 47.8 59.6 47.1 45.6 73.5 84.3 43.6 55.9</cell></row><row><cell cols="2">UGN-B [Song et al., 2021b]</cell><cell cols="7">43.3 48.1 63.4 49.5 48.2 72.9 90.8 59.0 60.0</cell></row><row><cell cols="2">HMP-PS [Song et al., 2021c]</cell><cell cols="7">38.0 45.9 65.2 50.9 50.8 76.0 93.3 [67.6] 61.0</cell></row><row><cell>Ours (ResNet-50)</cell><cell></cell><cell cols="7">[54.6] 47.1 [72.9] [54.0] [55.7] [76.7] 91.1 53.0 [63.1]</cell></row><row><cell cols="2">Ours (Swin Transformer-Base)</cell><cell cols="7">52.5 45.7 [76.1] 51.8 46.5 76.1 92.9 57.6 [62.4]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>F1 scores (in %) achieved for 8 AUs on DISFA dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>AUC results achieved for 12 AUs on BP4D dataset.</figDesc><table><row><cell>AU</cell><cell cols="4">DRML SRERL Ours (Res) Ours (Swin)</cell></row><row><cell>1</cell><cell>53.3</cell><cell>76.2</cell><cell>90.0</cell><cell>88.6</cell></row><row><cell>2</cell><cell>53.2</cell><cell>80.9</cell><cell>88.5</cell><cell>89.0</cell></row><row><cell>4</cell><cell>60.0</cell><cell>79.1</cell><cell>94.2</cell><cell>92.3</cell></row><row><cell>6</cell><cell>54.9</cell><cell>80.4</cell><cell>92.5</cell><cell>91.5</cell></row><row><cell>9</cell><cell>51.5</cell><cell>76.5</cell><cell>91.5</cell><cell>92.7</cell></row><row><cell>12</cell><cell>54.6</cell><cell>87.9</cell><cell>95.9</cell><cell>95.1</cell></row><row><cell>25</cell><cell>45.6</cell><cell>90.9</cell><cell>99.1</cell><cell>98.5</cell></row><row><cell>26</cell><cell>45.3</cell><cell>73.4</cell><cell>91.2</cell><cell>89.4</cell></row><row><cell>Avg.</cell><cell>52.3</cell><cell>80.7</cell><cell>92.9</cell><cell>92.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>AUC results achieved for 8 AUs on DISFA dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Aula-caps: Lifecycle-aware capsule networks for spatio-temporal analysis of facial actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Churamani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>Ciprian Corneanu</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="298" to="313" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the European Conference on Computer Vision (ECCV)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Facial action coding system: a technique for the measurement of facial movement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<editor>Jacob and Stenger, 2021] Geethu Miriam Jacob and Bjorn Stenger</editor>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Palo Alto</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2583" to="2596" />
		</imprint>
	</monogr>
	<note>Eac-net: Deep nets with enhancing and cropping for facial action unit detection</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic relationships guided representation learning for facial action unit recognition</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
	<note>IEEE Transactions on Affective Computing</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Local relationship learning with person-specific shape regularization for facial action unit detection</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Tal Ridnik, Emanuel Ben-Baruch</addrLine></address></meeting>
		<imprint>
			<publisher>Asaf Noy</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="82" to="91" />
		</imprint>
	</monogr>
	<note type="report_type">Nadav Zamir</note>
	<note>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep adaptive attention for joint facial action unit detection and face alignment</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="705" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Siyang Song, Shashank Jaiswal, Linlin Shen, Michel Valstar, and Hatice Gunes. Personality recognition by modelling person-specific cognitive processes using graph representation</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
	<note>Jaa-net: joint facial action unit detection and face alignment via adaptive attention</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning graph representation of person-specific cognitive processes from audio-visual behaviours for automatic personality recognition</title>
		<idno type="arXiv">arXiv:2110.13570</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<editor>Song et al., 2021b] Tengfei Song, Lisha Chen, Wenming Zheng, and Qiang Ji</editor>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>Tengfei Song</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6267" to="6276" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting semantic embedding and visual feature for facial action unit detection</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="964" to="975" />
		</imprint>
	</monogr>
	<note>Multitask convolutional neural network for pose-invariant face recognition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Kaili Zhao, Wen-Sheng Chu, and Honggang Zhang. Deep region and multi-label learning for facial action unit detection</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="3391" to="3399" />
		</imprint>
	</monogr>
	<note>Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
