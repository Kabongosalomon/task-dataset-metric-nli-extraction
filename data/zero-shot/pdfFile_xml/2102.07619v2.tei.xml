<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models by Instance-Guided Mask</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sina Weibo Corp</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyun</forename><surname>She</surname></persName>
							<email>qingyun_she@163.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Sina Weibo Corp</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sina Weibo Corp</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models by Instance-Guided Mask</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<note>ACM Reference Format: Zhiqiang Wang, Qingyun She, Junlin Zhang. 2021. MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models by Instance-Guided Mask. In Proceedings of DLP-KDD 2021. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Click-Through Rate(CTR) estimation has become one of the most fundamental tasks in many real-world applications and it's important for ranking models to effectively capture complex high-order features. Shallow feed-forward network is widely used in many state-of-the-art DNN models such as FNN, DeepFM and xDeepFM to implicitly capture high-order feature interactions. However, some research has proved that addictive feature interaction, particular feed-forward neural networks, is inefficient in capturing common feature interaction. To resolve this problem, we introduce specific multiplicative operation into DNN ranking system by proposing instance-guided mask which performs element-wise product both on the feature embedding and feed-forward layers guided by input instance. We also turn the feed-forward layer in DNN model into a mixture of addictive and multiplicative feature interactions by proposing MaskBlock in this paper. MaskBlock combines the layer normalization, instance-guided mask, and feed-forward layer and it is a basic building block to be used to design new ranking model under various configurations. The model consisting of MaskBlock is called MaskNet in this paper and two new MaskNet models are proposed to show the effectiveness of MaskBlock as basic building block for composing high performance ranking systems. The experiment results on three real-world datasets demonstrate that our proposed MaskNet models outperform state-of-the-art models such as DeepFM and xDeepFM significantly, which implies MaskBlock is an effective basic building unit for composing new high performance ranking systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Click-through rate (CTR) prediction is to predict the probability of a user clicking on the recommended items. It plays important role in personalized advertising and recommender systems. Many models Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. DLP-KDD 2021, August 15, 2021, Singapore ? 2021 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn have been proposed to resolve this problem such as Logistic Regression (LR) <ref type="bibr" target="#b16">[16]</ref>, Polynomial-2 (Poly2) <ref type="bibr" target="#b17">[17]</ref>, tree-based models <ref type="bibr" target="#b7">[7]</ref>, tensor-based models <ref type="bibr" target="#b12">[12]</ref>, Bayesian models <ref type="bibr" target="#b5">[5]</ref>, and Field-aware Factorization Machines (FFMs) <ref type="bibr" target="#b11">[11]</ref>. In recent years, employing DNNs for CTR estimation has also been a research trend in this field and some deep learning based models have been introduced such as Factorization-Machine Supported Neural Networks(FNN) <ref type="bibr" target="#b24">[24]</ref>, Attentional Factorization Machine (AFM) <ref type="bibr" target="#b2">[3]</ref>, wide &amp; deep(W&amp;D) <ref type="bibr" target="#b22">[22]</ref>, DeepFM <ref type="bibr" target="#b6">[6]</ref>, xDeepFM <ref type="bibr" target="#b13">[13]</ref> etc.</p><p>Feature interaction is critical for CTR tasks and it's important for ranking model to effectively capture these complex features. Most DNN ranking models such as FNN , W&amp;D, DeepFM and xDeepFM use the shallow MLP layers to model high-order interactions in an implicit way and it's an important component in current state-ofthe-art ranking systems.</p><p>However, Alex Beutel et.al <ref type="bibr" target="#b1">[2]</ref> have proved that addictive feature interaction, particular feed-forward neural networks, is inefficient in capturing common feature crosses. They proposed a simple but effective approach named "latent cross" which is a kind of multiplicative interactions between the context embedding and the neural network hidden states in RNN model. Recently, Rendle et.al's work <ref type="bibr" target="#b18">[18]</ref> also shows that a carefully configured dot product baseline largely outperforms the MLP layer in collaborative filtering. While a MLP can in theory approximate any function, they show that it is non-trivial to learn a dot product with an MLP and learning a dot product with high accuracy for a decently large embedding dimension requires a large model capacity as well as many training data. Their work also proves the inefficiency of MLP layer's ability to model complex feature interactions.</p><p>Inspired by "latent cross" <ref type="bibr" target="#b1">[2]</ref> and Rendle's work <ref type="bibr" target="#b18">[18]</ref>, we care about the following question: Can we improve the DNN ranking systems by introducing specific multiplicative operation into it to make it efficiently capture complex feature interactions?</p><p>In order to overcome the problem of inefficiency of feed-forward layer to capture complex feature cross, we introduce a special kind of multiplicative operation into DNN ranking system in this paper. First, we propose an instance-guided mask performing elementwise production on the feature embedding and feed-forward layer. The instance-guided mask utilizes the global information collected from input instance to dynamically highlight the informative elements in feature embedding and hidden layer in a unified manner. There are two main advantages for adopting the instance-guided mask: firstly, the element-wise product between the mask and hidden layer or feature embedding layer brings multiplicative operation into DNN ranking system in unified way to more efficiently capture complex feature interaction. Secondly, it's a kind of finegained bit-wise attention guided by input instance which can both weaken the influence of noise in feature embedding and MLP layers while highlight the informative signals in DNN ranking systems.</p><p>By combining instance-guided mask, a following feed-forward layer and layer normalization, MaskBlock is proposed by us to turn the commonly used feed-forward layer into a mixture of addictive and multiplicative feature interactions. The instance-guided mask introduces multiplicative interactions and the following feedforward hidden layer aggregates the masked information in order to better capture the important feature interactions. While the layer normalization can ease optimization of the network.</p><p>MaskBlock can be regarded as a basic building block to design new ranking models under some kinds of configuration. The model consisting of MaskBlock is called MaskNet in this paper and two new MaskNet models are proposed to show the effectiveness of MaskBlock as basic building block for composing high performance ranking systems.</p><p>The contributions of our work are summarized as follows: <ref type="bibr" target="#b0">(1)</ref> In this work, we propose an instance-guided mask performing element-wise product both on the feature embedding and feed-forward layers in DNN models. The global context information contained in the instance-guided mask is dynamically incorporated into the feature embedding and feed-forward layer to highlight the important elements. (2) We propose a basic building block named MaskBlock which consists of three key components: instance-guided mask, a following feed-forward hidden layer and layer normalization module. In this way, we turn the widely used feed-forward layer of a standard DNN model into a mixture of addictive and multiplicative feature interactions. (3) We also propose a new ranking framework named MaskNet to compose new ranking system by utilizing MaskBlock as basic building unit. To be more specific, the serial MaskNet model and parallel MaskNet model are designed based on the MaskBlock in this paper. The serial rank model stacks MaskBlock block by block while the parallel rank model puts many MaskBlocks in parallel on a sharing feature embedding layer. (4) Extensive experiments are conduct on three real-world datasets and the experiment results demonstrate that our proposed two MaskNet models outperform state-of-the-art models significantly. The results imply MaskBlock indeed enhance DNN model's ability of capturing complex feature interactions through introducing multiplicative operation into DNN models by instance-guided mask. The rest of this paper is organized as follows. Section 2 introduces some related works which are relevant with our proposed model. We introduce our proposed models in detail in Section 3. The experimental results on three real world datasets are presented and discussed in Section 4. Section 5 concludes our work in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 State-Of-The-Art CTR Models</head><p>Many deep learning based CTR models have been proposed in recent years and it is the key factor for most of these neural network based models to effectively model the feature interactions. <ref type="bibr" target="#b24">[24]</ref> is a feed-forward neural network using FM to pre-train the embedding layer. Wide &amp; Deep Learning <ref type="bibr" target="#b22">[22]</ref> jointly trains wide linear models and deep neural networks to combine the benefits of memorization and generalization for recommender systems. However, expertise feature engineering is still needed on the input to the wide part of Wide &amp; Deep model. To alleviate manual efforts in feature engineering, DeepFM <ref type="bibr" target="#b6">[6]</ref> replaces the wide part of Wide &amp; Deep model with FM and shares the feature embedding between the FM and deep component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factorization-Machine Supported Neural Networks (FNN)</head><p>While most DNN ranking models process high-order feature interactions by MLP layers in implicit way, some works explicitly introduce high-order feature interactions by sub-network. Deep &amp; Cross Network (DCN) <ref type="bibr" target="#b21">[21]</ref> efficiently captures feature interactions of bounded degrees in an explicit fashion. Similarly, eXtreme Deep Factorization Machine (xDeepFM) <ref type="bibr" target="#b13">[13]</ref> also models the loworder and high-order feature interactions in an explicit way by proposing a novel Compressed Interaction Network (CIN) part. AutoInt <ref type="bibr" target="#b19">[19]</ref> uses a multi-head self-attentive neural network to explicitly model the feature interactions in the low-dimensional space. FiBiNET <ref type="bibr" target="#b9">[9]</ref> can dynamically learn feature importance via the Squeeze-Excitation network (SENET) mechanism and feature interactions via bilinear function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature-Wise Mask Or Gating</head><p>Feature-wise mask or gating has been explored widely in vision <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b20">20]</ref>, natural language processing <ref type="bibr" target="#b4">[4]</ref> and recommendation system <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref>. For examples, Highway Networks <ref type="bibr" target="#b20">[20]</ref> utilize feature gating to ease gradient-based training of very deep networks. Squeezeand-Excitation Networks <ref type="bibr" target="#b8">[8]</ref> recalibrate feature responses by explicitly multiplying each channel with learned sigmoidal mask values. Dauphin et al. <ref type="bibr" target="#b4">[4]</ref> proposed gated linear unit (GLU) to utilize it to control what information should be propagated for predicting the next word in the language modeling task. Gating or mask mechanisms are also adopted in recommendation systems. Ma et al. <ref type="bibr" target="#b15">[15]</ref> propose a novel multi-task learning approach, Multi-gate Mixture-of-Experts (MMoE), which explicitly learns to model task relationships from data. Ma et al. <ref type="bibr" target="#b14">[14]</ref> propose a hierarchical gating network (HGN) to capture both the long-term and short-term user interests. The feature gating and instance gating modules in HGN select what item features can be passed to the downstream layers from the feature and instance levels, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Normalization</head><p>Normalization techniques have been recognized as very effective components in deep learning. Many normalization approaches have been proposed with the two most popular ones being BatchNorm <ref type="bibr" target="#b10">[10]</ref> and LayerNorm <ref type="bibr" target="#b0">[1]</ref> . Batch Normalization (Batch Norm or BN) <ref type="bibr" target="#b10">[10]</ref> normalizes the features by the mean and variance computed within a mini-batch. Another example is layer normalization (Layer Norm or LN) <ref type="bibr" target="#b0">[1]</ref> which was proposed to ease optimization of recurrent neural networks. Statistics of layer normalization are not computed across the samples in a mini-batch but are estimated in a layer-wise manner for each sample independently. Normalization methods have shown success in accelerating the training of deep networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR PROPOSED MODEL</head><p>In this section, we first describe the feature embedding layer. Then the details of the instance-guided mask, MaskBlock and MaskNet structure we proposed will be introduced. Finally the log loss as a loss function is introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding Layer</head><p>The input data of CTR tasks usually consists of sparse and dense features and the sparse features are mostly categorical type. Such features are encoded as one-hot vectors which often lead to excessively high-dimensional feature spaces for large vocabularies. The common solution to this problem is to introduce the embedding layer. Generally, the sparse input can be formulated as:</p><formula xml:id="formula_0">= [ 1 , 2 , ..., ]<label>(1)</label></formula><p>where denotes the number of fields, and ? R denotes a onehot vector for a categorical field with features and ? R is vector with only one value for a numerical field. We can obtain feature embedding for one-hot vector via:</p><formula xml:id="formula_1">= (2)</formula><p>where ? R ? is the embedding matrix of features and is the dimension of field embedding. The numerical feature can also be converted into the same low-dimensional space by:</p><formula xml:id="formula_2">= (3)</formula><p>where ? R is the corresponding field embedding with size . Through the aforementioned method, an embedding layer is applied upon the raw feature input to compress it to a low dimensional, dense real-value vector. The result of embedding layer is a wide concatenated vector:</p><formula xml:id="formula_3">V = (e 1 , e 2 , ..., e , ..., e )<label>(4)</label></formula><p>where denotes the number of fields, and e ? R denotes the embedding of one field. Although the feature lengths of input instances can be various, their embedding are of the same length ? , where is the dimension of field embedding. We use instance-guided mask to introduce the multiplicative operation into DNN ranking system and here the so-called "instance" means the feature embedding layer of current input instance in the following part of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Instance-Guided Mask</head><p>We utilize the global information collected from input instance by instance-guided mask to dynamically highlight the informative elements in feature embedding and feed-forward layer. For feature embedding, the mask lays stress on the key elements with more information to effectively represent this feature. For the neurons in hidden layer, the mask helps those important feature interactions to stand out by considering the contextual information in the input instance. In addition to this advantage, the instance-guided mask also introduces the multiplicative operation into DNN ranking system to capture complex feature cross more efficiently.</p><p>As depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, two fully connected (FC) layers with identity function are used in instance-guided mask. Notice that the input of instance-guided mask is always from the input instance, that is to say, the feature embedding layer.</p><p>The first FC layer is called "aggregation layer" and it is a relatively wider layer compared with the second FC layer in order to better collect the global contextual information in input instance. The aggregation layer has parameters 1 and here denotes the -th mask. For feature embedding and different MLP layers, we adopt different instance-guided mask owning its parameters to learn to capture various information for each layer from input instance.</p><p>The second FC layer named "projection layer" reduces dimensionality to the same size as feature embedding layer or hidden layer ? with parameters 2 , Formally,</p><formula xml:id="formula_4">= 2 ( ( 1 + 1 )) + 2<label>(5)</label></formula><p>where ? R = ? refers to the embedding layer of input instance, 1 ? R ? and 2 ? R ? are parameters for instanceguided mask, and respectively denotes the neural number of aggregation layer and projection layer, denotes the number of fields and is the dimension of field embedding. 1 ? R ? and 2 ? R ? are learned bias of the two FC layers. Notice here that the aggregation layer is usually wider than the projection layer because the size of the projection layer is required to be equal to the size of feature embedding layer or MLP layer. So we define the size = / as reduction ratio which is a hyper-parameter to control the ratio of neuron numbers of two layers.</p><p>Element-wise product is used in this work to incorporate the global contextual information aggregated by instance-guided mask into feature embedding or hidden layer as following:</p><formula xml:id="formula_5">V = V ? V V = V ? V ?<label>(6)</label></formula><p>where V denotes embedding layer and V ? means the feedforward layer in DNN model, ? means the element-wise production between two vectors as follows:</p><formula xml:id="formula_6">? = [ 1 ? 1 , 2 ? 2 , ..., ? ]<label>(7)</label></formula><p>here is the size of vector and The instance-guided mask can be regarded as a special kind of bitwise attention or gating mechanism which uses the global context information contained in input instance to guide the parameter optimization during training. The bigger value in implies that the model dynamically identifies an important element in feature embedding or hidden layer. It is used to boost the element in vector or ? . On the contrary, small value in will suppress the uninformative elements or even noise by decreasing the values in the corresponding vector or ? . The two main advantages in adopting the instance-guided mask are: firstly, the element-wise product between the mask and hidden layer or feature embedding layer brings multiplicative operation into DNN ranking system in unified way to more efficiently capture complex feature interaction. Secondly, this kind of fine-gained bit-wise attention guided by input instance can both weaken the influence of noise in feature embedding and MLP layers while highlight the informative signals in DNN ranking systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MaskBlock</head><p>To overcome the problem of the inefficiency of feed-forward layer to capture complex feature interaction in DNN models, we propose a basic building block named MaskBlock for DNN ranking systems in this work, as shown in <ref type="figure">Figure 2</ref> and <ref type="figure">Figure 3</ref>. The proposed MaskBlock consists of three key components: layer normalization module ,instance-guided mask, and a feed-forward hidden layer. The layer normalization can ease optimization of the network. The instance-guided mask introduces multiplicative interactions for feed-forward layer of a standard DNN model and feed-forward hidden layer aggregate the masked information in order to better capture the important feature interactions. In this way, we turn the widely used feed-forward layer of a standard DNN model into a mixture of addictive and multiplicative feature interactions.</p><p>First, we briefly review the formulation of LayerNorm. Layer Normalization:</p><p>In general, normalization aims to ensure that signals have zero mean and unit variance as they propagate through a network to reduce "covariate shift" <ref type="bibr" target="#b10">[10]</ref>. As an example, layer normalization (Layer Norm or LN) <ref type="bibr" target="#b0">[1]</ref> was proposed to ease optimization of recurrent neural networks. Specifically, let = ( 1 , 2 , ..., ) denotes the vector representation of an input of size to normalization layers. LayerNorm re-centers and re-scales input x as</p><formula xml:id="formula_7">h = g ? (x) + b, (x) = x ? , = 1 ?? =1 , = 1 ?? =1 ( ? ) 2<label>(8)</label></formula><p>where ? is the output of a LayerNorm layer. ? is an element-wise production operation. and are the mean and standard deviation of input. Bias b and gain g are parameters with the same dimension .</p><p>As one of the key component in MaskBlock, layer normalization can be used on both feature embedding and feed-forward layer. For the feature embedding layer, we regard each feature's embedding as a layer to compute the mean, standard deviation, bias and gain of LN as follows:</p><formula xml:id="formula_8">_ (V ) = (e 1 )</formula><p>, (e 2 ), ..., (e ), ..., (e ) (9) As for the feed-forward layer in DNN model, the statistics of are estimated among neurons contained in the corresponding hidden layer as follows: where X ? R refers to the input of feed-forward layer, W ? R ? are parameters for the layer, and respectively denotes the size of input layer and neural number of feed-forward layer. Notice that we have two places to put normalization operation on the MLP: one place is before non-linear operation and another place is after non-linear operation. We find the performance of the normalization before non-linear consistently outperforms that of the normalization after non-linear operation. So all the normalization used in MLP part is put before non-linear operation in our paper as formula <ref type="formula" target="#formula_3">(4)</ref> shows.</p><formula xml:id="formula_9">_ (V ? ) = ( (W X))<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MaskBlock on Feature Embedding:</head><p>We propose MaskBlock by combining the three key elements: layer normalization, instance-guided mask and a following feed-forward layer. MaskBlock can be stacked to form deeper network. According to the different input of each MaskBlock, we have two kinds of MaskBlocks: MaskBlock on feature embedding and MaskBlock on Maskblock. We will firstly introduce the MaskBlock on feature embedding as depicted in <ref type="figure">Figure 2</ref> in this subsection. The feature embedding V is the only input for MaskBlock on feature embedding. After the layer normalization operation on embedding V . MaskBlock utilizes instance-guided mask to highlight the informative elements in V by element-wise product, Formally,</p><formula xml:id="formula_10">V = V ? _ (V )<label>(11)</label></formula><p>where ? means an element-wise production between the instanceguided mask and the normalized vector (V ), V denote the masked feature embedding. Notice that the input of instance-guided mask V is also the feature embedding .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: Structure of Serial Model and Parallel Model</head><p>We introduce a feed-forward hidden layer and a following layer normalization operation in MaskBlock to better aggregate the masked information by a normalized non-linear transformation. The output of MaskBlock can be calculated as follows:</p><formula xml:id="formula_11">V = _ ( ) = ( (W (V ? _ (V ))))<label>(12)</label></formula><p>where W ? R ? are parameters of the feed-forward layer in the -th MaskBlock, denotes the size of V and means the size of neural number of the feed-forward layer.</p><p>The instance-guided mask introduces the element-wise product into feature embedding as a fine-grained attention while normalization both on feature embedding and hidden layer eases the network optimization. These key components in MaskBlock help the feedforward layer capture complex feature cross more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MaskBlock on MaskBlock:</head><p>In this subsection, we will introduce MaskBlock on MaskBlock as depicted in <ref type="figure">Figure 3</ref>. There are two different inputs for this MaskBlock: feature embedding and the output of the previous MaskBlock. The input of instance-guided mask for this kind of MaskBlock is always the feature embedding . MaskBlock utilizes instance-guided mask to highlight the important feature interactions in previous MaskBlock's output by element-wise product, Formally,</p><formula xml:id="formula_12">= ?<label>(13)</label></formula><p>where ? means an element-wise production between the instanceguided mask and the previous MaskBlock's output , denote the masked hidden layer. In order to better capture the important feature interactions, another feed-forward hidden layer and a following layer normalization are introduced in MaskBlock . In this way, we turn the widely used feed-forward layer of a standard DNN model into a mixture of addictive and multiplicative feature interactions to avoid the ineffectiveness of those addictive feature cross models. The output of MaskBlock can be calculated as follows:</p><formula xml:id="formula_13">V = _ ( ) = ( (W (V ? V )))<label>(14)</label></formula><p>where ? R ? are parameters of the feed-forward layer in the -th MaskBlock, denotes the size of V and means the size of neural number of the feed-forward layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">MaskNet</head><p>Based on the MaskBlock, various new ranking models can be designed according to different configurations. The rank model consisting of MaskBlock is called MaskNet in this work. We also propose two MaskNet models by utilizing the MaskBlock as the basic building block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Serial MaskNet:</head><p>We can stack one MaskBlock after another to build the ranking system , as shown by the left model in <ref type="figure">Figure 4</ref>. The first block is a MaskBlock on feature embedding and all other blocks are MaskBlock on Maskblock to form a deeper network. The prediction layer is put on the final MaskBlock's output vector. We call MaskNet under this serial configuration as SerMaskNet in our paper. All inputs of instance-guided mask in every MaskBlock come from the feature embedding layer V and this makes the serial MaskNet model look like a RNN model with sharing input at each time step. Parallel MaskNet: We propose another MaskNet by placing several MaskBlocks on feature embedding in parallel on a sharing feature embedding layer, as depicted by the right model in <ref type="figure">Figure 4</ref>. The input of each block is only the shared feature embedding V under this configuration. We can regard this ranking model as a mixture of multiple experts just as MMoE <ref type="bibr" target="#b15">[15]</ref> does. Each MaskBlock pays attention to specific kind of important features or feature interactions. We collect the information of each expert by concatenating the output of each MaskBlock as follows:</p><formula xml:id="formula_14">V = (V 1 , V 2 , ..., V , ..., V ) (15) where V</formula><p>? R is the output of the -th MaskBlock and means size of neural number of feed-forward layer in MaskBlock, is the MaskBlock number. To further merge the feature interactions captured by each expert, multiple feed-forward layers are stacked on the concatenation information V</p><p>. Let H 0 = V denotes the output of the concatenation layer, then H 0 is fed into the deep neural network and the feed forward process is:</p><formula xml:id="formula_15">H = (W H ?1 + ) (16)</formula><p>where is the depth and ReLU is the activation function. W , , H are the model weight, bias and output of the -th layer. The prediction layer is put on the last layer of multiple feed-forward networks.</p><p>We call this version MaskNet as "ParaMaskNet" in the following part of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Prediction Layer</head><p>To summarize, we give the overall formulation of our proposed model' s output as:^=</p><formula xml:id="formula_16">( 0 + ?? =1 )<label>(17)</label></formula><p>where^? (0, 1) is the predicted value of CTR, is the sigmoid function, is the size of the last MaskBlock's output(SerMaskNet) or feed-forward layer(ParaMaskNet), is the bit value of feedforward layer and is the learned weight for each bit value. For binary classifications, the loss function is the log loss:</p><formula xml:id="formula_17">L = ? 1 ?? =1 log(^) + (1 ? ) log(1 ?^)<label>(18)</label></formula><p>where is the total number of training instances, is the ground truth of -th instance and^is the predicted CTR. The optimization process is to minimize the following objective function:</p><formula xml:id="formula_18">= L + ???<label>(19)</label></formula><p>where denotes the regularization term and ? denotes the set of parameters, including those in feature embedding matrix, instanceguided mask matrix, feed-forward layer in MaskBlock, and prediction part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>In this section, we evaluate the proposed approaches on three realworld datasets and conduct detailed ablation studies to answer the following research questions:</p><p>? RQ1 Does the proposed MaskNet model based on the MaskBlock perform better than existing state-of-the-art deep learning based CTR models? ? RQ2 What are the influences of various components in the MaskBlock architecture? Is each component necessary to build an effective ranking system? ? RQ3 How does the hyper-parameter of networks influence the performance of our proposed two MaskNet models? ? RQ4 Does instance-guided mask highlight the important elements in feature embedding and feed-forward layers according to the input instance? In the following, we will first describe the experimental settings, followed by answering the above research questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>4.1.1 Datasets. The following three data sets are used in our experiments:</p><p>(1) Criteo 1 Dataset: As a very famous public real world display ad dataset with each ad display information and corresponding user click feedback, Criteo data set is widely used in many CTR model evaluation. There are 26 anonymous categorical fields and 13 continuous feature fields in Criteo data set. (2) Malware 2 Dataset: Malware is a dataset from Kaggle competitions published in the Microsoft Malware prediction. The goal of this competition is to predict a Windows machine's probability of getting infected. The malware prediction task can be formulated as a binary classification problem like a typical CTR estimation task does. (3) Avazu 3 Dataset: The Avazu dataset consists of several days of ad click-through data which is ordered chronologically. For each click data, there are 23 fields which indicate elements of a single ad impression. We randomly split instances by 8 : 1 : 1 for training , validation and test while <ref type="table" target="#tab_0">Table 1</ref> lists the statistics of the evaluation datasets. RelaImp is also as work <ref type="bibr" target="#b23">[23]</ref> does to measure the relative AUC improvements over the corresponding baseline model as another evaluation metric. Since AUC is 0.5 from a random strategy, we can remove the constant part of the AUC score and formalize the RelaImp as:</p><formula xml:id="formula_19">= ( ) ? 0.5 ( ) ? 0.5 ? 1<label>(20)</label></formula><p>4.1.3 Models for Comparisons. We compare the performance of the following CTR estimation models with our proposed approaches: FM, DNN, DeepFM, Deep&amp;Cross Network(DCN), xDeepFM and Au-toInt Model, all of which are discussed in Section 2. FM is considered as the base model in evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Implementation Details.</head><p>We implement all the models with Tensorflow in our experiments. For optimization method, we use the Adam with a mini-batch size of 1024 and a learning rate is set to 0.0001. Focusing on neural networks structures in our paper, we make the dimension of field embedding for all models to be a fixed value of 10. For models with DNN part, the depth of hidden layers is set to 3, the number of neurons per layer is 400, all activation function is ReLU. For default settings in MaskBlock, the reduction ratio of instance-guided mask is set to 2. We conduct our experiments with 2 Tesla 40 GPUs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison (RQ1)</head><p>The overall performances of different models on three evaluation datasets are show in the <ref type="table" target="#tab_1">Table 2</ref>. From the experimental results, we can see that:</p><p>( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study of MaskBlock (RQ2)</head><p>In order to better understand the impact of each component in MaskBlock, we perform ablation experiments over key components of MaskBlock by only removing one of them to observe the performance change, including mask module, layer normalization(LN) and feed-forward network(FFN). <ref type="table" target="#tab_3">Table 3</ref> shows the results of our two full version MaskNet models and its variants removing only one component. From the results in <ref type="table" target="#tab_3">Table 3</ref>, we can see that removing either instance-guided mask or layer normalization will decrease model's performance and this implies that both the instance-guided mask and layer normalization are necessary components in MaskBlock for its effectiveness. As for the feed-forward layer in MaskBlock, its effect on serial model or parallel model shows difference. The Serial model's performance dramatically degrades while it seems do no harm to parallel model if we remove the feed-forward layer in MaskBlock. We deem this implies that the feed-forward layer in MaskBlock is important for merging the feature interaction information after instance-guided mask. For parallel model, the multiple feed-forward layers above parallel MaskBlocks have similar function as feed-forward layer in MaskBlock does and this may produce performance difference between two models when we remove this component.  <ref type="table" target="#tab_4">Table 4</ref> show the impact of the number of feature embedding size on model performance. It can be observed that the performances of both models increase when embedding size increases at the beginning. However, model performance degrades when the embedding size is set greater than 50 for SerMaskNet model and 30 for ParaMaskNet model. The experimental results tell us the models benefit from larger feature embedding size.    <ref type="table" target="#tab_5">Table 5</ref>. For Ser-MaskNet model, the performance increases with more blocks at the beginning until the number is set greater than 5. While the performance slowly increases when we continually add more MaskBlock into ParaMaskNet model. This may indicates that more experts boost the ParaMaskNet model's performance though it's more time consuming.</p><p>Reduction Ratio in Instance-Guided Mask. In order to explore the influence of the reduction ratio in instance-guided mask, We conduct some experiments to adjust the reduction ratio from 1 to 5 by changing the size of aggregation layer. Experimental results are shown in <ref type="table" target="#tab_6">Table 6</ref> and we can observe that various reduction ratio has little influence on model's performance. This indicates that we can adopt small reduction ratio in aggregation layer in real life applications for saving the computation resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Instance-Guided Mask Study(RQ4)</head><p>As discussed in Section in 3.2, instance-guided mask can be regarded as a special kind of bit-wise attention mechanism to highlight important information based on the current input instance. We can utilize instance-guided mask to boost the informative elements and suppress the uninformative elements or even noise in feature embedding and feed-forward layer.</p><p>To verify this, we design the following experiment: After training the SerMaskNet with 3 blocks, we input different instances into the model and observe the outputs of corresponding instance-guided masks. Firstly, we randomly sample 100000 different instances from Criteo dataset and observe the distributions of the produced values by instance-guided mask from different blocks. <ref type="figure">Figure 5</ref> shows the result. We can see that the distribution of mask values follow normal distribution. Over 50% of the mask values are small number near zero and only little fraction of the mask value is a relatively larger number. This implies that large fraction of signals in feature embedding and feed-forward layer is uninformative or even noise which is suppressed by the small mask values. However, there is some informative information boosted by larger mask values through instance-guided mask.</p><p>Secondly, we randomly sample two instances and compare the difference of the produced values by instance-guided mask. The results are shown in <ref type="figure">Figure 6</ref>. We can see that: As for the mask values for feature embedding, different input instances lead the mask to pay attention to various areas. The mask outputs of instance A pay more attention to the first few features and the mask values of instance B focus on some bits of other features. We can observe the similar trend in the mask values in feed-forward layer. This indicates the input instance indeed guide the mask to pay attention to the different part of the feature embedding and feed-forward layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we introduce multiplicative operation into DNN ranking system by proposing instance-guided mask which performs element-wise product both on the feature embedding and feedforward layers. We also turn the feed-forward layer in DNN model into a mixture of addictive and multiplicative feature interactions by proposing MaskBlock by bombing the layer normalization, instanceguided mask, and feed-forward layer. MaskBlock is a basic building block to be used to design new ranking model. We also propose two specific MaskNet models based on the MaskBlock. The experiment results on three real-world datasets demonstrate that our proposed models outperform state-of-the-art models such as DeepFM and xDeepFM significantly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Neural Structure of Instance-Guided Mask</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>MaskBlock on Feature Embedding MaskBlock on MaskBlock</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Distribution of Mask Values Mask Values of Two Expamples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the evaluation datasets</figDesc><table><row><cell cols="4">Datasets #Instances #fields #features</cell></row><row><cell>Criteo</cell><cell>45M</cell><cell>39</cell><cell>30M</cell></row><row><cell>Avazu</cell><cell>40.43M</cell><cell>23</cell><cell>9.5M</cell></row><row><cell>Malware</cell><cell>8.92M</cell><cell>82</cell><cell>0.97M</cell></row></table><note>4.1.2 Evaluation Metrics. AUC (Area Under ROC) is used in our experiments as the evaluation metric. AUC's upper bound is 1 and larger value indicates a better performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Overall performance (AUC) of different models on three datasets(feature embedding size=10,our proposed two models both have 3 MaskBlocks with same default settings.)</figDesc><table><row><cell></cell><cell cols="2">Criteo</cell><cell cols="2">Malware</cell><cell cols="2">Avazu</cell></row><row><cell></cell><cell>AUC</cell><cell>RelaImp</cell><cell>AUC</cell><cell>RelaImp</cell><cell>AUC</cell><cell>RelaImp</cell></row><row><cell>FM</cell><cell>0.7895</cell><cell>0.00%</cell><cell>0.7166</cell><cell>0.00%</cell><cell>0.7785</cell><cell>0.00%</cell></row><row><cell>DNN</cell><cell>0.8054</cell><cell>+5.35%</cell><cell>0.7246</cell><cell>+3.70%</cell><cell>0.7820</cell><cell>+1.26%</cell></row><row><cell>DeepFM</cell><cell>0.8057</cell><cell>+5.46%</cell><cell>0.7293</cell><cell>+5.86%</cell><cell>0.7833</cell><cell>+1.72%</cell></row><row><cell>DCN</cell><cell>0.8058</cell><cell>+5.49%</cell><cell>0.7300</cell><cell>+6.19%</cell><cell>0.7830</cell><cell>+1.62%</cell></row><row><cell>xDeepFM</cell><cell>0.8064</cell><cell>+5.70%</cell><cell>0.7310</cell><cell>+6.65%</cell><cell>0.7841</cell><cell>+2.01%</cell></row><row><cell>AutoInt</cell><cell>0.8051</cell><cell>+5.39%</cell><cell>0.7282</cell><cell>+5.36%</cell><cell>0.7824</cell><cell>+1.40%</cell></row><row><cell>SerMaskNet</cell><cell>0.8119</cell><cell cols="5">+7.74% 0.7413 +11.40% 0.7877 +3.30%</cell></row><row><cell cols="6">ParaMaskNet 0.8124 +7.91% 0.7410 +11.27% 0.7872</cell><cell>+3.12%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1) Both the serial model and parallel model achieve better performance on all three datasets and obtains significant improvements over the state-of-the-art methods. It can boost the accuracy over the baseline FM by 3.12% to 11.40%, baseline DeepFM by 1.55% to 5.23%, as well as xDeepFM baseline by 1.27% to 4.46%. We also conduct a significance test to verify that our proposed models outperforms baselines with the significance level = 0.01.</figDesc><table><row><cell>Though maskNet model lacks similar module such as CIN in</cell></row><row><cell>xDeepFM to explicitly capture high-order feature interaction,</cell></row><row><cell>it still achieves better performance because of the existence</cell></row><row><cell>of MaskBlock. The experiment results imply that MaskBlock</cell></row><row><cell>indeed enhance DNN Model's ability of capturing complex</cell></row><row><cell>feature interactions through introducing multiplicative op-</cell></row><row><cell>eration into DNN models by instance-guided mask on the</cell></row><row><cell>normalized feature embedding and feed-forward layer.</cell></row><row><cell>(2) As for the comparison of the serial model and parallel model,</cell></row><row><cell>the experimental results show comparable performance on</cell></row><row><cell>three evaluation datasets. It explicitly proves that MaskBlock</cell></row><row><cell>is an effective basic building unit for composing various high</cell></row><row><cell>performance ranking systems.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Overall performance (AUC) of MaskNet models removing different component in MaskBlock on Criteo dataset(feature embedding size=10, MaskNet model has 3 MaskBlocks.)In the following part of the paper, we study the impacts of hyperparameters on two MaskNet models, including 1) the number of feature embedding size; 2) the number of MaskBlock; and 3) the reduction ratio in instance-guided mask module. The experiments are conducted on Criteo dataset via changing one hyper-parameter while holding the other settings. The hyper-parameter experiments show similar trend in other two datasets.</figDesc><table><row><cell cols="3">Model Name SerMaskNet ParaMaskNet</cell></row><row><cell>Full</cell><cell>0.8119</cell><cell>0.8124</cell></row><row><cell>-w/o Mask</cell><cell>0.8090</cell><cell>0.8093</cell></row><row><cell>-w/o LN</cell><cell>0.8106</cell><cell>0.8103</cell></row><row><cell>-w/o FFN</cell><cell>0.8085</cell><cell>0.8122</cell></row><row><cell cols="3">4.4 Hyper-Parameter Study(RQ3)</cell></row></table><note>Number of Feature Embedding Size. The results in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Overall performance (AUC) of different feature embedding size of MaskNet Models on Criteo dataset(the number of MaskBlock is 3)</figDesc><table><row><cell>Embedding Size</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>50</cell><cell>80</cell></row><row><cell>SerMaskNet</cell><cell cols="5">0.8119 0.8123 0.8121 0.8125 0.8121</cell></row><row><cell>ParaMaskNet</cell><cell cols="5">0.8124 0.8128 0.8131 0.8129 0.8129</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="6">Overall performance (AUC) of different num-</cell></row><row><cell cols="6">bers of MaskBlocks in MaskNet model on Criteo</cell></row><row><cell cols="2">dataset(embedding size= 10)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Block Number</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9</cell></row><row><cell>SerMaskNet</cell><cell cols="5">0.8110 0.8119 0.8126 0.8117 0.8115</cell></row><row><cell>ParaMaskNet</cell><cell cols="5">0.8113 0.8124 0.8127 0.8128 0.8132</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Overall performance (AUC) of different size of Hidden Layer in Mask Module of MastBlock on Criteo dataset.(embedding size= 10, number of MaskBlock is 3) Number of MaskBlock. For understanding the influence of the number of MaskBlock on model's performance, we conduct experiments to stack MaskBlock from 1 to 9 blocks for both MaskNet models. The experimental results are listed in the</figDesc><table><row><cell>Reduction ratio</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>SerMaskNet</cell><cell cols="5">0.8118 0.8119 0.8120 0.8117 0.8119</cell></row><row><cell>ParaMaskNet</cell><cell cols="5">0.8124 0.8124 0.8122 0.8122 0.8124</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models by Instance-Guided Mask DLP-KDD 2021, August 15, 2021, Singapore</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Criteo http://labs.criteo.com/downloads/download-terabyte-click-logs/ 2 Malware https://www.kaggle.com/c/microsoft-malware-prediction 3 Avazu http://www.kaggle.com/c/avazu-ctr-prediction</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent cross: Making use of context in recurrent recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagar</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vince</forename><surname>Gatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ispir</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st workshop on deep learning for recommender systems</title>
		<meeting>the 1st workshop on deep learning for recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft&apos;s bing search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><forename type="middle">Quinonero</forename><surname>Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Borchert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Omnipress</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">DeepFM: a factorization-machine based neural network for CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04247</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Practical lessons from predicting clicks on ads at facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ou</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Bowers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Workshop on Data Mining for Online Advertising</title>
		<meeting>the Eighth International Workshop on Data Mining for Online Advertising</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="169" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fieldaware factorization machines for CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">xdeepfm: Combining explicit and implicit feature interactions for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongxia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1754" to="1763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical gating networks for sequential recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="825" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling task relationships in multi-task learning with multi-gate mixture-ofexperts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1930" to="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ad Click Prediction: A View from the Trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2487575.2488200</idno>
		<ptr target="https://doi.org/10.1145/2487575.2488200" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD &apos;13)</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Data Mining. IEEE</title>
		<imprint>
			<biblScope unit="page" from="995" to="1000" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Steffen Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anderson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09683</idno>
		<title level="m">Neural Collaborative Filtering vs. Matrix Factorization Revisited</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autoint: Automatic feature interaction learning via selfattentive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yewen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1161" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep &amp; cross network for ad click predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ADKDD&apos;17</title>
		<meeting>the ADKDD&apos;17</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Attentional factorization machines: Learning the weight of feature interactions via attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>Hao Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04617</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep Feedback Network for Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/345</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2020/345" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2491" to="2497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning over multi-field categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
