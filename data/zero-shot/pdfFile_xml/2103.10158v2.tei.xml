<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">G</forename><surname>M?ller</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Table 1: TrivialAugment compares very favourably to previous augmentation methods. In this table we sum-marize some results from Table 2 and present augmentation search overhead estimates.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic augmentation methods have recently become a crucial pillar for strong model performance in vision tasks. While existing automatic augmentation methods need to trade off simplicity, cost and performance, we present a most simple baseline, TrivialAugment, that outperforms previous methods for almost free. TrivialAugment is parameter-free and only applies a single augmentation to each image. Thus, TrivialAugment's effectiveness is very unexpected to us and we performed very thorough experiments to study its performance. First, we compare Triv-ialAugment to previous state-of-the-art methods in a variety of image classification scenarios. Then, we perform multiple ablation studies with different augmentation spaces, augmentation methods and setups to understand the crucial requirements for its performance. Additionally, we provide a simple interface to facilitate the widespread adoption of automatic augmentation methods, as well as our full code base for reproducibility 1 . Since our work reveals a stagnation in many parts of automatic augmentation research, we end with a short proposal of best practices for sustained future progress in automatic augmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample strength Input image</head><p>Sample augmentation and apply it 1. https://github.com/automl/trivialaugment</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: A visualization of TA. For each image, TA (uniformly) samples an augmentation strength and an augmentation. This augmentation is then applied to the image with the sampled strength.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Data Augmentation is a very popular approach to increase generalization of machine learning models by generating additional data. It is applied in many areas, such as machine translation <ref type="bibr" target="#b3">[4]</ref>, object detection <ref type="bibr" target="#b5">[6]</ref> or semisupervised learning <ref type="bibr" target="#b21">[20]</ref>. In this work, we focus on the application of data augmentation to image classification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Image augmentations for image classification generate novel images based on images in a dataset, which are likely to still belong to the same classification category. This way the dataset can grow based on the biases that come with the augmentations. While data augmentations can yield considerable performance improvements, they do require domain knowledge. An example of an augmentation, with a likely class-preserving behaviour, is the rotation of an image by some small number of degrees. The image's class is still recognized by humans and so this allows the model to generalize in a way humans expect it to generalize.</p><p>Automatic augmentation methods are a set of methods that design augmentation policies automatically. They have been shown to improve model performance significantly across tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b21">20]</ref>.</p><p>Automatic augmentation methods have flourished especially for image classification in recent years <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b8">9]</ref> with many different approaches that learn policies over aug-mentation combinations. The promise of this field is to learn custom augmentation policies that are strong for a particular model and dataset. While the application of an augmentation policy found automatically is cheap, the search for it can be much more expensive than the training itself.</p><p>In this work, we challenge the belief that the resulting augmentation policies of current automatic augmentation methods are actually particularly well fit to the model and dataset. We do this by introducing a trivial baseline method that performs comparably to more expensive augmentation methods without learning a specific augmentation policy per task. Our method does not even combine augmentations in any way. We fittingly call it TrivialAugment (TA).</p><p>The contributions of this paper are threefold:</p><p>? We analyze the minimal requirements for wellperforming automatic augmentation methods and propose TrivialAugment (TA), a trivial augmentation baseline that poses state-of-the-art performance in most setups. At the same time, TA is the most practical automatic augmentation method to date.</p><p>? We comprehensively analyze the performance of TA and multiple other automatic augmentation methods in many setups, using a unified open-source codebase to compare apples to apples.</p><p>? We make recommendations on the practical usage of automatic augmentation methods and collect best practices for automatic augmentation research. Additionally, we provide our code for easy application and future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Many automatic augmentation methods have been proposed in recent years with multiple different setups. Still, all automatic augmentation methods we consider share one property: They work on augmentation spaces that consist of i) a set of prespecified augmentations A and ii) a set of possible strength settings with which augmentations in A can be called (in this work {0, . . . , 30}). One member of A might, for example, be the aforementioned rotation operation, where the strength would correspond to the number of degrees. Automatic augmentation methods now learn how to use these augmentations together on training data to yield a well-performing final classifier.</p><p>In this section, we provide a thorough overview of relevant previous methods. As the compute requirements for automatic augmentation methods can dominate the training costs, we order this recount by the total cost of each method.</p><p>We begin with the first automatic augmentation method, AutoAugment (AA) <ref type="bibr" target="#b0">[1]</ref>, which also happens to be the most expensive, spending over half a GPU-year of compute to yield a classifier on CIFAR-10. AA uses a recurrent neural network (RNN), which is trained with reinforcement learning methods, to predict a parameterization of augmentation policies. Reward is given for the validation accuracy of a particular model trained on a particular dataset with the predicted policy. AA makes use of multiple sub-policies each consisting of multiple augmentations, which in turn are applied sequentially to an input image. Additionally, augmentations are left out with a specified probability. This allows one sub-policy to represent multiple combinations of augmentations. Since AA is costly it uses not the task at hand for augmentation search, but a reduced dataset and a smaller model variant.</p><p>The second most expensive method is Augmentationwise Sharing for AutoAugment (AWS) <ref type="bibr" target="#b20">[19]</ref>. It builds on the same optimization procedure as AA, but uses a simpler search space. The search space consists of a distribution over pairs of augmentations that are applied together. Different from AA, AWS learns the augmentation policy for the last few epochs of training only. It does this on the full dataset with a small model.</p><p>A very different approach, called Population-based Augmentation (PBA) <ref type="bibr" target="#b8">[9]</ref>, is to learn the augmentation policy online as the training goes. PBA does so by using multiple workers that each use a different policy and are updated in an evolutionary fashion. It uses yet another policy parameterization: a vector of augmentations where each augmentation has an attached strength and leave-out probability. From this vector augmentations are sampled uniformly at random and applied with the given strength or left out, depending on the leave-out probability.</p><p>Another method based on multiple parallel workers is Online Hyper-Parameter Learning for Auto-Augmentation (OHL) <ref type="bibr" target="#b15">[14]</ref>. Here, the policy is defined like for AWS and its parameters are trained using reinforcement learning. The major difference with AWS is that its reward is the accuracy on held-out data after a part of training like for PBA, rather than final accuracy. As an additional way of tuning the neural network weights in the parallel run, the weights of the worker with maximal accuracy are used to initialize all workers in the next part of training.</p><p>Adversarial AutoAugment (Adv. AA) <ref type="bibr" target="#b23">[22]</ref> is another slightly cheaper method that uses multiple workers and learns the augmentation policy online. It trains only a single model, though. Here, a single batch is copied to eight different workers and each worker applies its own policies to it, similar to the work by Hoffer et al. <ref type="bibr" target="#b9">[10]</ref>. The worker policies are sampled at the beginning of each epoch from a policy distribution. The policy distribution has a similar form to that of AA. After each epoch, Adv. AA makes a reinforcement-learning based update and rewards the policy yielding the lowest accuracy training accuracy, causing the policy distribution to shift towards progressively stronger augmentations over the course of training.</p><p>Recently, Cubuk et al. proposed RandAugment (RA) <ref type="bibr" target="#b1">[2]</ref>. It is much simpler, but only slightly cheaper, compared to the previous methods. RA only tunes two scalar parameters for each task: (i) a single augmentation strength m ? {0, . . . , 30} which is applied to all augmentations and (ii) the number of augmentations to combine for each image n ? {1, 2, 3}. RA therefore reduces the number of hyperparameters from all the weights of an RNN (for AA) or a distribution over more than a thousand augmentation combinations (for AWS and OHL) to just two. This radical simplification, contrary to expectations, does not hurt accuracy scores compared to many other methods. The authors give indication that the strong performance might be due to the fact that n and m are tuned for the exact task at hand and not for a pruned dataset, as is done, for example, in AA. The big downside of RA is that it ends up performing an exhaustive search over a set of options for n and m incurring up to 80? overhead over a single training 2 .</p><p>Fast AutoAugment (Fast AA) <ref type="bibr" target="#b13">[13]</ref> is the cheapest of the learned methods. It is based on AA, but does not directly search for policies with strong validation performance. Rather, it searches for augmentation policies by finding well-performing inference augmentation policies for networks trained on a split of raw, non-augmented, images. All inference augmentations found on different splits are then joined to build a training time augmentation policy. The intuition behind this can be summarized as follows: If a neural network trained on real data generalizes to examples augmented with some policy then this policy produces images that lie in the domain of the class, as approximated by the neural network. The augmentations therefore are classpreserving and useful. This objective stands in contrasts to the approach followed by Adv. AA. Fast AA tries to find augmentations that yield high accuracy when applied to validation data, while Adv. AA tries to find augmentations that yield low accuracy when applied to training data.</p><p>Finally, in an unpublished arXiv paper, Lingchen et al. <ref type="bibr" target="#b16">[15]</ref> very recently suggested UniformAugment (UA), which works almost like RA. Unlike RA, it fixes the number of augmentations to N = 2 and drops each augmentation with a fixed probability of 0.5. Furthermore, the strength m is sampled uniformly at random for each applied operation.</p><p>In contrast to all above methods methods, we propose TrivialAugment (TA), an augmentation algorithm that is parameter-free like UA, but even simpler. At the same time, TA performs better than any of the comparatively cheap augmentation strategies, making it the most practical automatic augmentation method to date.</p><p>Different from all of the work discussed above, which improves final in-distribution test performance with augmentation strategies, AugMix <ref type="bibr" target="#b7">[8]</ref> aims to improve model robustness by combining multiple augmentations in application chains, mixing their outputs, and applying a consis- tency loss to several augmented images. The only metric we evaluated for which AugMix was evaluated, too, is ResNet-50 performance on the ImageNet test set. Here, TA outperforms AugMix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TrivialAugment</head><p>In this section, we present the simplest augmentation algorithm we could come up with that still performs well: TrivialAugment (TA). TA employs the same augmentation style that previous work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">15]</ref> used: An augmentation is defined as a function a mapping an image x and a discrete strength parameter m to an augmented image. The strength parameter is not used by all augmentations, but most use it to define how strongly to distort the image.</p><p>TA works as follows. It takes an image x and a set of augmentations A as input. It then simply samples an augmentation from A uniformly at random and applies this augmentation to the given image x with a strength m, sampled uniformly at random from the set of possible strengths {0, . . . , 30}, and returns the augmented image. We outline this very simple and parameter-free procedure as pseudocode in Algorithm 1 and visualize it in <ref type="figure">Figure 1</ref>. We emphasize that TA is not a special case of RandAugment (RA), since RA uses a fixed optimized strength for all images while TA samples this strength anew for each image.</p><p>While previous methods used multiple subsequent augmentations, TA only applies a single augmentation to each image. This allows viewing the distribution of the TAaugmented dataset as an average of the |A| data distributions generated by each of the augmentations applied to the full dataset. In <ref type="figure" target="#fig_0">Figure 2</ref> we visualize this notion for deterministic augmentations without a strength parameter. Un-2. In the original setups, the authors also used a different choice of n and m for the search on each task. This can be hard to do for new tasks or with less intuition for a task. Return a(x, m) 5: end procedure like previous work, we do not generate complex distributions out of stochastic combinations of augmentation methods, but simply mean the data distributions of the augmentations applied to the given dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we empirically demonstrate TA's surprisingly strong performance, as well as its behaviour across many ablation settings. In all non-ablation experiments we use either the RA augmentation space (RA), i.e., the set of augmentations and their strength parameterization from the RA paper <ref type="bibr" target="#b1">[2]</ref>, or the wide augmentation space (Wide) for TA. We list the augmentations and their arguments for all augmentation spaces in the appendix in <ref type="table">Table 8</ref>. We run each experiment ten times, if not stated otherwise. In addition to the average over runs we report a confidence interval, which will contain the true mean with probability p = 95%, under the assumption of normal distributed accuracies. In our code we provide a function to compute this interval. Results that lie within the confidence interval of the best performer for each task are typeset in bold font.</p><p>We evaluate our method on five different datasets. i) CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b10">[11]</ref> are standard datasets for image classification and each contain 50K training images. We trained Wide-ResNets <ref type="bibr" target="#b22">[21]</ref> as well as a ShakeShake model <ref type="bibr" target="#b4">[5]</ref>. We follow previous work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> with our setup. ii) SVHN <ref type="bibr" target="#b18">[17]</ref> consists of images of house numbers. It comes with a core set of 73K training images, but offers an additional 531K simpler images as extension of the dataset. We perform experiments with and without the additional images on a Wide-ResNet-28-10. iii) Finally, we perform experiments on ImageNet, a very large image classification corpus with 1000 classes and over 1.2 million images. This experiment is particularly interesting, since it was shown previously that there are augmentations, such as cutout, that do not generalize well to ImageNet. We train a ResNet-50 <ref type="bibr" target="#b6">[7]</ref> following the setup of <ref type="bibr" target="#b0">[1]</ref>. We use warmup and 32 workers due to cluster limitations, which is less than <ref type="bibr" target="#b0">[1]</ref>. We scale the learning rate appropriately. See Appendix A for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison to State-of-the-Art</head><p>It is non-trivial to compare automatic augmentation methods fairly. We therefore compare our method with the previous state-of-the-art in three different setups.</p><p>In Section 4.1.1, we follow the majority of previous work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b16">15]</ref> and perform a comparison with other methods that use the same model and training pipeline. This setup allows for different search costs of different methods and compares methods with the same inference and training costs.</p><p>In Section 4.1.2, we compare in a similar way as above, but against reproductions of other methods in our codebase. This avoids confounding factors, making sure that the methods, and not setup details, explain the differences between results. We reproduced a total of four other methods in our codebase, including the cheapest three previous methods.</p><p>In Section 4.1.3, we compare the total cost of each method, both search and model training, with the final accuracy. This comparison has the upside that it can consider work with different pipelines and models more fairly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Comparison to Published Results</head><p>In <ref type="table" target="#tab_3">Table 2</ref>, we compare TA to all methods that used the setup of AutoAugment <ref type="bibr" target="#b0">[1]</ref> or a very similar setup in terms of hyper-parameters, number of epochs and models.</p><p>TA performs as well or better than previous methods in almost all tasks. The SVHN datasets are the only exception, with RA performing somewhat better. This might, however, be due to our training pipeline, since, as we show in Section 4.1.2, we were not able to reproduce RA's performance for SVHN Core with our pipeline and the original training pipeline is not available.</p><p>For ImageNet, TA outperformed all other methods in terms of both top-1 accuracy and top-5 accuracy. We used an image width of 244 like RA <ref type="bibr" target="#b1">[2]</ref>, but even with a lower width of 224 (as was used for AA <ref type="bibr" target="#b0">[1]</ref>), TA outperformed the previously best methods (with a 77.97 ? .21 top-1 accuracy and 93.98 ? .07 top-5 accuracy; not listed in the table).</p><p>In this comparison, we cannot compare to all previous methods, since some use different setups. The best-known setup we had to leave out is Adv. AA. Therefore, we perform an extra set of experiments following its setup closely.</p><p>Adv. AA uses eight times the compute for its final training compared to other methods and therefore has a significant advantage compared to other methods. Adv. AA is based on batch augmentation <ref type="bibr" target="#b9">[10]</ref>, where a set of workers in a data parallel setting each compute gradients with respect to the same batch of examples, but apply different augmentations to the images in it. We re-created this setup, including all hyper-parameters and batch augmentation, for TA. In <ref type="table">Table 3</ref>, we compare TA with Adv. AA with a Wide-ResNet-28-10 and a ShakeShake-26-2x96d for both CIFAR-10 and CIFAR-100. We show that TA's trivial uniform sampling of a single augmentation achieves the same performance as their complex (and unavailable) reinforcement learning pipeline.   <ref type="table">Table 3</ref>: A comparison of TA with Adv. AA in the augmented batch setting on a Wide-ResNet-28-10. We report the average over five runs.</p><p>We conclude from this section that, for almost all considered benchmarks across datasets, models and even the way augmentations are applied, TA is among the top-performing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Comparison of Reproduced Results in a Fixed</head><p>Training Setup</p><p>While in the previous section, we tried to mitigate confounding factors by comparing results obtained with very similar setups with each other, in this section, we go one step further. We reproduce the results of four methods and compare our baseline method with these reproductions in order to yield a true apples-to-apples comparison.</p><p>As we present a very cheap and simple augmentation method we picked RA, Fast AA and UA as other cheap and simple augmentation methods for our comparison. Additionally, we compare to AA, as an important, common baseline. Moreover, for all of these methods relevant information for reproduction was published 3 .</p><p>For RA, AA and Fast AA we used the published policies and did not search for an augmentation policy from scratch. We based both our RA and AA implementations on a public codebase 4 by the authors of both RA and AA that implements AA for the CIFAR datasets. Likewise, for Fast AA we based our implementation on a public codebase. No code is published for UA, and there are multiple hyperparameters missing in the paper; in these cases, we used the hyper-parameters from RA. For our reproduction of UA, we also adopted the same discretization of the augmentation strengths into 31 values used by the other methods. In addition to the original augmentation space of UA we also perform experiments with the RA augmentation space.</p><p>We reran experiments for CIFAR-10, CIFAR-100 and SVHN Core, and present the results in <ref type="table" target="#tab_5">Table 4</ref>. For each method we ran the benchmarks included in the original work. Generally, we could reproduce most results or even improve upon published results. The only severe exception is RA for which we tried multiple changes to the setup, but were not able to reach their published scores -neither for CIFAR nor for SVHN Core.</p><p>In this evaluation, TA (Wide) performed best across all methods for each benchmark with a Wide-Resnet-28-10, and TA (RA) performed best for both Wide-Resnet-40-2 benchmarks.</p><p>In addition to the reproductions of published policies, we applied RandAugment to the Wide-ResNet-40-2 on CIFAR-10, which was originally not considered in the RA paper. We therefore had to search for a policy first. Depending on the task, Cubuk et al. <ref type="bibr" target="#b1">[2]</ref> considered different subsets of the full range of the augmentation strengths M ? {1, . . . , 30} and the number of consecutive augmentations N ? {1, . . . , 3}. In order to avoid missing the best candidates and to not require human intuition we searched on all 90 resulting combinations of RA's parameters. We split up a validation set of 10000 examples like in the original RandAugment method to evaluate the settings. We then picked the best setting and compared it to TA. <ref type="table">Table  5</ref> clearly shows that TA performs better than the costly RA setup, even though the RA setup in total required 91 full trainings, compared to a single training for TA.</p><p>Finally, we consider three more evaluations in the appendix: (i) We show that TA performs comparably or better on the same augmentation space with other automatic augmentation methods (see Appendix B), (ii) we show that TA generalizes to more peculiar datasets (see Appendix C) and (iii) we show TA's effectiveness with the EfficientNet Architecture <ref type="bibr" target="#b19">[18]</ref> (see Appendix D).   <ref type="table">Table 5</ref>: Average over ten runs on CIFAR-10 with a Wide-ResNet-40-2. TA performs better than the over 80-times more expensive exhaustive search over RA's parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Comparison by Total Compute Costs</head><p>In the previous sections, we compared different augmentation methods for a fixed training setup. We now consider the other extreme, comparing all methods across models and setups by their compute requirements.</p><p>In <ref type="figure" target="#fig_3">Figure 3</ref>, we plot this comparison for many CIFAR-100 setups across the literature. The question this plot answers is: given some compute budget, what method should we choose for the best final accuracy? For this plot, we used the accuracy numbers published in the literature and estimated the compute costs in RTX 2080 Ti GPU-hours. See Appendix E for a detailed account of the information used to calculate the compute cost approximations for all setups. We had to restrict the set of models we considered to the set of models for which we know from our experiments how expensive they are to run, namely all Wide-ResNet setups and the ShakeShake-26-2x96d. We tried to be as conservative as possible regarding the compute requirements of other methods, to not give TA an unfair advantage.</p><p>In the figure, for all considered budgets, TA and its vari-    ant with augmented batch (TA x8) perform among the best methods. TA also has a clear benefit compared to the popular cheap methods Fast AA and RA for all compute budgets; finally, it is dramatically cheaper than AA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Understanding the Minimal Requirements of TA</head><p>While so far, we have demonstrated that in many circumstances TA's approach of only using a single augmentation per image is enough or yields even better performance than more complicated methods, in this section we will dissect other properties of TA.</p><p>We first analyse how TA behaves across augmentation spaces from the literature. We then look at its performance after we apply random changes to its augmentation space. Finally, we consider sets of different augmentation strengths from which TA samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">TA with Different Hand-Picked Augmentation Spaces</head><p>For this evaluation, we carefully reimplemented the augmentation spaces of AA, UA and OHL, besides the one of RA. Additionally, we consider a larger augmentation space (Full), which is a super set of AA, and additionally contains a blur, a smooth, a horizontal and a vertical flip. Especially the vertical flip is likely not useful for very many classification tasks. See <ref type="table">Table 8</ref> in the appendix for an overview of the augmentation spaces. <ref type="table" target="#tab_7">Table 6</ref> indeed shows that TA performs worse on the full augmentation space than on all other augmentation spaces for a Wide-ResNet-28-10 on both SVHN Core and CIFAR-10.</p><p>We also included another augmentation space not considered in the previous literature: a variant of the AA augmentation space, where we removed the extreme invert operation, which maps each pixel x to 255?x. We can see that this augmentation space performs very well for CIFAR-10, but not great for SVHN Core. This aligns well with observations made by earlier work, indicating that the invert <ref type="bibr" target="#b3">4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Augmentations</head><p>Accuracy (%) <ref type="figure">Figure 4</ref>: The performance of WRN-40-2 models depending on the size of sampled subsets of the RA augmentations on CIFAR-10. We performed 10 evaluations per subset size.</p><p>augmentation fosters generalization on SVHN, but not on the other datasets <ref type="bibr" target="#b0">[1]</ref>. A peculiarity of the OHL augmentation space is that it only uses three strengths, unlike all other methods which consider 31 strengths. Interestingly, this is not harmful and OHL yields the best score for SVHN Core.</p><p>We can see that the performance of TA is rather stable between augmentation spaces, but still there seems to be room for improvement by a more sophisticated method to choose the augmentation space for TA depending on the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">TA's Behavior With Randomly Pruned Augmentation Spaces</head><p>While we assessed performance with different hand-crafted augmentation spaces above, now we want to analyze how performance is impacted if we only use random subsets of the 14 augmentations in the RA augmentation space (which we used in the other experiments unless otherwise stated).</p><p>In <ref type="figure">Figure 4</ref>, we analyze the performance and its variance for multiple augmentation subset sizes for a Wide-ResNet-40-2 on CIFAR-10. We performed 10 evaluations per sample size, where in each evaluation we picked a random sample of augmentations. While performance decreases as fewer and fewer augmentations are considered, we can see that it drops very slowly. We can throw away 4 of 14 augmentations and still obtain performance close to the original performance. Another trend is that with fewer augmentations the variance increases. This is likely due to the randomness of the subset choice per run, which increases for smaller subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">The Impact of the Set of Strengths on TA's Performance</head><p>Before, we mostly considered the impact of different sets of augmentations; now we consider the other component of the augmentation space: the set of strengths.  <ref type="table">Table 7</ref>: A comparison of the performance of TA (RA) on different datasets with a Wide-ResNet-28-10 using different subsets of strengths.</p><p>In <ref type="table">Table 7</ref>, we analyze the performance of TA with a Wide-ResNet-28-10 and different subsets of the original set of possible strengths {0, . . . , 30} on the RA augmentation space. We can see that the CIFAR-10 setup seems to be relatively agnostic to the set of strengths. Performance on CIFAR-100, on the other hand, is very negatively impacted by choosing the subset {30}. In general, performance improves on CIFAR-100 with larger sets. For SVHN Core, the opposite is the case: performance improves when only considering {30}. A reason for this could be that the majority of the augmentations are color based and changing the colors of a single-color background and a single-color number drastically, still in most cases yields valid house numbers.</p><p>Another observation we made is that it does not matter so much for any setup whether we reduce to three or just two augmentation strengths, compared to all 31. This seems to point towards the importance of a mixture of strong and weak augmentations. At the same time three different strengths, compared to 31, seem to be enough for these settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Automatic Augmentation Methods in Practice</head><p>While there are many expensive or hard to reproduce automatic augmentation methods, it is important that augmentation methods are practical: the impact of automatic augmentation methods unfolds in the application to new setups and problems. We evaluated many different settings and augmentation methods and we would like to pass on the gained knowledge.</p><p>First, we have compiled a short summary of learnings for the application of augmentation methods in Appendix F.</p><p>Second, in addition to our full codebase, we provide a simple one-file python library that implements the more practical augmentation methods: RA, UA and TA. It even allows choosing from all augmentation spaces considered in this work. For example, to get an image augmenter for TA and transform a PIL image img, one can call 1 aug = TrivialAugment(n,m) 2 augmented_img = aug(img)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Best Practices Proposal for Research</head><p>We found that it is difficult to reimplement many of the published methods, see <ref type="table" target="#tab_1">Table 12</ref> in the appendix. We also found that many methods performed similarly to the simple TA baseline, when we follow their setup. Here, we compile a short bullet point list of best practices we believe are important for sustainable research in this field.</p><p>? Share code as much as possible for easy entry of beginners and to make sure that setups are similar across papers. Otherwise, differences between the actual implementation and its description in the paper can impair reproducibility.</p><p>? Compare fairly to other methods and baselines with the same setup, train budget and augmentation space, or reproduce results of previous methods in your setup and mention differences.</p><p>? Report confidence intervals to discern "outperforming" from "performing comparably".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Limitations</head><p>While we could not find settings where TA failed for image classification, we found that TA does not work out-ofthe-box for object detection setups and also needs tuning to work for this task. So far, we can only wholeheartedly recommend the use of TA for image classification; its application to other computer vision tasks requires further study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>Most of the approaches considered as automatic augmentation methods are complicated. In this work, we presented TA, a very simple augmentation algorithm from which we can learn three main things.</p><p>First, TA teaches us about a crucial baseline missing for automatic augmentation methods.</p><p>Second, TA teaches us to never overlook the simplest solutions. There are a lot of complicated methods to automatically find augmentation policies, but the simplest method was so-far overlooked, even though it performs comparably or better.</p><p>Third, randomness in the chosen strengths appears to be very important for good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Settings</head><p>For all setups we normalize the images by training set mean and standard deviation after the application of all augmentations, besides a final cutout, if applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. CIFAR</head><p>Following previous work we apply the vertical flip and the pad-and-crop augmentations and finally a 16 pixel cutout <ref type="bibr" target="#b2">[3]</ref> after TA or generally any augmentation method. We trained Wide-ResNet models <ref type="bibr" target="#b22">[21]</ref> in the Wide-ResNet-40-2 and the larger Wide-ResNet-28-10 settings. We trained these models for 200 epochs using SGD with Nesterov Momentum and a learning rate of 0.1, a batch size of 128, a 5e-4 weight decay, cosine learning rate decay <ref type="bibr" target="#b17">[16]</ref>.</p><p>We trained ShakeShake-26-2x96d for 1600 epochs using SGD with Nesterov Momentum, a learning rate of 0.01, a batch size of 128, 1e-3 weight decay and a cosine learning rate decay.</p><p>For the augmented batch setups we followed Zhang et al. <ref type="bibr" target="#b23">[22]</ref>. We used the settings above for the Wide-ResNet-28-10 evaluations. And like Zhang slightly different settings for ShakeShake. We use 600 epochs, with a 0.2 learning rate and a 1e-4 weight decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. SVHN</head><p>Unlike for CIFAR we do not apply extra augmentations for SVHN, besides a final 16 pixel cutout <ref type="bibr" target="#b2">[3]</ref>. For the full dataset we trained for 160 epochs using SGD with Nesterov Momentum of 0.9, a learning rate of 0.005, a batch size of 128, a 1e-3 weight decay and cosine learning rate decay. For SVHN Core we train with the same settings, except that we trained for 200 epochs and used a larger weight decay of 5e-3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. ImageNet</head><p>Like for the other datasets we performed the standard augmentations of the dataset after the learned augmentations. That is we performed a randomly resized crop and scales between 0.08 and 1.0 using bicubic interpolation. We augmented with horizontal flips, applied a color jitter with brightness, contrast and saturation strength set to 0.4 and we applied lighting noise with an alpha of 0.1.</p><p>We trained on Imagenet with a ResNet-50 <ref type="bibr" target="#b6">[7]</ref> and followed the setup of AA <ref type="bibr" target="#b0">[1]</ref>. We train for 270 epochs with a batch size of 2048 distributed among 32 workers. We use image crops of height 224 considered both a 244 width of the images, like RA, and a 224 width, like AA. The initial learning rate of 0.1 is scaled proportional to the batch size divided by 256. As learning rate schedule we apply a stepwise 10-fold reduction after 90, 180 and 240 epochs with a linear warmup of factor 4 over the first 3 epochs. We use Nesterov Momenutm with a momentum parameter of 0.9 and a weight decay of 1e-4.</p><p>Unlike <ref type="bibr" target="#b0">[1]</ref> we only use 32 instead of 64 workers out of cluster limitations and scale the learning rate accordingly.  <ref type="table">Table 8</ref>: An overview of the augmentation spaces. The unmarked operations are shared by all augmentation spaces and make up the RA augmentation space. The UA augmentation space additionally contains the dash underlined operations and the . . . . . . OHL augmentation space additionally contains the dotted underlined operations. The ranges given here are the ones used for AA and RA with a discretization to thirty values. The UA augmentation space allows translation up to 14 pixels, but inherits all other settings from RA. The Wide augmentation space we use for batch augmentation has the same operations as RA, but uses the strength ranges in parantheses. The OHL augmentation space uses different ranges and a discretization to three values, see <ref type="bibr" target="#b15">[14]</ref> for more details. All methods are defined as part of Pillow (https://github.com/ python-pillow/Pillow), as part of ImageEnhance, ImageOps or as image attribute, besides cutout <ref type="bibr" target="#b2">[3]</ref>. We also provide operations with the exact same names in our code.</p><formula xml:id="formula_0">PIL operation range PIL operation range identity - auto contrast - equalize - rotate ?30 ? -+30 ? ( ?135 ? -+135 ? )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison of Different Methods on the Same Augmentation Space</head><p>While in the above experiments we used the augmentation space corresponding to each method in the evaluations, in this section, we probe the impact of these differences. We follow the setup of section 4.1.2 and compare our reproduced results of each method to TA on the exact same augmentation space as in the paper introducing the respective method. <ref type="table" target="#tab_11">Table 9</ref> shows that TA's improvements generalize across augmentation spaces and methods.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation on EfficientNet-B1</head><p>While we tried to evaluatate on as relevant setups as possible, we also had to make sure that we can compare with previous work for the main evaluation in Section 4.1.1. Here, we add an Evaluation of an EfficientNet-B1 following the ImageNet setup described in the original paper <ref type="bibr" target="#b19">[18]</ref> closely. None of the methods we compare to compares on this task, thus we re-implemented UA and RA as baselines. For RA we performed a search over 3 settings for m, namely 8, 14 and 21, and fixed the number of augmentations n to 2 following the EfficientNet evaluations in <ref type="bibr" target="#b1">[2]</ref>. <ref type="bibr">RA</ref> UA TA (Wide) EfficientNet-B1 78.75 ? .16 78.83 ? .23 78.99 ? .12 <ref type="table" target="#tab_1">Table 11</ref>: The average performance of an EfficientNet-B1 across 5 re-runs on ImageNet with different augmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Approximation of the Compute Costs for Different Methods</head><p>In this section, we discuss the data underlying our performance per compute comparison. To fairly compare methods, we do not rely on published GPU times as much as possible, but instead calculate all costs for a RTX 2080 Ti for which we know many training times. Therefore, we can only compare methods for which we ran the models. That means for CIFAR-100 we consider only, the consider the Wide-ResNets as well as Shake-Shake-26-2x96d.</p><p>Our estimates for the cost of one epoch on the full CIFAR-10 dataset with 50,000 examples for each model:</p><p>? Wide-ResNet-28-2: 16s</p><p>? Wide-ResNet-40-2: 40s</p><p>? Wide-ResNet-28-10: 101s</p><p>? Shake-Shake-26-2x96d: 83s AA In the AA paper <ref type="bibr" target="#b0">[1]</ref> the policy is trained over 15'000 evaluations of a Wide-ResNet-40-2 on 120 epochs of 4000 examples from CIFAR-10 for all models. We therefore estimate the search cost of AA as 15000 ? 4000/50000 ? 120 ? 40s = 1600h. Additionally we add the standard time for 200 epochs of standard training for each mode. Wide-ResNet-40-2: 1600h + 40s ? 200/60/60, Wide-ResNet-28-10: 1600h + 101s ? 200/60/60, Shake96: 1600h + 83s ? 1800/60/60.</p><p>Fast AA For Fast AA <ref type="bibr" target="#b13">[13]</ref>, we estimate, based on the GPU times in the paper that the search costs more than one full training. We therefore estimate the compute cost as one training.</p><p>UA and TA No search costs. Therefore the total cost simply is the cost of a single training. This is #epochs ? costperepoch.</p><p>Adv. AA and TA x8 We assume for both setups no costs, even though this of course is only a lower bound on the compute requirements of Adv. AA. We simply multiply the number of epochs with the cost per epoch and 8, the number of workers.</p><p>RA The authors of RA use a search space of 5 settings each is evaluated on 90% of the full dataset with the same number of epochs and model. So we have a factor of 5?9/10 with which we multiply the standard costs to get the search costs.</p><p>OHL OHL uses 300 epochs for the Wide-ResNets and trains with 8 parallel workers. We therefore have a factor of 8 ? 300/200 compared to standard costs for search and training combined.</p><p>AWS For AWS the data is not completely clear. First, we have an earlier version of the paper that says it evaluates 800 policies, but in a later version this was corrected down to 500. We therefore assume only 500 policy evaluations to be conservative. They used a Wide-ResNet-28-10 for the augmentation search CIFAR-100 experiments. During augmentation search they train on 80% of the training set for 200 epochs first, and then for 10 epochs for each policy evaluation. This yields 0.8 ? (200 + 500 ? 10) ? 101 = 117h.</p><p>For AWS's x8 setting (8-times augmented batches), we assume the same search costs as above and 8-times the training costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Recommendations for the Application of Automatic Augmentation Methods</head><p>Based on our intense study of automatic augmentation methods for different image classification tasks using different models we recommend the following steps when applying automatic augmentation methods. In the application of an automatic augmentation method it is of course important to know, whether a method is easy to reimplement. We thus put together <ref type="table" target="#tab_1">Table 12</ref> for easy guidance.</p><p>Standard Model and Dataset If the model and dataset combination you are using is part of automatic augmentation literature, we recommend to simply use the best published method for your setup with published code and policies.</p><p>Novel model or Novel dataset If you are using a setup not evaluated in the automatic augmentation literature, it is a good approach to try both the best performing model on a similar task as well as a parameter-free baseline. The parameter-free baseline, like UniformAugment or Triv-ialAugment, especially can be expected to generalize to the new task, since they generalized to all standard automatic evluation benchmarks without any tuning. If you have tuning budget, you can of course tune something like PBA to your particular task. This likely is a good idea if your images are very dissimilar to the automatic augmentation benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An exemplary visualization of a 2-D dataset with two classes, crosses and circles, separated by a decision boundary, the dotted line. The colored crosses represent deterministic augmentations of the cross class. TA now uniformly samples from all crosses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 2 : 3 :</head><label>123</label><figDesc>TrivialAugment Procedure 1: procedure TA(x: image) Sample an augmentation a from A Sample a strength m from {0, . . . , 30} 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of the final test accuracy on CIFAR-100 in comparison to RTX2080ti GPU-hours compute invested for augmentation search and final model training across a set of models. Methods marked with x8 use batch augmentations<ref type="bibr" target="#b9">[10]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>TrivialAugment compares very favourably to previous augmentation methods. In this table we summarize some results fromTable 2and present augmentation search overhead estimates.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>The average test accuracies from ten runs, besides for ImageNet, where we used five runs. The 95% confidence interval is noted with ?. The trivial TA is in all benchmarks among the top-performers. The only exception is the comparison to RA's performance on the SVHN benchmarks, but this difference was non-existent in our reimplementation in 4.1.2.Adv. AA TA (Wide) CIFAR-10 Wide-ResNet-28-10 98.10 ? .15 98.04 ? .06 ShakeShake-26-2x96d 98.15 ? .12 98.12 ? .12 CIFAR-100 Wide-ResNet-28-10 84.51 ? .18 84.62 ? .14 ShakeShake-26-2x96d 85.90 ? .15 86.02 ? .13</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: A reproduction of the results of previous work with</cell></row><row><cell>a Wide-ResNet-28-10 on CIFAR (a) and SVHN Core (c),</cell></row><row><cell>and with a Wide-ResNet-40-2 on CIFAR (b). We report the</cell></row><row><cell>relative performance difference to the published results in</cell></row><row><cell>parentheses.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>? .06 97.24 ? .03 AA 98.04 ? .02 97.47 ? .11 AA -{Invert} 97.97 ? .08 97.55 ? .06 RA 98.05 ? .02 97.46 ? .09 Wide 98.11 ? .03 97.46 ? .06 UA 98.06 ? .04 97.42 ? .07 OHL 98.10 ? .02 97.45 ? .05</figDesc><table><row><cell cols="2">Augmentation space SVHN Core</cell><cell>CIFAR-10</cell></row><row><cell>Full</cell><cell>97.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Evaluation of TA on SVHN Core and CIFAR-10 with a set of 7 different augmentation spaces. Note that RA = AA ? {SamplePairing, Invert, Cutout} and UA = AA ? {SamplePairing}.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>? .05 82.98 ? .22 98.16 ? .03 {0, 30} 97.51 ? .08 83.46 ? .10 98.02 ? .02 {0, 15, 30} 97.46 ? .06 83.43 ? .24 98.04 ? .03 {0, . . . , 30} 97.46 ? .09 83.54 ? .12 98.05 ? .02</figDesc><table><row><cell>Strengths</cell><cell>CIFAR-10</cell><cell>CIFAR-100 SVHN Core</cell></row><row><cell>{30}</cell><cell>97.45</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Method 97.31 ? .22 97.43 ? .09 97.12 ? .14 97.46 ? .14 TA 97.55 ? .06 97.51 ? .09 97.46 ? .09 97.42 ? .07 CIFAR-100 Method 82.91 ? .41 83.27 ? .13 83.1 ? .32 83.08 ? .27 TA 83.34 ? .10 83.36 ? .15 83.54 ? .12 83.33 ? .14 SVHN Method 97.99 ? .06 -98.06 ? .04 98.05 ? .04 Core TA 98.04 ? .02 97.84 ? .03 98.05 ? .02 98.06 ? .04</figDesc><table><row><cell>Dataset</cell><cell>Setup</cell><cell>AA</cell><cell>FAA</cell><cell>RA</cell><cell>UA</cell></row><row><cell>CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Comparisons of various methods (in our reimplementation) to TA, using the exact same augmentation space. E.g., for CIFAR-10 on the AA space, AA reached 97.31 ? .22 and TA reached 97.55 ? .06. No policy is published for FAA on SVHN Core, since this setup was not part of the FAA paper. Therefore, we do not reproduce FAA on SVHN Core.C. Evaluation on Special DatasetsTA (RA) Occ. CIFAR-10 94.99 ? .11 95.2 ? .08 95.52 ? .18 95.72 ? .09 Stanford Cars 90.21 ? .16 92.47 ? .17 -92.77 ? .12</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>A comparison on non-standard datasets. The RA setting of BruteForce-RA is searched in the same large set as in Section 4.1.2. Transfer-RA is transferred from Wide-ResNet-28-10 on CIFAR-10 and ResNet-50 on ImageNet, respectively. BF-RA for S. Cars was not feasible in the given time.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. SeeTable 12in the appendix for an overview of the published materials of different methods 4. https://github.com/tensorflow/models/tree/ fd34f711f319d8c6fe85110d9df6e1784cc5a6ca/ research/autoaugment</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">To further show that this method generalizes to more particular image classification datasets without fine-tuning, we considered two more datasets, following the settings of Section 4.1.2. (i) Since we are not aware of an image recognition dataset that contains occlusions, we created an occlusion variant of CIFAR-10 (Occ. CIFAR-10), where a 14x14 square is occluded by a black box, in each image including the test images; we evaluate a WRN-28-10 on Occ. CIFAR-10. We follow the settings for CIFAR-10 closely for this experiment. (ii) Additionally, we evaluate an RN-50 on the Stanford Cars dataset, which is a dataset in which visual details are important to distinguish car models. We train for 1000 epochs.Table 10shows that TA continues to perform well in these settings, outperforming even brute-force tuned RA.MethodBaseline Transfer-RA BF-RA</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We want to thank Ildoo Kim for his open-source codebase which ours forks from, and the reviewers for their insightful comments. We acknowledge funding by the Robert Bosch GmbH and the European Research Council (ERC) under the European Union Horizon 2020 research and innovation programme through grant no. 716721.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Policies for Training Code for training Code for meta-training</head> <ref type="table">Table 12</ref><p>: In this table we compare the reproducibility of different methods in three categories. (i) Whether the augmentation policies used for model trainings are available, (ii) whether the authors provide code for training a model with the policies on which they report their performance and (iii) whether there is code available to run the search for training policies, code for a meta-training. We mark entries with -if it is not an applicable category for the given augmentation method and additionally use the following symbols. ? s : Only available for subset of experiments, ? i : Not available for ImageNet trainings, which for PBA was also not considered in the paper, ? p : there is publicly work in progress.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data augmentation for low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="567" to="573" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Shake-shake regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gastaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Piotr Doll?r, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">AugMix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Population based augmentation: Efficient learning of augmentation policy schedules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2731" to="2741" />
		</imprint>
	</monogr>
	<note>Ion Stoica, and Pieter Abbeel</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In P</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Proceedings of the 26th International Conference on Advances in Neural Information Processing Systems (NeurIPS&apos;12)</title>
		<meeting>the 26th International Conference on Advances in Neural Information Processing Systems (NeurIPS&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fast autoaugment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="6665" to="6675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online hyper-parameter learning for auto-augmentation strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Uniformaugment: A search-free probabilistic data augmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ching Lingchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ava</forename><surname>Khonsari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Lashkari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mina</forename><forename type="middle">Rafi</forename><surname>Nazari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaspreet</forename><surname>Singh Sambee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><forename type="middle">A</forename><surname>Nascimento</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR&apos;17</title>
		<meeting>the International Conference on Learning Representations (ICLR&apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving auto-augment via augmentation-wise weight sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6256" to="6268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMCV&apos;16</title>
		<meeting>BMCV&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial autoaugment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning data augmentation strategies for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="566" to="583" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
