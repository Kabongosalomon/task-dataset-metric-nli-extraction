<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Long-and Short-Term Temporal Patterns with Deep Neural Networks Multivariate Time Series, Neural Network, Autoregressive models ACM Reference Format</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
							<email>guokun@cs.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
							<email>wchang2@andrew.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
							<email>yiming@cs.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
							<email>hanxiaol@cs.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Long-and Short-Term Temporal Patterns with Deep Neural Networks Multivariate Time Series, Neural Network, Autoregressive models ACM Reference Format</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>KEYWORDS. 2018. Mod-eling Long-and Short-Term Temporal Patterns with Deep Neural Networks. In Proceedings of ACM Conference (SIGIR&apos;18). ACM, New York, NY, USA, 11 pages. https://doi.org/10.475/123_4</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multivariate time series forecasting is an important machine learning problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. Temporal data arise in these real-world applications often involves a mixture of long-term and short-term patterns, for which traditional approaches such as Autoregressive models and Gaussian Process may fail. In this paper, we proposed a novel deep learning framework, namely Long-and Short-term Time-series network (LSTNet), to address this open challenge. LSTNet uses the Convolution Neural Network (CNN) and the Recurrent Neural Network (RNN) to extract short-term local dependency patterns among variables and to discover long-term patterns for time series trends. Furthermore, we leverage traditional autoregressive model to tackle the scale insensitive problem of the neural network model. In our evaluation on real-world data with complex mixtures of repetitive patterns, LSTNet achieved significant performance improvements over that of several state-of-the-art baseline methods. All the data and experiment codes are available online.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Multivariate time series data are ubiquitous in our everyday life ranging from the prices in stock markets, the traffic flows on highways, the outputs of solar power plants, the temperatures across different cities, just to name a few. In such applications, users are often interested in the forecasting of the new trends or potential hazardous events based on historical observations on time series signals. For instance, a better route plan could be devised based on Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR'18, July 2018, Ann Arbor, MI, USA ? 2018 Copyright held by the owner/author(s). ACM ISBN 123-4567-24-567/08/06. . . $15.00 https://doi.org/10.475/123_4 the predicted traffic jam patterns a few hours ahead, and a larger profit could be made with the forecasting of the near-future stock market.</p><p>Multivariate time series forecasting often faces a major research challenge, that is, how to capture and leverage the dynamics dependencies among multiple variables. Specifically, real-world applications often entail a mixture of short-term and long-term repeating patterns, as shown in <ref type="figure">Figure 1</ref> which plots the hourly occupancy rate of a freeway. Apparently, there are two repeating patterns, daily and weekly. The former portraits the morning peaks vs. evening peaks, while the latter reflects the workday and weekend patterns. A successful time series forecasting model should be capture both kinds of recurring patterns for accurate predictions. As another example, consider the task of predicting the output of a solar energy farm based on the measured solar radiation by massive sensors over different locations. The long-term patterns reflect the difference between days vs. nights, summer vs. winter, etc., and the shortterm patterns reflect the effects of cloud movements, wind direction changes, etc. Again, without taking both kinds of recurrent patterns into account, accurate time series forecasting is not possible. However, traditional approaches such as the large body of work in autoregressive methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b35">35]</ref> fall short in this aspect, as most of them do not distinguish the two kinds of patterns nor model their interactions explicitly and dynamically. Addressing such limitations of existing methods in time series forecasting is the main focus of this paper, for which we propose a novel framework that takes advantages of recent developments in deep learning research.</p><p>Deep neural networks have been intensively studied in related domains, and made extraordinary impacts on the solutions of a broad range of problems. The recurrent neural networks (RNN) models <ref type="bibr" target="#b8">[9]</ref>, for example, have become most popular in recent natural language processing (NLP) research. Two variants of RNN in particular, namely the Long Short Term Memory (LSTM) <ref type="bibr" target="#b14">[15]</ref> and the Gated Recurrent Unit (GRU) <ref type="bibr" target="#b5">[6]</ref>, have significantly improved the state-of-the-art performance in machine translation, speech recognition and other NLP tasks as they can effectively capture the meanings of words based on the long-term and short-term dependencies among them in input documents <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref>.In the field of computer vision, as another example, convolution neural network (CNN) models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">21]</ref> have shown outstanding performance by successfully extracting local and shift-invariant features (called "shapelets" sometimes) at various granularity levels from input images. Occupancy rate(%) <ref type="figure">Figure 1</ref>: The hourly occupancy rate of a road in the bay area for 2 weeks</p><p>Deep neural networks have also received an increasing amount of attention in time series analysis. A substantial portion of the previous work has been focusing on time series classification, i.e., the task of automated assignment of class labels to time series input. For instance, RNN architectures have been studied for extracting informative patterns from health-care sequential data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">23]</ref> and classifying the data with respect diagnostic categories. RNN has also been applied to mobile data, for classifying the input sequences with respect to actions or activities <ref type="bibr" target="#b12">[13]</ref>. CNN models have also been used in action/activity recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">31]</ref>, for the extraction of shift-invariant local patterns from input sequences as the features of classification models.</p><p>Deep neural networks have also been studied for time series forecasting <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">33]</ref>, i.e., the task of using observed time series in the past to predict the unknown time series in a look-ahead horizon -the larger the horizon, the harder the problem. Efforts in this direction range from the early work using naive RNN models <ref type="bibr" target="#b6">[7]</ref> and the hybrid models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35]</ref> combining the use of ARIMA <ref type="bibr" target="#b2">[3]</ref> and Multilayer Perceptron (MLP), to the recent combination of vanilla RNN and Dynamic Boltzmann Machines in time series forecasting <ref type="bibr" target="#b7">[8]</ref>.</p><p>In this paper, we propose a deep learning framework designed for the multivariate time series forecasting, namely Long-and Shortterm Time-series Network (LSTNet), as illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>. It leverages the strengths of both the convolutional layer to discover the local dependency patterns among multi-dimensional input variables and the recurrent layer to captures complex long-term dependencies. A novel recurrent structure, namely Recurrent-skip, is designed for capturing very long-term dependence patterns and making the optimization easier as it utilizes the periodic property of the input time series signals. Finally, the LSTNet incorporates a traditional autoregressive linear model in parallel to the non-linear neural network part, which makes the non-linear deep learning model more robust for the time series with violate scale changing. In the experiment on the real world seasonal time series datasets, our model consistently outperforms the traditional linear models and GRU recurrent neural network.</p><p>The rest of this paper is organized as follows. Section 2 outlines the related background, including representative auto-regressive methods and Gaussian Process models. Section 3 describe our proposed LSTNet. Section 4 reports the evaluation results of our model in comparison with strong baselines on real-world datasets. Finally, we conclude our findings in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED BACKGROUND</head><p>One of the most prominent univariate time series models is the autoregressive integrated moving average (ARIMA) model. The popularity of the ARIMA model is due to its statistical properties as well as the well-known Box-Jenkins methodology <ref type="bibr" target="#b1">[2]</ref> in the model selection procedure. ARIMA models are not only adaptive to various exponential smoothing techniques <ref type="bibr" target="#b25">[25]</ref> but also flexible enough to subsume other types of time series models including autoregression (AR), moving average (MA) and Autoregressive Moving Average (ARMA). However, ARIMA models, including their variants for modeling long-term temporal dependencies <ref type="bibr" target="#b1">[2]</ref>, are rarely used in high dimensional multivariate time series forecasting due to their high computational cost.</p><p>On the other hand, vector autoregression (VAR) is arguably the most widely used models in multivariate time series <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">24]</ref> due to its simplicity. VAR models naturally extend AR models to the multivariate setting, which ignores the dependencies between output variables. Significant progress has been made in recent years in a variety of VAR models, including the elliptical VAR model <ref type="bibr" target="#b27">[27]</ref> for heavy-tail time series and structured VAR model <ref type="bibr" target="#b26">[26]</ref> for better interpretations of the dependencies between high dimensional variables, and more. Nevertheless, the model capacity of VAR grows linearly over the temporal window size and quadratically over the number of variables. This implies, when dealing with long-term temporal patterns, the inherited large model is prone to overfitting. To alleviate this issue, <ref type="bibr" target="#b32">[32]</ref> proposed to reduce the original high dimensional signals into lower dimensional hidden representations, then applied VAR for forecasting with a variety choice of regularization.</p><p>Time series forecasting problems can also be treated as standard regression problems with time-varying parameters. It is therefore not surprising that various regression models with different loss functions and regularization terms are applied to time series forecasting tasks. For example, linear support vector regression (SVR) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref> learns a max margin hyperplane based on the regression loss with a hyper-parameter ? controlling the threshold of prediction errors. Ridge regression is yet another example which can be recovered from SVR models by setting ? to zeros. Lastly, <ref type="bibr" target="#b22">[22]</ref> applied LASSO models to encourage sparsity in the model parameters so that interesting patterns among different input signals could be manifest. These linear methods are practically more efficient for multivariate time series forecasting due to high-quality off-the-shelf solvers in the machine learning community. Nonetheless, like VARs, those linear models may fail to capture complex non-linear relationships of multivariate signals, resulting in an inferior performance at the cost of its efficiency.</p><p>Gaussian Processes (GP) is a non-parametric method for modeling distributions over a continuous domain of functions. This contrasts with models defined by a parameterized class of functions  such as VARs and SVRs. GP can be applied to multivariate time series forecasting task as suggested in <ref type="bibr" target="#b28">[28]</ref>, and can be used as a prior over the function space in Bayesian inference. For example, <ref type="bibr" target="#b9">[10]</ref> presented a fully Bayesian approach with the GP prior for nonlinear state-space models, which is capable of capturing complex dynamical phenomena. However, the power of Gaussian Process comes with the price of high computation complexity. A straightforward implementation of Gaussian Process for multivariate time-series forecasting has cubic complexity over the number of observations, due to the matrix inversion of the kernel matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FRAMEWORK</head><p>In this section, we first formulate the time series forecasting problem, and then discuss the details of the proposed LSTNet architecture ( <ref type="figure" target="#fig_2">Figure 2</ref>) in the following part. Finally, we introduce the objective function and the optimization strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>In this paper, we are interested in the task of multivariate time series forecasting. More formally, given a series of fully observed time series signals Y = {y 1 , y 2 , . . . , y T } where y t ? R n , and n is the variable dimension, we aim at predicting a series of future signals in a rolling forecasting fashion. That being said, to predict y T +h where h is the desirable horizon ahead of the current time stamp, we assume {y 1 , y 2 , . . . , y T } are available. Likewise, to predict the value of the next time stamp y T +h+1 , we assume {y 1 , y 2 , . . . , y T , y T +1 } are available. We hence formulate the input matrix at time stamp T as X T = {y 1 , y 2 , . . . , y T } ? R n?T .</p><p>In the most of cases, the horizon of the forecasting task is chosen according to the demands of the environmental settings, e.g. for the traffic usage, the horizon of interest ranges from hours to a day; for the stock market data, even seconds/minutes-ahead forecast can be meaningful for generating returns. <ref type="figure" target="#fig_2">Figure 2</ref> presents an overview of the proposed LSTnet architecture. The LSTNet is a deep learning framework specifically designed for multivariate time series forecasting tasks with a mixture of longand short-term patterns. In following sections, we introduce the building blocks for the LSTNet in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolutional Component</head><p>The first layer of LSTNet is a convolutional network without pooling, which aims to extract short-term patterns in the time dimension as well as local dependencies between variables. The convolutional layer consists of multiple filters of width ? and height n (the height is set to be the same as the number of variables). The k-th filter sweeps through the input matrix X and produces</p><formula xml:id="formula_0">h k = RELU (W k * X + b k )<label>(1)</label></formula><p>where * denotes the convolution operation and the output h k would be a vector, and the RELU function is RELU (x) = max(0, x). We make each vector h k of length T by zero-padding on the left of input matrix X . The output matrix of the convolutional layer is of size d c ? T where d c denotes the number of filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Recurrent Component</head><p>The output of the convolutional layer is simultaneously fed into the Recurrent component and Recurrent-skip component (to be described in subsection 3.4). The Recurrent component is a recurrent layer with the Gated Recurrent Unit (GRU) <ref type="bibr" target="#b5">[6]</ref> and uses the RELU function as the hidden update activation function. The hidden state of recurrent units at time t is computed as,</p><formula xml:id="formula_1">r t = ? (x t W xr + h t ?1 W hr + b r ) u t = ? (x t W xu + h t ?1 W hu + b u ) c t = RELU (x t W xc + r t ? (h t ?1 W hc ) + b c ) h t = (1 ? u t ) ? h t ?1 + u t ? c t<label>(2)</label></formula><p>where ? is the element-wise product, ? is the sigmoid function and x t is the input of this layer at time t. The output of this layer is the hidden state at each time stamp. While researchers are accustomed to using tanh function as hidden update activation function, we empirically found RELU leads to more reliable performance, through which the gradient is easier to back propagate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Recurrent-skip Component</head><p>The Recurrent layers with GRU <ref type="bibr" target="#b5">[6]</ref> and LSTM <ref type="bibr" target="#b14">[15]</ref> unit are carefully designed to memorize the historical information and hence to be aware of relatively long-term dependencies. Due to gradient vanishing, however, GRU and LSTM usually fail to capture very long-term correlation in practice. We propose to alleviate this issue via a novel recurrent-skip component which leverages the periodic pattern in real-world sets. For instance, both the electricity consumption and traffic usage exhibit clear pattern on a daily basis. If we want to predict the electricity consumption at t o'clock for today, a classical trick in the seasonal forecasting model is to leverage the records at t o'clock in historical days, besides the most recent records. This type of dependencies can hardly be captured by off-the-shelf recurrent units due to the extremely long length of one period (24 hours) and the subsequent optimization issues. Inspired by the effectiveness of this trick, we develop a recurrent structure with temporal skipconnections to extend the temporal span of the information flow and hence to ease the optimization process. Specifically, skip-links are added between the current hidden cell and the hidden cells in the same phase in adjacent periods. The updating process can be formulated as,</p><formula xml:id="formula_2">r t = ? (x t W x r + h t ?p W hr + b r ) u t = ? (x t W xu + h t ?p W hu + b u ) c t = RELU (x t W xc + r t ? (h t ?p W hc ) + b c ) h t = (1 ? u t ) ? h t ?p + u t ? c t (3)</formula><p>where the input of this layer is the output of the convolutional layer, and p is the number of hidden cells skipped through. The value of p can be easily determined for datasets with clear periodic patterns (e.g. p = 24 for the hourly electricity consumption and traffic usage datasets), and has to be tuned otherwise. In our experiments, we empirically found that a well-tuned p can considerably boost the model performance even for the latter case. Furthermore, the LSTNet could be easily extended to contain variants of the skip length p.</p><p>We use a dense layer to combine the outputs of the Recurrent and Recurrent-skip components. The inputs to the dense layer include the hidden state of Recurrent component at time stamp t, denoted by h R t , and p hidden states of Recurrent-skip component from time</p><formula xml:id="formula_3">stamp t ? p + 1 to t denoted by h S t ?p+1 , h S t ?p+2 . . . , h S t .</formula><p>The output of the dense layer is computed as,</p><formula xml:id="formula_4">h D t = W R h R t + p?1 i=0 W S i h S t ?i + b<label>(4)</label></formula><p>where h D t is the prediction result of the neural network (upper) part in the <ref type="figure" target="#fig_2">Fig.2</ref> at time stamp t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Temporal Attention Layer</head><p>However, the Recurrent-skip layer requires a predefined hyperparameter p, which is unfavorable in the nonseasonal time series prediction, or whose period length is dynamic over time. To alleviate such issue, we consider an alternative approach, attention mechanism <ref type="bibr" target="#b0">[1]</ref>, which learns the weighted combination of hidden representations at each window position of the input matrix. Specifically, the attention weights ? t ? R q at current time stamp t are calculated as</p><formula xml:id="formula_5">? t = AttnScore(H R t , h R t ?1 ) where H R t = [h R t ?q , . . . , h R t ?1 ]</formula><p>is a matrix stacking the hidden representation of RNN column-wisely and AttnScore is some similarity functions such as dot product, cosine, or parameterized by a simple multi-layer perceptron.</p><p>The final output of temporal attention layer is the concatenation of the weighted context vector c t = H t ? t and last window hidden representation h R t ?1 , along with a linear projection operation</p><formula xml:id="formula_6">h D t = W [c t ; h R t ?1 ] + b.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Autoregressive Component</head><p>Due to the non-linear nature of the Convolutional and Recurrent components, one major drawback of the neural network model is that the scale of outputs is not sensitive to the scale of inputs. Unfortunately, in specific real datasets, the scale of input signals constantly changes in a non-periodic manner, which significantly lowers the forecasting accuracy of the neural network model. A concrete example of this failure is given in Section 4.6. To address this deficiency, similar in spirit to the highway network <ref type="bibr" target="#b29">[29]</ref>, we decompose the final prediction of LSTNet into a linear part, which primarily focuses on the local scaling issue, plus a non-linear part containing recurring patterns. In the LSTNet architecture, we adopt the classical Autoregressive (AR) model as the linear component. Denote the forecasting result of the AR component as h L t ? R n , and the coefficients of the AR model as W ar ? R q ar and b ar ? R, where q ar is the size of input window over the input matrix. Note that in our model, all dimensions share the same set of linear parameters. The AR model is formulated as follows,</p><formula xml:id="formula_7">h L t,i = q ar ?1 k=0 W ar k y t ?k,i + b ar<label>(5)</label></formula><p>The final prediction of LSTNet is then obtained by by integrating the outputs of the neural network part and the AR component:</p><formula xml:id="formula_8">Y t = h D t + h L t<label>(6)</label></formula><p>where? t denotes the model's final prediction at time stamp t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Objective function</head><p>The squared error is the default loss function for many forecasting tasks, the corresponding optimization objective is formulated as,</p><formula xml:id="formula_9">minimize ? t ?? T r ain ||Y t ?? t ?h || 2 F<label>(7)</label></formula><p>where ? denotes the parameter set of our model, ? T r ain is the set of time stamps used for training, || ? || F is the Frobenius norm, and h is the horizon as mentioned in Section 3.1. The traditional linear regression model with the square loss function is named as Linear Ridge, which is equivalent to the vector autoregressive model with ridge regularization. However, experiments show that the Linear Support Vector Regression (Linear SVR) <ref type="bibr" target="#b30">[30]</ref> dominates the Linear Ridge model in certain datasets. The only difference between Linear SVR and Linear Ridge is the objective function. The objective function for Linear SVR is,</p><formula xml:id="formula_10">minimize ? 1 2 ||?|| 2 F + C t ?? T r ain n?1 i=0 ? t,i subject to |? t ?h,i ? Y t,i | ? ? t,i + ?, t ? ? T r ain ? t,i ? 0<label>(8)</label></formula><p>where C and ? are hyper-parameters. Motivated by the remarkable performance of the Linear SVR model, we incorporate its objective function in the LSTNet model as an alternative of the squared loss. For simplicity, we assume ? = 0 1 , and the objective function above reduces to absolute loss (L1-loss) function as follows:</p><formula xml:id="formula_11">minimize ? t ?? T r ain n?1 i=0 |Y t,i ?? t ?h,i |<label>(9)</label></formula><p>The advantage of the absolute loss function is that it is more robust to the anomaly in the real time series data. In the experiment section, we use the validation set to decide to use which objective function, square loss Eq.7 or absolute one Eq.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Optimization Strategy</head><p>In this paper, our optimization strategy is the same as that in the traditional time series forecasting model. Supposing the input time series is Y t = {y 1 , y 2 , . . . , y t }, we define a tunable window size q, and reformulate the input at time stamp t as X t = {y t ?q+1 , y t ?q+2 , . . . , y t }. The problem then becomes a regression task with a set of feature-value pairs {X t , Y t +h }, and can be solved by Stochastic Gradient Decent (SGD) or its variants such as Adam <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>We conducted extensive experiments with 9 methods (including our new methods) on 4 benchmark datasets for time series forecasting tasks. All the data and experiment codes are available online 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methods for Comparison</head><p>The methods in our comparative evaluation are the follows.</p><p>? AR stands for the autoregressive model, which is equivalent to the one dimensional VAR model. ? LRidge is the vector autoregression (VAR) model with L2regularization, which has been most popular for multivariate time series forecasting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? LSVR is the vector autoregression (VAR) model with Support</head><p>Vector Regression objective function <ref type="bibr" target="#b30">[30]</ref> . ? TRMF is the autoregressive model using temporal regularized matrix factorization by <ref type="bibr" target="#b32">[32]</ref>. <ref type="bibr" target="#b0">1</ref> One could keep ? to make the objective function more faithful to the Linear SVR model without modifying the optimization strategy. We leave this for future study. <ref type="bibr" target="#b1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>the link is anonymous due to the double blind policy</head><p>? GP is the Gaussian Process for time series modeling. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">28]</ref> ? VAR-MLP is the model proposed in <ref type="bibr" target="#b35">[35]</ref> that combines Multilayer Perception (MLP) and autoregressive model. ? RNN-GRU is the Recurrent Neural Network model using GRU cell. ? LSTNet-skip is our proposed LSTNet model with skip-RNN layer. ? LSTNet-Attn is our proposed LSTNet model with temporal attention layer. For the single output methods above such as AR, LRidge, LSVR and GP, we just trained n models independently, i.e., one model for each of the n output variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics</head><p>We used three conventional evaluation metrics defined as:</p><p>? Root Relative Squared Error (RSE):</p><formula xml:id="formula_12">RSE = (i,t )?? T e s t (Y it ?? it ) 2 (i,t )?? T e s t (Y it ? mean(Y )) 2<label>(10)</label></formula><p>? Empirical Correlation Coefficient (CORR)</p><formula xml:id="formula_13">CORR = 1 n n i=1 t Y it ? mean(Y i ) ? it ? mean(? i ) t Y it ? mean(Y i ) 2 ? it ? mean(? i ) 2<label>(11)</label></formula><p>where Y ,? ? R n?T are ground true signals and system prediction signals, respectively. The RSE are the scaled version of the widely used Root Mean Square Error(RMSE), which is design to make more readable evaluation, regardless the data scale. For RSE lower value is better, while for CORR higher value is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Data</head><p>We used four benchmark datasets which are publicly available.  In order to examine the existence of long-term and/or shortterm repetitive patterns in time series data, we plot autocorrelation graph for some randomly selected variables from the four datasets in <ref type="figure">Figure 3</ref>. Autocorrelation, also known as serial correlation, is the correlation of a signal with a delayed copy of itself as a function of delay defined below</p><formula xml:id="formula_14">R(? ) = E[(X t ? ?)(X t +? ? ?)] ? 2</formula><p>where X t is the time series signals, ? is mean and ? 2 is variance. In practice, we consider the empirical unbiased estimator to calculate the autocorrelation.</p><p>We can see in the graphs (a), (b) and (c) of <ref type="figure">Figure 3</ref>, there are repetitive patterns with high autocorrelation in the Traffic, Solar-Energy and Electricity datasets, but not in the Exchange-Rate dataset. Furthermore, we can observe a short-term daily pattern (in every 24 hours) and long-term weekly pattern (in every 7 days) in the graph of the Traffic and Electricity dataset, which perfect reflect the expected regularity in highway traffic situations and electricity consumptions. On the other hand, in graph (d) of the Exchange-Rate dataset, we hardly see any repetitive long-term patterns, expect some short-term local continuity. These observations are important for our later analysis on the empirical results of different methods. That is, for the methods which can properly model and successfully leverage both short-term and long-term repetitive patterns in data, they should outperform well when the data contain such repetitive patterns (like in Electricity, Traffic and Solar-Energy).</p><p>On the other hand, if the dataset does not contain such patterns (like in Exchange-Rate), the advantageous power of those methods may not lead a better performance than that of other less powerful methods. We will revisit this point in Section 4.7 with empirical justifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Details</head><p>We conduct grid search over all tunable hyper-parameters on the held-out validation set for each method and dataset. Specifically, all methods share the same grid search range of the window size q ranging from {2 0 , 2 1 , . . . , 2 9 } if applied. For LRidge and LSVR, the regularization coefficient ? is chosen from {2 ?10 , 2 ?8 , . . . , 2 8 , 2 10 }. For GP, the RBF kernel bandwidth ? and the noise level ? are chosen from {2 ?10 , 2 ?8 , . . . , 2 8 , 2 10 }. For TRMF, the hidden dimension is chosen from {2 2 , . . . , 2 6 } and the regularization coefficient ? is chosen from <ref type="figure">{0.1, 1, 10}</ref>. For LST-Skip and LST-Attn, we adopted the training strategy described in Section 3.8. The hidden dimension of the Recurrent and Convolutional layer is chosen from {50, 100, 200}, and {20, 50, 100} for Recurrent-skip layer. The skip-length p of Recurrent-skip layer is set as 24 for the Traffic and Electricity dataset, and tuned range from 2 1 to 2 <ref type="bibr" target="#b5">6</ref> for the Solar-Energy and Exchange-Rate datasets. The regularization coefficient of the AR component is chosen from {0.1, 1, 10} to achieve the best performance. We perform dropout after each layer, except input and output ones, and the rate usually is set to 0.1 or 0.2. The Adam <ref type="bibr" target="#b17">[18]</ref> algorithm is utilized to optimize the parameters of our model. <ref type="table">Table 2</ref> summarizes the evaluation results of all the methods (8) on all the test sets (4) in all the metrics (3). We set horizon = {3, 6, 12, 24}, respectively, which means the horizons was set from 3 to 24 hours for the forecasting over the Electricity and Traffic data, from 30 to 240 minutes over the Solar-Energy data, and from 3 to 24 days over the Exchange-Rate data. The larger the horizons, the harder the prediction tasks. The best result for each (data, metric) pair is highlighted in bold face in this table. The total count of the bold-faced results is 17 for LSTNet-Skip (one version of the proposed LSTNet), 7 for LSTNet-Attn (the other version of our LSTNet), and between 0 to 3 for the rest of the methods. Clearly, the two proposed models, LSTNet-skip and LSTNet-Attn, consistently enhance over state-of-the-art on the datasets with periodic pattern, especially in the settings of large horizons. Besides, LSTNet outperforms the strong neural baseline RNN-GRU by 9.2%, 11.7%, 22.2% in RSE metric on Solar-Energy, Traffic and Electricity dataset respectively when the horizon is 24, demonstrating the effectiveness of the framework design for complex repetitive patterns. What's more, when the periodic pattern q is not clear from applications, users may consider LSTNet-attn as alternative over LSTNet-skip, given the former still yield considerable improvement over the baselines. But the proposed LSTNet is slightly worse than AR and LRidge on the Exchange-Rate dataset. Why? Recall that in Section 4.3 and <ref type="figure">Figure 3</ref> we used the autocorrelation curves of these  <ref type="table" target="#tab_0">Methods  Metrics  3  6  12  24  3  6  12  24  3  6  12  24  3  6  12</ref>   <ref type="table">Table 2</ref>: Results summary (in RSE and CORR) of all methods on four datasets: 1) each row has the results of a specific method in a particular metric; 2) each column compares the results of all methods on a particular dataset with a specific horizon value;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Main Results</head><p>3) bold face indicates the best result of each column in a particular metric; and 4) the total number of bold-faced results of each method is listed under the method name within parentheses.</p><p>datasets to show the existence of repetitive patterns in the Solar-Energy, Traffic and Electricity datasets but not in Exchange-Rate.</p><p>The current results provide empirical evidence for the success of LSTNet models in modeling long-term and short-term dependency patterns when they do occur in data. Otherwise, LSTNet performed comparably with the better ones (AR and LRidge) among the representative baselines.</p><p>Compared the results of univariate AR with that of the multivariate baseline methods (LRidge, LSVR and RNN), we see that in some datasets, i.e. Solar-Energy and Traffic, the multivariate approaches is stronger, but weaker otherwise, which means that the richer input information would causes overfitting in the traditional multivariate approaches. In contrast, the LSTNet has robust performance in different situations, partly due to its autoregressive component, which we will discuss further in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study</head><p>To demonstrate the efficiency of our framework design, a careful ablation study is conducted. Specifically, we remove each component one at a time in our LSTNet framework. First, we name the LSTNet without different components as follows. For different baselines, we tune the hidden dimension of models such that they have similar numbers of model parameters to the completed LSTNet model, removing the performance gain induced by model complexity.</p><p>The test results measured using RSE and CORR are shown in <ref type="figure">Figure 5</ref>  <ref type="bibr" target="#b5">6</ref> . Several observations from these results are worth highlighting:</p><p>? The best result on each dataset is obtained with either LST-Skip or LST-Attn. The conclusion is that our architecture design is most robust across all experiment settings, especially with the large horizons.</p><p>As for why the AR component would have such an important role, our interpretation is that AR is generally robust to the scale changing in data. To empirically validate this intuition we plot one dimension (one variable) of the time series signals in the electricity consumption dataset for the duration from 1 to 5000 hours in <ref type="figure">Figure  6</ref>, where the blue curve is the true data and the red curve is the system-forecasted signals. We can see that the true consumption suddenly increases around the 1000th hour, and that LSTNet-Skip successfully captures this sudden change but LSTw/oAR fails to react properly. In order to better verify this assumption, we conduct a simulation experiment. First, we randomly generate an autoregressive process with the scale changing by the following steps. Firstly, we randomly sample a vector, w ? N (0, I ), w ? R p , where p is a given window size. Then the generated autoregressive process x t can be described as</p><formula xml:id="formula_15">x t = p i=1 w i x t ?i + ?<label>(12)</label></formula><p>where ? ? N (?, 1). To inject the scale changing, we increase the mean of Gaussian noise by ? 0 everyT timestamp. Then the Gaussian noise of time series x t can be written as</p><formula xml:id="formula_16">? ? N (?t/T ? ? 0 , 1)<label>(13)</label></formula><p>where the ??? denotes the floor function. We split the time series as the training set and test in chronological order, and test the RNN-GRU and the LSTNet models. The result is illustrated in <ref type="figure" target="#fig_7">Figure  4</ref>. In summary, this ablation study clearly justifies the efficiency of our architecture design. All components have contributed to the excellent and robust performance of LSTNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Mixture of long-and short-term patterns</head><p>To illustrate the success of LSTNet in modeling the mixture of short-term and long-term recurring patterns in time series data, <ref type="figure">Figure 7</ref> compares the performance of LSTNet and VAR on an specific time series (one of the output variables) in the Traffic dataset. As discussed in Section 4.3, the Traffic data exhibit two kinds of repeating patterns, i.e. the daily ones and the weekly ones. We can see in <ref type="figure">Figure 7</ref> that the true patterns (in blue) of traffic occupancy are very different on Fridays and Saturdays, and another on Sunday and Monday. The <ref type="figure">Figure 7</ref> is the prediction result of the VAR model (part (a)) and LSTNet (part (b)) of a traffic flow monitor sensor, where their hyper-parameters are chosen according to the RMSE result on the validation set. The figure shows that the VAR model is only capable to deal with the short-term patterns. The pattern of prediction results of the VAR model only depend on the day before the predictions. We can clearly see that the results of it in Saturday (2rd and 9th peaks) and Monday (4th and 11th peaks) is different from the ground truth, where the ground truth of Monday (weekday) has two peaks, one peak for Saturday (weekend). In the contrary, our proposed LSTNet model performs two patterns for weekdays and weekends respectfully. This example proves the ability of LSTNet model to memorize short-term and long-term recurring patterns simultaneously, which the traditional forecasting model is not equipped, and it is crucial in the prediction task of the real world time series signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we presented a novel deep learning framework (LST-Net) for the task of multivariate time series forecasting. By combining the strengths of convolutional and recurrent neural networks and an autoregressive component, the proposed approach significantly improved the state-of-the-art results in time series forecasting on multiple benchmark datasets. With in-depth analysis and empirical evidence, we show the efficiency of the architecture of LSTNet model, and that it indeed successfully captures both shortterm and long-term repeating patterns in data, and combines both linear and non-linear models for robust prediction.</p><p>For future research, there are several promising directions in extending the work. Firstly, the skip length p of the skip-recurrent layer is a crucial hyper-parameter. Currently, we manually tune it based on the validation dataset. How to automatically choose p according to data is an interesting problem. Secondly, in the convolution layer we treat each variable dimension equally, but in the real world dataset, we usually have rich attribute information. Integrating them into the LSTNet model is another challenging problem.   <ref type="figure">Figure 7</ref>: The true time series (blue) and the predicted ones (red) by VAR (a) and by LSTNet (b) for one variable in the Traffic occupation dataset. The X axis indicates the week days and the forecasting horizon = 24. VAR inadequately predicts similar patterns for Fridays and Saturdays, and ones for Sundays and Mondays, while LSTNet successfully captures both the daily and weekly repeating patterns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>M</head><label></label><figDesc>o n T u e W e s T h u Fr i S a t S u n M o n T u e W e s T h u Fr i S a t S u n 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>An overview of the Long-and Short-term Time-series network (LSTNet)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(a ) Figure 3 :</head><label>)3</label><figDesc>Traffic dataset (b) Solar-Energy dataset (c) Electricity dataset (d) Exchange-Rate dataset Autocorrelation graphs of sampled variables form four datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>24 AR RSE 0 .</head><label>240</label><figDesc>2435 0.3790 0.5911 0.8699 0.5991 0.6218 0.6252 0.6293 0.0995 0.1035 0.1050 0.1054 0.0228 0.0279 0.0353 0.0445 (3) CORR 0.9710 0.9263 0.8107 0.5314 0.7752 0.7568 0.7544 0.7519 0.8845 0.8632 0.8591 0.8595 0.9734 0.9656 0.9526 0.9357 LRidge RSE 0.2019 0.2954 0.4832 0.7287 0.5833 0.5920 0.6148 0.6025 0.1467 0.1419 0.2129 0.1280 0.0184 0.0274 0.0419 0.0675 (3) CORR 0.9807 0.9568 0.8765 0.6803 0.8038 0.8051 0.7879 0.7862 0.8890 0.8594 0.8003 0.8806 0.9788 0.9722 0.9543 0.9305 LSVR RSE 0.2021 0.2999 0.4846 0.7300 0.5740 0.6580 0.7714 0.5909 0.1523 0.1372 0.1333 0.1180 0.0189 0.0284 0.0425 0.0662 (1) CORR 0.9807 0.9562 0.8764 0.6789 0.7993 0.7267 0.6711 0.7850 0.8888 0.8861 0.8961 0.8891 0.9782 0.9697 0.9546 0.9370 TRMF RSE 0.2473 0.3470 0.5597 0.9005 0.6708 0.6261 0.5956 0.6442 0.1802 0.2039 0.2186 0.3656 0.0351 0.0875 0.0494 0.0563 (0) CORR 0.9703 0.9418 0.8475 0.5598 0.6964 0.7430 0.7748 0.7278 0.8538 0.8424 0.8304 0.7471 0.9142 0.8123 0.8993 0.8678 GP RSE 0.2259 0.3286 0.5200 0.7973 0.6082 0.6772 0.6406 0.5995 0.1500 0.1907 0.1621 0.1273 0.0239 0.0272 0.0394 0.0580 (1) CORR 0.9751 0.9448 0.8518 0.5971 0.7831 0.7406 0.7671 0.7909 0.8670 0.8334 0.8394 0.8818 0.8713 0.8193 0.8484 0.8278 VARMLP RSE 0.1922 0.2679 0.4244 0.6841 0.5582 0.6579 0.6023 0.6146 0.1393 0.1620 0.1557 0.1274 0.0265 0.0304 0.0407 0.0578 (0) CORR 0.9829 0.9655 0.9058 0.7149 0.8245 0.7695 0.7929 0.7891 0.8708 0.8389 0.8192 0.8679 0.8609 0.8725 0.8280 0.7675 RNN-GRU RSE 0.1932 0.2628 0.4163 0.4852 0.5358 0.5522 0.5562 0.5633 0.1102 0.1144 0.1183 0.1295 0.0192 0.0264 0.0408 0.0626 (0) CORR 0.9823 0.9675 0.9150 0.8823 0.8511 0.8405 0.8345 0.8300 0.8597 0.8623 0.8472 0.8651 0.9786 0.9712 0.9531 0.9223 LST-Skip RSE 0.1843 0.2559 0.3254 0.4643 0.4777 0.4893 0.4950 0.4973 0.0864 0.0931 0.1007 0.1007 0.0226 0.0280 0.0356 0.0449 (17) CORR 0.9843 0.9690 0.9467 0.8870 0.8721 0.8690 0.8614 0.8588 0.9283 0.9135 0.9077 0.9119 0.9735 0.9658 0.9511 0.9354 LST-Attn RSE 0.1816 0.2538 0.3466 0.4403 0.4897 0.4973 0.5173 0.5300 0.0868 0.0953 0.0984 0.1059 0.0276 0.0321 0.0448 0.0590 (7) CORR 0.9848 0.9696 0.9397 0.8995 0.8704 0.8669 0.8540 0.8429 0.9243 0.9095 0.9030 0.9025 0.9717 0.9656 0.9499 0.9339</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>?</head><label></label><figDesc>LSTw/oskip: The LSTNet models without the Recurrentskip component and attention component. ? LSTw/oCNN: The LSTNet-skip models without the Convolutional component. ? LSTw/oAR: The LSTNet-skip models without the AR component.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>?</head><label></label><figDesc>Removing the AR component (in LSTw/oAR) from the full model caused the most significant performance drops on most of the datasets, showing the crucial role of the AR component in general.? Removing the Skip and CNN components in (LSTw/oCNN or LSTw/oskip) caused big performance drops on some datasets but not all. All the components of LSTNet together leads to the robust performance of our approach on all the datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Simulation Test: Left side is the training set and right side is test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Results of LSTNet in the ablation tests on the Solar-Energy, Traffic and Electricity dataset The predicted time series (red) by LSTw/oAR (a) and by LST-Skip (b) vs. the true data (blue) on Electricity dataset with horizon = 24 Fr i S a t S u n M o n T u e W e s T h u Fr i S a t S u n M o n T u Fr i S a t S u n M o n T u e W e s T h u Fr i S a t S u n M o n T u e 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>summarizes the corpus statistics.</cell></row><row><cell>? Traffic 3 : A collection of 48 months (2015-2016) hourly data</cell></row><row><cell>from the California Department of Transportation. The data</cell></row><row><cell>describes the road occupancy rates (between 0 and 1) mea-</cell></row><row><cell>sured by different sensors on San Francisco Bay area free-</cell></row><row><cell>ways.</cell></row><row><cell>? Solar-Energy 4 : the solar power production records in the</cell></row><row><cell>year of 2006, which is sampled every 10 minutes from 137</cell></row><row><cell>PV plants in Alabama State.</cell></row><row><cell>? Electricity 5 : The electricity consumption in kWh was recorded</cell></row><row><cell>every 15 minutes from 2012 to 2014, for n = 321 clients. We</cell></row><row><cell>converted the data to reflect hourly consumption;</cell></row><row><cell>? Exchange-Rate: the collection of the daily exchange rates of</cell></row><row><cell>eight foreign countries including Australia, British, Canada,</cell></row><row><cell>Switzerland, China, Japan, New Zealand and Singapore rang-</cell></row><row><cell>ing from 1990 to 2016.</cell></row><row><cell>All datasets have been split into training set (60%), validation set</cell></row><row><cell>(20%) and test set (20%) in chronological order. To facilitate future</cell></row><row><cell>research in multivariate time series forecasting, we publicize all</cell></row><row><cell>raw datasets and the one after preprocessing in the website.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Dataset Statistics, where T is length of time series, D is number of variables, L is the sample rate.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Both RNN and LSTNet can memorize the pattern in training set (left side). But, the RNN-GRU model cannot follow the scale changing pattern in the test set (right side). Oppositely, the LSTNet model fits the test set much better. In other words, the normal RNN module, or says the neural-network component in LSTNet, may not be sufficiently sensitive to violated scale fluctuations in data (which is typical in Electricity data possibly due to random events for public holidays or temperature turbulence, etc.), while the simple linear AR model can make a proper adjustment in the forecasting.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://pems.dot.ca.gov 4 http://www.nrel.gov/grid/solar-power-data.html 5 https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We omit the results in RAE as it shows similar comparison with respect to the relative performance among the methods.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Time series analysis: forecasting and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Ljung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distribution of residual autocorrelations in autoregressive-integrated moving average time series models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Pierce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">332</biblScope>
			<biblScope unit="page" from="1509" to="1526" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Support vector machine with adaptive parameters in financial time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E H</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1506" to="1518" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01865</idno>
		<title level="m">Recurrent neural networks for multivariate time series with missing values</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent networks and narma modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Atlas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="301" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonlinear dynamic boltzmann machines for timeseries prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Osogami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI-17. Extended research report available at goo. gl/Vd0wna</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bayesian inference and learning in gaussian process state-space models with particle mcmc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frigola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lindsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Sch?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frigola-Alcade</surname></persName>
		</author>
		<title level="m">Bayesian Time Series Learning with Gaussian Processes</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Time series analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Hamilton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Princeton university press Princeton</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep, convolutional, and recurrent models for human activity recognition using wearables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Halloran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ploetz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.08880</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hybrid neural network models for hydrologic time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="585" to="592" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Financial time series forecasting using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="307" to="319" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks: A unified approach to action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Workshops</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Forecasting macroeconomic time series: Lasso-based approaches and their forecast combinations with dynamic factor models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="996" to="1015" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning to diagnose with lstm recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wetzell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03677</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">New introduction to multiple time series analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">General exponential smoothing and the equivalent arma process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mckenzie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="344" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Melnyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06606</idno>
		<title level="m">Estimating structured vector autoregressive model</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust estimation of transition matrices in high dimensional heavy-tailed vector autoregressive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caffo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1843" to="1851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gaussian processes for time-series modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ebden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reece</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aigrain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phil. Trans. R. Soc. A</title>
		<imprint>
			<biblScope unit="volume">371</biblScope>
			<biblScope unit="page">20110550</biblScope>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Support vector method for function approximation, regression estimation, and signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Golowich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="281" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks on multichannel time series for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence (IJCAI)<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="25" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Temporal regularized matrix factorization for high-dimensional time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="847" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning: A generic approach for extreme condition traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Demiryurek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 SIAM International Conference on Data Mining</title>
		<meeting>the 2017 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="777" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Forecasting with artificial neural networks:: The state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Patuwo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of forecasting</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="62" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Time series forecasting using a hybrid arima and neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="159" to="175" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
