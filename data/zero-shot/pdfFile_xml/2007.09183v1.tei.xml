<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bi-directional Cross-Modality Feature Propagation with Separation-and-Aggregation Gate for RGB-D Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
							<email>jbwang@ie.cuhk.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="department">The Chinese</orgName>
								<orgName type="institution">University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
							<email>qianchen@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<email>hsli@ee.cuhk.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="department">The Chinese</orgName>
								<orgName type="institution">University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
							<email>zeng@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bi-directional Cross-Modality Feature Propagation with Separation-and-Aggregation Gate for RGB-D Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>RGB-D Semantic Segmentation, Cross-Modality Feature Propagation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depth information has proven to be a useful cue in the semantic segmentation of RGB-D images for providing a geometric counterpart to the RGB representation. Most existing works simply assume that depth measurements are accurate and well-aligned with the RGB pixels and models the problem as a cross-modal feature fusion to obtain better feature representations to achieve more accurate segmentation. This, however, may not lead to satisfactory results as actual depth data are generally noisy, which might worsen the accuracy as the networks go deeper. In this paper, we propose a unified and efficient Cross-modality Guided Encoder to not only effectively recalibrate RGB feature responses, but also to distill accurate depth information via multiple stages and aggregate the two recalibrated representations alternatively. The key of the proposed architecture is a novel Separation-and-Aggregation Gating operation that jointly filters and recalibrates both representations before cross-modality aggregation. Meanwhile, a Bi-direction Multi-step Propagation strategy is introduced, on the one hand, to help to propagate and fuse information between the two modalities, and on the other hand, to preserve their specificity along the long-term propagation process. Besides, our proposed encoder can be easily injected into the previous encoder-decoder structures to boost their performance on RGB-D semantic segmentation. Our model outperforms state-of-the-arts consistently on both in-door and out-door challenging datasets 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation, which aims at assigning each pixel with different semantic labels, is a long-standing task. Besides exploiting various contextual information from the visual cues <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b42">43]</ref>, depth data have recently been utilized as supplementary information to RGB data to achieve improved segmentation accuracy <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19]</ref>. Depth data naturally complements RGB signals by providing the 3D geometry to 2D visual information, which is robust to illumination changes and helps better distinguishing various objects.</p><p>Although significant advances have been achieved in RGB semantic segmentation, directly feeding the complementary depth data into existing RGB semantic segmentation frameworks <ref type="bibr" target="#b24">[25]</ref> or simply ensemble results of two modalities <ref type="bibr" target="#b5">[6]</ref> might lead to inferior performance. The key challenges lie in two aspects. <ref type="bibr" target="#b0">(1)</ref> The substantial variations between RGB and Depth modalities. RGB and depth data show different characteristics. How to effectively identify their differences and unify the two types of information into an efficient representation for semantic segmentation is still an open problem. <ref type="bibr" target="#b1">(2)</ref> The uncertainty of depth measurements. Depth data provided with existing benchmarks are mainly captured by Time-of-Flight or structured light cameras, such as Kinect, AsusXtion and Re-alSense etc. The depth measurements are generally noisy due to different object materials and limited distance measurement range. The noise is more apparent for out-door scenes and results in undesirable segmentation, as shown in <ref type="figure" target="#fig_0">Fig 1.</ref> Most existing RGB-D based methods mainly focus on tackling the first challenge. Standard practice is to use the depth data 2 as another input and adopt Fully Convolutional Network (FCN)-like architectures with feature fusion schemas, e.g., convolution and modality-based affinity etc., to fuse the features of two modalities <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38]</ref>. The fused feature is then used to recalibrate the subsequent RGB feature responses or predicted results. Although these methods provide plausible solutions to unify the two types of information, the assumption of the input depth data being accurate and well-aligned with RGB signals might not be true, making these methods sensitive to in-the-wild samples. Moreover, how to ensure that the network fully utilizes information from both modalities remains an open problem. Recently, some works <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b38">39]</ref> attempt to tackle the second challenge by diminishing the network's sensitivity to the quality of depth measurements. Instead of utilizing depth data as an extra input, they propose to distill the depth features via multi-task learning and regard depth data as extra supervision for training. Specifically, <ref type="bibr" target="#b38">[39]</ref> introduces a two-stage framework, which first predicts several intermediate tasks including depth estimation and then uses the outputs of these intermediate tasks as the multi-modal input to final tasks. <ref type="bibr" target="#b43">[44]</ref> proposes a pattern-affinitive propagation with jointly predicting depth, surface normal and semantic segmentation to capture correlative information between modalities. We argue that there exists an inherent inefficacy in such design, i.e. the interaction and correlation of RGB and depth information are only implicitly modeled. The complementarity of the two types of data for semantic segmentation was not well studied in this way.</p><p>Motivated by the above observations, we propose to tackle both two challenges in a simple yet effective framework by introducing a novel cross-modality guided encoder to FCN-like RGB-D semantic segmentation backbones. The key idea of the proposed framework is to leverage both channel-wise and spatialwise correlation of the two modalities to firstly squeeze the exceptional feature responses of depth, which effectively suppresses feature responses from the lowquality depth measurements, and then use the suppressed depth representations to refine RGB features. In practice, we devise the steps bi-directionally due to the in-door RGB sources also contain noisy features. In contrast to depth data, the RGB noisy features are usually caused by similar appearance of different neighboring objects. We denote the above process as depth-feature recalibration and RGB-feature recalibration, respectively. We therefore introduce a new gate unit, namely the Separation-and-Aggregation Gate (SA-Gate), to improve the quality of the multi-modality representation by encouraging the network to recalibrate and spotlight the modality-specific feature of each modality first, and then selectively aggregate the informative features from both modalities for the final segmentation. To effectively take advantage of the differences of features between the two modalities, we further introduce the Bi-direction Multi-step Propagation (BMP) that encourages the two streams to better preserve their specificity during the information interaction process in the encoder stage.</p><p>Our contributions can be summarized into three-fold:</p><p>-We propose a novel bi-directional cross-modality guided encoder for RGB-D semantic segmentation. With the proposed SA-Gate and BMP modules, we could effectively diminish the influence of noisy depth measurements, and also allow incorporating sufficiently complementary information to form discriminative representations for segmentation. -Comprehensive evaluation on the NYUD V2 dataset shows significant improvements by our approach when integrated into state-of-the-art RGB semantic segmentation networks, which demonstrate the generalization of our encoder as a plug-and-play module. -The proposed method achieves state-of-the-art performances on both in-door and challenging out-door semantic segmentation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RGB-D Semantic Segmentation</head><p>With the development of depth sensors, recently there is a surge of interest in leveraging depth data as a geometry augmentation for RGB semantic segmentation task, dubbed as RGB-D semantic segmentation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b2">3]</ref>. According to specific functionality of depth information suited in different architectures, current RGB-D based methods could be roughly divided into two categories. Most of the works treat depth data as an additional input source to recalibrate the RGB feature responses either implicitly or explicitly. Long et al. <ref type="bibr" target="#b24">[25]</ref> shows simply averaging final score maps of RGB and D modalities helps enforce the inter-object discrimination in the in-door setting. Li et al. <ref type="bibr" target="#b22">[23]</ref> utilize the LSTM layers to selectively fuse the feature from the two modalities input. With a similar target, <ref type="bibr" target="#b5">[6]</ref> proposes locality-sensitive deconvolution networks along with a gated fusion module. Several recent works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17]</ref> extend the RGB feature recalibration process from the final outputs of a dual-path network to different stages of the backbone, encouraging better recalibration with multi-level cross-modality feature fusion. To guide the recalibration with explicit cross-modality interaction modeling, some works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37]</ref> tailor general 2D operations to 2.5D behaviors with depth guidance. For example, <ref type="bibr" target="#b32">[33]</ref> proposes depth-aware convolution and pooling operations to help recalibrating RGB feature responses in depth-consistent regions. <ref type="bibr" target="#b19">[20]</ref> proposes a depth-aware gate module that adaptively selects the pooling field size in a CNN according to object scale. 3DGNN <ref type="bibr" target="#b26">[27]</ref> introduces a 3D graph neural network to model accurate context with geometry cues provided by depth. Alternatively, some approaches regard the depth data as an extra supervised signal to recalibrate the RGB counterpart in a multi-task learning manner. For example, <ref type="bibr" target="#b43">[44]</ref> proposes a pattern affinity propagation network to regularize and boost complementary tasks.</p><p>[39] introduces a multi-modal distillation model to pass the valid messages from depth to RGB features.</p><p>Different from previous works that hold the ideal assumption of depth source's quality and mainly focus on in-door setting, we try to extend the task to the in-the-wild environment, e.g., CityScapes dataset. The out-door setting is more challenging due to the inevitable noisy signals contained in the depth data. In this work, we try to recalibrate RGB feature responses from a filtered depth representation and vice versa, which effectively enhance the strength of representations for both modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention Mechanism</head><p>Attention mechanisms have been widely utilized in kinds of computer vision tasks, serving as the tools to spotlight the most representative and informative regions of input signals <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34]</ref>. For example, to improve the performance of the image/video classification task, SENet <ref type="bibr" target="#b15">[16]</ref> introduces a self recalibrate gating mechanism by model importance among different channels of feature maps. Based on similar spirits, SKNet <ref type="bibr" target="#b20">[21]</ref> designs a channel-wise attention module to select kernel sizes to adaptively adjust its receptive field size based on multiple scales of input information. <ref type="bibr" target="#b33">[34]</ref> introduces a non-local operation which explores the similarity of each pair of points in space. For the segmentation task, a well-designed attention module could encourage the network to learn helpful context information effectively. For instance, DFN <ref type="bibr" target="#b40">[41]</ref> introduces a channel attention block to select the more discriminative features from multi-level feature maps to get more accurate semantic information. DANet <ref type="bibr" target="#b10">[11]</ref> proposes two types of attention modules to model the semantic inter-dependencies in spatial and channel dimensions respectively.</p><p>However, the main challenge of RGB-D semantic segmentation task is how to make full use of cross-modality data under the substantial variations and noisy signals between modalities. The proposed SA-Gate is the first to focus on the noisy features of cross-modalities by tailoring the attention mechanisms. The SA-Gate module is specialized for suppressing the exceptional noisy feature of depth data and recalibrate its counterpart RGB feature responses in a unified manner at first, and then fuses the cross-modality information with a softmax gating that is guided by the recalibrated features, achieving effective and efficient cross-modality feature aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>RGB-D semantic segmentation needs to aggregate features from both RGB and depth modalities. However, both modalities have inevitably noisy information. Specifically, depth measurements are inaccurate due to the characteristics of depth sensors and RGB features might generate confusing results due to the high appearance similarity between the objects. An effective cross-modality aggregation scheme should be able to identify their strengths from each feature as well as unify the most informative cross-modality features into an efficient representation. To this end, we put forward a novel cross-modality guided encoder. The overall framework of the proposed approach is depicted in <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>, which consists of a cross-modality guided encoder and a segmentation decoder. Given RGB-D data as inputs 3 , our encoder recalibrates and fuses the complementary information from the two modalities via the SA-Gate unit, and then propagates the fused multi-modal features along with modality-specific features via the Bi-direction Multi-step Propagation (BMP) module. The information is then decoded by a segmentation decoder network to generate the segmentation map. We will detail each component in the remaining parts of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bi-direction Guided Encoder</head><p>Separation-and-Aggregation (SA) Gate. To ensure informative feature propagation between modalities, the SA-Gate is designed with two operations. One is feature recalibration on each single modality, and the other is cross-modality feature aggregation. The operations are in terms of Feature Separation (FS) and Feature Aggregation (FA) parts, as illustrated in <ref type="figure" target="#fig_1">Fig 2 (b)</ref>.</p><p>Feature Separation (FS). We take depth stream for example. Due to physical characteristics of depth sensors, noisy signals in depth modality frequently show up in regions close to object's boundaries or partial surfaces outside the scope of depth sensors, as shown in the second column of <ref type="figure">Fig. 3</ref>. Hence, the network is expected to first filter noisy signals surrounding these local regions to avoid misleading information propagation on the process of recalibrating complementary RGB modality and aggregating cross-modality features. In practice, we exploit high confident activations in RGB stream to filter out exceptional depth activations at the same level. To do so, global spatial information of both modalities should be embedded and squeezed to obtain a cross-modality attention vector first. We achieve this by a global average pooling along the channel-wise dimensions of two modalities, which is followed by concatenation and a MLP operation to obtain attention vector. Suppose we have two input feature maps denoted as RGB in ? R C?H?W and HHA in ? R C?H?W , above operations could be formulated as</p><formula xml:id="formula_0">I = F gp (RGB in HHA in ),<label>(1)</label></formula><p>where denotes the concatenation of feature maps from two modalities, F gp refers to global average pooling, I = (I 1 , . . . , I k , . . . , I 2C ) is the cross-modality global descriptor for collecting expressive statistics for the whole inputs. Then, the cross-modality attention vector for the depth input is learned by</p><formula xml:id="formula_1">W hha = ?(F mlp (I)), W hha ? R C ,<label>(2)</label></formula><p>where F mlp denotes MLP network, ? denotes sigmoid function scaling the weight value into (0, 1). By doing so, the network can take advantage of the most informative visual appearance and geometry features, and thus tends to effectively suppress the importance of noisy features in depth stream. Then, we could obtain a less noisy depth representation, namely Filtered HHA, through a channel-wise multiplication between input depth feature maps and the cross-modality gate:</p><formula xml:id="formula_2">HHA filtered = HHA in W hha .<label>(3)</label></formula><p>With a filtered depth representation counterpart, the RGB feature responses could be recalibrated with more accurate depth information. We devise the recalibration operation as the summation of the two modalities:</p><formula xml:id="formula_3">RGB rec = HHA filtered + RGB in ,<label>(4)</label></formula><p>where RGB rec denotes recalibrated RGB feature maps. The general idea behind the formula is that, instead of directly using element-wise product to reweight RGB feature with regarding depth features as recalibrate coefficients, the proposed operation using summation could be viewed as some kind of offset to refine RGB feature responses at corresponding positions, as demonstrated in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>In practice, we implement recalibration step in a symmetric and bi-directional manner, such that low confident activations in RGB stream could also be suppressed in the same manner and filtered RGB information RGB filtered could inversely recalibrate the depth feature responses to form a more robust depth representation HHA rec . We visualize feature maps of HHA before and after Feature Separation Part in <ref type="figure">Fig. 3</ref>. The RGB counterpart is shown in the supplementation. <ref type="figure">Fig. 3</ref>. Visualization of depth features before and after FSP on CityScapes validation set. We can observe that objects have more precise shapes after FSP and invalid partial surfaces are completed. More explanation is illustrated in the supplemental material Feature Aggregation (FA). RGB and D features are strongly complementary to each other. To make full use of their complementarity, we need to complementarily aggregate the cross-modality features at a certain position in space according to their characterization capabilities. To achieve this, we consider both characteristics of these two modalities and generate spatial-wise gates for both RGB in and HHA in to control information flow of each modality feature map with soft attention mechanism, which is visualized in <ref type="figure" target="#fig_1">Figure 2</ref> (b) and marked by the second red frame. To make the gate more precise, we use recalibrated RGB and HHA feature maps from FS part, i.e., RGB rec ? R C?H?W and HHA rec ? R C?H?W , to generate the gate. We first concatenate these two feature maps to combine their features at a certain position in space. Then we define two mapping functions to map high-dimensional feature to two different spatial-wise gates:</p><formula xml:id="formula_4">F rgb :F concat2 ? G rgb ? R 1?H?W ,<label>(5)</label></formula><formula xml:id="formula_5">F hha :F concat2 ? G hha ? R 1?H?W ,<label>(6)</label></formula><p>where F concat2 ? R 2C?H?W is the concatenated feature, G rgb is the spatial-wise gate for RGB feature map, and G hha is the spatial-wise gate for HHA feature map. In practice, we use a 1?1 convolution to implement this mapping function. A softmax function is applied on these two gates:</p><formula xml:id="formula_6">A (i,j) rgb = e G (i,j) rgb e G (i,j) rgb + e G (i,j) hha , A (i,j) hha = e G (i,j) hha e G (i,j) rgb + e G (i,j) hha<label>(7)</label></formula><p>where A rgb , A hha ? R 1?H?W and A</p><formula xml:id="formula_7">(i,j) rgb +A (i,j) hha = 1. G (i,j)</formula><p>rgb is the weight assigned to each position in the RGB feature map and G (i,j) hha is the weight assigned to each position in the HHA feature map. The final merged feature M can be obtained by weighting the RGB and HHA maps:</p><formula xml:id="formula_8">M i,j = RGB (i,j) in ? A (i,j) rgb + HHA (i,j) in ? A (i,j) hha .<label>(8)</label></formula><p>So far, we have added gated RGB and HHA feature maps to obtain the fused feature maps M . Since SA-Gate is injected into the encoder stage, we then average the fused features and the original input to obtain RGB out and HHA out respectively, which share similar spirits with residual learning. Bi-directional Multi-step Propagation (BMP). By normalizing the sum of two weights at each position to 1, the numerical scale of the weighted feature will not significantly differ from the input RGB or HHA. Therefore, it has no negative influence on the learning of the encoder or the loading of the pre-trained parameters. For each layer l, we use the output M l generated by the l-th SA-Gate to refine the raw output of the l-th layer in the encoder: RGB l out = (RGB l in + M l )/2, HHA l out = (HHA l in + M l )/2. This is a bi-directional propagation process and the refined results will be propagated to the next layer in the encoder for more accurate and efficient encoding of the two modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Segmentation Decoder</head><p>The decoder can adopt almost any design of decoder from SOTA RGB-based segmentation networks, since SA-Gate is a plug-and-play module and can make good use of complementary information of cross-modality on encoder stage. We show results of combining our encoder with different decoders in <ref type="table" target="#tab_4">Table 6</ref>. We choose DeepLabV3+ <ref type="bibr" target="#b1">[2]</ref> as our decoder for it achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct comprehensive experiments on in-door NYU Depth V2 and outdoor CityScapes datasets in terms of two metrics: mean Intersection-over-Union (mIoU ) and pixel accuracy (pixel acc.). We also evaluate our model on SUN-RGBD dataset (Please refer to the supplemental material for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>NYU Depth V2 <ref type="bibr" target="#b27">[28]</ref> contains 1449 RGB-D images with 40-class labels, in which 795 images are used for training and the rest 654 images are for testing. CityScapes <ref type="bibr" target="#b7">[8]</ref> contains images from 27 cities. There are 2975 images for training, 500 for validation and 1525 for testing. Each image has a resolution of 2048 ? 1024 and is fine-annotated with pixel-level labels of 19 semantic classes. We do not use additional coarse annotations in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We use PyTorch framework. For data augmentation, we use random horizontal flipping and scaling with scales [0.5,1.75]. When comparing with SOTA methods, we adopt flipping and multi-scale inference strategies as a test-time augmentation to boost the performance. More details are shown in the supplemental material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Efficiency Analysis</head><p>To verify whether the proposed cross-modality feature propagation helps and is efficient, we compare the final model with the RGB-D baseline. We average predictions of two parallel DeepLab V3+ as RGB-D baseline. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the proposed method achieves better performance with significantly less memory requirement and computational cost when compared with baseline.</p><p>The results indicate that aimlessly adding parameters to a multi-modality network will not bring extra representational power to better recognize objects. In contrast, a well-design cross-modality mechanism, like proposed cross-modality feature propagation, helps to learn more powerful representations to improve performance more efficiently. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We perform ablation studies on our design choices under same hyperparameters. Feature Separation. We employ the FS operation before the feature aggregation in SA-Gate, to filter out noisy features for bi-directional recalibration step. To verify effectiveness of this operation, we ablate each design of FS in <ref type="table" target="#tab_1">Table 2</ref>. Note that we ablate four different architectures and replace all FS parts in the network for comparison. 'Concat' represents we concatenate RGB in and HHA in feature maps and directly pass them to feature aggregation part. 'Self-global' represents we filter single modality features with its own global information. 'Cross-global' represents the filtered RGB is added to input RGB and vice versa. The filtering guidance comes from cross-modality global information. 'Product' means we multiply RGB in by HHA filtered and vice versa. We see that from column 2 to 4, not using cross-modality information to filter noisy feature or refine features without explicit cross-modality recalibration lead to about 1% drop. On the other hand, the last two columns indicate the cross-modality guidance (E.q 4) is more appropriate and effective than cross-modality re-weighting when doing cross-modality recalibration. Overall, these results show that proposed FS operator effectively filters incorrect messages and recalibrates feature responses, achieving the best performance among all compared designs. Feature Aggregation. We employ the SA-Gating mechanism to adaptively select the feature from the cross-modal data, according to their different characteristics at each spatial location. This gate can effectively control information flow of multimodal data. To evaluate the validity of the design, we perform ablation study on feature aggregation, as shown in <ref type="table">Table 3</ref>. The experiment setting is kept the same as above. 'Addition' represents directly adding the recalibrated RGB and HHA feature maps. 'Conv' represents conducting convolution on the concatenated feature map. 'Proposed' represents the FA operator. We see that FA operator leads to the best result, since it considers the spatial-wise relationship between two modalities and can better explore the complementary information.  Design of Encoder. We verify and analyze the effectiveness of proposed BMP to our encoder, and how it functions with the SA-Gate. Toward this end, we conduct two ablation studies as shown in <ref type="table">Table 4</ref> &amp; 5. We use ResNet-50 as our backbone here and directly upsampling the final score map by a factor of 16, without using a segmentation decoder. The first row in <ref type="table">Table 4</ref> &amp; 5 is the baseline that averages score maps generated by two ResNet-50 (RGB &amp; D). For the first ablation, we gradually embed SA-Gate unit behind different layers of ResNet50. Note that we generate score maps for both two sides and average them as final segmentation result. This setting is different from those above, because last block of ResNet may not be equipped with a SA-Gate in this part, i.e., no fused feature is generated from last block. From <ref type="table">Table 4</ref>, we observe that if SA-Gate is embedded into a higher stage, it will lead to relatively worse performance. Besides, when stacking SA-Gate stage by stage, the additional gain continuously reduces. These two phenomena show that features of different modalities are more different in lower stage and an early fusing will achieve better performance. <ref type="table" target="#tab_3">Table 5</ref> shows results of second experiment. We observe that both SA-Gate and BMP can boost performance. Meanwhile, they complement each other and performs better in the presence of the other component. Moreover, when associating <ref type="table" target="#tab_3">Table 5</ref> &amp; 2, we see that SA-Gate helps BMP better propagate valid information than other gate mechanisms. It demonstrates effectiveness and importance of a more accurate representation to the feature propagation. The Plug-and-Play Property of Proposed Encoder. We conduct ablation study to validate the flexibility and effectiveness of our method for different types  of decoders. Following recent RGB-based semantic segmentation algorithms, we splice their decoders with our model to form modified RGB-D versions (i.e., RGB-D w SA-Gate), as shown in <ref type="table" target="#tab_4">Table 6</ref>. We see that in the column 2 and 4, our method consistently helps achieving significant improvements against original RGB versions. Besides, comparing with naive RGB-D modifications, our method also boosts the performance at least 1.5% mIoU. Especially, with the decoders in Deeplab V3+ <ref type="bibr" target="#b1">[2]</ref>, our method achieves 3.7% mIoU improvements. The results verify both the flexibility and effectiveness of our method for various decoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Visualization of SA-Gate</head><p>We visualize first SA-Gate in our model to see what it has learned, as shown in <ref type="figure">Fig 4.</ref> Note that the black region in GT represents ignored pixels when calculating IoU. We reproduce RDFNet-101 <ref type="bibr" target="#b25">[26]</ref> in PyTorch with 48.7% mIoU on NYU Depth V2, which is close to the result in the original paper (49.1%). Red represents a higher weight assigned to RGB and blue represents a higher weight assigned to HHA. From column 4, we can see that RGB has a stronger response - at boundary and HHA responds well in glare and dark areas. The phenomenon is reasonable since RGB feature has more details in high contrast areas and HHA feature is not affected by lighting conditions. From row 1, details inside yellow boxes are lost in HHA while obvious in RGB. Our method successfully identifies chair legs and distinguishes table that looks similar to chair. In row 2, glare blurs the border of the photo frame. Since our model focuses more on HHA in this area, it predicts the photo frame more completely than RDFNet. Besides, our model captures more details than RDFNet on clothes stand. In row 3, cabinet in dark red is hard to recognize in RGB but with identifiable features in HHA. Improper fusion of RGB and HHA leads to erroneous semantics for this area (column 3). While our model pays more attention to HHA in this area to achieve more precise results.</p><formula xml:id="formula_9">- - - - - - - - - - - - - - - - - - 71.3 Shu Kong et al. * [20] - - - - - - - - - - - - - - - - - - - 78.2 PADNet * [39] - - - - - - - - - - - - - - - - - - - 80.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.6</head><p>Comparing with State-of-the-arts NYU Depth V2. Results are shown in <ref type="table" target="#tab_6">Table 7</ref>. Our model achieves leading performance. On the consideration of a fair comparison to <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b38">39]</ref> that utilize ResNet-50 as backbone, we also use same backbone and achieve 51.3% mIoU, which is still better than these methods. Specifically, <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b16">17]</ref> try to use channelwise attention or vanilla convolution to extract complementary feature, which are more implicit than our model in selecting valid feature from complementary information. Besides, we can see that utilizing depth data as extra supervision (such as <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b38">39]</ref>) could make network more robust than general RGB-D methods that take both RGB and depth as input sources <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27]</ref>. However, our results demonstrate that once the input RGB-D information could be effectively recalibrated and aggregated, higher performance could be obtained.</p><p>CityScapes. We achieve 81.7% mIoU on validation set and 82.8% mIoU on test set, which are both leading performances. <ref type="table" target="#tab_7">Table 8</ref> shows results on test set. We observe that due to serious noise of depth measurements in this dataset, most of previous RGB-D based methods even worse than RGB-based methods. However, our method effectively distills depth feature and extracts valid information in it and boosts the performance. Note that <ref type="bibr" target="#b6">[7]</ref> is a contemporary work and we outperform them by 0.7%. We exclude the results of GSCNN <ref type="bibr" target="#b28">[29]</ref> for fair comparison, since it uses a stronger backbone WideResNet instead of ResNet-101. However, we still outperform GSCNN by 0.9% mIoU on the validation set and achieve the same performance as it on test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a cross-modality guided encoder along with SA-Gate and BMP modules to address two key challenges in RGB-D semantic segmentation, i.e., the effective unified representation for different modalities and the robustness to low-quality depth source. Meanwhile, our proposed encoder can act as a plug-and-play module, which can be easily injected to current state-ofthe-art RGB semantic segmentation frameworks to boost their performances.</p><p>45. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In: CVPR (2017) 46. Zhuang, Y., Tao, L., Yang, F., Ma, C., Zhang, Z., Jia, H., Xie, X.: Relationnet:</p><p>Learning deep-aligned representation for semantic image segmentation. In: ICPR. IEEE (2018)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This supplementary material presents: (1) more implementation details based on the main paper; (2) additional experimental analysis and qualitative results of our approach on NYU Depth V2, CityScapes val set and SUN-RGBD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Implementation Details</head><p>We use PyTorch framework to implement our experiments. We set batch size to 16 for all experiments. We adopt mini-batch SGD with momentum to train our model. The momentum is fixed as 0.9 and the weight decay is set to 0.0005. We employ a poly learning rate policy where the initial learning rate is multiplied by (1 ? iter max iter ) 0.9 . For NYU Depth V2, we randomly crop the image to 480 ? 480 and train 800 epochs with base learning rate set to 0.02. We employ cross-entropy loss on both the final output and the intermediate feature map output from ResNet-101 block4, where the weight over the final loss is 1 and the auxiliary loss is 0.2.</p><p>For SUN-RGBD, we randomly crop the image to 480 ? 480 and train 80 epochs with base learning rate set to 0.02. Cross-entropy loss is used for the final output.</p><p>For CityScapes, we randomly crop the image to 800 ? 800 and train 240 epochs with base learning rate set to 0.04. We use OHEM loss for better learning. For data augmentation, we use random horizontal flipping and random scaling with scale {0.5, 0.75, 1, 1.25, 1.5, 1.75}. When comparing with the state-of-the-art methods, we adopt flipping and multi-scale inference strategies as a test-time augmentation to boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>Besides the results analyzed in the main paper, we also conduct experiments on CityScapes val set and SUN-RGBD dataset to further verify the effectiveness and generalization ablity of our approach. Meanwhile, we conduct more ablation studies on NYU Depth V2 to verify the robustness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results on CityScapes</head><p>Comparison with State-of-the-art Methods on CityScapes Val Set. Tabel 9 shows the results on CityScapes val set comparing with state-of-theart RGB-D based methods. We also list the results of RGB based methods for reference. We see that from row 6 and 7, due to the serious noisy depth measurements on this out-door dataset, a simple multi-modal fusion mechanism (RGB-D baseline) can not help explore the strength of depth data to boost the <ref type="table">Table 9</ref>. CityScapes val set results in terms of mIoU metric. We also list the results of RGB-based methods for reference</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Depth Data Backbone mIoU(%) GSCNN <ref type="bibr" target="#b28">[29]</ref> WideResNet-101 80.8 CCNet <ref type="bibr" target="#b17">[18]</ref> ResNet-101 81.3 DANet <ref type="bibr" target="#b10">[11]</ref> ResNet-101 81.5 ACFNet <ref type="bibr" target="#b42">[43]</ref> ResNet-101 81. performance compared with its single modality version (RGB baseline). However, with our proposed SA-Gate and BMP strategy, our final model could filter the noisy features and aggregate the cross-modality features more effectively. Thus, the proposed approach could still gain 1.2% mIoU increase to baselines. On the other hand, we see that from row 4 to row 9, our method is more robust than state-of-the-art RGBD-based methods that predict depth value either as a second stage multi-modal input <ref type="bibr" target="#b38">[39]</ref> or as a gating module for RGB-feature <ref type="bibr" target="#b19">[20]</ref>. Our final model achieves 2.6% mIoU improvement compared with <ref type="bibr" target="#b19">[20]</ref> under the setting of ResNet-101 backbone and 4.6% mIoU improvement compared with <ref type="bibr" target="#b38">[39]</ref> under the setting of ResNet-50 backbone. Comparing with raw depth source, although the predicted one avoids noisy information from the raw data, it will lead to the loss of depth information due to the over-smooth predicted depth values between objects. The results demonstrate the effectiveness of our crossmodality feature propagation and the potential superiority of directly forwarding the 'raw' depth source to the network than prediction ones as long as the noisy information can be effectively suppressed and multi-modality information can be fully explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results on SUN-RGBD</head><p>We further perform experiments on SUN-RGBD dataset to evaluate the effectiveness of our method. SUN-RGBD dataset contains images from several different datasets. It has 37 categories of objects and consists of 10335 RGB-D images.</p><p>There are 5285 images for training and 5050 images for testing. We keep all hyper-parameters the same as NYU Depth V2 except the number of epochs. Quantitative results are shown in <ref type="table" target="#tab_0">Table 10</ref>. We outperform most of the stateof-the-art methods, while slightly lower than PAP <ref type="bibr" target="#b43">[44]</ref> on this dataset. When compare with our baselines, our final model could boost RGB baseline by 3.4% mIoU and RGBD baseline by 1.9% mIoU.  <ref type="bibr" target="#b32">[33]</ref> 42.0 -Kong et al. <ref type="bibr" target="#b19">[20]</ref> 45.1 80.3 3DGNN <ref type="bibr" target="#b26">[27]</ref> 45.9 -RDF-152 <ref type="bibr" target="#b25">[26]</ref> 47.7 81.5 CFN <ref type="bibr" target="#b23">[24]</ref> 48.1 -ACNet <ref type="bibr" target="#b16">[17]</ref> 48.1 -PAP <ref type="bibr" target="#b43">[44]</ref> 50 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Robustness to Noisy Signals Existing in the Input</head><p>As claimed in the main paper, one of our goal is to devise the module that could effectively suppress noisy signals existing in the input depth measurement and highlight its positive features. To further verify this, we add different level Gaussian noise (mean=0, std ranges from 10 to 120) to the input HHA map and take our final model and RGB-D baseline (dual-branch DeepLab V3+) as a comparison. Note that the value of the input HHA map ranges from 0 to 255, just like the RGB image. Experimental results are listed in <ref type="table" target="#tab_0">Table 11</ref>. We do not use multi-scale inference strategy here. From the table, we observe several interesting phenomena as follows. (1) When adding small Gaussian noise with std=10, the input HHA map does not change considerably and the performance of the baseline and our final model only drop a little. However, our model has a smaller decrease than the baseline, which illustrates our model is more robust. (2) When we add large Gaussian noise with std=40, the performance of the baseline decreases more quickly than our final model (-44.6?VS -22.7?). When adding Gaussian noise with std=120, the performance of baseline drops -72.8?, while our final model only drops -28.0?. We attribute the robustness of our method to the filtering and recalibration operation in the SA-Gate, which may adaptively and effectively filter the noise of the input HHA map. Besides, since there is no clean ground-truth depth information to explicitly supervise the acts of SA-Gate, maybe exploring more explicit constraints on modules like SA-Gate will further enhance the robustness of the network to noisy scenes. We leave this to future work. In <ref type="figure" target="#fig_2">Figure 5 4</ref> and <ref type="figure">Figure 6</ref>, we highlight several representative feature responses samples before and after Feature Separation Part of proposed SA-Gate on both out-door and in-door datasets, to show how cross-modality feature filtering and recalibration can help refine primitive noisy single modality features in a more intuitive way. We select the feature embedding computed by the first layer in our network and follow [46] to compress the feature to three dimensions by the PCA and convert it to an RGB image for visualization. Note that the change of color does not directly relate to different feature responses. Instead, the color consistency inside objects indicates whether the module learns appropriate features.</p><p>We first visualize the response of HHA features on CityScapes val set in <ref type="figure" target="#fig_2">Figure 5</ref>. In the first row, the streetlight is totally missing in the HHA image. After FSP, we can observe that the feature map shows a good response to the location of the streetlight. In the second row, the outline of the pole is more precise after FSP. In the third row, some objects which don't exist in the HHA images show up after FSP.</p><p>Different from out-door environments, in-door scenes are more likely to decrease the validity of RGB modality due to the lighting and similar appearance of objects. Therefore, we also visualize the response of RGB features on NYU Depth V2 test set in <ref type="figure">Figure 6</ref>. In the first and the second rows, some unnecessary texture information is removed after FSP and we get a much smoother surface on the ground. In the third row, the effect of strong lighting is eliminated after FSP. In the fourth row, areas with inconspicuous contrast in the RGB image are enhanced after FSP.</p><p>In conclusion, the cross-modality feature filtering and recalibration in the proposed SA-Gate module could help make full use of the advantages of multimodal data to supplement the missing signal and suppress unnecessary noisy feature responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head><p>HHA RGB in RGB rec <ref type="figure">Fig. 6</ref>. Visualization of RGB feature before and after FSP on NYU test set</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Qualitative Results and Discussion</head><p>In this section, we present qualitative segmentation results of our method on in-door and out-door datasets, to show how cross-modality feature propagation could help semantic segmentation in various ways. <ref type="figure">Figure 7</ref> shows some qualitative results on CityScapes. We observe that although the quality of depth source in CityScapes is very noisy and ambiguous, our model still achieve accurate segmentation results. Taking the first row as an example, the poles in the HHA image are indistinct, while our method successfully identifies the shape of the poles with the help of the RGB image. <ref type="figure">Figure 8</ref> visualizes results on NYU Depth V2. In the first row, the desk is correctly identified in the RGB baseline, which is wrong in the RGB-D's. We can observe that part of the desk is misidentified as 'wall', since it has the same orientation as the wall. The situation of the chair near the desk is just the opposite. Our method perfectly combines the characteristics of RGB and HHA to make both objects well recognized. In the second row, our method generates smoother object boundaries and gains better intra-class consistency. In the third row, the carpet in the lower-left corner is missing in the RGB-D baseline because it is attached to the ground, but it is well recognized with the proposed method. <ref type="figure">Figure 9</ref> shows our results on SUN-RGBD. We observe that our model handles the details very well and achieve satisfactory intra-class consistency and inter-class distinction. <ref type="figure" target="#fig_0">Figure 10</ref> also reveals the strengths of our method. For example, in the first row, the two sofas are missing in the ground truth. In the second row, books on the bookshelf are missing in the ground truth, which is due to coarse labeling. In the third and the fourth rows, many meaningful areas are set to invalid areas (marked as black). In the fifth row, the chair is mislabeled as a RGB HHA Ground Truth Ours <ref type="figure">Fig. 7</ref>. Qualitative results on CityScapes val set. Better viewed in color and zoom in table. In the last row, a large region of the floor is mislabeled as chairs. However, our model recognizes these miss labeled objects correctly in the scene, since our method can make full use of complementary information in multi-modal data. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a)RGB-D baseline, which is designed with a habitual cross-modality fusion schema, results in inaccurate classification on the area that exists substantial variations between RGB and Depth modalities. (b) The depth measurements in out-door environments are noisy. Without proposed modules, the results will degrade dramatically 1 Introduction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>(a)The overview of our network. We employ an encoder-decoder architecture. The input of the network is a pair of RGB-HHA images. During training, each pair of feature maps (e.g., outputs of RGB-Layer1 and HHA-Layer1) are fused by a SA-Gate and propagated to the next stage of the encoder for further feature transformation. Fusion results of the first and the last SA-Gates would be propagated to the segmentation decoder (DeepLab V3+). (b) The architecture of the SA-Gate, which contains two parts, Feature Separation (FS) and Feature Aggregation (FA)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of depth feature before and after FSP on CityScapes val set 5 Does Filtering and Recalibration Help?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8 .Fig. 9 .Fig. 10 .</head><label>8910</label><figDesc>Results on NYU Depth V2 test set. From left to right: (1) RGB, (2) HHA, (3) result of RGB baseline, (4) result of RGB-D baseline, (5) result of ours, (Qualitative segmentation examples on SUN-RGBD Some incorrect ground-truth labels on SUN-RGBD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of efficiency on NYUDV2 test set. We use ResNet-50 as backbone and DeepLab V3+<ref type="bibr" target="#b1">[2]</ref> as decoder. FLOPS are estimated for input of 3 ? 480 ? 480</figDesc><table><row><cell>Methods</cell><cell cols="3">Params/M FLOPs/G mIoU(%)</cell></row><row><cell>RGB-D baseline</cell><cell>78.2</cell><cell>269.6</cell><cell>46.7</cell></row><row><cell>Ours</cell><cell>63.4</cell><cell>204.9</cell><cell>50.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on feature separation (FS) part on NYU Depth V2 test set. No decoder is used here</figDesc><table><row><cell cols="2">Backbone Concat Self-global Cross-global Product Proposed mIoU(%)</cell></row><row><cell>Res50</cell><cell>47.8</cell></row><row><cell>Res50</cell><cell>47.5</cell></row><row><cell>Res50</cell><cell>47.8</cell></row><row><cell>Res50</cell><cell>47.5</cell></row><row><cell>Res50</cell><cell>48.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Ablation study on feature aggregation (FA) part on NYU Depth V2 test set. No decoder is used here Ablation study on encoder design on NYU Depth V2 test set. '*' means we average two outputs of RGB and HHA to get final output. No decoder is used here</figDesc><table><row><cell cols="2">Backbone Addition Conv Proposed mIoU(%)</cell></row><row><cell>Res50</cell><cell>47.8</cell></row><row><cell>Res50</cell><cell>48.0</cell></row><row><cell>Res50</cell><cell>48.6</cell></row><row><cell cols="2">Backbone Block1 Block2 Block3 Block4 mIoU(%)</cell></row><row><cell>Res50  *</cell><cell>45.9</cell></row><row><cell>Res50  *</cell><cell>47.8</cell></row><row><cell>Res50  *</cell><cell>47.5</cell></row><row><cell>Res50  *</cell><cell>46.8</cell></row><row><cell>Res50  *</cell><cell>44.3</cell></row><row><cell>Res50  *</cell><cell>47.9</cell></row><row><cell>Res50  *</cell><cell>48.3</cell></row><row><cell>Res50  *</cell><cell>48.0</cell></row><row><cell>Res50</cell><cell>48.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Ablation study for BMP and SA-Gate. No decoder is used here</figDesc><table><row><cell>Method</cell><cell>mIoU(%)</cell></row><row><cell>Res50 (Average of Dual Path)</cell><cell>45.9</cell></row><row><cell>Res50 + SA-Gate</cell><cell>47.4 (1.5% ?)</cell></row><row><cell>Res50 + BMP</cell><cell>47.8 (1.9% ?)</cell></row><row><cell>Res50 + BMP + SA-Gate</cell><cell>48.6 (2.7% ?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>The plug-and-play property evaluation of the proposed model on NYU Depth V2 test set. Method indicates different decoders, SA-Gate indicates the proposed fusion module. RGB: RGB image as inputs; RGB-D: the simple method which only average final score maps of RGB path and HHA path. Note that we reproduce these methods using official open-source code and all experiments use the same setting as our method</figDesc><table><row><cell>Method</cell><cell cols="3">RGB(%mIoU ) RGB-D(%mIoU ) RGB-D w SA-Gate(%mIoU )</cell></row><row><cell>DeepLab V3 [1]</cell><cell>44.7</cell><cell>46.5</cell><cell>49.1 (2.6 ?)</cell></row><row><cell>PSPNet [45]</cell><cell>43.1</cell><cell>46.2</cell><cell>48.2 (2.0 ?)</cell></row><row><cell>DenseASPP [40]</cell><cell>42.3</cell><cell>45.7</cell><cell>47.8 (2.1 ?)</cell></row><row><cell>OCNet [42]</cell><cell>44.5</cell><cell>47.6</cell><cell>49.1 (1.5 ?)</cell></row><row><cell>DeepLab V3+ [2]</cell><cell>44.3</cell><cell>46.7</cell><cell>50.4 (3.7 ?)</cell></row><row><cell>DANet [11]</cell><cell>43.0</cell><cell>45.5</cell><cell>48.6 (3.1 ?)</cell></row><row><cell>FastFCN [36]</cell><cell>45.4</cell><cell>47.6</cell><cell>50.1 (2.5 ?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>State-of-the-art comparison experiments on NYU Depth V2 test set</figDesc><table><row><cell>Method</cell><cell>mIoU(%)</cell><cell>Pixel Acc.(%)</cell></row><row><cell>3DGNN [27]</cell><cell>43.1</cell><cell>-</cell></row><row><cell>Kong et al. [20]</cell><cell>44.5</cell><cell>72.1</cell></row><row><cell>LS-DeconvNet [6]</cell><cell>45.9</cell><cell>71.9</cell></row><row><cell>CFN [24]</cell><cell>47.7</cell><cell>-</cell></row><row><cell>ACNet [17]</cell><cell>48.3</cell><cell>-</cell></row><row><cell>RDF-101 [26]</cell><cell>49.1</cell><cell>75.6</cell></row><row><cell>PADNet [39]</cell><cell>50.2</cell><cell>75.2</cell></row><row><cell>PAP [44]</cell><cell>50.4</cell><cell>76.2</cell></row><row><cell>Ours</cell><cell>52.4</cell><cell>77.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Cityscapes test set accuracies. '*' means RGB-D based methods 86.1 93.5 56.1 63.3 69.7 77.3 81.3 93.9 72.9 95.7 87.3 72.9 96.2 76.8 89.4 86.5 72.2 78.2 77.6 DenseASPP [40] 98.7 87.1 93.4 60.7 62.7 65.6 74.6 78.5 93.6 72.5 95.4 86.2 71.9 96.0 78.0 90.3 80.7 69.7 76.8 80.6 87.0 93.5 59.8 63.4 68.9 76.8 80.9 93.7 72.8 95.5 87.0 72.1 96.0 77.6 89.0 86.9 69.2 77.6 81.4 DANet [11] 98.6 86.1 93.5 56.1 63.3 69.7 77.3 81.3 93.9 72.9 95.7 87.3 72.9 96.2 76.8 89.4 86.5 72.2 78.2 81.5</figDesc><table><row><cell>Method</cell><cell>roa.</cell><cell>sid.</cell><cell>bui.</cell><cell>wal.</cell><cell>fen.</cell><cell>pol.</cell><cell>lig.</cell><cell>sig.</cell><cell>veg.</cell><cell>ter.</cell><cell>sky</cell><cell>per.</cell><cell>rid.</cell><cell>car</cell><cell>tru.</cell><cell>bus</cell><cell>tra.</cell><cell>mot.</cell><cell>bic.</cell><cell>mIoU</cell></row><row><cell cols="2">DUC [32] 98.6 CCNet [18] -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.4</cell></row><row><cell cols="21">BFP [10] 98.7 GALD [22] 98.7 87.2 93.8 59.3 61.9 71.4 79.2 82.0 93.9 72.8 95.6 88.4 74.8 96.3 74.1 90.6 81.1 73.4 79.8 81.8</cell></row><row><cell>ACFNet [43]</cell><cell cols="20">98.7 87.1 93.9 60.2 63.9 71.1 78.6 81.5 94.0 72.9 95.9 88.1 74.1 96.5 76.6 89.3 81.5 72.1 79.2 81.8</cell></row><row><cell>LDFNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* [19]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 .</head><label>10</label><figDesc>Segmentation results on SUN-RGBD test set</figDesc><table><row><cell>Method</cell><cell>mIoU(%)</cell><cell>Pixel Acc.(%)</cell></row><row><cell>Depth-aware CNN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 .</head><label>11</label><figDesc>Robustness test on NYU Depth V2 test set. Std=x means we add Gaussian noise with mean 0 and std x to the input HHA map. Results are shown in mIoU(%)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>.5</cell><cell>83.8</cell><cell></cell></row><row><cell></cell><cell cols="2">RGB baseline</cell><cell>46.0</cell><cell>81.0</cell><cell></cell></row><row><cell></cell><cell cols="2">RGBD baseline</cell><cell>47.5</cell><cell>81.5</cell><cell></cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell>49.4</cell><cell>82.5</cell><cell></cell></row><row><cell>Method</cell><cell>No Noise</cell><cell>Std=10</cell><cell>Std=40</cell><cell>Std=80</cell><cell>Std=120</cell></row><row><cell>RGB-D Base</cell><cell>48.91</cell><cell cols="4">48.79(-2.5?) 46.73(-44.6?) 46.18(-55.8?) 45.35(-72.8?)</cell></row><row><cell>Ours</cell><cell>51.50</cell><cell cols="4">51.41(-1.7?) 50.33(-22.7?) 50.23(-24.7?) 50.06(-28.0?)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Raw depth map or its encoded representation-HHA map, which includes horizontal disparity, height above ground and norm angle. For more detail about HHA, please refer to<ref type="bibr" target="#b12">[13]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that we use HHA map to encode the depth measurements.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">It has been shown in the main paper, but we list again here for more detailed explanations and discussions</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">3d sketch-aware semantic scene completion via semi-supervised structure prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">3d neighborhood convolution: Learning depthaware features for rgb-d and rgb semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<idno>3DV. IEEE</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Spgnet: Semantic prediction guidance for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Locality-sensitive deconvolution networks with gated fusion for rgb-d indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cars can&apos;t fly up in the sky: Improving urbanscene segmentation via height-driven attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rfbnet: Deep multimodal networks with residual fusion blocks for rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00135</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Boundary-aware feature propagation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adaptive context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dynamic multi-scale filters for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Std2p: Rgbd semantic segmentation using spatio-temporal data-driven pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Acnet: Attention based network to exploit complementary features for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10089</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Incorporating luminance, depth and color information by a fusion-based network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Hang</surname></persName>
		</author>
		<editor>ICIP. IEEE</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Recurrent scene parsing with perspective understanding in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07229</idno>
		<title level="m">Global aggregation then local distribution in fully convolutional networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Lstm-cf: Unifying context modeling and fusion with lstms for rgb-d scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cascaded feature network for semantic segmentation of rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Rdfnet: Rgb-d multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Gated-scnn: Gated shape cnns for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning common and specific features for RGB-D semantic segmentation with deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Depth-aware cnn for rgb-d segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fastfcn: Rethinking dilated convolution in the backbone for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11816</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">2.5 d convolution for rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP. IEEE</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Coupling two-stream rgb-d semantic segmentation network by idempotent mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<editor>ICIP. IEEE</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pad-net: Multi-tasks guided predictionand-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Acfnet: Attentional class feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
