<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Generalization Using a Mixture of Multiple Latent Domains</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Matsuura</surname></persName>
							<email>matsuura@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
							<email>harada@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">RIKEN</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Domain Generalization Using a Mixture of Multiple Latent Domains</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When domains, which represent underlying data distributions, vary during training and testing processes, deep neural networks suffer a drop in their performance. Domain generalization allows improvements in the generalization performance for unseen target domains by using multiple source domains. Conventional methods assume that the domain to which each sample belongs is known in training. However, many datasets, such as those collected via web crawling, contain a mixture of multiple latent domains, in which the domain of each sample is unknown. This paper introduces domain generalization using a mixture of multiple latent domains as a novel and more realistic scenario, where we try to train a domain-generalized model without using domain labels. To address this scenario, we propose a method that iteratively divides samples into latent domains via clustering, and which trains the domain-invariant feature extractor shared among the divided latent domains via adversarial learning. We assume that the latent domain of images is reflected in their style, and thus, utilize style features for clustering. By using these features, our proposed method successfully discovers latent domains and achieves domain generalization even if the domain labels are not given. Experiments show that our proposed method can train a domaingeneralized model without using domain labels. Moreover, it outperforms conventional domain generalization methods, including those that utilize domain labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In the development of deep neural networks (DNNs), many methods that achieve good performance in computer vision tasks have been proposed <ref type="bibr" target="#b12">(Ren et al. 2015;</ref>. A domain represents an underlying data distribution, and these methods assume that the domains given in training (source domain) and in testing (target domain) are the same. However, it is known that DNNs suffer a drop in their performance due to domain shift <ref type="bibr" target="#b15">(Torralba and Efros 2011)</ref>.</p><p>To address this problem, extensive research has been carried out on domain generalization, which aims to train a domain-generalized model that performs well for the unseen target domain by using labeled data from multiple source Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.  domains. Considering the situation where a DNN is used for autonomous driving or robots in the real world, it is desirable to perform well under different conditions (e.g., illumination, types of objects) from the data given in training. Because we can access no samples in the target domain, domain generalization can be considered a more difficult and a more important task than domain adaptation <ref type="bibr" target="#b11">(Long et al. 2015;</ref><ref type="bibr" target="#b5">Ganin and Lempitsky 2015)</ref>, in which we can access labeled/unlabeled samples of the target domain in training.</p><p>To achieve domain generalization, several domain generalization methods have been proposed, including methods that train the feature extractor so that the feature distributions among multiple source domains are matched <ref type="bibr" target="#b10">(Li et al. 2018a;</ref><ref type="bibr" target="#b11">Li et al. 2018b)</ref>, or methods that train models for each domain and which combine them in testing <ref type="bibr" target="#b12">(Mancini et al. 2018b;</ref>. These conventional methods require domain labels, which represent the domain to which each sample in multiple source domains belongs. However, most datasets, such as those collected via web crawling, are a mixture of multiple latent domains, and it is difficult to know the domain labels. For example, there are several types of image search results for "dog", such as close-up photos of a face, photos of a dog figure in na-ture, and drawings of a dog. In this scenario, domain labels have to be attached manually to use conventional methods, but this process may be costly and time-consuming. Moreover, it is not obvious how to divide a mixture of multiple latent domains into each domain because those underlying data distributions are unknown.</p><p>JiGen <ref type="bibr" target="#b1">(Carlucci et al. 2019)</ref> achieves domain generalization without domain labels by combining supervised learning and self-supervised learning to solve jigsaw puzzles of the training images. However, it does not take advantage of the fact that there exist several latent domains in the source domain. Therefore, in this paper, we propose a novel and realistic scenario called domain generalization using a mixture of multiple latent domains, in which the source domain contains multiple latent domains, and the domain to which each sample belongs is unknown. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, in the proposed scenario, we try to train a model that performs well for the unseen target domain using a mixture of multiple latent domains. Moreover, we propose a novel method to solve this scenario. First, we assume that the latent domain of images is reflected in their style. Although other factors may also be considered, such as the background, location, and pose change, domain mismatches may be more severe when image styles are different, such as photos, NIR images, paintings, or sketches. Therefore, we utilize style features proposed in the research field of style transfer as domaindiscriminative features to discover latent domains. Specifically, we utilize a stack of convolutional feature statistics (i.e., mean and standard deviation) that are known to be capable of capturing image styles <ref type="bibr" target="#b9">(Li et al. 2017c</ref>). Once domain-discriminative features are obtained, our method iteratively assigns pseudo domain labels by clustering them, and trains a domain-invariant feature extractor shared among multiple latent domains by adversarial learning.</p><p>Experiments with benchmark datasets show that our proposed method is effective for domain generalization using a mixture of multiple latent domains, and it outperforms conventional domain generalization methods that use domain labels. Moreover, it is found that the use of pseudo domain labels obtained by clustering style features improves the classification performance compared with the use of original domain labels annotated by humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Here, we explain domain adaptation and domain generalization methods. Moreover, we explain style-transfer methods because as domain-discriminative features, our proposed method utilizes style features that were originally proposed in the research field of style transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Adaptation</head><p>To deal with domain shift <ref type="bibr" target="#b15">(Torralba and Efros 2011)</ref>, domain adaptation and domain generalization have been studied. Domain adaptation aims to generalize a model from the source domain to the target domain with data in both domains. In unsupervised domain adaptation, several methods are employed to match the distribution in pixel space <ref type="bibr" target="#b1">(Bousmalis et al. 2017;</ref> or feature space <ref type="bibr" target="#b11">(Long et al. 2015;</ref><ref type="bibr" target="#b5">Ganin and Lempitsky 2015)</ref>. Although these methods assume single-source and target domains, multi-source domain adaptation methods <ref type="bibr" target="#b16">(Xu et al. 2018;</ref><ref type="bibr" target="#b14">Schoenauer-Sebag et al. 2019)</ref> utilize multiple source domains for domain adaptation to learn domain relations.</p><p>Moreover, for the case in which the domains to which each sample belongs are unknown, <ref type="bibr" target="#b12">Mancini et al. (Mancini et al. 2018a</ref>) proposed a deep architecture that automatically discovers multiple latent domains, and it uses this information to align the distributions of the internal feature representations of sources and target domains. In contrast to our proposed method, this method is suitable for domain adaptation, and requires target samples in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Generalization</head><p>Domain generalization aims to train a domain-generalized model for the unseen target domain by using multiple source domains. Unlike domain adaptation, target samples are not given in training. The representative methods for domain generalization match the feature distributions among multiple source domains by using an auto-encoder <ref type="bibr" target="#b6">(Ghifary et al. 2015;</ref><ref type="bibr" target="#b10">Li et al. 2018a)</ref> or using adversarial learning <ref type="bibr" target="#b11">(Li et al. 2018b;</ref><ref type="bibr" target="#b14">Shao et al. 2019)</ref>. In addition, several methods have been proposed, such as a method that is based on meta learning <ref type="bibr" target="#b9">(Li et al. 2017b;</ref><ref type="bibr" target="#b0">Balaji, Sankaranarayanan, and Chellappa 2018)</ref>, one that uses domain-specific aggregation modules , and a method that combines supervised learning and self-supervised learning to solve jigsaw puzzles <ref type="bibr" target="#b1">(Carlucci et al. 2019)</ref>.</p><p>Most conventional domain generalization methods require domain labels, which represent the domains to which each sample belongs. However, in the scenario of domain generalization using a mixture of multiple domains, we cannot apply these methods because domain labels are not given. Although <ref type="bibr">JiGen (Carlucci et al. 2019)</ref> does not require domain labels in training, it is different from our proposed method, which assumes that the source domain contains multiple latent domains and take advantage of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Style Transfer</head><p>Style transfer enables us to transfer the style of an image called style image to that of an image called content image while preserving its content. Neural style transfer <ref type="bibr" target="#b6">(Gatys, Ecker, and Bethge 2016)</ref> utilizes Gram matrices of the neural activations from different layers of a convolutional neural network (CNN) to represent the artistic style of an image. Li et al. <ref type="bibr" target="#b9">(Li et al. 2017c</ref>) theoretically showed that matching the Gram matrices of neural activations is equivalent to minimizing the maximum mean discrepancy with the secondorder polynomial kernel, and constructed another style loss by aligning the convolutional feature statistics (i.e., mean and standard deviation) of two feature maps between style and generated images. AdaIN (Huang and Belongie 2017) enables arbitrary style transfer in real-time by replacing the convolutional feature statistics of the content image with those of the style image. Inspired by these methods, we assume that the latent domain of images is reflected in their style and utilize convolutional feature statistics as domaindiscriminative features. </p><formula xml:id="formula_0">{?(? 1 (x)), ?(? 1 (x)), !!! , ?(? M (x)), ?(? M (x))} Assign</formula><p>Domain-discriminative feature ddf(x) Clustering <ref type="figure">Figure 2</ref>: Illustration of our proposed method: Our method iteratively assigns pseudo domain labels by clustering domaindiscriminative features extracted from lower layers of the feature extractor, and trains the domain-invariant feature extractor via adversarial learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Generalization Using a Mixture of Multiple Latent Domains</head><p>In conventional domain generalization, the model trained</p><formula xml:id="formula_1">with K source domains D s = {D k s } K k=1</formula><p>, which share the same tasks (input x and label spaces y) but have different data distributions, accurately works for the new target domain D t . In this paper, we focus on the image classification task and set the number of object categories to C. Moreover, when the k-th source domain D k s has N k s samples, the dataset given in training is</p><formula xml:id="formula_2">D s = {D k s } K k=1 , D k s = {(x k i , y k i )} N k s i=1</formula><p>. This can also be represented using</p><formula xml:id="formula_3">D s = {(x i , y i , d i )} Ns i=1</formula><p>, when the domain to which each sample belongs and the total number of samples included in all source domains are defined as d i and N s , respectively. Namely, conventional domain generalization methods train the model that works well for the unseen target domain by using input images x i , object category labels y i , and domain labels d i . However, as we described above, a real dataset may be a mixture of multiple latent domains, and it is difficult to obtain domain labels in this case. Therefore, we propose a scenario called domain generalization using a mixture of multiple latent domains, where the given dataset is</p><formula xml:id="formula_4">D s = {(x i , y i )} Ns i=1 because domain labels d i are unknown.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>In this section, we explain the details of our proposed method. An overview of our method is shown in <ref type="figure">Fig. 2</ref>. Our method utilizes adversarial learning with a domain discriminator to train the domain-invariant feature extractor from among multiple latent domains; this approach is also used in conventional domain adaptation or generalization methods (Ganin and Lempitsky 2015; <ref type="bibr" target="#b11">Li et al. 2018b</ref>). Although adversarial domain generalization methods require domain labels, they are not given in domain generalization using a mixture of multiple latent domains. Therefore, our method iteratively reassigns pseudo domain labels by clustering domain-discriminative features obtained from the model.</p><p>The key point is how to extract domain-discriminative features from the model in order to cluster samples by their latent domains. Clustering features obtained from the model may generally divide samples by their object categories, and not by their domains. Moreover, our method aims to train a domain-invariant feature extractor by making the outputs domain-invariant, which hinders the extraction of domaindiscriminative features from the model. To solve this problem, we assume that the latent domain of images is reflected in their style, and we thus propose to utilize style features used in style transfer. Specifically, we utilize a stack of convolutional feature statistics (i.e., mean and standard deviations) obtained from lower layers of the feature extractor. In this way, our method can divide samples into each latent domain and achieve domain generalization. In the rest of the section, we describe the details of each component of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial Domain Generalization</head><p>Adversarial learning, which is developed from generative adversarial networks (GANs) (Goodfellow et al. 2014), has been used for research in domain adaptation (Ganin and Lempitsky 2015) and generalization <ref type="bibr" target="#b11">(Li et al. 2018b</ref>). Generally, a deep learning model can be divided into a feature extractor F f and a classifier F c . These models can be trained with the following classification loss L cls .</p><formula xml:id="formula_5">L cls = ? 1 N s Ns i=1 C c=1 1l [c=yi] log F c (F f (x i ))<label>(1)</label></formula><p>In addition to these components, adversarial learning introduces a domain discriminator F d , which is trained to discriminate the domains when outputs of the feature extractor are inputted. Conversely, the feature extractor is trained to extract features that make it difficult for the domain discriminator to discriminate their domains. This makes it possible to extract domain-invariant features from among multiple source domains, which generalizes the model for the unseen target domain. The adversarial loss L adv is defined as follows.</p><formula xml:id="formula_6">L adv = ? 1 N s Ns i=1K k=1 1l [k=di] log F d (F f (x i ))<label>(2)</label></formula><p>Although conventional methods use known domain labels d i and the known number of domains K, our proposed method uses pseudo domain labelsd i by assigning samples intoK pseudo domains using clustering. It is known that adversarial learning tends to generate ambiguous features near the decision boundary by trying to simply match the distributions among multiple source domains <ref type="bibr" target="#b14">(Saito et al. 2018)</ref>. Therefore, we introduce the entropy loss L ent (Grandvalet and Bengio 2005), which is used in some domain adaptation methods <ref type="bibr" target="#b11">(Long et al. 2016;</ref><ref type="bibr" target="#b17">Zhang et al. 2019)</ref> to train a more discriminative model for target samples by encouraging low-density separation between object categories. Although previous domain adaptation methods adapt it to only unlabeled target samples, our method adapts it to all labeled source samples as follows.</p><formula xml:id="formula_7">L ent = ? 1 N s Ns i=1 H(F c (F f (x i )))<label>(3)</label></formula><p>Here, H(?) represents the entropy function. This entropy loss enables us to extract discriminative features for object categories and to improve the classification accuracy. The total training objective is described as follows.</p><formula xml:id="formula_8">min F f ,Fc = L cls (F f , F c ) + ?(L ent (F f , F c ) ? L adv (F f , F d )) min F d = L adv (F f , F d )<label>(4)</label></formula><p>Here, ? denotes the trade-off parameter to suppress the noise signal of two losses L adv , L ent in the early stage of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain-discriminative Features</head><p>As domain-discriminative features, we utilize style features proposed in the style transfer <ref type="bibr" target="#b6">(Gatys, Ecker, and Bethge 2016;</ref><ref type="bibr" target="#b9">Li et al. 2017c;</ref><ref type="bibr" target="#b7">Huang and Belongie 2017)</ref>. Style transfer aims to generate a stylized image given a content image and a reference style image. Li et al. <ref type="bibr" target="#b9">(Li et al. 2017c)</ref> proposed a new style loss L sty to align the convolutional feature statistics (i.e., mean and standard deviation) between the generated image x gen and the style image x sty as follows.</p><formula xml:id="formula_9">L sty = M m=1 ?(? m (x gen )) ? ?(? m (x sty )) 2 + M m=1 ?(? m (x gen )) ? ?(? m (x sty )) 2<label>(5)</label></formula><p>Here, each ? m (x) denotes the output in a layer used to compute the style loss, and mean ?(x) and standard deviation Algorithm 1 Training algorithm.</p><p>Require: Data:</p><formula xml:id="formula_10">D s = {(x s i , y s i )} Ns i=1 Initialized i ,d i with zero while not end of epoch do Calculate {ddf(x i )} Ns i=1 using Eq. 8 Obtain {a i } Ns i=1 by clustering {ddf(x i )} Ns i=1</formula><p>Calculate? using Eq. 9 Updated i with?(a i ) while not end of minibatch do Sample a minibatch of x i , y i ,d i Update parameters using Eq. 4 end while Updated i withd i end while ?(x) are calculated across spatial dimensions independently for each channel c.</p><formula xml:id="formula_11">? c (x) = 1 HW H h=1 W w=1 x chw (6) ? c (x) = 1 HW H h=1 W w=1 (x chw ? ? c (x)) 2 +<label>(7)</label></formula><p>In our method, we assume that the latent domain of images is reflected in their style, and we thus utilize convolutional feature statistics as domain-discriminative features. Further, to combine multi-scale style features obtained from different convolutional layers, we define a stack of them as domain-discriminative features. Namely, the domaindiscriminative feature ddf(x) is calculated using multiple layers' outputs ? 1 (x), ? ? ? , ? M (x) as follows.</p><formula xml:id="formula_12">ddf(x) = {?(? 1 (x)), ?(? 1 (x)), ? ? ? , ?(? M (x)), ?(? M (x))}<label>(8)</label></formula><p>Training Procedure</p><p>After obtaining domain-discriminative features for all training samples using Eq. 8, our method divides them intoK clusters by clustering, and utilizes the cluster assignments a i as pseudo domain labelsd i . We use a standard clustering algorithm, k-means (Macqueen 1967), although other clustering algorithms can be used in our method. The overall training procedure is shown in Alg. 1. Our method iteratively reassigns pseudo domain labels in training. This is because domain-discriminative features can be extracted more successfully as the training progresses. In particular, we determine that the reassignment of pseudo domain labels is conducted for each epoch.</p><p>The problem here is that clustering can divide samples into each cluster but cannot properly decide which domain label should be assigned to each cluster. If the reassigned pseudo domain labels are shifted largely with those before one epoch, it negatively impacts the training. Therefore, we use the following equation to convert the cluster assignment a i into the pseudo domain labeld i by calculating the permutation? so as to maximize the rate of agreement between the cluster assignments {a i } Ns i=1 and pseudo domain labels before one epoch</p><formula xml:id="formula_13">{d i } Ns i=1 . ? = arg max ??? 1 N s Ns i=1 1l [d i =?(ai)]<label>(9)</label></formula><p>Here, the optimal permutation? can be computed using the Kuhn-Munkres algorithm (Munkres 1957).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets</head><p>To evaluate our proposed method, we perform experiments using two datasets for domain generalization.  <ref type="formula" target="#formula_5">. 2019)</ref>, we use three domains as the source domain, and the other as the target. For the same reason, we split 10% (in the case of PACS) and 30% (in the case of VLCS) of the source samples as validation datasets. In testing, all target samples are used to calculate the accuracy of the model that achieves the best accuracy in the validation dataset. Because domain labels are not given in domain generalization using a mixture of multiple latent domains, we do not use them when using our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>As the feature extractor, we use AlexNet and ResNet-18 pre-trained on ImageNet by removing the last layer. As the classifier, we initialize one fully connected layer to have the same number of inputs as before, and to have the same number of outputs as the number of object categories. As the domain discriminator, we use three fully connected layers (1024?1024?K). Note that we weight the loss function in Eq. 2 by the inverse of the size of pseudo domain labels. This is because if the number of images per pseudo domain is highly imbalanced, minimizing Eq. 2 results in a trivial parametrization where the model will predict the same output regardless of the input. To acquire the domaindiscriminative features of Eq. 8, we use relu2 and relu3 in the case of AlexNet, and conv2 x and conv3 x in the case of ResNet-18. To conduct adversarial learning in Eq. 4, we insert a gradient reversal layer (GRL) (Ganin and Lempitsky 2015) between the feature extractor and the domain discriminator, and we use the same schedule for ? of Eq. 4 as follows: ? = 2 1+exp(?10?p) ? 1. Here, p is linearly changed from 0 to 1 as training progresses. To reduce the computational cost of clustering, we reduce the dimension of domain-discriminative features to 256.</p><p>Basically, we utilize the other hyper-parameters employed by JiGen <ref type="bibr" target="#b1">(Carlucci et al. 2019)</ref>. In other words, we train the model for 30 epochs using the mini-batch stochastic gradient descent (SGD) with a momentum of 0.9, a weight decay of 5e ? 4, and a batch size of 128. We set the initial learning rate to 1e ? 3, and scale it by a factor of 0.1 after 80% of the training epochs. In the experiment with the VLCS dataset, we set the initial learning rate to 1e?4 because it is observed that a high learning rate causes early convergence and overfitting in the source domain. Moreover, we set the learning rate of the classifier and the domain discriminator to be 10 times larger than that of the feature extractor because they are trained from scratch. For pre-processing, we crop images to random sizes and aspect ratios, horizontally flip them randomly, change their brightness/contrast/saturation/hue randomly, and normalize them using ImageNet's statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>We compare our method with the following recent domain generalization methods. Deep All: Pre-trained Alexnet or ResNet-18 fine-tuned on the aggregation of all source domains with only the classification loss. TF <ref type="bibr" target="#b8">(Li et al. 2017a)</ref>:</p><p>The low-rank parameterized neural network, which reduces the number of parameters to be trained. CIDDG <ref type="bibr" target="#b11">(Li et al. 2018b</ref>): The conditional-invariant deep domain generalization method, which matches conditional distributions by considering the changes in the class prior. MLDG <ref type="bibr" target="#b9">(Li et al. 2017b</ref>): The meta-learning method by meta-optimization on simulated train/test splits with the domain shift. CCSA <ref type="bibr" target="#b12">(Motiian et al. 2017</ref>): The deep model in mixture with the classification and contrastive semantic alignment loss to address supervised domain adaptation and generalization. MMD-AAE <ref type="bibr" target="#b10">(Li et al. 2018a</ref>): A model that trains feature representations by jointly optimizing a multi-domain autoencoder regularized by the maximum mean discrepancy distance, a discriminator, and a classifier with adversarial learning. SLRC <ref type="bibr" target="#b4">(Ding and Fu 2018)</ref>: The structured lowrank constraint to transfer the knowledge between domainspecific networks and the domain-invariant one. D-SAM (D'Innocente and Caputo 2018): Domain-specific aggregation modules, which enable us to merge generic and specific information in an effective manner using an aggregation layer strategy. JiGen <ref type="bibr" target="#b1">(Carlucci et al. 2019)</ref>: Jigsaw puzzlebased generalization method, which focuses on the unsupervised task to solve jigsaw puzzles.</p><p>Note that methods other than Deep All and JiGen cannot be applied for domain generalization using a mixture of multiple latent domains because they require domain labels in training. Therefore, for these methods, we use the score in the scenario of general domain generalization where domain labels are given. Results <ref type="table" target="#tab_2">Table 1</ref> and <ref type="table" target="#tab_4">Table 2</ref> show the experimental results with the PACS and VLCS datasets, respectively. The scores shown in the tables are the average over five repetitions for each run, andK denotes the number of pseudo domains used in our method. For all datasets, our method achieves results that surpass those of existing methods regardless of the number of pseudo domainsK. Below, we discuss the influence of the number of pseudo domainsK. In the PACS dataset, our method has a significant advantage with respect to the corresponding Deep All baseline. The results show that training the domain-invariant feature extractor using adversarial learning is effective for domain generalization among more diverse domains such as the PACS dataset. This good performance is achieved without using any domain labels, unlike other methods excluding JiGen. Our method can discover latent domains and assign pseudo domain labels by focusing on the image styles. Moreover, even in the VLCS dataset, where domain shifts are inside only photo images, our method can improve the classification accuracy compared to other methods. The results show that even if the original domain labels of datasets are not separated by the image styles, our method can improve the generalization performance by assigning pseudo domain labels by focusing on them.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further Analysis Ablation Study</head><p>In this section, we describe an ablation study to investigate the effect of different components of our method using the PACS dataset and AlexNet. The variants of our method used in the experiments are as follows. Our method without L adv :</p><p>The model that removes the adversarial loss in Eq. 2. Our method without L ent : The model that removes the entropy loss in Eq. 3. Our method without stat.: The model that simply uses outputs of the convolutional layer (relu2 in this experiment) as domain-discriminative features for clustering instead of a stack of convolutional feature statistics in Eq 8. Our method without iter.: The model that uses the first assigned pseudo domain labels to the end without iteratively reassigning them. Our method without clus.: The model that uses original domain labels instead of assigning pseudo domain labels by clustering. <ref type="table" target="#tab_5">Table 3</ref> shows the experimental results obtained when the number of pseudo domains is set to three. The results of our method without L adv and our method without L ent indicate that the adversarial loss in Eq. 2 is effective for domain generalization, and it is further improved by using the entropy loss in Eq. 3. The result of our method without stat. indicates that simply using the outputs of convolutional layers cannot sufficiently extract domain-discriminative features, and it cannot achieve domain generalization so well. The result of our method without iter. indicates that iteratively reassigning pseudo domain labels improve the classification performance compared with those assigned at the start of training to the end. This may be because domain-discriminative features can be extracted more successfully by using models trained with samples of each domain, rather than using a pretrained model. Finally, the result of our method without clus. indicates that the use of iteratively reassigned pseudo domain labels improves the classification accuracy compared with the use of original domain labels. It appears that pseudo domain labels are suitable for training the domain-invariant feature extractor because they are based on the model's inner features and capture image styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Varying the Number of Pseudo Domains</head><p>In domain generalization using a mixture of multiple latent domains, the number of multiple latent domains in the source domain is unknown. Although our method divides samples intoK pseudo domains by clustering, we have to set the number of pseudo domains in advance. It is unclear whether our method works accurately if the number of pseudo domains is not the same as the number of original domains. Therefore, we check the performance of our method when changing the number of pseudo domains. We use the same experimental setting of the previous paragraph with the PACS dataset and ResNet-18. We consider four experiments in which the target domains are changed as one set, and repeat it five times. <ref type="figure">Fig. 3</ref> shows the mean and standard deviation results of our proposed method and Deep All. Note that in reality, the number of original domains is three. Based on the results obtained, there is no significant correlation between the number of pseudo domains and the classification accuracy, which highlights the robustness of our method to varying numbers of pseudo domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clustering Evaluation</head><p>Our method assigns pseudo domain labels by clustering.</p><p>There is a concern that clustering is not performed by domains but by object categories, although it does not neces-sarily have to divide samples by original domains. Therefore, we evaluate the clustering by calculating the normalized mutual information (NMI) between pseudo domain labels and object category labels, original domain labels, and pseudo domain labels before one epoch. Moreover, we visualize the distribution of domain-discriminative features using t-SNE (van der Maaten and Hinton 2008). We use the same experiment setting of the previous paragraph with the PACS dataset and AlexNet, set the number of pseudo domains to three, and set Art-painting to the target domain. <ref type="figure">Fig. 4</ref> shows that the NMI between pseudo domain labels and the original domain labels is large, while that between pseudo domain labels and object category labels is small. Moreover, the NMI between pseudo domain labels and original domain labels remains almost unchanged over the whole training period. These indicate that clustering domain-discriminative features divides samples not by object categories but original domains over the whole training period. This fact can also be seen in <ref type="figure">Fig. 5</ref>, where the distributions of domain-discriminative features are roughly divided by their original domains. Moreover, the NMI between pseudo domain labels and the previous assignment gradually converges to 1.0 as the training proceeds, which indicates that clustering results become gradually stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this study, we proposed a new scenario called domain generalization using a mixture of multiple latent domains. To address this scenario, we proposed a new method that extracts a stack of convolutional feature statistics representing the image styles as domain-discriminative features, assigns pseudo domain labels by clustering them, and trains the domain-invariant feature extractor from among latent domains using adversarial learning. In the experiments, our method without domain labels achieved a better performance than conventional methods that use them.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Photo</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Unlike conventional domain generalization, domain generalization using a mixture of multiple latent domains aims to train a domain-generalized model without domain labels (e.g., Photo, Art, Sketch), which represent the domain to which each sample belongs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Results obtained when varying the number of pseudo domains. The accuracy is the average of five sets. NMI between pseudo domain labels and object category labels, original domain labels, and previous assignments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>A stack of convolutional feature statistics</figDesc><table><row><cell>n Assign pseudo domain labels</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Adversarial loss</cell><cell>Pseudo domain label</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Domain discriminator</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>F d</cell><cell>Entropy</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>loss</cell><cell></cell></row><row><cell>Mixture of multiple latent domains x</cell><cell>Feature extractor F f</cell><cell>Domain-invariant feature</cell><cell>Classifier F c</cell><cell>Classification loss</cell><cell>Object category label</cell></row></table><note>n Train the domain-invariant feature extractor</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results in the PACS dataset. The title of each column indicates the name of the domain used as the target. The methods with an asterisk use domain labels, but Deep All, JiGen, and our method do not use them. The respective scores are obtained from each method's original paper.</figDesc><table><row><cell>PACS</cell><cell>Art.</cell><cell cols="2">Cartoon Sketch Photo Avg.</cell></row><row><cell></cell><cell></cell><cell>AlexNet</cell><cell></cell></row><row><cell>Deep All</cell><cell>63.30</cell><cell>63.13</cell><cell>54.07 87.70 67.05</cell></row><row><cell>TF  *</cell><cell>62.86</cell><cell>66.97</cell><cell>57.51 89.50 69.21</cell></row><row><cell>Deep All</cell><cell>57.55</cell><cell>67.04</cell><cell>58.52 77.98 65.27</cell></row><row><cell>CIDDG  *</cell><cell>62.70</cell><cell>69.73</cell><cell>64.45 78.65 68.88</cell></row><row><cell>Deep All</cell><cell>64.91</cell><cell>64.28</cell><cell>53.08 86.67 67.24</cell></row><row><cell>MLDG  *</cell><cell>66.23</cell><cell>66.88</cell><cell>58.96 88.00 70.01</cell></row><row><cell>Deep All</cell><cell>64.44</cell><cell>72.07</cell><cell>58.07 87.50 70.52</cell></row><row><cell>D-SAM  *</cell><cell>63.87</cell><cell>70.70</cell><cell>64.66 85.55 71.20</cell></row><row><cell>Deep All</cell><cell>66.68</cell><cell>69.41</cell><cell>60.02 89.98 71.52</cell></row><row><cell>JiGen</cell><cell>67.63</cell><cell>71.71</cell><cell>65.18 89.00 73.38</cell></row><row><cell>Deep All</cell><cell>68.09</cell><cell>70.23</cell><cell>61.80 88.86 72.25</cell></row><row><cell cols="2">Ours (K=2) 66.99</cell><cell>70.64</cell><cell>67.78 89.35 73.69</cell></row><row><cell cols="2">Ours (K=3) 69.27</cell><cell>72.83</cell><cell>66.44 88.98 74.38</cell></row><row><cell cols="2">Ours (K=4) 68.84</cell><cell>72.53</cell><cell>65.90 88.75 74.01</cell></row><row><cell></cell><cell></cell><cell>ResNet-18</cell><cell></cell></row><row><cell>Deep All</cell><cell>77.87</cell><cell>75.89</cell><cell>69.27 95.19 79.55</cell></row><row><cell>D-SAM  *</cell><cell>77.33</cell><cell>72.43</cell><cell>77.83 95.30 80.72</cell></row><row><cell>Deep All</cell><cell>77.85</cell><cell>74.86</cell><cell>67.74 95.73 79.05</cell></row><row><cell>JiGen</cell><cell>79.42</cell><cell>75.25</cell><cell>71.35 96.03 80.51</cell></row><row><cell>Deep All</cell><cell>78.34</cell><cell>75.02</cell><cell>65.24 96.21 78.70</cell></row><row><cell cols="2">Ours (K=2) 81.28</cell><cell>77.16</cell><cell>72.29 96.09 81.83</cell></row><row><cell cols="2">Ours (K=3) 79.64</cell><cell>76.75</cell><cell>71.22 95.86 80.87</cell></row><row><cell cols="2">Ours (K=4) 80.07</cell><cell>75.06</cell><cell>74.21 95.73 81.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">: Results in the VLCS dataset. The respective scores</cell></row><row><cell cols="4">are obtained from each method's original paper. For details</cell></row><row><cell cols="4">about the meaning of columns and use of asterisks, see Ta-</cell></row><row><cell>ble 1.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PACS</cell><cell>Art.</cell><cell cols="2">Cartoon Sketch Photo Avg.</cell></row><row><cell></cell><cell></cell><cell>AlexNet</cell><cell></cell></row><row><cell>Deep All</cell><cell>68.09</cell><cell>70.23</cell><cell>61.80 88.86 72.25</cell></row><row><cell cols="2">Ours w/o L adv 67.66</cell><cell>70.45</cell><cell>62.56 88.94 72.40</cell></row><row><cell cols="2">Ours w/o L ent 68.31</cell><cell>71.13</cell><cell>65.26 89.38 73.52</cell></row><row><cell cols="2">Ours w/o stat. 67.37</cell><cell>70.22</cell><cell>63.12 89.20 72.48</cell></row><row><cell cols="2">Ours w/o iter. 69.13</cell><cell>70.72</cell><cell>65.41 89.11 73.59</cell></row><row><cell cols="2">Ours w/o clus. 68.49</cell><cell>72.24</cell><cell>66.31 89.27 74.08</cell></row><row><cell>Ours</cell><cell>69.27</cell><cell>72.83</cell><cell>66.44 88.98 74.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results of the ablation study in the PACS dataset. For details about the meaning of columns, seeTable 1.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code is publicly available at https://github.com/mil-tokyo/ dg mmld/.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by JST CREST Grant Number JPMJCR1403, and partially supported by JSPS KAKENHI Grant Number JP19H01115. We would like to thank Yusuke Mukuta, Antonio Tejero de Pablos, Atsuhiro Noguchi, Akihiro Nakamura for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using meta-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sankaranarayanan</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bousmalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Crdoco: Pixel-level domain transfer with cross-domain consistency</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting hierarchical context on a large database of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep domain generalization with structured low-rank constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="304" to="313" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>and Fu</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; D&amp;apos;</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html" />
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<meeting><address><addrLine>Fergus, and Perona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ecker</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>Goodfellow et al. 2014</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<editor>AAAI. [Li et al. 2017c</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Demystifying neural style transfer</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the fifth Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Best sources forward: Domain generalization through source-specific nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mancini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="32" to="38" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Labelme: A database and web-based tool for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR. Shao et al. 2019</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-adversarial discriminative deep domain generalization for face presentation attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
		</imprint>
	</monogr>
	<note>Visualizing data using t-SNE</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep cocktail network: Multi-source unsupervised domain adaptation with category shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain-symmetric networks for adversarial domain adaptation</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
