<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dialogue Relation Extraction with Document-Level Heterogeneous Graph Attention Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
							<email>hui_chen@mymail.sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">DeCLaRe Lab</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">DeCLaRe Lab</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">DeCLaRe Lab</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">DeCLaRe Lab</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
							<email>sporia@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">DeCLaRe Lab</orgName>
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dialogue Relation Extraction with Document-Level Heterogeneous Graph Attention Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dialogue relation extraction (DRE) aims to detect the relation between pairs of entities mentioned in a multi-party dialogue. It plays an essential role in constructing knowledge graphs from conversational data increasingly abundant on the internet and facilitating intelligent dialogue system development. The prior methods of DRE do not meaningfully leverage speaker information-they just prepend the utterances with the respective speaker names. Thus, they fail to model the crucial interspeaker relations that may provide additional context to relevant argument entities through pronouns and triggers. We present a graph attention network-based method for DRE where a graph that contains meaningfully connected speaker, entity, type, and utterance nodes is constructed. This graph is fed to a graph attention network for context propagation among relevant nodes, which effectively captures the dialogue context. We empirically show that this graph-based approach quite effectively captures the relations between different argument pairs in a dialogue as it outperforms the state-of-the-art approaches by a significant margin on the benchmark dataset DialogRE. Our code is released at: https://github. com/declare-lab/dialog-HGAT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The relation extraction (RE) task aims to identify relations between pairs of entities that exist in a document. It plays a pivotal role in understanding unstructured text and constructing knowledge bases <ref type="bibr" target="#b17">(Peng et al., 2017;</ref>. Although the task of document-level relation extraction has been studied extensively in the past, the task of relation extraction from dialogues has yet to receive extensive study.</p><p>Most previous works in this field focus on the professional and formal literature like biomedical documents <ref type="bibr" target="#b11">(Li et al., 2016;</ref><ref type="bibr" target="#b27">Wu et al., 2019)</ref> and Wikipedia articles <ref type="bibr" target="#b6">(Elsahar et al., 2018;</ref>   <ref type="bibr" target="#b13">Mesquita et al., 2019)</ref>. These kinds of datasets are well-formatted and logically coherent with clear referential semantics. Hence, for most NLP tasks, analyzing a few continuous sentences is enough to grasp pivotal information. However, in dialogue relation extraction, conversational text is sampled from daily chat, which is more casual in nature. Hence its logic is simpler but more entangled, and the referential ambiguity always occurs to an external reader. Compared with formal literature, it has lower information density <ref type="bibr" target="#b26">(Wang and Liu, 2011)</ref> and thus is more difficult for models to understand. Moreover, compared with other document-level RE datasets such as DocRED, dialogue text has much more cross-sentence relations <ref type="bibr" target="#b31">(Yu et al., 2020)</ref>. <ref type="figure">Fig. 1</ref> presents an example of the target dialogue, taken from DialogRE <ref type="bibr" target="#b31">(Yu et al., 2020)</ref> dataset. In order to infer the relation between Speaker1 and Emma, we may need to find some triggers to recognize the characteristics of Emma. Triggers are shreds of evidence that can support the inference. As we can see, the following utterances are talking about Emma, and the keyword baby daughter mentioned by Speaker1 is a trigger, which provides evidence that Emma is Speaker1's daughter.</p><p>Prior works show that triggers of arguments facilitate the document-level relation inference. Thus, the DocRED dataset <ref type="bibr" target="#b30">(Yao et al., 2019)</ref> provides several supporting evidence for each argument pair. Some efforts utilize the dependency paths of arguments to find possible triggers. For example, LSR model <ref type="bibr" target="#b14">(Nan et al., 2020)</ref> constructs meta dependency paths of each argument pair and aggregates all the word representations located in these paths to their model to enhance the model's reasoning ability. <ref type="bibr" target="#b21">Sahu et al. (2019)</ref> uses syntactic parsing and coreference resolution to find intra-and inter-related words of each argument.  proposes an edgeoriented graph to synthesize argument-related information. These models are graph-based and have proven powerful in encoding long-distance information. However, for dialogue relation extraction, interlocutors exist in every utterance of the dialogue, and they are often considered as an argument. Although these previous approaches have utilized entity features of arguments, most of them employ meta dependency paths to find the related words, which neglect necessary information related to speakers, since the speaker references have very little dependency features in each utterance. In this work, we formulate the dialogue relation extraction task as a classification problem, where we design a graph attention network to model semantic, syntactic, and speaker information. Compared with other graph-based models in the relation extraction task, our model is lightweight, without any costly matrix operation, and it can generalize to completely unseen graphs.</p><p>In this paper, we propose a simple yet effective attention-based heterogeneous graph neural network to tackle the dialogue relation extraction task in an inductive manner. We use multi-type features to create the graph and employ graph attention mechanism to propagate contextual information. Different from most of the previous works, our proposed model is customized for the relation extraction task in dialogue background, as we have specially modeled speaker information and designed a mechanism to propagate messages among different sentences for better inter-sentence representation learning.</p><p>The remainder of this paper is organized as fol-lows: Section 2 briefly discusses relevant works of heterogeneous graph neural networks; Section 3 elaborates on our proposed framework; Section 4 introduces the used dataset and baseline models; Section 5 lays out the experiment results and analysis; Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Graph-based models have raised widespread attention from NLP researchers, as it is demonstrated as a powerful mathematical tool to represent complicated syntactic and semantic relations among structured language data. Early work applies classic graph processing algorithms onto language graphs. <ref type="bibr" target="#b15">Pang and Lee (2004)</ref> constructed a text graph and adopt the minimum-cut method to cluster the nodes for sentiment analysis. <ref type="bibr" target="#b0">Agirre and Soroa (2009)</ref> leveraged PageRank algorithm on personalized subgraphs of a wordnet to disambiguate polysemous words according to connected context words.</p><p>Recently, graph neural networks (GNN) (Kipf and Welling, 2017) becomes popular in relation extraction tasks. For example, <ref type="bibr" target="#b17">Peng et al. (2017)</ref> tried to build a computation graph from syntactic parsing trees and employed graph LSTM to obtain better word embeddings for multi-ary relation extraction. <ref type="bibr" target="#b33">Zhang et al. (2018)</ref> designed a pruning algorithm for syntactic graphs and add a graph convolution layer on top of the sequential LSTM encoder in the learning process. The combination with typical attention-based language models such as transformer <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref> is also studied. <ref type="bibr" target="#b1">Cai and Lam (2020)</ref> and <ref type="bibr" target="#b29">Yao et al. (2020)</ref> used transformer-based graph convolutional networks to explicitly encode relations among distant syntactic nodes, to address the long-distance propagation issue.</p><p>Based on GNN, heterogeneous graph neural networks are proposed and have been applied in many NLP tasks, like text classification <ref type="bibr" target="#b12">(Linmei et al., 2019)</ref>, text summarization , user profiling <ref type="bibr" target="#b4">(Chen et al., 2019)</ref>, and event categorization . The prior work proves that heterogeneous graph neural network is a powerful tool in NLP. For the relation extraction task,  constructed an edge-oriented heterogeneous graph that contains sentence, mention, and entity information. However, syntactic information is neglected in their model. Differently, homogeneous nodes in our graph are all independent, and we take syntactic features to initialize sentence information as well as edges features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition</head><p>Given a dialogue containing N utterances D = {u 1 , u 2 , ..., u N } and a couple of argument pairs A = {(x 1 , y 1 ), (x 2 , y 2 ), . . . }, where subject x i and object y i are entities mentioned in the dialogue, the goal is to identify the relation between argument pairs (x i , y i ). For documentlevel relation extraction task, there are many crosssentence relations which are supported by various sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Overview</head><p>In this work, we introduce an attention-based graph network to tackle the problem where each conversation is represented as a heterogeneous graph. We first utilize an utterance encoder, which is composed of two Bidirectional long short-term memory networks to encode conversational information. These utterance encodings, along with word embeddings, speaker embeddings, argument embeddings, and type embedding, are logically connected to form a heterogeneous graph, which will be discussed in detail later in this section. Further, this graph is fed through five graph attention layers <ref type="bibr" target="#b24">(Veli?kovi? et al., 2018</ref>) that aggregate information from the neighboring nodes. Lastly, we concatenate the learned argument embeddings and feed them to a classifier. An overview of the proposed model is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Preprocessing</head><p>In the data preprocessing period, we use spaCy 1 to tokenize utterances, and at the same time, we obtain part-of-speech (POS) tags as well as named entity types of each token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Utterance Encoder</head><p>Given a dialogue D = {u 1 , u 2 , ..., u N }, we use GloVe <ref type="bibr" target="#b18">(Pennington et al., 2014)</ref> to initialize the word embeddings and then feed them to a contextual Bidirectional Long Short-Term Memory network (BiLSTM) to obtain contextualized representations. The operation of BiLSTM can be de-1 https://spacy.io fined as:</p><formula xml:id="formula_0">? ? h i j = LST M l ( ??? h i j+1 , e i j ) (1) ? ? h i j = LST M r ( ??? h i j?1 , e i j ) (2) h i j = [ ? ? h i j ; ? ? h i j ]<label>(3)</label></formula><p>where ? ? h i j and ? ? h i j denote the hidden representations in the j-th layer of utterance u i from two directions, h i j is the contextual representation which is the concatenation of ? ? h i j and ? ? h i j , and e i j stands for the embedding of the j-th token in utterance u i . Unlike the previous approaches <ref type="bibr" target="#b14">Nan et al., 2020)</ref> that only adopt semantic contextual features in utterance encoding, we add syntactic features such as POS tags and named entity types to the contextual representations. The embedding of each token in the utterance can be described as:</p><formula xml:id="formula_1">e = [e w ; e p ; e t ]<label>(4)</label></formula><p>where we concatenate word embedding e w initialized by GloVe <ref type="bibr" target="#b18">(Pennington et al., 2014)</ref>, syntactic POS embedding e p , and type embedding e t to form the token embedding e. Moreover, we believe conversation-level contextual features play an important role in understanding a conversation. To encode non-local contextual information between each utterance, we apply max pool operation to the hidden states of each utterance-level BiLSTM (local LSTM), and then feed the sequence c = {c 1 , c 2 , ..., c N } to a conversation-level BiLSTM (global LSTM). The operation of global LSTM is the same as Eqs. <ref type="formula">(1)</ref> to <ref type="formula" target="#formula_0">(3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Graph Construction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Node Construction</head><p>In our model, we design a heterogeneous graph network containing five types of nodes: utterance nodes, type nodes, word nodes, speaker nodes, and argument nodes. Each type of node is used to encode a type of information in the dialogue. In the task, only word nodes, speaker nodes and argument nodes are probable to attend the final classification process. In other words, only these types of nodes are possible arguments. For simplicity, we name them as basic nodes in our illustration.</p><p>Utterance and Type Nodes Utterance nodes are initialized by the utterance embeddings which we  obtain from the utterance encoder. They are connected with the basic nodes which constitute the utterance. Type nodes represent the entity types of words in an utterance, where a variety of named and numeric entities, such as PERSON or LOCA-TION, are included. Since one mention may have different types in one conversation, type nodes can facilitate information integration. For example, 'Frank' can be a string if it represents an alternative name, and at the same time, it can be a person if it refers to a speaker in the conversation. Type nodes are connected with the basic nodes having the type attribute in the conversation. Each type node is initialized with a specific type of embedding. We believe that type of information has a positive influence on the relation inference process.</p><p>Basic Nodes Word nodes represent the vocabulary of a conversation. Each word node is connected with the utterance, which contains the word and it is also connected with all the possible types that the word may have in the conversation. We initialize the states of word nodes with GloVe <ref type="bibr" target="#b18">(Pennington et al., 2014)</ref>.</p><p>Speaker nodes represent each unique speaker in the conversation. Each speaker node is connected with the utterances uttered by the speaker himself/herself. This type of node is initialized with some specific embeddings and can gather informa-tion from different speakers.</p><p>Argument nodes are two special nodes that are used to encode relative positional information of argument pairs. There are two argument nodes in each graph in total. One stands for the subject argument and the other represents the object argument. Similarly, argument nodes are also encoded by specific embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Edge Construction</head><p>The proposed graph is undirected but the propagation has directions. There are five types of edges: utterance-word, utterance-argument, utterance-speaker, type-word, and type-argument edges. Each edge has its own type. These edges are randomly initialized except the utterance-word edge.</p><p>For the edge between utterance and word nodes, we adopt POS tags to initialize the edge features. This type of edge aggregates not only global semantic features of the conversation but also local syntactic features to the word nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Graph Attention Mechanism</head><p>We use graph attention mechanism <ref type="bibr" target="#b24">(Veli?kovi? et al., 2018)</ref> to aggregate neighboring information to the target node. Suppose we have a node i and some neighborhood nodes j, the graph attention mechanism can be described as:</p><formula xml:id="formula_2">F(h i , h j ) = LeakyReLU(a T (W i h i ; W j h j ; E ij ) (5) ? ij = softmax(F(h i , h j )) (6) = exp(F(h i , h j )) k exp(F(h i , h k ))<label>(7)</label></formula><formula xml:id="formula_3">h i = || K k=1 ?( j ? k ij W k q h j )<label>(8)</label></formula><p>where h i and h j are representations of node i and nodes j, W i , W j , W q and a T are trainable weight matrices, E ij is the edge weight matrix that is mapped to the multi-dimensional embedding space, ? ij is the attention weight between i and j, ? is an activation function, and || is concatenation operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.4">Message Propagation</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, there are five layers in our proposed graph module, where each layer represents an aggregation. There are four types of layers that we mark in the figure. LayerA and LayerD contain the message propagation between utterance nodes and basic nodes, and similarly, LayerB and Lay-erC are the message propagation between basic nodes and type nodes. We would call the whole message propagation path meta path. Different meta path strategies may lead to different performance. Our meta path in this work can be described as follows: First, we use utterance nodes to update word nodes, speaker nodes, and argument nodes; secondly, the updated word nodes and argument nodes pass messages to type nodes; then type nodes conversely update the word nodes and argument nodes; next we use word nodes, speaker nodes, and argument nodes to update utterance nodes; and lastly the updated utterance nodes update word nodes, speaker nodes and argument nodes. The path can be denoted as</p><formula xml:id="formula_4">V u ? V b ? V t ? V b ? V u ? V b , where V u , V b</formula><p>, and V t refer to utterance nodes, basic nodes, and type nodes.</p><p>Following , we add a residual connection <ref type="bibr" target="#b8">(He et al., 2016)</ref> to avoid gradient vanishing during updating:</p><formula xml:id="formula_5">h i =h i + h i<label>(9)</label></formula><p>whereh i is the output learned in the graph attention layer, and h i is the original input of the graph attention layer.</p><p>In message passing, except for graph attention operation, there is also a two-layer feed-forward network which can be denoted as:</p><formula xml:id="formula_6">h new i = FFN(? i )<label>(10)</label></formula><p>Suppose we have the initial embeddings of utterance nodes, basic nodes and type nodes, denoted as embedding matrices H u = {H u , H b , H t }, the message propagating process can be written as:</p><formula xml:id="formula_7">H 1 b = GAT(H 0 b , H 0 u )<label>(11)</label></formula><formula xml:id="formula_8">H 1 t = GAT(H 0 t , H 1 b )<label>(12)</label></formula><formula xml:id="formula_9">H 2 b = GAT(H 1 b , H 1 t )<label>(13)</label></formula><formula xml:id="formula_10">H 1 u = GAT(H 0 u , H 2 b )<label>(14)</label></formula><formula xml:id="formula_11">H 3 b = GAT(H 2 b , H 1 u )<label>(15)</label></formula><p>where the GAT operation is the same as Eqs. <ref type="formula">(5)</ref> to <ref type="formula" target="#formula_6">(10)</ref>. The superscripts represent the n th update of the matrix and 0 marks the initial state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Relation Classifier</head><p>After the message propagation in the heterogeneous graph, we obtain new representations of all entities. We select the argument nodes ? x and ? y , as well as the corresponding word nodes e x and e y from basic nodes, and concatenate them. Finally, they are fed to a linear transformation and a sigmoid function to get the predictions: </p><formula xml:id="formula_12">e x = [maxpool(? x ); maxpool(e x )]<label>(16</label></formula><formula xml:id="formula_13">P (r|e x , e y ) = ?(W e e + b e ) r<label>(18)</label></formula><p>where P (r|e x , e y ) is the probability of relation type r given argument pair (e x , e y ), W e and b e are linear transformation weight and bias vector, maxpool is max pooling operation, and ? is sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Used</head><p>We evaluate the proposed framework on the Di-alogRE dataset <ref type="bibr" target="#b31">(Yu et al., 2020)</ref>, which contains 1,788 dialogues and 10,168 relational triples. The data statistics are shown in <ref type="table" target="#tab_1">Table 1</ref>. DialogRE is adapted from the complete transcripts of Friends, a widely used corpus in dialogue research these years <ref type="bibr" target="#b3">(Chen et al., 2017;</ref><ref type="bibr" target="#b34">Zhou and Choi, 2018;</ref><ref type="bibr" target="#b28">Yang and Choi, 2019;</ref><ref type="bibr" target="#b19">Poria et al., 2019)</ref>, and there are 36 possible relation types, most of which focus on biographical attributes of person entities. Each dialogue contains several relational triples (x, y, r), and the task is to predict the relation r between each argument pair (x, y). In the experiments, the dataset is partitioned into train, dev, and test set with a roughly 60/20/20 ratio. Following the evaluation metrics of DialogRE, we report macro F 1 scores of the proposed model and all the baselines in both the standard and conversational settings. In the following sections, we use F 1 c to represent F 1 scores in the conversational setting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Sequence-based Models</head><p>We select convolutional neural networks (CNN) <ref type="bibr" target="#b32">(Zeng et al., 2014)</ref>, LSTM, and BiL-STM <ref type="bibr" target="#b2">(Cai et al., 2016)</ref> as the sequence-based baselines. These models take word embeddings, mention embeddings, and type embeddings as features. Concretely, they use GloVe and spaCy to get word embeddings and label named-entity types, and then take an average of all the embeddings of mention names for each entity to get mention embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Graph-based Models</head><p>As our proposed model is graph-based, we also select two graph-based models AGGCN  and LSR <ref type="bibr" target="#b14">(Nan et al., 2020)</ref> as the baselines. AGGCN directly feeds the full dependency tree of each sentence to a graph convolutional network, which takes self-attention weights as soft edges. It achieves state-of-the-art results in various relation extraction tasks. LSR adopts an adaptation of Kirchhoff's Matrix-Tree Theorem <ref type="bibr" target="#b22">(Tutte, 1984;</ref><ref type="bibr" target="#b10">Koo et al., 2007)</ref> to induce the latent dependency structure of each document and then feeds the latent structure to a densely connected graph convolutional network to inference the relations. These graph-based models both utilize dependency information to construct the inference graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Result and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison with Baselines</head><p>We present our main results on DialogRE dataset in <ref type="table" target="#tab_5">Table 2</ref>. As shown in <ref type="table" target="#tab_5">Table 2</ref>, our model surpasses the state-of-the-art method by 9.6%/7.5% F 1 scores, and 8.4%/5.7% F 1 c scores in both validation and test sets, which demonstrates the effectiveness of the information propagation along task-specific functional meta-paths in the heterogeneous graph. As a result, inter-sentence communication usually passes through a long distance, which causes information loss or degradation. However, this kind of information transmission is critically important for dialog-style text, because logical connections are not locally compact within adjacent sentences, instead, they are spread over the whole conversations. Our proposed model constructs a heterogeneous graph with shorter distances between logically closed but syntactically faraway word pairs. Hence the longdistance issue is mitigated.</p><p>We also compare the model sizes as an efficiency indicator. Although creating numerous nodes and edges inevitably brings overhead, the total number of parameters is still moderate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>To understand the impact of our model's components, we perform ablation studies using our proposed model on the DialogRE dataset. The ablation results are shown in <ref type="table" target="#tab_6">Table 3</ref>. First, we remove local LSTM and global LSTM. The dropping accuracy proves that the contextual encoder plays an important role in semantic feature extraction. Second, we remove the specific argument nodes and have observed that F 1 and F 1 c scores decrease to 55.0% and 50.2% on test set. This proves that our design on argument nodes effectively synthesizes argument features to the model. Further, we test the performance of the syntactic features we inject by removing POS embedding, NER embedding, and POS edge features. The scores record a decrease under all these experiment settings. Notably, removing POS embedding leads to even about 2% drops in all the evaluation Model Dev (%) Test (%) #params F 1 F 1 c F 1 F 1 c Majority <ref type="bibr" target="#b31">(Yu et al., 2020</ref><ref type="bibr">) -38.9 38.7 35.8 35.8 CNN (Yu et al., 2020</ref> -46.1 43.7 48.0 45.0 LSTM <ref type="bibr" target="#b31">(Yu et al., 2020</ref><ref type="bibr">) -46.7 44.2 47.4 44.9 BiLSTM (Yu et al., 2020</ref> 4.1M 48.1 44.3 48.6 45.0 AGGCN  3.7M 46.6 40.5 46.2 39.5 LSR <ref type="bibr" target="#b14">(Nan et al., 2020)</ref> 20.5M 44.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of the Meta Path</head><p>We test the performance of our message propagation strategy via changing meta-path strategies. In our proposed model, there are five layers in the heterogeneous graph. Those basic nodes, corresponding to different types of words, speakers, and arguments, are updated totally three times, i.e., they are first updated by utterance nodes, second updated by type nodes, and ultimately updated by utterance again. To investigate the meta path's effect, we compare our proposed five-layer graph module with different strategies where the numbers of layers are one, seven, and nine in <ref type="table" target="#tab_8">Table 4</ref>. In Strategy1, we only set up one LayerA, where the basic nodes are updated by the initialized utterance nodes once. We observe that all the macro F 1 scores drop dramatically, showing the onelayer structure is not deep enough to grasp complex dependencies. To make node features more informative, we would add more layers. At this time, we may be curious about how many layers the module should have to induct an optimal structure in this task. In Strategy2 and Strategy3, we design a seven-layer module and a nine-layer module, respectively. For Strategy2, the order of layers is A-B-C-D-A-D-A, where A,B,C, and D are layer labels introduced in <ref type="figure" target="#fig_0">Fig. 2</ref>. Compared with our proposed module, scores on validation set decrease about 1% and scores on test set decrease 1.7% and 0.6 % with the standard-setting and the conversational setting, respectively. However, the module with nine layers in Strategy3 shows a larger gap between itself and the best performance, where the order of layers is A-B-C-D-A-B-C-D-A. We think this is probably because the structure is so complicated, which causes an oversmooth problem and prevents itself from learning meaningful hidden representations.  </p><formula xml:id="formula_15">Strategy Dev (%) Test (%) F 1 F 1c F 1 F 1c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Case Studies</head><p>In the dataset, 95% of argument pairs span in at least two consecutive sentences instead of being restricted to the same sentence. Therefore, it is crucial that the model can tackle long-distance learning issues. Compared with the LSTM model, direct connections among different types of nodes in HGNN reduce the length of information propagation paths between pairs of argument nodes. Considering the following example in <ref type="figure">Fig. 3</ref>, sub- <ref type="figure">Figure 3</ref>: An example to show the effective message propagation between argument pairs ject a -'Mindy' and object b -'Speaker 1' share the relationship 'per:friends', which is indicated by the trigger 'my best friend' in the first utterance. The entity information is relayed from 'Mindy' to 'Speaker 1' in the update process: 'speaker 1' node aggregates utterance level information from its neighbor nodes containing a. the relation trigger 'best friend'. b. in BiLSTM model, the key information has to travel a long journey from the subject entity word to the object one as there are too many words between them in the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Error Analysis</head><p>Type information involves in the information propagation process and thus affects the contents of output embeddings. The model is prone to make incorrectly and biased predictions. If it fails to receive enough certainty from other information sources and then can only rely on the entity types of the two arguments. For example, given an argument pair of two human names, both are named entity type 'PER-SON'. Sometimes the model inclines to deem the relationship between the two arguments to be 'per:alternate_name' instead of the correct answer 'per:alumni' or 'per:roommate'. This is because among all of these classes, 'PERSON-PERSON' is a preferable type pair. However, the class 'per:alternate_name' (22.01%) presents more frequently than 'per:alumni' (1.83%) and 'per:roommate' (1.29%) in the dataset. When information aggregated from all sources other than the argument pair is not evident for judgment, en-tity bias misguides the model to the wrong classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we present an attention-based heterogeneous graph network to deal with the dialogue relation extraction task in an inductive manner. This heterogeneous graph attention network has modeled multi-type features of the conversation, such as utterance, word, speaker, argument, and entity type information. On the benchmark Dialo-gRE dataset, our proposed framework outperforms the strongest baselines and the state-of-the-art approaches by a significant margin, which proves the proposed framework can effectively capture relations between different entities in the conversation. Future work will focus on making use of latent relations between entities that exist in dialogue history to develop intelligent conversational agents.</p><p>In our experiments, we tune the parameters of batch size, learning rate, and BiLSTM hidden size by testing the performance on the validation set.   <ref type="table">Table 6</ref> shows statistics of relation labels in Di-alogRE dataset. In the train set and test set, there are 35 types of relations, while in the dev set, there are 37 types. 'gpe:birth_in_place' and 'per:place_of_birth' only exist in the dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Statistics of Relation Labels</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An overview of the proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Yeah, and it was uhm... it was like a real little person laugh too. It was... it was like uhm... Only... only not creepy. Well... well, what did you do to make her laugh? I uhm... Well, I sang... well actually I rapped... Baby Got Back... You WHAT? You sang... to our baby daughter... An example adapted from DialogRE dataset. Words with red and blue background represent subject and object entities. Words with yellow background represent triggers that facilitate the relation inference. Solid and dash lines stand for intra-and inter-utterance relations.</figDesc><table><row><cell>per:alterna te_names</cell><cell>You WHAT? And I missed it? Because I was giving a makeover to that stupid I just finished getting Phoebe all dressed to meet Mike's parents. She's per:girl/boyfriend so nervous, it's so sweet! Speaker 1</cell><cell>Guess what? I made Emma laugh today. Speaker 2 pe r: ch ild re n</cell><cell>Relation Trigger Argument Trigger</cell></row><row><cell></cell><cell>hippie?</cell><cell></cell><cell></cell></row><row><cell cols="2">Figure 1:</cell><cell></cell><cell></cell></row></table><note>et al., 2019;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>[ 1 ]</head><label>1</label><figDesc>Speaker1: Any sign of your brother? [2] Speaker2: No, but he's always late. [3] Speaker1: I thought you only met him once? [4] Speaker2: Yeah, I did. I think it sounds y'know big sistery, ?'Frank's always late.'</figDesc><table><row><cell>[1]</cell><cell>[2]</cell><cell>[3]</cell><cell>[4]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Graph Attention</cell></row><row><cell></cell><cell cols="2">Utterance</cell><cell></cell><cell>LayerA</cell><cell>Aggregation</cell><cell>LayerB</cell></row><row><cell></cell><cell cols="2">Encoder</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>LayerC</cell><cell>LayerD</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Classifier</cell></row><row><cell></cell><cell></cell><cell>Word</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Utterance Speaker</cell><cell>LayerA</cell><cell>Relation Labels</cell></row><row><cell></cell><cell></cell><cell cols="2">Argument</cell><cell>Entity</cell></row><row><cell></cell><cell></cell><cell>Type</cell><cell></cell><cell>Embeddings</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Main results on DialogRE dataset. Values in the #params column refer to parameter sizes of the models. F 1 and F 1 c are macro F 1 scores under standard setting and conversational setting, respectively. Word embeddings of the models are captured by GloVe<ref type="bibr" target="#b18">(Pennington et al., 2014)</ref>.</figDesc><table><row><cell>metrics.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Dev (%) F 1 F 1c</cell><cell>Test (%) F 1 F 1c</cell></row><row><cell>Full model</cell><cell cols="2">57.7 52.7 56.1 50.7</cell></row><row><cell>w/o Local BiLSTM</cell><cell cols="2">54.9 50.0 55.3 50.3</cell></row><row><cell>w/o Global BiLSTM</cell><cell cols="2">54.7 50.2 53.5 48.7</cell></row><row><cell>w/o Argument nodes</cell><cell cols="2">56.0 51.3 55.0 50.2</cell></row><row><cell>w/o POS embedding</cell><cell cols="2">54.6 50.9 53.0 48.5</cell></row><row><cell>w/o NER embedding</cell><cell cols="2">56.8 51.5 54.2 49.2</cell></row><row><cell cols="3">w/o POS edge weights 56.9 52.4 54.7 50.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Ablation results on DialogRE dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Comparison with different meta-path strategies on DialogRE dataset. 'L' means the number of layers in the graph module.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc>lists the major parameters used in our experiments.</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Word embedding dimension</cell><cell>300</cell></row><row><cell>NER embedding dimension</cell><cell>30</cell></row><row><cell>POS embedding dimension</cell><cell>30</cell></row><row><cell>Local BiLSTM hidden Size</cell><cell>200</cell></row><row><cell>Local BiLSTM layers</cell><cell>2</cell></row><row><cell>Global BiLSTM hidden Size</cell><cell>128</cell></row><row><cell>Global BiLSTM layers</cell><cell>2</cell></row><row><cell># Multihead attention</cell><cell>10</cell></row><row><cell>Learning rate</cell><cell>0.0005</cell></row><row><cell>Batch size</cell><cell>16</cell></row><row><cell>Edge embedding dimension</cell><cell>50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Parameter settings.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Personalizing pagerank for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009)</title>
		<meeting>the 12th Conference of the European Chapter of the ACL (EACL 2009)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="33" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph transformer for graph-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7464" to="7471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional neural network for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="756" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust coreference resolution and entity linking on dialogues: Character identification on TV show transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="216" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised user profiling with heterogeneous graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJ-CAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2116" to="2122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Connecting the dots: Document-level neural relation extraction with edge-oriented graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4927" to="4938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">T-rex: A large scale alignment of natural language with knowledge base triples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlos</forename><surname>Vougiouklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslen</forename><surname>Remaci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederique</forename><surname>Laforest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Simperl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<publisher>LREC</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention guided graph convolutional networks for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="241" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structured prediction models via the matrix-tree theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Biocreative v cdr task corpus: a resource for chemical disease relation extraction. Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hsuan</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention networks for semi-supervised short text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianchi</forename><surname>Hu Linmei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houye</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4823" to="4832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Knowledgenet: A benchmark dataset for knowledge base population</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipe</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Cannaviccio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Schmidek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paramita</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="749" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1546" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fine-grained event categorization with heterogeneous graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiran</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanxing</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunfeng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3238" to="3245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph lstms. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Meld: A multimodal multi-party dataset for emotion recognition in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
	<note>Erik Cambria, and Rada Mihalcea</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction beyond the sentence boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inter-sentence relation extraction with document-level graph convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Sunil Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4309" to="4316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Thomas Tutte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>Clarendon Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural networks for extractive document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6209" to="6219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A pilot study of opinion summarization in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the Association for Computational Linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the Association for Computational Linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Renet: A deep learning approach for extracting gene-disease associations from literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hing-Fung</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wah</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Research in Computational Molecular Biology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="272" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Friendsqa: Open-domain question answering on tv show transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 20th Annual SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer for graphto-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaowei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7145" to="7154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Docred: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="764" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dialogue-based relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4927" to="4940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2205" to="2215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">They exist! introducing plural mentions to coreference resolution and entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
