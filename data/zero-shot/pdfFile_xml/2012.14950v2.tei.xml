<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">2D or not 2D? Adaptive 3D Convolution Selection for Efficient Video Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengduo</forename><surname>Li</surname></persName>
							<email>hdli@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
							<email>zxwu@fudan.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
							<email>abhinav@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">2D or not 2D? Adaptive 3D Convolution Selection for Efficient Video Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D convolutional networks are prevalent for video recognition. While achieving excellent recognition performance on standard benchmarks, they operate on a sequence of frames with 3D convolutions and thus are computationally demanding. Exploiting large variations among different videos, we introduce Ada3D, a conditional computation framework that learns instance-specific 3D usage policies to determine frames and convolution layers to be used in a 3D network. These policies are derived with a twohead lightweight selection network conditioned on each input video clip. Then, only frames and convolutions that are selected by the selection network are used in the 3D model to generate predictions. The selection network is optimized with policy gradient methods to maximize a reward that encourages making correct predictions with limited computation. We conduct experiments on three video recognition benchmarks and demonstrate that our method achieves similar accuracies to state-of-the-art 3D models while requiring 20% ? 50% less computation across different datasets. We also show that learned policies are transferable and Ada3D is compatible to different backbones and modern clip selection approaches. Our qualitative analysis indicates that our method allocates fewer 3D convolutions and frames for "static" inputs, yet uses more for motionintensive clips. * Corresponding author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Barbequing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Breakdancing</head><p>Golf driving 1 Here, we use "clip" in a broad sense; for 2D models, a clip is a single RGB frame while for 3D models it is a stack of frames.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Videos are expected to make up a staggering 82% of Internet traffic by 2022 <ref type="bibr" target="#b0">[1]</ref>, which demands approaches that can understand video content like actions and events accurately and efficiently. Key to video recognition is temporal modeling to capture relationships among different frames. Towards this goal, extensive studies have been conducted with 3D convolutional networks by extending 2D convolutions over time <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b44">45]</ref>. While of- <ref type="figure">Figure 1</ref>: A conceptual overview of our approach. Ada3D learns to adaptively keep/discard 3D convolutional layers and frames conditioned on input clips for efficient video recognition. Fewer 3D convolutions and frames are kept for clips that contain discriminative static cues and contextual information, while more are used for motionintensive clips, in pursuit of a reduced overall computational cost without sacrificing recognition accuracy. Black mask indicates the frame is discarded.</p><p>fering excellent recognition accuracy on standard benchmarks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>, 3D models are often computationally expensive due to the costly convolution operations along the temporal axis on a large number of stacked frames. For example, at the clip-level 1 , a standard ResNet50 <ref type="bibr" target="#b13">[14]</ref> model only requires 4.1 GFLOPs (giga floating-point operations) to compute predictions for a single image, while a SlowFast network <ref type="bibr" target="#b8">[9]</ref> with the same ResNet50 backbone needs 16 times more computation (65.7 GFLOPs). Furthermore, the computational cost linearly grows with the number of clips uniformly sampled through the entire sequence for videolevel prediction aggregation.</p><p>But are 3D convolutions really important for recognizing different types of videos? Do we really need them through-out the network? Is it necessary to perform 3D convolution on a fixed number of stacked frames for all different samples? Intuitively, 3D convolutions are critical for capturing changing patterns among inputs. However, due to large intra-class and inter-class variations, some videos are relatively more "static" than others, for which using a computationally expensive 3D model on redundant inputs might be unnecessary. This paper seeks to develop a computationally efficient framework for video recognition by learning how many frames to use and whether to use 3D convolutions in 3D networks. This is an orthogonal yet complementary direction to existing work on fast video recognition, which either designs lightweight 3D architectures <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b33">34]</ref> or develops clip selection schemes to use fewer clips for classification <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>With this in mind, we introduce Ada3D, an end-to-end framework that learns adaptive 3D convolution usage conditioned on each input clip sample for efficient video recognition. For each clip, deriving a dynamic inference strategy entails (1) learning how many frames are used as inputs to the 3D network; (2) conditioned on these selected frames, determining how many 3D convolutional layers are activated; (3) and most importantly, making correct predictions while only using a small number of input frames and 3D convolutions. By doing so, Ada3D allocates more computational resources to videos with complicated motion patterns while performing economical inference for "easy static" videos, enabling efficient video classification while maintaining reliable classification accuracy. While appealing, learning whether to keep/discard input frames and 3D convolutions is a non-trivial task, as it requires making binary decisions that are non-differentiable.</p><p>To this end, Ada3D is built upon a reinforcement learning framework <ref type="bibr" target="#b31">[32]</ref>. In particular, given a video clip, Ada3D trains a two-head selection network to produce a frame usage policy and a convolution usage policy, indicating which frames in the input stack and which 3D convolutions in the network should be kept or discarded, respectively. Then, conditioned on the derived policies, dynamic inference is performed on a pretrained 3D network with selected frames and 3D convolutions for fast recognition. The selection network is optimized with policy gradient methods <ref type="bibr" target="#b31">[32]</ref> to maximize a reward function that is carefully designed to incentivize using as few computational resources as possible while making correct predictions. We further jointly finetune the selection network with the 3D network such that the 3D model is able to adapt to the adaptive inference paradigm. It worth nothing that the selection network is designed to be lightweight so that its computational overhead is negligible.</p><p>We conduct extensive experiments to evaluate Ada3D on ActivityNet <ref type="bibr" target="#b15">[16]</ref>, FCVID <ref type="bibr" target="#b18">[19]</ref>, Mini-Kinetics-200 <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b3">4]</ref>, and demonstrate that Ada3D is able to save 20% to 50% computation on different datasets while maintaining similar recognition performance compared with baselines. We show policies learned on Mini-Kinetics-200 can be further transferred to the full Kinetics dataset <ref type="bibr" target="#b3">[4]</ref>. In addition, we show the approach is compatible with different 3D models and it is also complementary to other clip-level selection methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b47">48]</ref>. We also demonstrate qualitatively that our method learns to allocate fewer 3D convolutions and frames for clips that are relatively more static, while applying more computation to motion-intensive clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep neural networks for video recognition. Existing work typically designs video recognition architectures by equipping state-of-the-art 2D models with the ability for temporal modeling, and can be roughly categorized into two directions. In particular, the first applies 2D models on a per-frame basis and then model temporal relationships across frames by aggregating features along the temporal axis with operations such as pooling <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b9">10]</ref>, recurrent networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b22">23]</ref>, and using inputs with explicit temporal information such as optical flow <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38]</ref>. The other <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8]</ref> directly transforms 2D models into 3D models with 3D convolutions applied on stacked RGB frames (clips). While achieving state-of-the-art performance on various benchmarks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b12">13]</ref>, 3D models are computationally expensive, limiting their deployment in real-world applications with limited resources. Our work aims to reduce the computational cost of 3D models by learning instance-specific 3D policies using fewer frames and 3D convolutions in a 3D model conditioned on inputs while making correct predictions at the same time.</p><p>Efficient video recognition. Extensive studies have been conducted on designing efficient network architectures for video recognition <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34]</ref>. Recent advances in efficient 2D ConvNets, e.g. group convolution <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30]</ref>, have been explored in 3D models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b33">34]</ref>. In addition, some lightweight temporal aggregation operations are introduced to speed up inference such as a relational module in TRN <ref type="bibr" target="#b48">[49]</ref> and a shift module in TSM <ref type="bibr" target="#b23">[24]</ref>. More recently, X3D <ref type="bibr" target="#b7">[8]</ref> expands a tiny model across several dimensions for a good efficiency/accuracy trade-off. However, all these approaches use a fixed input sampling scheme (i.e., number of frames and frame rate) and compute predictions with a "one-size-fits-all" model for all inputs clips, regardless of the large temporal variations among them. In contrast, we learn dynamic frame usage policies and convolution usage policies conditioned on input clips, in pursuit of computational efficiency without sacrificing accuracy. It is worth pointing out that our method is model-agnostic, and can be used in tandem with these efficient networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptive computation.</head><p>Many adaptive computation  <ref type="figure">Figure 2</ref>: An overview of our approach. Given an input clip, the selection network produces features for each frame in the clip, which are further aggregated uniformly to derive a frame usage policy and a convolution usage policy simultaneously. These policies activate a subset of frames and 3D convolutions in the 3D network for inference. Then, conditioned on the prediction, two rewards are computed to evaluate the frame and convolution policy, respectively. See texts for more details.</p><p>(a.k.a, conditional computation) methods have been developed in the image domain, achieving reduced computation by dynamically selecting channels <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25]</ref>, skipping layers <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b36">37]</ref>, performing early exiting with auxiliary structures <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b45">46]</ref>, adaptively switching input resolutions <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46]</ref>, etc. There are also a few recent studies exploring adaptive computation for videos. These approaches adaptively select salient clips for faster inference with one <ref type="bibr" target="#b42">[43]</ref> or more <ref type="bibr" target="#b40">[41]</ref> agents to aggregate videolevel predictions. Compressed video <ref type="bibr" target="#b19">[20]</ref> and audio <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref> are also utilized for further improvement in clip selection. More recently, a dynamic resolution selection strategy is introduced in <ref type="bibr" target="#b25">[26]</ref>. Our method is closely related yet orthogonal to these approaches. They focus on selecting informative clips throughout the entire sequence to achieve fast inference, aiming to improve the widely used uniform sampling baseline for video recognition. For each selected clip, the same amount of computational resource is used. In contrast, we allocate computation conditioned on the complexity of the input video clip. This can be considered as dynamic routing in a network and is complementary to those clip-selection methods (as will be shown empirically) <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20]</ref>, which are a form of routing across different time steps in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Ada3D reduces the computational cost of 3D networks by learning instance-specific 3D usage policies that encourage using fewer computational resources, in the forms of frames and 3D convolutions, while producing accurate pre-dictions. To this end, we first revisit popular 3D networks used for temporal modeling in Sec. 3.1, and then elaborate different components of Ada3D in Sec. 3.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D Networks for Video Recognition</head><p>Operating on stacked RGB frames, 3D video models typically extend state-of-the-art 2D networks by replacing a number of 2D convolutions with 3D convolutions for temporal modeling over time. Formally, taking as inputs an input clip V with T frames {v 1 , v 2 , ..., v T }, 3D models obtain final predictions through a stack of 2D (k 1?d?d ) and 3D (k t?d?d ) convolutional layers, where t denotes the temporal extent of 3D convolutional filters which is typically set to 3 and 5 in practice, and d denotes the spatial height and width. In common instantiations of 3D video models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8]</ref>, 3D convolutions are inserted into the building blocks of 2D networks, and these 3D blocks are organized based on heuristics such as using them in early <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b34">35]</ref> or late <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> stages of the network, if not applied in all stages <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b32">33]</ref>. Note that state-ofthe-art frameworks usually perform temporal convolutions in a non-degenerate form <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref>, i.e., taking in T frames and outputting T convolved frames. While achieving state-ofthe-art recognition performance, 3D video models are often computationally expensive since a number of costly 3D convolutions are applied on a sequence of stacked frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ada3D: Adaptive 3D Convolution Selection</head><p>Ada3D learns 3D convolution usage policies conditioned on input video clips to reduce the computational cost of 3D models. We achieve this with a lightweight selection network that is trained to determine which frames to use as inputs to a pretrained 3D model and which convolution layers to activate in the network for those selected frames. This involves making binary decisions that are non-differentiable, and thus not applicable for supervised frameworks. Instead, we formulate learning the selection network as Markov Decision Process (MDP) <ref type="bibr" target="#b27">[28]</ref>. We define the state space of the MDP as the input video clip; actions in the model involve keeping/discarding frames and 3D convolutions in 3D networks. The reward balances between recognition accuracy and computation. The MDP is single-step: a video clip is observed, actions are taken, and a reward is computed-this can also be considered as a contextual bandit <ref type="bibr" target="#b20">[21]</ref>.</p><p>More formally, given an input clip V of length T and a 3D ResNet video classifier F with K 3D convolution stages 2 , the selection network f p , parameterized by w, computes features for each frame in the input clip; these features are then aggregated as inputs to two parallel branches, outputting two vectors m ? R T and n ? R K :</p><formula xml:id="formula_0">m, n = sigmoid(f p (V; w)).<label>(1)</label></formula><p>Here, each entry in m and n is normalized to be in the range [0, 1] with the sigmoid(x) = 1 1+exp(?x) function, indicating the likelihood of keeping the corresponding frame and 3D convolution stage 2 .</p><p>We then define a frame usage policy ? f and a convolution usage policy ? c with a T -dimensional and a Kdimensional Bernoulli distribution, respectively:</p><formula xml:id="formula_1">? f (u | V) = T t=1 m ut t (1 ? m t ) 1?ut (2) ? c (v | V) = K k=1 n v k k (1 ? n k ) 1?v k .<label>(3)</label></formula><p>where u ? {0, 1} T and v ? {0, 1} K are actions based on m and n, and u t = 1 indicates the t-th frame in V is used; similarly v k = 1 means the k-th 3D convolution stage in the 3D model is activated. Zero entries in u and v represent inactive frames and convolutions, respectively. During training, u and v are produced by sampling from the corresponding policy, and a greedy approach is used at test time. Given these actions, a subset V of the full clip V is formed based on u. Similarly, according to v, certain 3D convolution layers are changed to 2D by taking only the center channel of its 3D convolutional filter along the temporal axis, i.e., the slicing operation k t?d?d [ t 2 , :, :] in Py-Torch style. Then, conditioned on V , we run a forward pass with the 3D network where certain 3D convolutions are degraded, and a prediction is then computed. To encourage correct predictions with limited computation, we evaluate these actions with a reward function:</p><formula xml:id="formula_2">R(x) = 1 ? O(x) for correct prediction ?? else (4)</formula><p>where O(x) represents the normalized computational cost of the action and x ? {u, v}. Based on Eqn. 4, we compute two rewards for frame actions and convolution actions respectively, encouraging using as little computation as possible when making correct predictions while penalizing incorrect predictions with a negative reward, i.e., ??. Note that ? also balances the speed-accuracy trade-off with different values. While we instantiate O(u) and O(v) as ( ||u||0 T ) and ( ||v||0 K ) 2 -the normalized usage of the number of frames and 3D convolutions-there are also other options such as FLOPs <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15]</ref>. The selection network is then optimized to maximize the expected reward:</p><formula xml:id="formula_3">max w L = E u?? f ,v??c [R(u) + R(v)].<label>(5)</label></formula><p>We use policy gradient methods <ref type="bibr" target="#b31">[32]</ref> to learn the parameters w for the selection network and the expected gradient can be derived as:</p><formula xml:id="formula_4">? w L = E [R(u)? w log ? f (u | V) + R(v)? w log ? c (v | V)].<label>(6)</label></formula><p>Eqn. 6 can be estimated with many samples at a time, and thus we use samples in mini-batches to compute the expected gradient and then Eqn. 6 is approximated by:</p><formula xml:id="formula_5">? w L ? 1 B B i=1 [R(u i )? w log ? f (u i | V i ) + R(v i )? w log ? c (v i | V i )],<label>(7)</label></formula><p>where B is the total number of samples in the mini-batch. The gradient is then propagated back to train the policy network with SGD. We further reduce variance by adding a baseline function to the reward <ref type="bibr" target="#b31">[32]</ref>. So far we have only trained the selection network while keeping the pretrained video model fixed. The selection network is able to learn decent policies that use fewer frames and 3D convolutions while maintaining prediction accuracies. However, input distributions to the 3D model are no longer the same as those used to train the original network, where all frames and 3D convolutions are used. As a result, the 3D model is not equipped with the ability to deal with inputs with varying number of frames and 3D convolutions that are adaptively turned on/off. To remedy this, we further jointly fine-tune the 3D model with the selection network such that it is able to accustomed to such adaptive inference paradigm. The objective function then becomes: where ? denotes the weights of the 3D network F and the first term is the cross-entropy loss for an input clip V with one-hot label y for classification training. Algorithm 1 summarizes algorithm of Ada3D.</p><formula xml:id="formula_6">min w,? ? j=1 y j log(F(V; ?) j ) ? L(w)<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets and evaluation metrics.</p><p>We evaluate our approach on three video recognition datasets: Ac-tivityNet (ACTIVITYNET) <ref type="bibr" target="#b15">[16]</ref>, Fudan-Columbia Video Datasets (FCVID) <ref type="bibr" target="#b18">[19]</ref> and Mini-Kinetics-200 (MINI-KINETICS) <ref type="bibr" target="#b43">[44]</ref>. ACTIVITYNET contains around 20K Youtube videos of 200 action classes, with an average duration of 117 seconds. We use the latest version 1.3 and its official split with 10, 024 training videos, 4, 926 validation videos and 5, 044 testing videos. We report results on the validation set as the labels of testing videos are not publicly available. FCVID consists of 91, 223 Youtube videos belonging to 239 categories, with an average duration of 167 seconds. The official split is adopted with a training set of 45, 611 videos and a testing set of 45, 612 videos. MINI-KINETICS is a publicly released subset of KINETICS <ref type="bibr" target="#b3">[4]</ref> initially introduced in <ref type="bibr" target="#b43">[44]</ref>, consisting of 200 classes with the most training samples in Kinetics; 400 and 25 videos are sampled from each action class for training and validation, forming a training set with 80, 000 videos and a validation set with 5, 000 videos. Here we use the identical samples as <ref type="bibr" target="#b43">[44]</ref>. To demonstrate the transferability of the selection network, we experiment with the Kinetics full set, which contains 240K training videos and 20K validation videos.</p><p>Following official instructions, we report mean aver-age precision (mAP) on ACTIVITYNET and FCVID. For MINI-KINETICS and KINETICS, we report Top-1 accuracy.</p><p>Network architectures. We use an I3D <ref type="bibr" target="#b3">[4]</ref> with a backbone of ResNet-50 <ref type="bibr" target="#b13">[14]</ref> as the 3D video model if not mentioned otherwise, due to its popularity and competitive recognition performance across various benchmarks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>. Our implementation follows <ref type="bibr" target="#b6">[7]</ref>, where 3D convolutions are factorized spatially and temporally in a similar way as R(2+1)-D <ref type="bibr" target="#b34">[35]</ref>, which is already a more efficient architecture than original I3D. In addition, we also experiment with the Slowonly model introduced in <ref type="bibr" target="#b8">[9]</ref> to demonstrate the compatibility of our approach with more recent networks. We use a lightweight architecture for the selection network with negligible computational overhead. Specifically, we use MobileNetV2 <ref type="bibr" target="#b29">[30]</ref> as the backbone of the selection network. The inputs to the network are downsampled to 112 ? 112 per frame, and it only requires 0.08 GFLOPs to compute features for each frame.</p><p>Implementation details. All 3D networks are fine-tuned from models provided by <ref type="bibr" target="#b6">[7]</ref>, which are pre-trained on Kinetics. We fine-tune 3D models for 40 epochs on FCVID and ACTIVITYNET and 20 epochs on MINI-KINETICS, with a cosine learning rate schedule starting at 0.01 and a batch size of 64. The MobileNetV2 backbone of the selection network is also pre-trained on these datasets with the same schedules to speed up convergence. We first fix the pretrained 3D models and train the selection network for 40 epochs with a learning rate of 0.0001 and a batch size of 256. Finally, the whole pipeline is jointly fine-tuned for 60 epochs with the same learning rate described above. SGD with momentum 0.9 is used for optimization. We use 8 GPUs for all experiments.</p><p>Regarding network inputs during training, we follow <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref> by randomly sampling a clip with 8/16 frames using a temporal stride of 8 (sampling rate) from a given video. For the spatial domain, 224 ? 224 pixels are randomly cropped from the sampled clip during training. For inference, we follow the common practice <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b38">39]</ref> and uniformly sample 10 clips with a spatial size of 256 ? 256 from a testing video. Video-level prediction is obtained by averaging the clip-level predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head><p>We compare our proposed method with various baselines under different input settings (8/16 frames per clip) and report results in <ref type="table" target="#tab_1">Table 1</ref>. The baselines we use include:</p><p>? Random: Based on the frame usage and convolution usage produced by Ada3D, we generate random policies that use a similar amount of computational resources compared to Ada3D.</p><p>? Random FT: The 3D model is further jointly fine-tuned with the random policies.  ? Upper: The original pretrained 3D model with all 3D convolutions and all frames used, which can be viewed as a performance "upperbound" of our method.</p><p>As shown in <ref type="table" target="#tab_1">Table 1</ref>, under the 8-frame input setting, Ada3D obtains an mAP (accuracy for MINI-KINETICS) of 81.9%, 82.6% and 78.9%, requiring an average of 35.6, 42.2 and 42.4 GFLOPs per clip on FCVID, ACTIVITYNET, MINI-KINETICS respectively. Ada3D achieves comparable recognition performance but brings 40%, 28% and 27% computational savings. This confirms that Ada3D is able to learn effective 3D convolution and frame usage policies by saving computational resources and preserving accuracies at the same time across different datasets. Similar patterns are also observed under the 16-frame input setting for all three datasets.</p><p>Using similar computational resources, Ada3D improves the Random baseline by 3.5% to 5% mAP/accuracy on three datasets. Ada3D also outperforms Random FT by 1% to 2.5%. These results verify that Ada3D produces adaptive polices and allocates computational resources on a per-input basis to maintain recognition performance. It is worth noting that there are slightly differences in computational savings on different datasets. This results from the fact that video categories in these datasets are different. For example, FCVID contains some classes of static objects and scenes like "bridge" and "temple", and thus we observe more computational savings than ACTIV-ITYNET and MINI-KINETICS, which are more activityfocused; on MINI-KINETICS, where categories are motionintensive, more computational resources are needed compared to FCVID and ACTIVITYNET.</p><p>Recognition with varying computational budgets. As discussed in Section 3.2, the choice of ? in Eqn. 4 adjusts the amount of penalty on policies that produce incorrect predictions, and thus it controls the speed/accuracy trade-off. Here we report recognition accuracies of Ada3D under different computational budgets. As demonstrated in <ref type="figure" target="#fig_3">Fig. 3</ref>, our method is able to cover a wide range of speed/accuracy trade-offs and consistently outperforms Random FT with different computational budgets. For example, on ACTIV-ITYNET, Ada3D obtains an mAP of 82.6%, 81.9% and 80.9% with an average of 42. Extension to clip selection. As mentioned in Sec. 2, our method is orthogonal and thus could be complementary to the line of clip selection methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12]</ref> for efficient video recognition. We validate our hypothesis by combining our method with AdaFrame <ref type="bibr" target="#b42">[43]</ref>. Specifically, we use Ada3D as the backbone of AdaFrame to dynamically allocate computational resources conditioned on each input clip, as opposed to the original AdaFrame that uses the   same amount of computation with a fixed backbone for all clips. Following <ref type="bibr" target="#b42">[43]</ref>, we train three variants of AdaFrame which operates on 3, 5, and 10 clips for different computational budgets. As demonstrated in <ref type="table" target="#tab_3">Table 2</ref>, extending our approach with adaptive clip selection further decreases the computational cost while producing comparable performance with the Upper. For example, it reduces the number of clips sampled from each testing video from 10 to 7.4 and obtains an mAP of 82.0% that is on par with Upper (82.1%). Additionally, we believe our method is also complementary to other clip selection methods leveraging multi-modal inputs such as audio <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b11">12]</ref>, as well as adaptive spatial resolution modulating methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc GFLOPs # 3D # Frame  Transferring learned policies. We now analyze whether the policies learned by our method can be transferred to novel action categories. To this end, we take the selection network trained on MINI-KINETICS and fine-tune a pretrained I3D model with a ResNet-50 as its backbone on full Kinetics. We keep the weights of the selection network fixed during fine-tuning. Details of training and testing are the same as joint fine-tuning as described in Sec. 4.1. As shown in <ref type="table" target="#tab_5">Table 3</ref>, policies learned on MINI-KINETICS can reduce the overall computational cost of the fine-tuned video model by 25% on Kinetics with negligible difference in recognition accuracy compared to the Upper baseline, indicating that our method learns strategies that are transferable to unseen classes and videos. It is worth noting that the I3D baseline we use obtains superior recognition performance on Kinetics that is higher than <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b43">44]</ref> and competitive compared to results reported in <ref type="bibr" target="#b38">[39]</ref> using 32 frames per input clip.</p><p>Compatibility with different 3D architectures. Next, we evaluate the compatibility of our approach with different 3D networks. We use a more efficient 3D network architecture recently introduced in <ref type="bibr" target="#b8">[9]</ref> termed as Slowonly and evaluate our approach. In particular, it only uses 3D convolutions in the 4-th and 5-th stage of a ResNet50, resulting in competitive recognition performance with less computational cost. As shown in <ref type="table" target="#tab_7">Table 4</ref>, our method still obtains 20% to 40% savings in GFLOPs with similar recognition performance, indicating Ada3D is compatible with different 3D models. Our method by design is model-agnostic, for which we believe it could be complementary to recent work on designing efficient 3D models such as X3D <ref type="bibr" target="#b7">[8]</ref> as well.  Qualitative analysis. In addition to the quantitative results presented above, we also qualitatively analyze our method.</p><p>In particular, we observe that our method produces policies with fewer 3D convolutions and frames for input clips that are more "static", while uses more for motion-intensive instances. As shown in <ref type="figure">Fig. 4</ref>, a smaller number of 3D convolutions and frames are applied on clips with discriminative static cue. For instance, the presence of "bass" and "book binder" for class "playing bass guitar" and "book binding" suffice to produce correct predictions, and the scene of a "court" serves as a strong contextual signal for "hurling". On the other hand, for motion-intensive action classes and instances, especially those related to human movement such as "breakdancing", "somersaulting" and "Tai Chi", more computational resources are allocated by our method to capture finer temporal relationships among frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion</head><p>Impact of joint finetuning. Recall that we first train the seletion network with the 3D model fixed and then jointly fine-tune both of them. Here we analyze the performance of our method without the first selection network training stage (Tr) or the joint fine-tuning stage (FT). For faster evaluation, we uniformly sample 3 clips from each test video. Results are shown in <ref type="table">Table 5</ref>.</p><p>As can be seen, joint fine-tuning is crucial to further improve the recognition performance (75.9 vs. 77.8). This indicates that fine-tuning the video model together with learned policies indeed helps the 3D model to adapt to the adaptive inference paradigm brought by the selection network. It is worth noting that skipping the first training  <ref type="figure">Figure 4</ref>: Qualitative results. Black mask indicates the frame is discarded. Left: Fewer 3D convolutions and frames are used for action classes and instances that are more "static", i.e. containing discriminative static cue and contextual information.</p><p>Right: For motion-intensive instances, more computation is allocated for probing finer temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FCVID ACTIVITYNET</head><p>Tr FT mAP GFLOPs mAP GFLOPs  <ref type="table">Table 5</ref>: Ablation on the effectiveness of two training stages.</p><p>stage (i.e., directly training the selection network with the 3D model jointly) leads to a lower recognition performance (76.5 vs. 77.8). We posit the reason is that adding another objective (the classification loss) while training the selection network from random initialization further increases the instability of network learning under such a reinforcement learning setting; and thus the selection network converges to sub-optimal policies.  <ref type="table">Table 6</ref>: Ablation on the usefulness of 3D convolution usage and frame usage policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions of convolution and frame usage policies.</head><p>To demonstrate the effectiveness of 3D convolution usage and frame usage policies learned by the two-head selection network, we conduct experiments to analyze contributions of the two components. In particular, we replace each/both components with randomly generated policies similar to Random FT. Here we use 3-clip testing as well. As shown in <ref type="table">Table 6</ref>, applying either 3D or frame usage policy improves recognition performance under the same computational budget, while using both achieves the best performance with 1% improvement over the single-component settings, indicating the double-head architecture can learn to produce policies cooperatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented Ada3D, a framework that learns to derive adaptive 3D convolution and frame usage policiesdetermining which 3D convolutions in a pretrained 3D video model and which frames in the input clip to use on a per-input basis-for efficient video recognition. In particular, a two-head selection network is trained with policy gradient methods to produce these policies, reducing overall computational cost while maintaining recognition performance. Extensive experimental results on three large-scale video recognition datasets indicate that Ada3D achieves 20%-50% computational savings on state-of-theart 3D video models while achieving similar accuracies. We further demonstrate Ada3D is compatible with different backbones of 3D model and other clip selection methods, and qualitatively show that more computational resource is allocated on motion-intensive instances but less on static ones by Ada3D.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 : 6 p 7 R?</head><label>167</label><figDesc>Training algorithm of our approach. Input: An input video clip V, the number of epochs of for training the selection network E 1 , the number of epochs of joint fine-tuning E 2 1 Obtain a pretrained video classifier F 2 Randomly initialize selection network w 3 for e ? 0 to E 1 do 4 m, n = sigmoid(f p (V; w)) 5 u, v ? ? w (u|V), ? w (v|V) // Eqn. 3 = F(V|u, v) // Apply actions on F and forward = R(u) + R(v) = w ? ??L cls // Eqn. 8 14 end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>mAP GFLOPs # 3D # Frame mAP GFLOPs # 3D # Frame Acc GFLOPs # 3D # Frame</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2, 30.1 and 24.4 GFLOPs per clip respectively, while Random FT obtains 81.1%, 80.7% and 79.8% with 42.9, 32.6 and 25.4 GFLOPS per clip on average. Same patterns are also observed on FCVID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Recognition performance under different computational budgets controlled by ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>...... ......</figDesc><table><row><cell>Spatial Downsampling</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Frame Policy</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">3D Video Model</cell><cell></cell></row><row><cell>Selection Network</cell><cell>3D Convolution Policy</cell><cell>2D</cell><cell>3D</cell><cell>2D</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Recognition performance and computational cost of our method vs. baselines. Two input settings are experimented, i.e. 8-frame setting (Top) and 16-frame setting (Bottom). # 3D and # Frame denote the number of 3D convolutions and frames usage per input clip respectively, averaged over the entire test set. See texts for more details.</figDesc><table><row><cell>8-frame per clip</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Upper</cell><cell>82.1</cell><cell>58.6</cell><cell>5.0</cell><cell>8.0</cell><cell>82.6</cell><cell>58.6</cell><cell>5.0</cell><cell>8.0</cell><cell>79.0</cell><cell>58.6</cell><cell>5.0</cell><cell>8.0</cell></row><row><cell>Random</cell><cell>78.1</cell><cell>36.1</cell><cell>2.2</cell><cell>5.8</cell><cell>79.2</cell><cell>42.9</cell><cell>3.0</cell><cell>6.6</cell><cell>74.0</cell><cell>42.2</cell><cell>2.0</cell><cell>6.9</cell></row><row><cell>Random FT</cell><cell>80.7</cell><cell>36.1</cell><cell>2.2</cell><cell>5.8</cell><cell>81.1</cell><cell>42.9</cell><cell>3.0</cell><cell>6.6</cell><cell>77.4</cell><cell>41.5</cell><cell>1.9</cell><cell>6.8</cell></row><row><cell>Ours</cell><cell>81.9</cell><cell>35.6</cell><cell>2.2</cell><cell>5.7</cell><cell>82.6</cell><cell>42.2</cell><cell>3.1</cell><cell>6.6</cell><cell>78.9</cell><cell>42.4</cell><cell>1.9</cell><cell>6.9</cell></row><row><cell>16-frame per clip</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Upper</cell><cell cols="2">84.4 117.3</cell><cell cols="2">5.0 16.0</cell><cell cols="2">84.4 117.3</cell><cell cols="2">5.0 16.0</cell><cell cols="2">79.6 117.3</cell><cell cols="2">5.0 16.0</cell></row><row><cell>Random</cell><cell>79.2</cell><cell>63.2</cell><cell cols="2">2.1 10.3</cell><cell>80.4</cell><cell>73.3</cell><cell cols="2">3.0 11.2</cell><cell>75.2</cell><cell>75.8</cell><cell cols="2">2.9 11.8</cell></row><row><cell>Random FT</cell><cell>82.0</cell><cell>65.3</cell><cell cols="2">2.1 10.6</cell><cell>82.8</cell><cell>71.3</cell><cell cols="2">3.0 11.1</cell><cell>78.2</cell><cell>78.0</cell><cell cols="2">2.9 12.0</cell></row><row><cell>Ours</cell><cell>84.3</cell><cell>66.6</cell><cell cols="2">2.1 10.7</cell><cell>84.0</cell><cell>70.1</cell><cell cols="2">3.0 11.1</cell><cell>79.2</cell><cell>73.8</cell><cell cols="2">2.9 11.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Extension to clip-level selection.</figDesc><table><row><cell>Combining</cell></row><row><cell>Ada3D with AdaFrame [43] offers computational savings</cell></row><row><cell>for video-level aggregation. # Clip denotes number of clips</cell></row><row><cell>used per testing video; Van (Vanilla) and Ada denote our</cell></row><row><cell>method without and with AdaFrame, respectively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Transferring learned policies.</figDesc><table><row><cell>We fine-tune a</cell></row><row><cell>Kinetics pretrained model on Kinetics full training set, with</cell></row><row><cell>policies learned on Mini-Kinetics, and evaluate on Kinetics</cell></row><row><cell>validation set.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Results on FCVID [19] using Slowonly [9] archi- tecture as 3D model. Top: 8-frame input setting. Bottom: 16-frame input setting.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We consider turning off an entire 3D convolution stage that contains multiple 3D convolutional layers to save more computation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This work is supported by IARPA via Department of Interior/Interior Business Center (DOI/IBC) contract number D17PC00345.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The state of online video for 2020</title>
		<ptr target="https://www.forbes.com/sites/tjmccue/2020/02/05/looking-deep-into-the-state-of-online-video-for-2020/.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Batch-shaping for learning conditional channel gated networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Babak Ehteshami Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>ICLR, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive neural networks for fast test-time prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-fiber networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Haoqi Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pyslowfast</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/slowfast,2020.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatially adaptive computation time for residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Listen to look: Action recognition by previewing audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Amc: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-scale dense networks for resource efficient image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploiting feature and class relationships in video categorization with regularized deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scsampler: Sampling salient clips from video for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The epoch-greedy algorithm for multi-armed bandits with side information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved techniques for training adaptive deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Videolstm convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Runtime neural pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ar-net: Adaptive frame resolution for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Ching</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 3</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Autofocus: Efficient multi-scale inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Markov decision processes: discrete stochastic dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin L Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">C3d: Generic features for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lubomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning when and where to zoom with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burak</forename><surname>Uzkent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional networks with adaptive inference graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Skipnet: Learning dynamic routing in convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning based frame sampling for effective untrimmed video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Blockdrop: Dynamic inference paths in residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adaframe: Adaptive frame selection for fast video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal pyramid network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Resolution adaptive networks for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamic sampling networks for efficient action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Dong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamaljeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
