<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RigNet: Repetitive Image Guided Network for Depth Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PCA Lab</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
							<email>kunwang@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">PCA Lab</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PCA Lab</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PCA Lab</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
							<email>junli@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">PCA Lab</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PCA Lab</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RigNet: Repetitive Image Guided Network for Depth Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>depth completion</term>
					<term>image guidance</term>
					<term>repetitive design</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depth completion deals with the problem of recovering dense depth maps from sparse ones, where color images are often used to facilitate this task. Recent approaches mainly focus on image guided learning frameworks to predict dense depth. However, blurry guidance in the image and unclear structure in the depth still impede the performance of the image guided frameworks. To tackle these problems, we explore a repetitive design in our image guided network to gradually and sufficiently recover depth values. Specifically, the repetition is embodied in both the image guidance branch and depth generation branch. In the former branch, we design a repetitive hourglass network to extract discriminative image features of complex environments, which can provide powerful contextual instruction for depth prediction. In the latter branch, we introduce a repetitive guidance module based on dynamic convolution, in which an efficient convolution factorization is proposed to simultaneously reduce its complexity and progressively model high-frequency structures. Extensive experiments show that our method achieves superior or competitive results on KITTI benchmark and NYUv2 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Depth completion, the technique of converting sparse depth measurements to dense ones, has a variety of applications in the computer vision field, such as autonomous driving <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b49">50]</ref>, augmented reality <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b44">45]</ref>, virtual reality <ref type="bibr" target="#b0">[1]</ref>, and 3D scene reconstruction <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b41">42]</ref>. The success of these applications heavily depends on reliable depth predictions. Recently, multi-modal information from various sensors is involved to help generate dependable depth results, such as color images <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b2">3]</ref>, surface normals <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b37">38]</ref>, confidence maps <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b48">49]</ref>, and even binaural echoes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35]</ref>. Particularly, the latest image guided methods <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b46">47]</ref> principally concentrate on using color images to guide the recovery of dense depth maps, achieving outstanding performance. However, due to the challenging environments and limited depth measurements, it's difficult for existing image guided methods to produce clear image guidance and structure-detailed depth features arXiv:2107.13802v5 [cs.CV] 13 Jul 2022 (d) Our repetitive model <ref type="figure">Fig. 1</ref>. To obtain dense depth Prediction, most existing image guided methods employ tandem models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36]</ref> (a) or parallel models <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b16">17]</ref> (b,c) with various inputs (e.g., Boundary/Confidence/Normal/RGB-D), whilst we propose the repetitive mechanism (d), aiming at providing gradually refined image/depth Guidance.</p><p>(see Figs. 2 and 6). To deal with these issues, in this paper we develop a repetitive design in both the image guidance branch and depth generation branch.</p><p>In the image guidance branch: Existing image guided methods are not sufficient to produce very precise details to provide perspicuous image guidance, which limits the content-complete depth recovery. For example, the tandem models ( <ref type="figure">Fig. 1(a)</ref>) tend to only utilize the final layer features of a hourglass unit. The parallel models conduct scarce interaction between multiple hourglass units ( <ref type="figure">Fig. 1(b)</ref>), or refer to image guidance encoded only by single hourglass unit ( <ref type="figure">Fig. 1(c)</ref>). Different from them, as shown in <ref type="figure">Fig. 1(d)</ref>, we present a vertically repetitive hourglass network to make good use of RGB features in multi-scale layers, which contain image semantics with much clearer and richer contexts.</p><p>In the depth generation branch: It is known that gradients near boundaries usually have large mutations, which increase the difficulty of recovering structure-detailed depth for convolution <ref type="bibr" target="#b47">[48]</ref>. As evidenced in plenty of methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36]</ref>, the depth values are usually hard to be predicted especially around the region with unclear boundaries. To moderate this issue, in this paper we propose a repetitive guidance module based on dynamic convolution <ref type="bibr" target="#b46">[47]</ref>. It first extracts the high-frequency components by channel-wise and cross-channel convolution factorization, and then repeatedly stacks the guidance unit to progressively produce refined depth. We also design an adaptive fusion mechanism to effectively obtain better depth representations by aggregating depth features of each repetitive unit. However, an obvious drawback of the dynamic convolution is the large GPU memory consumption, especially under the case of our repetitive structure. Hence, we further introduce an efficient module to largely reduce the memory cost but maintain the accuracy.</p><p>Benefiting from the repetitive strategy with gradually refined image/depth representations, our method performs better than others, as shown in Figs. 4, 5 and 6, and reported in <ref type="table" target="#tab_4">Tables 3, 4</ref>, 5 and 6. In short, our contributions are: -We propose the effective but lightweight repetitive hourglass network, which can extract legible image features of challenging environments to provide clearer guidance for depth recovery. -We present the repetitive guidance module based on dynamic convolution, including an adaptive fusion mechanism and an efficient guidance algorithm, which can gradually learn precise depth representations. -Extensive experimental results demonstrate the effectiveness of our method, which achieves outstanding performances on three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Depth only approaches. For the first time in 2017, the work <ref type="bibr" target="#b47">[48]</ref> proposes sparsity invariant CNNs to deal with sparse depth. Since then, lots of depth completion works <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49]</ref> input depth without using color image. Distinctively, Lu et al. <ref type="bibr" target="#b31">[32]</ref> take sparse depth as the only input with color image being auxiliary supervision when training. However, single-modal based methods are limited without other reference information. As technology quickly develops, plenty of multi-modal information is available, e.g., surface normal and optic flow images, which can significantly facilitate the depth completion task. Image guided methods. Existing image guided depth completion methods can be roughly divided into two patterns. One pattern is that various maps are together input into tandem hourglass networks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b51">52]</ref>. For example, S2D <ref type="bibr" target="#b32">[33]</ref> directly feeds the concatenation into a simple Unet <ref type="bibr" target="#b40">[41]</ref>. CSPN <ref type="bibr" target="#b4">[5]</ref> studies the affinity matrix to refine coarse depth maps with spatial propagation network (SPN). CSPN++ <ref type="bibr" target="#b3">[4]</ref> further improves its effectiveness and efficiency by learning adaptive convolutional kernel sizes and the number of iterations for propagation.</p><p>As an extension, NLSPN <ref type="bibr" target="#b35">[36]</ref> presents non-local SPN which focuses on relevant non-local neighbors during propagation. Another pattern is using multiple independent branches to model different sensor information and then fuse them at multi-scale stages <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b16">17]</ref>. For example, PENet <ref type="bibr" target="#b16">[17]</ref> employs feature addition to guide depth learning at different stages. ACMNet <ref type="bibr" target="#b58">[59]</ref> chooses graph propagation to capture the observed spatial contexts. GuideNet <ref type="bibr" target="#b46">[47]</ref> seeks to predict dynamic kernel weights from the guided image and then adaptively extract the depth features. However, these methods still cannot provide very sufficient semantic guidance for the specific depth completion task. Repetitive learning models. To extract more accurate and abundant feature representations, many approaches <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref> propose to repeatedly stack similar components. For example, PANet <ref type="bibr" target="#b29">[30]</ref> adds an extra bottom-up path aggregation which is similar with its former top-down feature pyramid network (FPN). NAS-FPN <ref type="bibr" target="#b12">[13]</ref> and BiFPN <ref type="bibr" target="#b45">[46]</ref> conduct repetitive blocks to sufficiently encode discriminative image semantics for object detection. FCFRNet <ref type="bibr" target="#b28">[29]</ref> argues that the feature extraction in one-stage frameworks is insufficient, and thus proposes a two-stage model, which can be regarded as a special case of the repetitive design. On this basis, PENet <ref type="bibr" target="#b16">[17]</ref> further improves its performance by utilizing confidence maps and varietal CSPN++. Different from these methods, <ref type="figure">Fig. 2</ref>. Overview of our repetitive image guided network, which contains an image guidance branch and a depth generation branch. The former consists of a repetitive hourglass network (RHN) and the latter has the similar structure as RHN1. In the depth branch, we perform our novel repetitive guidance module (RG, elaborated in <ref type="figure" target="#fig_0">Fig. 3</ref>) to refine depth. In addition, an efficient guidance algorithm (EG) and an adaptive fusion mechanism (AF) are proposed to further improve the performance of the module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Guidance Branch Depth Generation Branch</head><p>in our image branch we first conduct repetitive CNNs units to produce clearer guidance in multi-scale layers. Then in our depth branch we perform repetitive guidance module to generate structure-detailed depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Repetitive Design</head><p>In this section, we first introduce our repetitive hourglass network (RHN), then elaborate the proposed repetitive guidance module (RG), including an efficient guidance algorithm (EG) and an adaptive fusion mechanism (AF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Repetitive Hourglass Network</head><p>For autonomous driving in challenging environments, it is important to understand the semantics of color images in view of the sparse depth measurement. The problem of blurry image guidance can be mitigated by a powerful feature extractor, which can obtain context-clear semantics. In this paper we present our repetitive hourglass network shown in <ref type="figure">Fig. 2</ref>. RHN i is a symmetrical hourglass unit like Unet. The original color image is first encoded by a 5?5 convolution and then input into RHN 1 . Next, we repeatedly utilize the similar but lightweight unit, each layer of which consists of two convolutions, to gradually extract high- level semantics. In the encoder of RHN i , E ij takes E i(j?1) and D (i?1)j as input.</p><formula xml:id="formula_0">D ij e 1j ? ? Conv Conv d j1 d j2 d j RG d jk ? EG 1 EG 2 EG k AF Conv GAP Conv C C?1?1 C?C C?H?W channel-wise</formula><p>In the decoder of RHN i , D ij inputs E ij and D i(j+1) . When i &gt; 1, the process is</p><formula xml:id="formula_1">E ij = Conv D (i?1)j , j = 1, Conv E i(j?1) + D (i?1)j , 1 &lt; j ? 5, D ij = Conv (E i5 ) , j = 5, Deconv D i(j+1) + E ij , 1 ? j &lt; 5,<label>(1)</label></formula><p>where Deconv (?) denotes deconvolution function, and E 1j = Conv(E 1(j?1) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Repetitive Guidance Module</head><p>Depth in challenging environments is not only extremely sparse but also diverse. Most of the existing methods suffer from unclear structures, especially near the object boundaries. Since gradual refinement is proven effective <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b51">52]</ref> to tackle this issue, we propose our repetitive guidance module to progressively generate dense and structure-detailed depth maps. As illustrated in <ref type="figure">Fig. 2</ref>, our depth generation branch has the same architecture as RHN 1 . Given the sparse depth input and color image guidance features D ij in the decoder of the last RHN, our depth branch generates final dense predictions. At the stage of the depth branch's encoder, our repetitive guidance module (left of <ref type="figure" target="#fig_0">Fig. 3</ref>) takes D ij and e 1j as input and employs the efficient guidance algorithm (in Sec. 3.2) to produce refined depth d jk step by step. Then we fuse the refined d jk by our adaptive fusion mechanism (in Sec. 3.2), obtaining the depth d j ,</p><formula xml:id="formula_2">d j = RG (D ij , e 1j ) ,<label>(2)</label></formula><p>where RG (?) refers to the repetitve guidance function. Efficient Guidance Algorithm Suppose the size of inputs D ij and e 1j are both C ? H ? W . It is easy to figure out the complexity of the dynamic convolution is O(C ?C ?R 2 ?H ?W ), which generates spatial-variant kernels according to color image features. R 2 is the size of the filter kernel window. In fact, C, H, and W are usually very large, it's thus necessary to reduce the complexity of  the dynamic convolution. GuideNet <ref type="bibr" target="#b46">[47]</ref> proposes channel-wise and cross-channel convolution factorization, whose complexity is</p><formula xml:id="formula_3">M EG M DC = C?H?W +C?C C?C?R 2 ?H?W = H?W +C C?H?W ?R 2 M EG M CF = C?H?W +C?C C?R 2 ?H?W +C?C = H?W +C C+H?W ?R 2</formula><formula xml:id="formula_4">O(C ? R 2 ? H ? W + C ? C).</formula><p>However, our repetitive guidance module employs the convolution factorization many times, where the channel-wise process still needs massive GPU memory consumption, which is O(C ? R 2 ? H ? W ). As a result, inspired by SENet <ref type="bibr" target="#b15">[16]</ref> that captures high-frequency response with channel-wise differentiable operations, we design an efficient guidance unit to simultaneously reduce the complexity of the channel-wise convolution and encode high-frequency components, which is shown in the top right of <ref type="figure" target="#fig_0">Fig. 3</ref>. Specifically, we first concatenate the image and depth inputs and then conduct a 3 ? 3 convolution. Next, we employ the global average pooling function to generate a C ? 1 ? 1 feature. At last, we perform pixel-wise dot between the feature and the depth input. The complexity of our channel-wise convolution is only O(C ? H ? W ), reduced to 1/R 2 . The process is defined as</p><formula xml:id="formula_5">d jk = EG (D ij , e 1j ) , k = 1, EG (Conv (D ij ) , d k?1 ) , k &gt; 1,<label>(3)</label></formula><p>where EG (?) represents the efficient guidance function. Suppose the memory consumptions of the common dynamic convolution, convolution factorization, and our EG are M DC , M CF , and M EG , respectively. <ref type="table" target="#tab_0">Table 1</ref> shows the theoretical analysis of GPU memory consumption ratio. Under the setting of the second (4 in total) fusion stage in our depth generation branch, using 4-byte floating precision and taking C = 128, H = 128, W = 608, and R = 3, as shown in <ref type="table" target="#tab_1">Table 2</ref>, the GPU memory of EG is reduced from 42.75GB to 0.037GB compared with the common dynamic convolution, nearly 1155 times lower in one fusion stage. Compared to the convolution factorization in GuideNet <ref type="bibr" target="#b46">[47]</ref>, the memory of EG is reduced from 0.334GB to 0.037GB, nearly 9 times lower. Therefore, we can conduct our repetitive strategy easily without worrying much about GPU memory consumption.</p><p>Adaptive Fusion Mechanism Since many coarse depth features (d j1 , ? ? ? , d jk ) are available in our repetitive guidance module, it comes naturally to jointly utilize them to generate refined depth maps, which has been proved effective in various related methods <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b16">17]</ref>. Inspired by the selective kernel convolution in SKNet <ref type="bibr" target="#b26">[27]</ref>, we propose the adaptive fusion mechanism to refine depth, which is illustrated in the bottom right of <ref type="figure" target="#fig_0">Fig. 3</ref>. Specifically, given inputs (d j1 , ? ? ? , d jk ), we first concatenate them and then perform a 3 ? 3 convolution. Next, the global average pooling is employed to produce a C ? 1 ? 1 feature map. Then another 3 ? 3 convolution and a softmax function are applied, obtaining (? 1 , ? ? ? , ? k ),</p><formula xml:id="formula_6">? k = Sof t (Conv (GAP (Conv (d j1 || ? ? ? ||d jk )))) ,<label>(4)</label></formula><p>where Sof t (?) and || refer to softmax function and concatenation. GAP (?) represents the global average pooling operation. Finally, we fuse the k coarse depth maps using ? k to produce the output d j ,</p><formula xml:id="formula_7">d j = k n=1 ? n d jn .<label>(5)</label></formula><p>The Eqs. 4 and 5 can be denoted as</p><formula xml:id="formula_8">d j = AF (d j1 , d j2 , ? ? ? , d jk ) ,<label>(6)</label></formula><p>where AF (?) represents the adaptive fusion function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RigNet</head><p>In this section, we describe the network architecture and the loss function for training. The proposed RigNet mainly consists of two parts: (1) an image guidance branch for the generation of hierarchical and clear semantics based on the repetitive hourglass network, and (2) a depth generation branch for structuredetailed depth predictions based on the novel repetitive guidance module with an efficient guidance algorithm and an adaptive fusion mechanism.</p><p>4.1 Network Architecture <ref type="figure">Fig. 2</ref> shows the overview of our network. In our image guidance branch, the RHN 1 encoder-decoder unit is built upon residual networks <ref type="bibr" target="#b14">[15]</ref>. In addition, we adopt the common connection strategy <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b2">3]</ref> to simultaneously utilize low-level and high-level features. RHN i (i &gt; 1) has the similar but lightweight architecture with RHN 1 , which is used to extract clearer image guidance semantics <ref type="bibr" target="#b53">[54]</ref>. The depth generation branch has the same structure as RHN 1 . In this branch, we perform repetitive guidance module based on dynamic convolution to gradually produce structure-detailed depth features at multiple stages, which is shown in <ref type="figure" target="#fig_0">Fig. 3</ref> and described in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Loss Function</head><p>During training, we adopt the mean squared error (MSE) to compute the loss, which is defined as</p><formula xml:id="formula_9">L = 1 m q?Qv ?GT q ? P q ? 2 ,<label>(7)</label></formula><p>where GT and P refer to ground truth depth and predicted depth respectively. Q v represents the set of valid pixels in GT , m is the number of the valid pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we first introduce the related datasets, metrics, and implementation details. Then, we carry out extensive experiments to evaluate the performance of our method against other state-of-the-art approaches. Finally, a number of ablation studies are employed to verify the effectiveness of our method.  <ref type="bibr" target="#b10">[11]</ref> is a synthetic dataset cloned from the real world KITTI video sequences. In addition, it also produces color images under various lighting (e.g., sunset, morning) and weather (e.g., rain, fog) conditions. Following GuideNet <ref type="bibr" target="#b46">[47]</ref>, we use the masks generated from sparse depths of KITTI dataset to obtain sparse samples. Such strategy makes it closed to real-world situation for the distribution of sparse depths. Sequences of 0001, 0002, 0006, and 0018 are used for training, 0020 with various lighting and weather conditions is used for testing. It contributes to 1,289 frames for fine-tuning and 837 frames for evaluating each condition. NYUv2 Dataset <ref type="bibr" target="#b43">[44]</ref> is comprised of video sequences from a variety of indoor scenes as recorded by both the color and depth cameras from the Microsoft Kinect. Paired color images and depth maps in 464 indoor scenes are commonly used. Following previous depth completion methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b46">47]</ref>, we train our model on 50K images from the official training split, and test on the 654 images from the official labeled test set. Each image is downsized to 320 ? 240, and then 304?228 center-cropping is applied. As the input resolution of our network must be a multiple of 32, we further pad the images to 320 ? 256, but evaluate only at the valid region of size 304 ? 228 to keep fair comparison with other methods. Metrics. For the outdoor KITTI depth completion dataset, following the KITTI benchmark and existing methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b16">17]</ref>, we use four standard metrics for evaluation, including RMSE, MAE, iRMSE, and iMAE. For the indoor NYUv2 dataset, following previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b28">29]</ref>, three metrics are selected for evaluation, including RMSE, REL, and ? i (i = 1.25, 1.25 2 , 1.25 3 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>The model is particularly trained with 4 TITAN RTX GPUs. We train it for 20 epochs with the loss defined in Eq. 7. We use ADAM <ref type="bibr" target="#b22">[23]</ref> as the optimizer with the momentum of ? 1 = 0.9, ? 2 = 0.999, a starting learning rate of 1 ? 10 ?3 , and weight decay of 1 ? 10 ?6 . The learning rate drops by half every 5 epochs. The synchronized cross-GPU batch normalization <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b54">55]</ref> is used when training.   <ref type="table" target="#tab_4">Table 3</ref> shows the quantitative results on KITTI benchmark, whose dominant evaluation metric is the RMSE. Our RigNet ranks 1st among publicly published papers when submitting, outperforming the 2nd with significant 17.42mm improvement while the errors of other methods are very closed. Here, the performance of our RigNet is also better than those approaches that employ additional dataset, e.g., DLiDAR <ref type="bibr" target="#b37">[38]</ref> utilizes CARLA <ref type="bibr" target="#b8">[9]</ref> to predict surface normals for better depth predictions. Qualitative comparisons with several state-of-the-art works are shown in <ref type="figure">Fig. 4</ref>. While all methods provide visually good results in general, our estimated depth maps possess more details and more accurate object boundaries. The corresponding error maps can offer supports more clearly. For example, among the marked iron pillars in the first row of <ref type="figure">Fig. 4</ref>, the error of our prediction is significantly lower than the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation on KITTI Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation on NYUv2 Dataset</head><p>To verify the performance of proposed method on indoor scenes, following existing approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b28">29]</ref>, we train our repetitive image guided network on the NYUv2 dataset <ref type="bibr" target="#b43">[44]</ref> with the setting 500 sparse samples. As illustrated in <ref type="table">Table 4</ref>, our model achieves the best performance among all traditional and latest approaches without using additional datasets, which proves that our network possesses stronger generalization capability. <ref type="figure" target="#fig_3">Fig. 5</ref> demonstrates the qualitative visualization results. Obviously, compared with those state-of-the-art methods, our RigNet can recover more detailed structures with lower errors at most pixels, including sharper boundaries and more complete object shapes. For example, among the marked doors in the last row of <ref type="figure" target="#fig_3">Fig. 5</ref>, our prediction is very close to the ground truth, while others either have large errors in the whole regions or have blurry shapes on specific objects.  <ref type="figure">Fig. 4</ref>. Qualitative results on KITTI depth completion test set, including (b) GuideNet <ref type="bibr" target="#b46">[47]</ref>, (c) FCFRNet <ref type="bibr" target="#b28">[29]</ref>, and (d) CSPN <ref type="bibr" target="#b4">[5]</ref>. Given sparse depth maps and the aligned color images (1st column), depth completion models output dense depth predictions (e.g., 2nd column). We provide error maps borrowed from the KITTI leaderboard for detailed discrimination. Warmer color in error maps refer to higher error.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation Studies</head><p>Here we employ extensive experiments to verify the effectiveness of each proposed component, including the repetitive hourglass network (RHN- <ref type="table">Table 5</ref>) and the repetitive guidance module (RG- <ref type="table">Table 6</ref>), which consists of the efficient guidance algorithm (EG) and the adaptive fusion mechanism (AF). Note that the batch size is set to 8 when computing the GPU memory consumption.</p><p>(1) Effect of Repetitive Hourglass Network.</p><p>The state-of-the-art baseline GuideNet <ref type="bibr" target="#b46">[47]</ref> employs 1 ResNet-18 as backbone and guided convolution G 1 to predict dense depth. To validate the effect of our RHN, we explore the backbone design of the image guidance branch for the specific depth completion task from four aspects, which are illustrated in <ref type="table">Table 5</ref>.</p><p>(i) Deeper single backbone vs. RHN. The second column of <ref type="table">Table 5</ref> shows that, when replacing the single ResNet-10 with ResNet-18, the error is reduced by 43mm. However, when deepening the baseline from 18 to 26/34/50, the  <ref type="table">Table 5</ref>. Ablation studies of RHN on KITTI validation set. 18-1 denotes that we use 1 ResNet-18 as backbone, which is also the baseline. 'Deeper'/'More' denotes that we conduct single&amp;deeper/multiple&amp;tandem hourglass units as backbone. Note that each layer of RHN2,3 only contains two convolutions while the RHN1 employs ResNet.</p><p>errors have barely changed, which indicate that simply increasing the network depth of image guidance branch cannot deal well with the specific depth completion task. Differently, with little sacrifice of parameters (?2 M), our RHN-10-3 and RHN-18-3 are 24mm and 10mm superior to Deeper-10-1 and Deeper-18-1, respectively. <ref type="figure" target="#fig_5">Fig. 6</ref> shows that the image feature of our parallel RHN-18-3 has much clearer and richer contexts than that of the baseline Deeper-18-1.</p><p>(ii) More tandem backbones vs. RHN. As shown in the third column of <ref type="table">Table 5</ref>, we stack the hourglass unit in series. The models of More-18-2, More-18-3, and More-18-4 have worse performances than the baseline Deeper-18-1. It turns out that the combination of tandem hourglass units is not sufficient to provide clearer image semantic guidance for the depth recovery. In contrast, our parallel RHN achieves better results with fewer parameters and smaller model sizes. These facts give strong evidence that the parallel repetitive design in image guidance branch is effective for the depth completion task.</p><p>(iii) Deeper-More backbones vs. RHN. As illustrated in the fourth column of <ref type="table">Table 5</ref>, deeper hourglass units are deployed in serial way. We can see that the Deeper-More combinations are also not very effective, since the errors of them are higher than the baseline while RHN's error is 10mm lower. It verifies again the effectiveness of the lightweight RHN design.</p><p>(2) Effect of Repetitive Guidance Module.</p><p>(i) Efficient guidance. Note that we directly output the features in EG 3 when not employing AF. <ref type="table" target="#tab_0">Tables 1 and 2</ref> have provided quantitative analysis in theory for EG design. Based on (a), we disable G 1 by replacing it with EG 1 . Comparing (b) with (a) in <ref type="table">Table 6</ref>, both of which carry out the guided convolution technology only once, although the error of (c) goes down a little bit, the GPU memory is heavily reduced by 11.95GB. These results give strong evidence that our new guidance design is not only effective but also efficient.</p><p>(ii) Repetitive guidance. When the recursion number k of EG increases, the errors of (c) and (d) are 6.3mm and 11.2mm significantly lower than that of (b) respectively. Meanwhile, as illustrated in <ref type="figure" target="#fig_5">Fig. 6</ref>  <ref type="table">Table 6</ref>. Ablation studies of RG/AF on KITTI validation set. RG-EG k refer to the case where we repeatedly use EG k times. '?0' refers to 23.37GB. G1 represents the raw guided convolution in GuideNet <ref type="bibr" target="#b46">[47]</ref>, which is used only once in one fusion stage. (iii) Adaptive fusion. Based on (d) that directly outputs the feature of RG-EG 3 , we choose to utilize all features of RG-EG k (k = 1, 2, 3) to produce better depth representations. (e), (f), and (g) refer to addition, concatenation, and our AF strategies, respectively. Specifically in (f), we conduct a 3 ? 3 convolution to control the channel to be the same as RG-EG 3 's after concatenation. As we can see from the 'AF' column of <ref type="table">Table 6</ref>, all of the three strategies improve the performance of the model with a little bit GPU memory sacrifice (about 0-0.06GB), which demonstrates that aggregating multi-step features in repetitive procedure is effective. Furthermore, our AF mechanism obtains the best result among them, outperforming (d) 5.3mm. These facts prove that our AF design benefits the system better than simple fusion strategies. Detailed difference of intermediate features produced by our repetitive design is shown in Figs. 2 and 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Generalization Capabilities</head><p>In this subsection, we further validate the generalization capabilities of our RigNet on different sparsity, including the number of valid points, various lighting and weather conditions, and the synthetic pattern of sparse data. The corresponding results are illustrated in Figs. 7 and 8.</p><p>(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Number of valid points</head><p>On KITTI selected validation split, we compare our method with four wellknown approaches with available codes, i.e., S2D <ref type="bibr" target="#b32">[33]</ref>, Fusion <ref type="bibr" target="#b48">[49]</ref>, NConv <ref type="bibr" target="#b9">[10]</ref>, and ACMNet <ref type="bibr" target="#b58">[59]</ref>. Note that, all models are pretrained on KITTI training split with raw sparsity, which is equivalent to sampling ratios of 1.0, but not finetuned on the generated depth inputs. Specifically, we first uniformly sample the raw depth maps with ratios (0.025, 0.05, 0.1, 0.2) and (0.4, 0.6, 0.8, 1.0) to produce the sparse depth inputs. Then we test the pretrained models on the inputs. <ref type="figure">Fig. 7</ref> shows our RigNet significantly outperforms others under all levels of sparsity in terms of both RMSE and MAE metrics. These results indicates that our method can deal well with complex data inputs.</p><p>(2) Lighting and weather condition The lighting condition of KITTI dataset is almost invariable and the weather condition is good. However, both lighting and weather conditions are vitally important for depth completion, especially for self-driving service. Therefore, we fine-tune our RigNet (trained on KITTI) on 'clone' of Virtual KITTI <ref type="bibr" target="#b10">[11]</ref> and test under all other different lighting and weather conditions. As shown in the right of <ref type="figure">Fig. 8</ref>, we compare 'RG' with '+' (replace RG with addition), our method outperforms '+' with large margin on RMSE. The left of <ref type="figure">Fig. 8</ref> further demonstrates that RigNet has better performance than GuideNet <ref type="bibr" target="#b46">[47]</ref> and ACMNet <ref type="bibr" target="#b58">[59]</ref> in complex environments. These results verify that our method is able to handle polytropic lighting and weather conditions.</p><p>In summary, all above-mentioned evidences demonstrate that the proposed approach has robust generalization capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we explored the repetitive design in our image guided network for depth completion task. We pointed out that there were two issues impeding the performance of existing outstanding methods, i.e., the blurry guidance in image and unclear structure in depth. To tackle the former issue, in our image guidance branch, we presented a repetitive hourglass network to produce discriminative image features. To alleviate the latter issue, in our depth generation branch, we designed a repetitive guidance module to gradually predict structure-detailed depth maps. Meanwhile, to model high-frequency components and reduce GPU memory consumption of the module, we proposed an efficient guidance algorithm. Furthermore, we designed an adaptive fusion mechanism to automatically fuse multi-stage depth features for better predictions. Extensive experiments show that our method achieves outstanding performances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Our repetitive guidance (RG) implemented by an efficient guidance algorithm (EG) and an adaptive fusion mechanism (AF). k refers to the repetitive number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results on NYUv2 test set. From left to right: (a) color image, (b) sparse depth, (c) NLSPN<ref type="bibr" target="#b35">[36]</ref>, (d) ACMNet<ref type="bibr" target="#b58">[59]</ref>, (e) CSPN<ref type="bibr" target="#b4">[5]</ref>, (f) our RigNet, and (g) ground truth. We present the results of these four methods under 500 samples. The circled rectangle areas show the recovery of object details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>59 63 65 71 84 72 81 91 89 107 104 60 61 64 65 Model size (M) 224 239 246 273 317 274 309 344 339 407 398 228 232 242 246 RMSE (mm) 822 779 780 778 777 802 816 811 807 801 800 803 798 772 769</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Visual comparisons of intermediate features of the baseline and our repetition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Comparisons under different levels of sparsity on KITTI validation split. The solid lines refer to our method while the dotted ones represent other approaches. Comparisons with existing methods (left) and itself (right) replacing 'RG' with '+', under different lighting and weather conditions on Virtual KITTI test split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Theoretical analysis on GPU memory consumption.</figDesc><table><row><cell>Method</cell><cell>DC</cell><cell>CF</cell><cell>EG</cell></row><row><cell cols="4">Memory (GB) 42.75 0.334 0.037</cell></row><row><cell cols="2">Times (-/EG) 1155</cell><cell>9</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Numerical analysis on GPU memory consumption.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Quantitative comparisons on KITTI depth completion benchmark. Method RMSE REL ?1.25 ? 1.25 2 ? 1.25 3</figDesc><table><row><cell></cell><cell>m</cell><cell>m</cell><cell></cell><cell></cell></row><row><cell cols="6">Bilateral [44] 0.479 0.084 92.4 97.6 98.9</cell></row><row><cell>Zhang [56]</cell><cell cols="5">0.228 0.042 97.1 99.3 99.7</cell></row><row><cell>S2D 18 [34]</cell><cell cols="5">0.230 0.044 97.1 99.4 99.8</cell></row><row><cell>DCoeff [20]</cell><cell cols="4">0.118 0.013 99.4 99.9</cell><cell>-</cell></row><row><cell>CSPN [5]</cell><cell cols="5">0.117 0.016 99.2 99.9 100.0</cell></row><row><cell cols="2">CSPN++ [4] 0.116</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="6">DLiDAR [38] 0.115 0.022 99.3 99.9 100.0</cell></row><row><cell cols="6">Xu et al. [51] 0.112 0.018 99.5 99.9 100.0</cell></row><row><cell cols="6">FCFRNet [29] 0.106 0.015 99.5 99.9 100.0</cell></row><row><cell cols="6">ACMNet [59] 0.105 0.015 99.4 99.9 100.0</cell></row><row><cell>PRNet [25]</cell><cell cols="5">0.104 0.014 99.4 99.9 100.0</cell></row><row><cell cols="6">GuideNet [47] 0.101 0.015 99.5 99.9 100.0</cell></row><row><cell>TWISE [19]</cell><cell cols="5">0.097 0.013 99.6 99.9 100.0</cell></row><row><cell>NLSPN [36]</cell><cell cols="5">0.092 0.012 99.6 99.9 100.0</cell></row><row><cell cols="6">RigNet (ours) 0.090 0.013 99.6 99.9 100.0</cell></row><row><cell cols="6">Table 4. Quantitative comparisons on</cell></row><row><cell cols="2">NYUv2 dataset.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>, since our repetition in depth (d) can continuously model high-frequency components, the intermediate depth feature possesses more detailed boundaries and the corresponding image guidance branch consistently has a high response nearby the regions. These facts forcefully demonstrate the effectiveness of our repetitive guidance design.</figDesc><table><row><cell cols="2">Method RHN3</cell><cell cols="4">RG G1 EG1 EG2 EG3 add concat ours (GB) (mm) AF Memory RMSE</cell></row><row><cell>baseline</cell><cell></cell><cell>?</cell><cell></cell><cell>?0</cell><cell>778.6</cell></row><row><cell>(a)</cell><cell cols="2">? ?</cell><cell></cell><cell cols="2">+1.35 769.0</cell></row><row><cell>(b)</cell><cell>?</cell><cell>?</cell><cell></cell><cell cols="2">-10.60 768.6</cell></row><row><cell>(c)</cell><cell>?</cell><cell>? ?</cell><cell></cell><cell cols="2">+2.65 762.3</cell></row><row><cell>(d)</cell><cell>?</cell><cell>? ? ?</cell><cell></cell><cell cols="2">+13.22 757.4</cell></row><row><cell>(e)</cell><cell>?</cell><cell>? ? ? ?</cell><cell></cell><cell cols="2">+13.22 755.8</cell></row><row><cell>(f)</cell><cell>?</cell><cell>? ? ?</cell><cell>?</cell><cell cols="2">+13.22 754.6</cell></row><row><cell>(g)</cell><cell>?</cell><cell>? ? ?</cell><cell></cell><cell cols="2">? +13.28 752.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgement</head><p>The authors would like to thank reviewers for their detailed comments and instructive suggestions. This work was supported by the National Science Fund of China under Grant Nos. U1713208, 62072242 and Postdoctoral Innovative Talent Support Program of China under Grant BX20200168, 2020M681608. Note that the PCA Lab is associated with, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, Nanjing University of Science and Technology.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Depth perception in virtual reality: distance estimations in peri-and extrapersonal space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Armbr?ster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kuhlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Spijkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fimm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cyberpsychology &amp; Behavior</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning joint 2d-3d representations for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10023" to="10032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cspn++: Learning context and resource aware convolutional spatial propagation networks for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning depth with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep convolutional compressed sensing for lidar depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-time dense mapping for self-driving vehicles using fisheye cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA. pp</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Tablet versus phone: Depth perception in handheld augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jarvis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sandor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Reitmayr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Carla: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">CoRL. pp</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Confidence propagation through cnns for guided sparse depth regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Visualechoes: Spatial image representation learning through echolocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schissler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d visual perception for self-driving cars using a multi-camera system: Calibration, mapping, localization, and obstacle detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Furgale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Penet: Towards precise and efficient image guided depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>In: ICRA (2021) 1, 2, 3, 6</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Indoor depth completion with boundary consistency and self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Depth completion with twin surface extrapolation at occlusion boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Depth coefficients for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sparse and dense data with cnns: Depth completion and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nashashibi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">8</biblScope>
		</imprint>
		<respStmt>
			<orgName>Computer Ence</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">In defense of classical image processing: Fast depth completion on the cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CRV. pp</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Depth completion using plane-residual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">U</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A multi-scale guided cascade hourglass network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>WACV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fcfr-net: Feature fusion based coarse-to-fine residual learning for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI. vol</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>2021) 1, 2, 3, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cbnet: A novel composite backbone network architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">From depth what can you see? depth completion via auxiliary image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Self-supervised sparse-to-dense: Selfsupervised depth completion from lidar and monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Cavalheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>In: ICRA (2019) 1, 2, 3, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beyond image to depth: Improving depth prediction using echoes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Parida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Non-local spatial propagation network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>In: ECCV (2020) 1, 2, 3, 5, 6, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Bayesian deep basis fitting for depth completion with uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Panoformer: Panorama transformer for indoor 360 depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Distortion-tolerant monocular depth estimation on omnidirectional images using dual-cubemap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Channel attention based iterative residual learning for depth map super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning guided convolutional network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<title level="m">Sparsity invariant cnns</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sparse and noisy lidar completion with rgb guidance and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MVA. pp</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Regularizing nighttime weirdness: Efficient self-supervised monocular depth estimation in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Depth completion from sparse lidar data with depth-normal constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deformable spatial propagation networks for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP. pp</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2020" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dense depth posterior (ddp) from single image and sparse range</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep depth completion of a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Adaptive context-aware multi-modal network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07895</idno>
		<title level="m">Robust depth completion with uncertainty-driven loss functions</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
