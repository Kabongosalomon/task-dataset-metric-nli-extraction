<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Speech Synthesis with Transformer Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naihan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">CETC Big Data Research Institute Co</orgName>
								<address>
									<settlement>Ltd, Guizhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
							<email>csmliu@uestc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqing</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft STC Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
							<email>szhao@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft STC Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">CETC Big Data Research Institute Co</orgName>
								<address>
									<settlement>Ltd, Guizhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<email>mingzhou@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Speech Synthesis with Transformer Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2) are proposed and achieve state-of-theart performance, they still suffer from two problems: 1) low efficiency during training and inference; 2) hard to model long dependency using current recurrent neural networks (RNNs). Inspired by the success of Transformer network in neural machine translation (NMT), in this paper, we introduce and adapt the multi-head attention mechanism to replace the RNN structures and also the original attention mechanism in Tacotron2. With the help of multi-head self-attention, the hidden states in the encoder and decoder are constructed in parallel, which improves training efficiency. Meanwhile, any two inputs at different times are connected directly by a self-attention mechanism, which solves the long range dependency problem effectively. Using phoneme sequences as input, our Transformer TTS network generates mel spectrograms, followed by a WaveNet vocoder to output the final audio results. Experiments are conducted to test the efficiency and performance of our new network. For the efficiency, our Transformer TTS network can speed up the training about 4.25 times faster compared with Tacotron2. For the performance, rigorous human tests show that our proposed model achieves state-of-the-art performance (outperforms Tacotron2 with a gap of 0.048) and is very close to human quality (4.39 vs 4.44 in MOS).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text to speech (TTS) is a very important task for user interaction, aiming to synthesize intelligible and natural audios which are indistinguishable from human recordings. Traditional TTS systems have two components: front-end and back-end. Front-end is responsible for text analysis and linguistic feature extraction, such as word segmentation, part of speech tagging, multi-word disambiguation and prosodic structure prediction; back-end is built for speech synthesis based on linguistic features from front-end, such as speech acoustic parameter modeling, prosody modeling and speech generation. In the past decades, concatenative and parametric speech synthesis systems were mainstream techniques. However, both of them have complex pipelines, and defining good linguistic features is often time-consuming and lan-guage specific, which requires a lot of resource and manpower. Besides, synthesized audios often have glitches or instability in prosody and pronunciation compared to human speech, and thus sound unnatural.</p><p>Recently, with the rapid development of neural networks, end-to-end generative text-to-speech models, such as Tacotron  and <ref type="bibr">Tacotron2 (Shen et al. 2017)</ref>, are proposed to simplify traditional speech synthesis pipeline by replacing the production of these linguistic and acoustic features with a single neural network. Tacotron and Tacotron2 first generate mel spectrograms directly from texts, then synthesize the audio results by a vocoder such as Griffin Lim algorithm <ref type="bibr" target="#b3">(Griffin and Lim 1984)</ref> or WaveNet <ref type="bibr" target="#b13">(Van Den Oord et al. 2016)</ref>. With the end-to-end neural network, quality of synthesized audios is greatly improved and even comparable with human recordings on some datasets. The end-to-end neural TTS models contain two components, an encoder and a decoder. Given the input sequence (of words or phonemes), the encoder tries to map them into a semantic space and generates a sequence of encoder hidden states, and the decoder, taking these hidden states as context information with an attention mechanism, constructs the decoder hidden states then outputs the mel frames. For both encoder and decoder, recurrent neural networks (RNNs) are usually leveraged, such as LSTM <ref type="bibr" target="#b4">(Hochreiter and Schmidhuber 1997)</ref> and <ref type="bibr">GRU (Cho et al. 2014)</ref>.</p><p>However, RNNs can only consume the input and generate the output sequentially, since the previous hidden state and the current input are both required to build the current hidden state. The characteristic of sequential process limits the parallelization capability in both the training and inference process. For the same reason, for a certain frame, information from many steps ahead may has been biased after multiple recurrent processing. To deal with these two problems, Transformer <ref type="bibr" target="#b13">(Vaswani et al. 2017</ref>) is proposed to replace the RNNs in NMT models.</p><p>Inspired by this idea, in this paper, we combine the advantages of Tacotron2 and Transformer to propose a novel end-to-end TTS model, in which the multi-head attention mechanism is introduced to replace the RNN structures in the encoder and decoder, as well as the vanilla attention network. The self-attention mechanism unties the sequential dependency on the last previous hidden state to improve the parallelization capability and relieve the long dis-tance dependency problem. Compared with the vanilla attention between the encoder and decoder, the multi-head attention can build the context vector from different aspects using different attention heads. With the phoneme sequences as input, our novel Transformer TTS network generates mel spectrograms, and employs WaveNet as vocoder to synthesize audios. We conduct experiments with 25-hour professional speech dataset, and the audio quality is evaluated by human testers. Evaluation results show that our proposed model outperforms the original Tacotron2 with a gap of 0.048 in CMOS, and achieves a similar performance (4.39 in MOS) with human recording (4.44 in MOS). Besides, our Transformer TTS model can speed up the training process about 4.25 times compared with Tacotron2. Audio samples can be accessed on https://neuraltts. github.io/transformertts/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we first introduce the sequence-to-sequence model, followed by a brief description about Tacotron2 and Transformer, which are two preliminaries in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sequence to Sequence Model</head><p>A sequence-to-sequence model <ref type="bibr" target="#b10">(Sutskever, Vinyals, and Le 2014;</ref><ref type="bibr" target="#b0">Bahdanau, Cho, and Bengio 2014)</ref> converts an input sequence (x 1 , x 2 , ..., x T ) into an output sequence (y 1 , y 2 , ..., y T ), and each predicted y t is conditioned on all previously predicted outputs y 1 , ..., y t?1 . In most cases, these two sequences are of different lengths (T = T ). In NMT, this conversion translates the input sentence in one language into the output sentence in another language, based on a conditional probability p(y 1 , ..., y T |x 1 , ..., x T ):</p><formula xml:id="formula_0">h t = encoder(h t?1 , x t ) (1) s t = decoder(s t?1 , y t?1 , c t )<label>(2)</label></formula><p>where c t is the context vector calculated by an attention mechanism:</p><formula xml:id="formula_1">c t = attention(s t?1 , h)<label>(3)</label></formula><p>thus p(y 1 , ..., y T |x 1 , ..., x T ) can be computed by</p><formula xml:id="formula_2">p(y 1 , ..., y T |x 1 , ..., x T ) = T t=1 p(y t |y &lt;t , x)<label>(4)</label></formula><p>and</p><formula xml:id="formula_3">p(y t |y &lt;t , x) = sof tmax(f (s t ))<label>(5)</label></formula><p>where f (?) is a fully connected layer. For translation tasks, this softmax function is among all dimensions of f (s t ) and calculates the probability of each word in the vocabulary. However, in the TTS task, the softmax function is not required and the hidden states s calculated by decoder are consumed directly by a linear projection to obtain the desired spectrogram frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tacotron2</head><p>Tacotron2 is a neural network architecture for speech synthesis directly from text, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref> . The embedding sequence of input is firstly processed with a 3-layer CNN to extract a longer-term context, and then fed into the encoder, which is a bi-directional LSTM. The previous mel spectrogram frame (the predicted one in inference, or the golden one in training time), is first processed with a 2-layer fully connected network (decoder pre-net), whose output is concatenated with the previous context vector, followed by a 2layer LSTM. The output is used to calculate the new context vector at this time step, which is concatenated with the output of the 2-layer LSTM to predict the mel spectrogram and stop token with two different linear projections respectively. Finally the predicted mel spectrogram is fed into a 5-layer CNN with residual connections to refine the mel spectrogram.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Transformer for NMT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural TTS with Transformer</head><p>Compared to RNN-based models, using Transformer in neural TTS has two advantages. First it enables parallel training by removing recurrent connections, as frames of an input sequence for decoder can be provided in parallel. The second one is that self attention provides an opportunity for injecting global context of the whole sequence into each input frame, building long range dependencies directly. Transformer shortens the length of paths forward and backward signals have to traverse between any combination of positions in the input and output sequences down to 1. This helps a lot in a neural TTS model, such as the prosody of synthesized waves, which not only depends on several words in the neighborhood, but also sentence level semantics. In this section we will introduce the architecture of our Transformer TTS model, and analyze the function of each part. The overall structure diagram is shown in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text-to-Phoneme Converter</head><p>English pronunciation has certain regularities, for example, there are two kinds of syllables in English: open and closed. The letter "a" is often pronounced as /e?/ when it's in an open syllable, while it is pronounced as /ae/ or /a:/ in closed syllables. We can rely on the neural network to learn such a regularity in the training process. However, it is difficult to learn all the regularities when, which is often the case, the training data is not sufficient enough, and some exceptions have too few occurrences for neural networks to learn. So we make a rule system and implement it as a text-to-phoneme converter, which can cover the vast majority of cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scaled Positional Encoding</head><p>Transformer contains no recurrence and no convolution so that if we shuffle the input sequence of encoder or decoder, we will get the same output. To take the order of the sequence into consideration, information about the relative or absolute position of frames is injected by triangle positional embeddings, shown in Eq. 7:</p><formula xml:id="formula_4">P E(pos, 2i) = sin( pos 10000 2i d model ) (6) P E(pos, 2i + 1) = cos( pos 10000 2i d model )<label>(7)</label></formula><p>where pos is the time step index, 2i and 2i + 1 is the channel index and d model is the vector dimension of each frame. In NMT, the embeddings for both source and target language are from language spaces, so the scales of these embeddings are similar. This condition doesn't hold in the TTS scenarioe, since the source domain is of texts while the target domain is of mel spectrograms, hence using fixed positional embeddings may impose heavy constraints on both the encoder and decoder pre-nets (which will be described in Sec. 3.3 and 3.4). We employ these triangle positional embeddings with a trainable weight, so that these embedding can adaptively fit the scales of both encoder and decoder prenets' output, as shown in Eq. 8:</p><formula xml:id="formula_5">x i = prenet(phoneme i ) + ?P E(i)<label>(8)</label></formula><p>where ? is the trainable weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Encoder Pre-net</head><p>In Tacotron2, a 3-layer CNN is applied to the input text embeddings, which can model the longer-term context in the input character sequence. In our Transformer TTS model, we input the phoneme sequence into the same network, which is called "encoder pre-net". Each phoneme has a trainable embedding of 512 dims, and the output of each convolution layer has 512 channels, followed by a batch normalization and ReLU activation, and a dropout layer as well. In addition, we add a linear projection after the final ReLU activation, since the output range of ReLU is [0, +?), while each dimension of these triangle positional embeddings is in</p><formula xml:id="formula_6">[?1, 1].</formula><p>Adding 0-centered positional information onto nonnegative embeddings will result in a fluctuation not centered on the origin and harm model performance, which will be demonstrated in our experiment. Hence we add a linear projection for center consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Decoder Pre-net</head><p>The mel spectrogram is first consumed by a neural network composed of two fully connected layers(each has 256 hidden units) with ReLU activation, named "decoder pre-net", and it plays an important role in the TTS system. Phonemes has trainable embeddings thus their subspace is adaptive, <ref type="figure">Figure 3</ref>: System architecture of our model.</p><p>while that of mel spectrograms is fixed. We infer that decoder pre-net is responsible for projecting mel spectrograms into the same subspace as phoneme embeddings, so that the similarity of a phoneme, mel f rame pair can be measured, thus the attention mechanism can work. Besides, 2 fully connected layers without non-linear activation are also tried but no reasonable attention matrix aligning the hidden states of encoder and decoder can be generated. In our other experiment, hidden size is enlarged from 256 to 512, however that doesn't generate significant improvement but needs more steps to converge. Accordingly, we conjecture that mel spectrograms have a compact and low dimensional subspace that 256 hidden units are good enough to fit. This conjecture can also be evidenced in our experiment, which is shown in Sec. 4.6, that the final positional embedding scale of decoder is smaller than that of encoder. An additional linear projection is also added like encoder pre-net not only for center consistency but also obtain the same dimension as the triangle positional embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Encoder</head><p>In Tacotron2, the encoder is a bi-directional RNN. We replace it with Transformer encoder which is described in Sec. 2.3 . Comparing to original bi-directional RNN, multi-head attention splits one attention into several subspaces so that it can model the frame relationship in multiple different aspects, and it directly builds the long-time dependency between any two frames thus each of them considers global context of the whole sequence. This is crucial for synthesized audio prosody especially when the sentence is long, as generated samples sound more smooth and natural in our experiments. In addition, employing multi-head attention instead of original bi-directional RNN can enable parallel computing to improve training speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Decoder</head><p>In Tacotron2, the decoder is a 2-layer RNN with locationsensitive attention <ref type="bibr" target="#b1">(Chorowski et al. 2015)</ref>. We replace it with Transformer decoder which is described in Sec. 2.3. Employing Transformer decoder makes two main differences, adding self-attention, which can bring similar advantages described in Sec. 3.5, and using multi-head attention instead of the location-sensitive attention. The multi-head attention can integrate the encoder hidden states in multiple perspectives and generate better context vectors. Taking attention matrix of previous decoder time steps into consideration, location-sensitive attention used in Tacotron2 can encourage the model to generate consistent attention results. We try to modify the dot product based multi-head attention to be location sensitive, but that doubles the training time and easily run out of memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Mel Linear, Stop Linear and Post-net</head><p>Same as Tacotron2, we use two different linear projections to predict the mel spectrogram and the stop token respectively, and use a 5-layer CNN to produce a residual to refine the reconstruction of mel spectrogram. It's worth mentioning that, for the stop linear, there is only one positive sample in the end of each sequence which means "stop", while hundreds of negative samples for other frames. This imbalance may result in unstoppable inference. We impose a positive weight (5.0 ? 8.0) on the tail positive stop token when calculating binary cross entropy loss, and this problem was efficiently solved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we conduct experiments to test our proposed Transformer TTS model with 25-hour professional speech pairs, and the audio quality is evaluated by human testers in MOS and CMOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Setup</head><p>We use 4 Nvidia Tesla P100 to train our model with an internal US English female dataset, which contains 25-hour professional speech (17584 text, wave pairs, with a few too long waves removed). 50ms silence at head and 100ms silence at tail are kept for each wave. Since the lengths of training samples vary greatly, fixed batch size will either run out of memory when long samples are added into a batch with a large size or waste the parallel computing power if the batch is small and into which short samples are divided. Therefore, we use the dynamic batch size where the maximum total number of mel spectrogram frames is fixed and one batch should contain as many samples as possible. Thus there are on average 16 samples in single batch per GPU. We try training on a single GPU, but the procedures are quiet instable or even failed, by which synthesized audios were like raving and incomprehensible. Even if training doesn't fail, synthesized waves are of bad quality and weird prosody, or even have some severe problems like missing phonemes. Thus we enable multi-GPU training to enlarge the batch size, which effectively solves those problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Text-to-Phoneme Conversion and Pre-process</head><p>Tacotron2 uses character sequences as input, while our model is trained on pre-normalized phoneme sequences. Word and syllable boundaries, punctuations are also included as special markers. The process pipeline to get training phoneme sequences contains sentence separation, text normalization, word segmentation and finally obtaining pronunciation. By text-to-phoneme conversion, mispronunciation problems are greatly reduced especially for those pronunciations that are rarely occurred in our training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">WaveNet Settings</head><p>We train a WaveNet conditioned on mel spectrogram with the same internal US English female dataset, and use it as the vocoder for all models in this paper. The sample rate of ground truth audios is 16000 and frame rate (frames per second) of ground truth mel spectrogram is 80. Our autoregressive WaveNet contains 2 QRNN layers and 20 dilated layers, and the sizes of all residual channels and dilation channels are all 256. Each frame of QRNN's final output is copied 200 times to have the same spatial resolution as audio samples and be conditions of 20 dilated layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training Time Comparison</head><p>Our model can be trained in parallel since there is no recurrent connection between frames. In our experiment, time consume in a single training step for our model is ?0.4s, which is 4.25 times faster than that of Tacotron2 (?1.7s) with equal batch size (16 samples per batch). However, since the parameter quantity of our model is almost twice than Tacotron2, it still takes ?3 days to converge comparing to ?4.5 days of that for Tacotron2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation</head><p>We  <ref type="formula" target="#formula_0">(2017)</ref>), and each tester listens less than 30 audios.</p><p>We train a Tacotron2 model with our internal US English female dataset as the baseline (also use phonemes as input), and gain equal MOS with our model. Therefore we test the comparison mean option score (CMOS) between samples generated by Tacotron2 and our model for a finer contrast. In the comparison mean option score (CMOS) test, testers listen to two audios (generated by Tacotron2 and our model with the same text) each time and evaluates how the latter feels comparing to the former using a score in [?3, 3] with intervals of 1. The order of the two audios changes randomly so testers don't know their sources. Our model wins by a gap of 0.048, and detailed results are shown in <ref type="table">Table 1</ref>.</p><p>We also select mel spectrograms generated by our model and Tacotron2 respectively with the same text, and com-  pare them together with ground truth, as shown in column 1,2 and 3 of <ref type="figure">Fig. 4</ref>. As we can see, our model does better in reconstructing details as marked in red rectangles, while Tacotron2 left out the detailed texture in high frequency region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Studies</head><p>In this section, we study the detail modification of network architecture, and conduct several experiments to show our improvements.</p><p>Re-centering Pre-net's Output As described in Sec. 3.3 and 3.4, we re-project both the encoder and decoder prenets' outputs for consistent center with positional embeddings. In contrast, we add no linear projection in encoder pre-net and add a fully connected layer with ReLU activation in decoder pre-net. The results imply that center-consistent positional embedding performs slightly better, as shown in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different Positional Encoding Methods</head><p>We inject positional information into both encoder's and decoder's input sequences as Eq. 8. <ref type="figure" target="#fig_2">Fig. 5</ref> shows that the final positional embedding scales of encoder and decoder are different, and <ref type="table" target="#tab_4">Table 3</ref> shows model with trainable scale performs slightly better. We think that the trainable scale relaxes the constraint on encoder and decoder pre-nets, making positional information more adaptive for different embedding spaces. We also try adding absolute position embeddings (each position has a trainable embedding) to the sequence, which also works but has some severe problems such as missing phonemes when the sequences became long. That's because long sample is relatively rare in the training set, so the embeddings for large indexes can hardly be trained and thus the   Ground Truth 4.47 ? 0.05 <ref type="table">Table 5</ref>: Ablation studies in different head numbers.</p><p>position information won't be accurate for rear frames in a long sample.</p><p>Model with Different Hyper-Parameter Both the encoder and decoder of the original Transformer is composed of 6 layers, and each multi-head attention has 8 heads. We compare performance and training speed with different layer and head numbers, as shown in <ref type="table" target="#tab_5">Table 4</ref>, 5 and 6. We find that reducing layers and heads both improve the training speed, but on the other hand, harm model performance in different degrees.</p><p>We notice that in both the 3-layer and 6-layer model, only alignments from certain heads of the beginning 2 layers' are interpretable diagonal lines, which shows the approximate correspondence between input and output sequence, while those of the following layers are disorganized. Even so, more layers can still lower the loss, refine the synthesized mel spectrogram and improve audio quality. The reason is that with residual connection between different layers, our model fits target transformation in a Taylor-expansion way: the starting terms account most as low ordering ones, while the subsequential ones can refine the function. Hence adding more layer makes the synthesized wave more natural, since it does better in processing spectrogram details (shown in column 4, <ref type="figure">Fig. 4</ref>). Fewer heads can slightly reduce training time cost since there are less production per layer, but also harm the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Traditional speech synthesis methods can be categorized into two classes: concatenative systems and parametric systems. Concatenative TTS systems <ref type="bibr" target="#b5">(Hunt and Black 1996;</ref><ref type="bibr" target="#b0">Black and Taylor 1997)</ref>     integrates the front-end and the back-end as one seq2seq <ref type="bibr" target="#b10">(Sutskever, Vinyals, and Le 2014;</ref><ref type="bibr" target="#b0">Bahdanau, Cho, and Bengio 2014)</ref> model and learns the whole process in an end-to-end way, predicting acoustic parameters followed by a SampleRNN <ref type="bibr" target="#b7">(Mehri et al. 2016)</ref> as the vocoder. However, acoustic parameters are still intermediate for audios, thus Char2Wav is not a really end-toend TTS model, and their seq2seq and SampleRNN models need to be separately pre-trained, while Tacotron, proposed by Wang et al. <ref type="formula" target="#formula_0">(2017)</ref>, is an end-to-end generative text-tospeech model, which can be trained by text, spectrogram pairs directly from scratch, and synthesizes speech audios with generated spectrograms by <ref type="bibr">Griffin Lim algorithm (Griffin and Lim 1984</ref>  <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2014)</ref> or CNN-based (e.g. ConvS2S <ref type="bibr" target="#b2">(Gehring et al. 2017</ref><ref type="bibr">), ByteNet (Kalchbrenner et al. 2016</ref>)) neural networks. For RNN-based models, both training and inference are sequential for each sample, while CNN-based models enable paral-lel training. Both RNN and CNN based models are difficult to learn dependencies between distant positions since RNNs have to traverse a long path and CNN has to stack many convolutional layers to get a large receptive field, while Transformer solves this using self attention in both its encoder and decoder. The ability of self-attention is also proved in SAGAN , where original GANs without self-attention fail to capture geometric or structural patterns that occur consistently in some classes (for example, dogs are often drawn without clearly defined separate feet). By adding self-attention, these failure cases are greatly reduced. Besides, multi-head attention is proposed to obtain different relations in multi-subspaces. Recently, Transformer has been applied in automatic speech recognition (ASR) <ref type="bibr" target="#b17">(Zhou et al. 2018a;</ref><ref type="bibr" target="#b17">Zhou et al. 2018b</ref>), proving its ability in acoustic modeling other than natural language process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We propose a neural TTS model based on Tacotron2 and Transformer, and make some modification to adapt Transformer to neural TTS task. Our model generates audio samples of which quality is very closed to human recording, and enables parallel training and learning long-distance dependency so that the training is sped up and the audio prosody is much more smooth. We find that batch size is crucial for training stability, and more layers can refine the detail of generated mel spectrograms especially for high frequency regions thus improve model performance.</p><p>Even thought Transformer has enabled parallel training, autoregressive model still suffers from two problems, which are slow inference and exploration bias. Slow inference is due to the dependency of previous frames when infer current frame, so that the inference is sequential, while exploration bias comes from the autoregressive error accumulation. We may solve them both at once by building a nonautoregressive model, which is also our current research in progress.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>System architecture of Tacotron2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>System architecture of Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>PE scale of encoder and decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: MOS comparison of whether re-centering pre-net's</cell></row><row><cell>output.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>MOS comparison of scaled and original PE.</figDesc><table><row><cell>Layer Number</cell><cell>MOS</cell></row><row><cell>3-layer</cell><cell>4.33 ? 0.06</cell></row><row><cell>6-layer</cell><cell>4.41 ?0.05</cell></row><row><cell cols="2">Ground Truth 4.44 ? 0.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies in different layer numbers.</figDesc><table><row><cell>Head Number</cell><cell>MOS</cell></row><row><cell>4-head</cell><cell>4.39 ? 0.05</cell></row><row><cell>8-head</cell><cell>4.44 ?0.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison of time consuming (in second) per training step of different layer and head numbers.</figDesc><table><row><cell>and stitch them by some algorithms such as Viterbi (Viterbi</cell></row><row><cell>1967) followed by signal process methods (Charpentier and</cell></row><row><cell>Stella 1986; Verhelst and Roelands 1993) to generate new</cell></row><row><cell>waves. Parametric TTS systems (Tokuda et al. 2000; Zen,</cell></row><row><cell>Tokuda, and Black 2009; Ze, Senior, and Schuster 2013;</cell></row><row><cell>Tokuda et al. 2013) convert speech waves into spectrograms,</cell></row><row><cell>and acoustic parameters, such as fundamental frequency and</cell></row><row><cell>duration, are used to synthesize new audio results.</cell></row><row><cell>Traditional speech synthesis methods require extensive</cell></row><row><cell>domain expertise and may contain brittle design choices.</cell></row><row><cell>Char2Wav</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>). Based on Tacotron, Tacotron2 (Shen et al. 2017), a unified and entirely neural model, generates mel spectrograms by a Tacotron-style neural network and then synthesizes speech audios by a modified WaveNet (Van Den Oord et al. 2016). WaveNet is an autoregressive generative model for waveform synthesis, composed of stacks of dilated convolutional layers and processes raw audios of very high temporal resolution (e.g., 24,000 sample rate), while suffering from very large time cost in inference. This problem is solved by Parallel WaveNet (Oord et al. 2017), based on the inverse autoregressive flow (IAF) (Kingma et al. 2016) and reaches 1000? real time. Recently, ClariNet (Ping, Peng, and Chen 2018), a fully convolutional text-towave neural architecture, is proposed to enable the fast endto-end training from scratch. Moreover, VoiceLoop (Taigman et al. 2018) is an alternative neural TTS method mimicking a person's voice based on samples captured in-thewild, such as audios of public speeches, and even with an inaccurate automatic transcripts. On the other hand, Transformer (Vaswani et al. 2017) is proposed for neural machine translation (NMT) and achieves state-of-the-art result. Previous NMT models are dominated by RNN-based</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Charpentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
	</analytic>
	<monogr>
		<title level="m">Neural machine translation by jointly learning to align and translate</title>
		<meeting><address><addrLine>Stella</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1986" />
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chorowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gehring</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<title level="m">Convolutional sequence to sequence learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Signal estimation from modified short-time fourier transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="243" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
	<note>Griffin and Lim</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unit selection in a concatenative speech synthesis system using a large speech database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W. ; L</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.10099</idno>
	</analytic>
	<monogr>
		<title level="m">Neural machine translation in linear time</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="373" to="376" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Acoustics, Speech, and Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Samplernn: An unconditional end-to-end neural audio generation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mehri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07837</idno>
		<idno>arXiv:1711.10433</idno>
	</analytic>
	<monogr>
		<title level="m">Parallel wavenet: Fast high-fidelity speech synthesis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Clarinet: Parallel wave generation in end-to-end text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen ;</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07281</idno>
		<idno>arXiv:1712.05884</idno>
	</analytic>
	<monogr>
		<title level="m">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Char2wav: End-to-end speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR 2017 workshop</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinyals</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><forename type="middle">;</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Voiceloop: Voice fitting and synthesis via a phonological loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speech parameter generation algorithms for hmm-based speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tokuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1234" to="1252" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>SSW, 125</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An overlap-add technique based on waveform similarity (wsola) for high quality time-scale modification of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Verhelst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roelands</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Viterbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1967" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="260" to="269" />
		</imprint>
	</monogr>
	<note>Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. Tacotron: A fully end-to-end text-to-speech synthesis model. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Statistical parametric speech synthesis using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schuster ; Ze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="7962" to="7966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Statistical parametric speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tokuda</forename><surname>Black ; Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1039" to="1064" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A comparison of modeling units in sequence-tosequence speech recognition with the transformer on mandarin chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<idno>arXiv:1804.10752</idno>
	</analytic>
	<monogr>
		<title level="m">Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Syllable-based sequence-to-sequence speech recognition with the transformer in mandarin chinese</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
