<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SimCSE: Simple Contrastive Learning of Sentence Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
							<email>tianyug@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
							<email>danqic@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SimCSE: Simple Contrastive Learning of Sentence Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using "entailment" pairs as positives and "contradiction" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to the previous best results. We also show-both theoretically and empirically-that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning universal sentence embeddings is a fundamental problem in natural language processing and has been studied extensively in the literature <ref type="bibr" target="#b10">(Kiros et al., 2015;</ref><ref type="bibr" target="#b5">Hill et al., 2016;</ref><ref type="bibr">Conneau et al., 2017;</ref><ref type="bibr" target="#b13">Logeswaran and Lee, 2018;</ref><ref type="bibr">Cer et al., 2018;</ref><ref type="bibr">Reimers and Gurevych, 2019, inter alia)</ref>.</p><p>In this work, we advance state-of-the-art sentence * The first two authors contributed equally (listed in alphabetical order). This work was done when Xingcheng visited the Princeton NLP group remotely. <ref type="bibr">1</ref> Our code and pre-trained models are publicly available at https://github.com/princeton-nlp/SimCSE. embedding methods and demonstrate that a contrastive objective can be extremely effective when coupled with pre-trained language models such as <ref type="bibr">BERT (Devlin et al., 2019)</ref> or RoBERTa <ref type="bibr" target="#b12">(Liu et al., 2019)</ref>. We present SimCSE, a simple contrastive sentence embedding framework, which can produce superior sentence embeddings, from either unlabeled or labeled data.</p><p>Our unsupervised SimCSE simply predicts the input sentence itself with only dropout <ref type="bibr" target="#b28">(Srivastava et al., 2014)</ref> used as noise <ref type="figure" target="#fig_1">(Figure 1(a)</ref>). In other words, we pass the same sentence to the pre-trained encoder twice: by applying the standard dropout twice, we can obtain two different embeddings as "positive pairs". Then we take other sentences in the same mini-batch as "negatives", and the model predicts the positive one among the negatives. Although it may appear strikingly simple, this approach outperforms training objectives such as predicting next sentences <ref type="bibr" target="#b13">(Logeswaran and Lee, 2018)</ref> and discrete data augmentation (e.g., word deletion and replacement) by a large margin, and even matches previous supervised methods. Through careful analysis, we find that dropout acts as minimal "data augmentation" of hidden representations while removing it leads to a representation collapse.</p><p>Our supervised SimCSE builds upon the recent success of using natural language inference (NLI) datasets for sentence embeddings <ref type="bibr">(Conneau et al., 2017;</ref><ref type="bibr" target="#b26">Reimers and Gurevych, 2019</ref>) and incorporates annotated sentence pairs in contrastive learning <ref type="figure" target="#fig_1">(Figure 1(b)</ref>). Unlike previous work that casts it as a 3-way classification task (entailment, neutral, and contradiction), we leverage the fact that entailment pairs can be naturally used as positive instances. We also find that adding corresponding contradiction pairs as hard negatives further improves performance. This simple use of NLI datasets achieves a substantial improvement compared to prior methods using the same datasets. We also compare to other labeled sentence-pair The pets are sitting on a couch. Different hidden dropout masks in two forward passes There are animals outdoors.</p><p>There is a man.</p><p>The man wears a business suit.</p><p>A kid is skateboarding.</p><p>A kit is inside the house.</p><p>Two dogs are running.</p><p>A man surfing on the sea.</p><p>A kid is on a skateboard.  datasets and find that NLI datasets are especially effective for learning sentence embeddings.</p><p>To better understand the strong performance of SimCSE, we borrow the analysis tool from <ref type="bibr" target="#b33">Wang and Isola (2020)</ref>, which takes alignment between semantically-related positive pairs and uniformity of the whole representation space to measure the quality of learned embeddings. Through empirical analysis, we find that our unsupervised Sim-CSE essentially improves uniformity while avoiding degenerated alignment via dropout noise, thus improving the expressiveness of the representations. The same analysis shows that the NLI training signal can further improve alignment between positive pairs and produce better sentence embeddings. We also draw a connection to the recent findings that pre-trained word embeddings suffer from anisotropy <ref type="bibr">(Ethayarajh, 2019;</ref><ref type="bibr" target="#b11">Li et al., 2020)</ref> and prove that-through a spectrum perspective-the contrastive learning objective "flattens" the singular value distribution of the sentence embedding space, hence improving uniformity.</p><p>We conduct a comprehensive evaluation of Sim-CSE on seven standard semantic textual similarity (STS) tasks <ref type="bibr">(Agirre et al., 2012</ref><ref type="bibr">(Agirre et al., , 2013</ref><ref type="bibr">(Agirre et al., , 2014</ref><ref type="bibr">(Agirre et al., , 2015</ref><ref type="bibr">(Agirre et al., , 2016</ref><ref type="bibr">Cer et al., 2017;</ref><ref type="bibr" target="#b15">Marelli et al., 2014)</ref> and seven transfer tasks <ref type="bibr">(Conneau and Kiela, 2018)</ref>. On the STS tasks, our unsupervised and supervised models achieve a 76.3% and 81.6% averaged Spearman's correlation respectively using BERT base , a 4.2% and 2.2% improvement compared to previous best results. We also achieve competitive performance on the transfer tasks. Finally, we identify an incoherent evaluation issue in the literature and consolidate the results of different settings for future work in evaluation of sentence embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Contrastive Learning</head><p>Contrastive learning aims to learn effective representation by pulling semantically close neighbors together and pushing apart non-neighbors <ref type="bibr" target="#b3">(Hadsell et al., 2006)</ref>. It assumes a set of paired examples</p><formula xml:id="formula_0">D = {(x i , x + i )} m i=1</formula><p>, where x i and x + i are semantically related. We follow the contrastive framework in Chen et al. (2020) and take a cross-entropy objective with in-batch negatives <ref type="bibr">(Chen et al., 2017;</ref><ref type="bibr">Henderson et al., 2017)</ref>: let h i and h + i denote the representations of x i and x + i , the training objective for (x i , x + i ) with a mini-batch of N pairs is:</p><formula xml:id="formula_1">i = ? log e sim(h i ,h + i )/? N j=1 e sim(h i ,h + j )/? ,<label>(1)</label></formula><p>where ? is a temperature hyperparameter and sim(h 1 , h 2 ) is the cosine similarity</p><formula xml:id="formula_2">h 1 h 2 h 1 ? h 2 .</formula><p>In this work, we encode input sentences using a pre-trained language model such as BERT <ref type="bibr">(Devlin et al., 2019)</ref> or RoBERTa <ref type="bibr" target="#b12">(Liu et al., 2019)</ref>: h = f ? (x), and then fine-tune all the parameters using the contrastive learning objective (Eq. 1).</p><p>Positive instances. One critical question in contrastive learning is how to construct (x i , x + i ) pairs. In visual representations, an effective solution is to take two random transformations of the same image (e.g., cropping, flipping, distortion and rotation) as <ref type="bibr">Dosovitskiy et al., 2014)</ref>. A similar approach has been recently adopted in language representations <ref type="bibr" target="#b16">Meng et al., 2021)</ref> by applying augmentation techniques such as word deletion, reordering, and substitution. However, data augmentation in NLP is inherently difficult because of its discrete nature. As we will see in ?3, simply using standard dropout on intermediate representations outperforms these discrete operators.</p><formula xml:id="formula_3">x i and x + i (</formula><p>In NLP, a similar contrastive learning objective has been explored in different contexts <ref type="bibr">(Henderson et al., 2017;</ref><ref type="bibr" target="#b1">Gillick et al., 2019;</ref><ref type="bibr" target="#b8">Karpukhin et al., 2020)</ref>. In these cases, (x i , x + i ) are collected from supervised datasets such as question-passage pairs. Because of the distinct nature of x i and x + i , these approaches always use a dual-encoder framework, i.e., using two independent encoders f ? 1 and f ? 2 for x i and x + i . For sentence embeddings, Logeswaran and Lee (2018) also use contrastive learning with a dual-encoder approach, by forming current sentence and next sentence as (x i , x + i ). Alignment and uniformity. Recently, <ref type="bibr" target="#b33">Wang and Isola (2020)</ref> identify two key properties related to contrastive learning-alignment and uniformityand propose to use them to measure the quality of representations. Given a distribution of positive pairs p pos , alignment calculates expected distance between embeddings of the paired instances (assuming representations are already normalized):</p><formula xml:id="formula_4">align E (x,x + )?ppos f (x) ? f (x + ) 2 .</formula><p>(2)</p><p>On the other hand, uniformity measures how well the embeddings are uniformly distributed:</p><formula xml:id="formula_5">uniform log E x,y i.i.d. ? p data e ?2 f (x)?f (y) 2 ,<label>(3)</label></formula><p>where p data denotes the data distribution. These two metrics are well aligned with the objective of contrastive learning: positive instances should stay close and embeddings for random instances should scatter on the hypersphere. In the following sections, we will also use the two metrics to justify the inner workings of our approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unsupervised SimCSE</head><p>The idea of unsupervised SimCSE is extremely simple: we take a collection of sentences {x i } m i=1 and use x + i = x i . The key ingredient to get this to work with identical positive pairs is through the use of independently sampled dropout masks for x i and x + i . In standard training of Transformers <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref>, there are dropout masks placed on fully-connected layers as well as attention probabilities (default p = 0.1). We denote h z i = f ? (x i , z) where z is a random mask for dropout. We simply feed the same input to the encoder twice and get  The two columns denote whether we use one encoder or two independent encoders. Next 3 sentences: randomly sample one from the next 3 sentences. Delete one word: delete one word randomly (see <ref type="table" target="#tab_2">Table 1</ref>).</p><p>two embeddings with different dropout masks z, z , and the training objective of SimCSE becomes:</p><formula xml:id="formula_6">i = ? log e sim(h z i i ,h z i i )/? N j=1 e sim(h z i i ,h z j j )/? ,<label>(4)</label></formula><p>for a mini-batch of N sentences. Note that z is just the standard dropout mask in Transformers and we do not add any additional dropout.</p><p>Dropout noise as data augmentation. We view it as a minimal form of data augmentation: the positive pair takes exactly the same sentence, and their embeddings only differ in dropout masks. We compare this approach to other training objectives on the STS-B development set (Cer et al., 2017) 2 .  </p><formula xml:id="formula_7">h = f ? (g(x)</formula><p>, z) and g is a (random) discrete operator on x. We note that even deleting one word would hurt performance and none of the discrete augmentations outperforms dropout noise. We also compare this self-prediction training objective to the next-sentence objective used in Logeswaran and Lee (2018), taking either one encoder or two independent encoders. As shown in <ref type="table" target="#tab_1">Table 2</ref>, we find that SimCSE performs much better than the next-sentence objectives (82.5 vs 67.4 on STS-B) and using one encoder instead of two makes a significant difference in our approach.</p><p>Why does it work? To further understand the role of dropout noise in unsupervised SimCSE, we try out different dropout rates in <ref type="table" target="#tab_3">Table 3</ref> and observe that all the variants underperform the default dropout probability p = 0.1 from Transformers. We find two extreme cases particularly interesting: "no dropout" (p = 0) and "fixed 0.1" (using default dropout p = 0.1 but the same dropout masks for the pair). In both cases, the resulting embeddings for the pair are exactly the same, and it leads to a dramatic performance degradation. We take the checkpoints of these models every 10 steps during training and visualize the alignment and uniformity metrics 3 in <ref type="figure">Figure 2</ref>, along with a simple data augmentation model "delete one word". As clearly shown, starting from pre-trained checkpoints, all models greatly improve uniformity. However, the alignment of the two special variants also degrades drastically, while our unsupervised SimCSE keeps a steady alignment, thanks to the use of dropout noise. It also demonstrates that starting from a pretrained checkpoint is crucial, for it provides good initial alignment. At last, "delete one word" improves the alignment yet achieves a smaller gain on the uniformity metric, and eventually underperforms unsupervised SimCSE.  <ref type="figure">Figure 2</ref>: alignuniform plot for unsupervised SimCSE, "no dropout", "fixed 0.1", and "delete one word". We visualize checkpoints every 10 training steps and the arrows indicate the training direction. For both align and uniform , lower numbers are better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Supervised SimCSE</head><p>We have demonstrated that adding dropout noise is able to keep a good alignment for positive pairs (x, x + ) ? p pos . In this section, we study whether we can leverage supervised datasets to provide better training signals for improving alignment of our approach. Prior work <ref type="bibr">(Conneau et al., 2017;</ref><ref type="bibr" target="#b26">Reimers and Gurevych, 2019)</ref> has demonstrated that supervised natural language inference (NLI) datasets <ref type="bibr">(Bowman et al., 2015;</ref><ref type="bibr" target="#b37">Williams et al., 2018)</ref> are effective for learning sentence embeddings, by predicting whether the relationship between two sentences is entailment, neutral or contradiction. In our contrastive learning framework, we instead directly take (x i , x + i ) pairs from supervised datasets and use them to optimize Eq. 1.</p><p>Choices of labeled data. We first explore which supervised datasets are especially suitable for constructing positive pairs (x i , x + i ). We experiment with a number of datasets with sentence-pair examples, including 1) QQP 4 : Quora question pairs; 2) Flickr30k <ref type="bibr" target="#b41">(Young et al., 2014)</ref>: each image is annotated with 5 human-written captions and we consider any two captions of the same image as a positive pair; 3) ParaNMT <ref type="bibr" target="#b35">(Wieting and Gimpel, 2018)</ref>: a large-scale back-translation paraphrase dataset 5 ; and finally 4) NLI datasets: SNLI <ref type="bibr">(Bowman et al., 2015)</ref> and MNLI <ref type="bibr" target="#b37">(Williams et al., 2018)</ref>.</p><p>We train the contrastive learning model (Eq. 1) with different datasets and compare the results in <ref type="table" target="#tab_5">Table 4</ref>. For a fair comparison, we also run experiments with the same # of training pairs. Among all the options, using entailment pairs from the NLI (SNLI + MNLI) datasets performs the best. We think this is reasonable, as the NLI datasets consist of high-quality and crowd-sourced pairs. Also, human annotators are expected to write the hypotheses manually based on the premises and two sentences tend to have less lexical overlap. For instance, we find that the lexical overlap (F1 measured between two bags of words) for the entailment pairs (SNLI + MNLI) is 39%, while they are 60% and 55% for QQP and ParaNMT.</p><p>Contradiction as hard negatives. Finally, we further take the advantage of the NLI datasets by using its contradiction pairs as hard negatives 6 . In NLI datasets, given one premise, annotators are required to manually write one sentence that is absolutely true (entailment), one that might be true (neutral), and one that is definitely false (contradiction). Therefore, for each premise and its entailment hypothesis, there is an accompanying contradiction hypothesis 7 (see <ref type="figure" target="#fig_1">Figure 1</ref> for an example).</p><p>Formally, we extend (</p><formula xml:id="formula_8">x i , x + i ) to (x i , x + i , x ? i ), where x i is the premise, x +</formula><p>i and x ? i are entailment and contradiction hypotheses. The training objective i is then defined by (N is mini-batch size):</p><formula xml:id="formula_9">? log e sim(h i ,h + i )/? N j=1 e sim(h i ,h + j )/? + e sim(h i ,h ? j )/? .</formula><p>(5) As shown in <ref type="table" target="#tab_5">Table 4</ref>, adding hard negatives can further improve performance (84.9 ? 86.2) and this is our final supervised SimCSE. We also tried to add the ANLI dataset <ref type="bibr" target="#b20">(Nie et al., 2020)</ref> or combine it with our unsupervised SimCSE approach, but didn't find a meaningful improvement. We also considered a dual encoder framework in supervised SimCSE and it hurt performance (86.2 ? 84.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Connection to Anisotropy</head><p>Recent work identifies an anisotropy problem in language representations <ref type="bibr">(Ethayarajh, 2019;</ref><ref type="bibr" target="#b11">Li et al., 2020)</ref>, i.e., the learned embeddings occupy a narrow cone in the vector space, which severely limits their expressiveness. <ref type="bibr">Gao et al. (2019)</ref>  <ref type="bibr">6</ref> We also experimented with adding neutral hypotheses as hard negatives. See Section 6.3 for more discussion. <ref type="bibr">7</ref> In fact, one premise can have multiple contradiction hypotheses. In our implementation, we only sample one as the hard negative and we did not find a difference by using more.  demonstrate that language models trained with tied input/output embeddings lead to anisotropic word embeddings, and this is further observed by Ethayarajh (2019) in pre-trained contextual representations.  show that singular values of the word embedding matrix in a language model decay drastically: except for a few dominating singular values, all others are close to zero. A simple way to alleviate the problem is postprocessing, either to eliminate the dominant principal components <ref type="bibr">(Arora et al., 2017;</ref><ref type="bibr" target="#b19">Mu and Viswanath, 2018)</ref>, or to map embeddings to an isotropic distribution <ref type="bibr" target="#b11">(Li et al., 2020;</ref><ref type="bibr" target="#b29">Su et al., 2021)</ref>. Another common solution is to add regularization during training <ref type="bibr">(Gao et al., 2019;</ref>. In this work, we show that-both theoretically and empirically-the contrastive objective can also alleviate the anisotropy problem.</p><p>The anisotropy problem is naturally connected to uniformity <ref type="bibr" target="#b33">(Wang and Isola, 2020)</ref>, both highlighting that embeddings should be evenly distributed in the space. Intuitively, optimizing the contrastive learning objective can improve uniformity (or ease the anisotropy problem), as the objective pushes negative instances apart. Here, we take a singular spectrum perspective-which is a common practice in analyzing word embeddings <ref type="bibr" target="#b19">(Mu and Viswanath, 2018;</ref><ref type="bibr">Gao et al., 2019;</ref>, and show that the contrastive objective can "flatten" the singular value distribution of sentence embeddings and make the representations more isotropic.</p><p>Following <ref type="bibr" target="#b33">Wang and Isola (2020)</ref>, the asymptotics of the contrastive learning objective (Eq. 1) can be expressed by the following equation when the number of negative instances approaches infinity (assuming f (x) is normalized):</p><formula xml:id="formula_10">? 1 ? E (x,x + )?ppos f (x) f (x + ) + E x?p data log E x ? ?p data e f (x) f (x ? )/? ,<label>(6)</label></formula><p>where the first term keeps positive instances similar and the second pushes negative pairs apart. When</p><formula xml:id="formula_11">p data is uniform over finite samples {x i } m i=1 , with h i = f (x i )</formula><p>, we can derive the following formula from the second term with Jensen's inequality:</p><formula xml:id="formula_12">E x?p data log E x ? ?p data e f (x) f (x ? )/? = 1 m m i=1 log ? ? 1 m m j=1 e h i h j /? ? ? ? 1 ? m 2 m i=1 m j=1 h i h j .<label>(7)</label></formula><p>Let W be the sentence embedding matrix corresponding to {x i } m i=1 , i.e., the i-th row of W is h i . Optimizing the second term in Eq. 6 essentially minimizes an upper bound of the summation of all elements in WW , i.e., Sum(WW ) = m i=1 m j=1 h i h j . Since we normalize h i , all elements on the diagonal of WW are 1 and then tr(WW ) (the sum of all eigenvalues) is a constant. According to <ref type="bibr" target="#b17">Merikoski (1984)</ref>, if all elements in WW are positive, which is the case in most times according to <ref type="figure">Figure G</ref>.1, then Sum(WW ) is an upper bound for the largest eigenvalue of WW . When minimizing the second term in Eq. 6, we reduce the top eigenvalue of WW and inherently "flatten" the singular spectrum of the embedding space. Therefore, contrastive learning is expected to alleviate the representation degeneration problem and improve uniformity of sentence embeddings.</p><p>Compared to post-processing methods in <ref type="bibr" target="#b11">Li et al. (2020)</ref>; <ref type="bibr" target="#b29">Su et al. (2021)</ref>, which only aim to encourage isotropic representations, contrastive learning also optimizes for aligning positive pairs by the first term in Eq. 6, which is the key to the success of SimCSE. A quantitative analysis is given in ?7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation Setup</head><p>We conduct our experiments on 7 semantic textual similarity (STS) tasks. Note that all our STS experiments are fully unsupervised and no STS training sets are used. Even for supervised SimCSE, we simply mean that we take extra labeled datasets for training, following previous work <ref type="bibr">(Conneau et al., 2017)</ref>. We also evaluate 7 transfer learning tasks and provide detailed results in Appendix E. We share a similar sentiment with <ref type="bibr" target="#b26">Reimers and Gurevych (2019)</ref> that the main goal of sentence embeddings is to cluster semantically similar sentences and hence take STS as the main result.</p><p>Semantic textual similarity tasks. We evaluate on 7 STS tasks: <ref type="bibr">STS 2012</ref><ref type="bibr">(Agirre et al., 2012</ref><ref type="bibr" target="#b10">, 2015</ref>, STS Benchmark (Cer et al., 2017) and SICK-Relatedness <ref type="bibr" target="#b15">(Marelli et al., 2014)</ref>. When comparing to previous work, we identify invalid comparison patterns in published papers in the evaluation settings, including (a) whether to use an additional regressor, (b) Spearman's vs Pearson's correlation, and (c) how the results are aggregated <ref type="table" target="#tab_15">(Table B</ref>.1). We discuss the detailed differences in Appendix B and choose to follow the setting of <ref type="bibr" target="#b26">Reimers and Gurevych (2019)</ref> in our evaluation (no additional regressor, Spearman's correlation, and "all" aggregation). We also report our replicated study of previous work as well as our results evaluated in a different setting in <ref type="table" target="#tab_15">Table B</ref>   <ref type="bibr" target="#b11">(Li et al., 2020)</ref> and whitening <ref type="bibr" target="#b29">(Su et al., 2021)</ref>, we only report the "NLI" setting (see <ref type="table" target="#tab_15">Table C</ref>.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Main Results</head><p>We compare unsupervised and supervised Sim-CSE to previous state-of-the-art sentence embedding methods on STS tasks. Unsupervised baselines include average GloVe embeddings (Pennington et al., 2014), average BERT or RoBERTa embeddings 10 , and post-processing methods such as BERT-flow <ref type="bibr" target="#b11">(Li et al., 2020)</ref> and BERTwhitening <ref type="bibr" target="#b29">(Su et al., 2021)</ref>. We also compare to several recent methods using a contrastive objective, including 1) IS-BERT <ref type="bibr" target="#b42">(Zhang et al., 2020)</ref>, which maximizes the agreement between global and local features; 2) DeCLUTR <ref type="bibr" target="#b2">(Giorgi et al., 2021)</ref>, which takes different spans from the same document as positive pairs; 3) CT (Carlsson et al., 2021), which aligns embeddings of the same sentence from two different encoders. 11 Other supervised 10 Following <ref type="bibr" target="#b29">Su et al. (2021)</ref>, we take the average of the first and the last layers, which is better than only taking the last. <ref type="bibr">11</ref> We do not compare to CLEAR , because they use their own version of pre-trained models, and the numbers appear to be much lower. Also note that CT is a concurrent work to ours. methods include InferSent (Conneau et al., 2017), Universal Sentence Encoder (Cer et al., 2018), and SBERT/SRoBERTa <ref type="bibr" target="#b26">(Reimers and Gurevych, 2019)</ref> with post-processing methods (BERT-flow, whitening, and CT). We provide more details of these baselines in Appendix C. <ref type="table" target="#tab_7">Table 5</ref> shows the evaluation results on 7 STS tasks. SimCSE can substantially improve results on all the datasets with or without extra NLI supervision, greatly outperforming the previous stateof-the-art models. Specifically, our unsupervised SimCSE-BERT base improves the previous best averaged Spearman's correlation from 72.05% to 76.25%, even comparable to supervised baselines. When using NLI datasets, SimCSE-BERT base further pushes the state-of-the-art results to 81.57%. The gains are more pronounced on RoBERTa encoders, and our supervised SimCSE achieves 83.76% with RoBERTa large .</p><p>In Appendix E, we show that SimCSE also achieves on par or better transfer task performance compared to existing work, and an auxiliary MLM objective can further boost performance.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation Studies</head><p>We investigate the impact of different pooling methods and hard negatives. All reported results in this section are based on the STS-B development set. We provide more ablation studies (normalization, temperature, and MLM objectives) in Appendix D.</p><p>Pooling methods. Reimers and Gurevych (2019); <ref type="bibr" target="#b11">Li et al. (2020)</ref> show that taking the average embeddings of pre-trained models (especially from both the first and last layers) leads to better performance than [CLS]. Hard negatives. Intuitively, it may be beneficial to differentiate hard negatives (contradiction examples) from other in-batch negatives. Therefore, we extend our training objective defined in Eq. 5 to incorporate weighting of different negatives:</p><formula xml:id="formula_13">? log e sim(h i ,h + i )/? N j=1 e sim(h i ,h + j )/? + ? 1 j i e sim(h i ,h ? j )/? ,<label>(8)</label></formula><p>where 1 j i ? {0, 1} is an indicator that equals 1 if and only if i = j. We train SimCSE with different values of ? and evaluate the trained models on the development set of STS-B. We also consider taking neutral hypotheses as hard negatives. As shown in <ref type="table" target="#tab_10">Table 7</ref>, ? = 1 performs the best, and neutral hypotheses do not bring further gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis</head><p>In this section, we conduct further analyses to understand the inner workings of SimCSE.</p><p>Uniformity and alignment. <ref type="figure" target="#fig_3">Figure 3</ref> shows uniformity and alignment of different sentence embedding models along with their averaged STS results. In general, models which have both better alignment and uniformity achieve better performance, confirming the findings in <ref type="bibr" target="#b33">Wang and Isola (2020)</ref>. We also observe that (1) though pre-trained embeddings have good alignment, their uniformity is poor (i.e., the embeddings are highly anisotropic);</p><p>(2) post-processing methods like BERT-flow and BERT-whitening greatly improve uniformity but also suffer a degeneration in alignment; (3) unsupervised SimCSE effectively improves uniformity of pre-trained embeddings whereas keeping a good alignment; (4) incorporating supervised data in SimCSE further amends alignment. In Appendix F, we further show that SimCSE can effectively flatten singular value distribution of pre-trained embeddings. In Appendix G, we demonstrate that SimCSE provides more distinguishable cosine similarities between different sentence pairs. Qualitative comparison. We conduct a smallscale retrieval experiment using SBERT base and SimCSE-BERT base . We use 150k captions from Flickr30k dataset and take any random sentence as query to retrieve similar sentences (based on cosine similarity). As several examples shown in <ref type="table" target="#tab_12">Table 8</ref>, the retrieved sentences by SimCSE have a higher quality compared to those retrieved by SBERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Early work in sentence embeddings builds upon the distributional hypothesis by predicting surrounding sentences of a given one <ref type="bibr" target="#b10">(Kiros et al., 2015;</ref><ref type="bibr">Hill SBERT base</ref> Supervised SimCSE-BERT base Query: A man riding a small boat in a harbor. #1 A group of men traveling over the ocean in a small boat. A man on a moored blue and white boat. #2 Two men sit on the bow of a colorful boat.</p><p>A man is riding in a boat on the water. #3 A man wearing a life jacket is in a small boat on a lake. A man in a blue boat on the water.</p><p>Query: A dog runs on the green grass near a wooden fence. #1 A dog runs on the green grass near a grove of trees.</p><p>The dog by the fence is running on the grass. #2 A brown and white dog runs through the green grass.</p><p>Dog running through grass in fenced area. #3 The dogs run in the green field.</p><p>A dog runs on the green grass near a grove of trees.   <ref type="table" target="#tab_1">Table 2</ref> <ref type="bibr" target="#b13">Logeswaran and Lee, 2018)</ref>. <ref type="bibr" target="#b21">Pagliardini et al. (2018)</ref> show that simply augmenting the idea of word2vec <ref type="bibr" target="#b18">(Mikolov et al., 2013)</ref> with n-gram embeddings leads to strong results. Several recent (and concurrent) approaches adopt contrastive objectives <ref type="bibr" target="#b42">(Zhang et al., 2020;</ref><ref type="bibr" target="#b2">Giorgi et al., 2021;</ref><ref type="bibr" target="#b16">Meng et al., 2021;</ref><ref type="bibr">Carlsson et al., 2021;</ref><ref type="bibr" target="#b9">Kim et al., 2021;</ref><ref type="bibr" target="#b40">Yan et al., 2021)</ref> by taking different views-from data augmentation or different copies of models-of the same sentence or document. Compared to these work, SimCSE uses the simplest idea by taking different outputs of the same sentence from standard dropout, and performs the best on STS tasks. Supervised sentence embeddings are promised to have stronger performance compared to unsupervised counterparts. <ref type="bibr">Conneau et al. (2017)</ref> propose to fine-tune a Siamese model on NLI datasets, which is further extended to other encoders or pre-trained models <ref type="bibr">(Cer et al., 2018;</ref><ref type="bibr" target="#b26">Reimers and Gurevych, 2019)</ref>. Furthermore, <ref type="bibr" target="#b35">Wieting and Gimpel (2018)</ref>; <ref type="bibr" target="#b36">Wieting et al. (2020)</ref> demonstrate that bilingual and back-translation corpora provide useful supervision for learning semantic similarity. Another line of work focuses on regularizing embeddings <ref type="bibr" target="#b11">(Li et al., 2020;</ref><ref type="bibr" target="#b29">Su et al., 2021;</ref><ref type="bibr" target="#b7">Huang et al., 2021)</ref> to alleviate the representation degeneration problem (as discussed in ?5), and yields substantial improvement over pre-trained language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this work, we propose SimCSE, a simple contrastive learning framework, which greatly improves state-of-the-art sentence embeddings on semantic textual similarity tasks. We present an unsupervised approach which predicts input sentence itself with dropout noise and a supervised approach utilizing NLI datasets. We further justify the inner workings of our approach by analyzing alignment and uniformity of SimCSE along with other baseline models. We believe that our contrastive objective, especially the unsupervised one, may have a broader application in NLP. It provides a new perspective on data augmentation with text input, and can be extended to other continuous representations and integrated in language model pre-training. We find that SimCSE is not sensitive to batch sizes as long as tuning the learning rates accordingly, which contradicts the finding that contrastive learning requires large batch sizes <ref type="bibr" target="#b8">(Chen et al., 2020)</ref>. It is probably due to that all SimCSE models start from pre-trained checkpoints, which already provide us a good set of initial parameters.  For both unsupervised and supervised SimCSE, we take the [CLS] representation with an MLP layer on top of it as the sentence representation. Specially, for unsupervised SimCSE, we discard the MLP layer and only use the [CLS] output during test, since we find that it leads to better performance (ablation study in ?6.3).</p><p>Finally, we introduce one more optional variant which adds a masked language modeling (MLM) objective <ref type="bibr">(Devlin et al., 2019)</ref> as an auxiliary loss to Eq. 1: + ? ? mlm (? is a hyperparameter). This helps SimCSE avoid catastrophic forgetting of token-level knowledge. As we will show in Table D.2, we find that adding this term can help improve performance on transfer tasks (not on sentence-level STS tasks). <ref type="table" target="#tab_15">Table B</ref>.2: Comparisons of our reproduced results using different evaluation protocols and the original numbers. ?: results from <ref type="bibr" target="#b26">Reimers and Gurevych (2019)</ref>; ?: results from <ref type="bibr" target="#b29">Su et al. (2021)</ref>; Other results are reproduced by us. From the table we see that SBERT takes the "all" evaluation and BERT-whitening takes the "wmean" evaluation. and Gurevych (2019). Since the "all" setting fuses data from different topics together, it makes the evaluation closer to real-world scenarios, and unless specified, we take the "all" setting. We list evaluation settings for a number of previous work in <ref type="table" target="#tab_15">Table B</ref>.1. Some of the settings are reported by the paper and some of them are inferred by comparing the results and checking their code. As we can see, the evaluation protocols are very incoherent across different papers. We call for unifying the setting in evaluating sentence embeddings for future research. We will also release our evaluation code for better reproducibility. Since previous work uses different evaluation protocols from ours, we further evaluate our models in these settings to make a direct comparison to the published numbers. We evaluate SimCSE with "wmean" and Spearman's correlation to directly compare to <ref type="bibr" target="#b11">Li et al. (2020)</ref> and <ref type="bibr" target="#b29">Su et al. (2021)</ref> in <ref type="table" target="#tab_15">Table B</ref>.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Baseline Models</head><p>We elaborate on how we obtain different baselines for comparison in our experiments:</p><p>? For average GloVe embedding <ref type="bibr" target="#b24">(Pennington et al., 2014)</ref>, <ref type="bibr">InferSent (Conneau et al., 2017)</ref> and Universal Sentence Encoder <ref type="bibr">(Cer et al., 2018)</ref>, we directly report the results from <ref type="bibr" target="#b26">Reimers and Gurevych (2019)</ref>, since our evaluation setting is the same as theirs.</p><p>? For BERT <ref type="bibr">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b12">(Liu et al., 2019)</ref>, we download the pretrained model weights from HuggingFace's Transformers 13 , and evaluate the models with our own scripts.</p><p>? For SBERT and SRoBERTa <ref type="bibr" target="#b26">(Reimers and Gurevych, 2019)</ref>, we reuse the results from the original paper. For results not reported by <ref type="bibr" target="#b26">Reimers and Gurevych (2019)</ref>, such as the performance of SRoBERTa on transfer tasks, we download the model weights from Sen-tenceTransformers 14 and evaluate them. ? For DeCLUTR <ref type="bibr" target="#b2">(Giorgi et al., 2021)</ref> and contrastive tension (Carlsson et al., 2021), we reevaluate their checkpoints in our setting.</p><p>? For BERT-flow <ref type="bibr" target="#b11">(Li et al., 2020)</ref>, since their original numbers take a different setting, we retrain their models using their code 15 , and evaluate the models using our own script.</p><p>? For BERT-whitening <ref type="bibr" target="#b29">(Su et al., 2021)</ref>, we implemented our own version of whitening script following the same pooling method in <ref type="bibr" target="#b29">Su et al. (2021)</ref>, i.e. first-last average pooling. Our implementation can reproduce the results from the original paper (see <ref type="table" target="#tab_15">Table B</ref>.2).</p><p>For both BERT-flow and BERT-whitening, they have two variants of postprocessing: one takes the NLI data ("NLI") and one directly learns the embedding distribution on the target sets ("target"). We find that in our evaluation setting, "target" is generally worse than "NLI" <ref type="table" target="#tab_15">(Table C</ref>.1), so we only report the NLI variant in the main results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Ablation Studies</head><p>Normalization and temperature. We train Sim-CSE using both dot product and cosine similarity with different temperatures and evaluate them on the STS-B development set. As shown in <ref type="table" target="#tab_15">Table D</ref> MLM auxiliary task. Finally, we study the impact of the MLM auxiliary objective with different ?. As shown in <ref type="table" target="#tab_15">Table D</ref>.2, the token-level MLM objective improves the averaged performance on transfer tasks modestly, yet it brings a consistent drop in semantic textual similarity tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Transfer Tasks</head><p>We evaluate our models on the following transfer tasks: MR <ref type="bibr" target="#b23">(Pang and Lee, 2005)</ref>, CR <ref type="bibr" target="#b6">(Hu and Liu, 2004)</ref>, SUBJ <ref type="bibr" target="#b22">(Pang and Lee, 2004)</ref>, MPQA <ref type="bibr" target="#b34">(Wiebe et al., 2005)</ref>, SST-2 <ref type="bibr" target="#b27">(Socher et al., 2013)</ref>, <ref type="bibr">TREC (Voorhees and Tice, 2000)</ref> and <ref type="bibr">MRPC (Dolan and Brockett, 2005)</ref>. A logistic regression classifier is trained on top of (frozen) sentence embeddings produced by different methods. We follow default configurations from SentEval 16 . <ref type="table" target="#tab_15">Table E</ref>.1 shows the evaluation results on transfer tasks. We find that supervised SimCSE performs on par or better than previous approaches, although the trend of unsupervised models remains unclear. We find that adding this MLM term consistently improves performance on transfer tasks, confirming our intuition that sentence-level objective may not directly benefit transfer tasks. We also experiment with post-processing methods (BERT-   <ref type="formula" target="#formula_1">(2019)</ref>; ?: results from <ref type="bibr" target="#b42">Zhang et al. (2020)</ref>. We highlight the highest numbers among models with the same pre-trained encoder. MLM: adding MLM as an auxiliary task with ? = 0.1. flow/whitening) and find that they both hurt performance compared to their base models, showing that good uniformity of representations does not lead to better embeddings for transfer learning. As we argued earlier, we think that transfer tasks are not a major goal for sentence embeddings, and thus we take the STS results for main comparison. <ref type="figure">Figure F</ref>.1 shows the singular value distribution of SimCSE together with other baselines. For both unsupervised and supervised cases, singular value drops the fastest for vanilla BERT or SBERT embeddings, while SimCSE helps flatten the spectrum distribution. Postprocessing-based methods such as BERT-flow or BERT-whitening flatten the curve even more since they directly aim for the goal of mapping embeddings to an isotropic distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Distribution of Singular Values</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Cosine-similarity Distribution</head><p>To directly show the strengths of our approaches on STS tasks, we illustrate the cosine similarity distributions of STS-B pairs with different groups of human ratings in <ref type="figure">Figure G.</ref>1. Compared to all the baseline models, both unsupervised and supervised SimCSE better distinguish sentence pairs with different levels of similarities, thus leading to a better performance on STS tasks. In addition, we observe that SimCSE generally shows a more scattered distribution than BERT or SBERT, but also preserves a lower variance on semantically similar sentence pairs compared to whitened distribution. This observation further validates that SimCSE can achieve a better alignment-uniformity balance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4-5</head><p>Avg. BERTbase 0-1 1-2 2-3 3-4</p><p>1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4-5</head><p>BERTbase-whitening 0-1 1-2 2-3 3-4</p><p>1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4-5</head><p>Unsupervised SimCSE-BERTbase 0-1 1-2 2-3 3-4</p><p>1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4-5</head><p>SBERTbase 0-1 1-2 2-3 3-4</p><p>1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4-5</head><p>SBERTbase-whitening 0-1 1-2 2-3 3-4</p><p>1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4-5</head><p>Supervised SimCSE-BERTbase <ref type="figure">Figure G</ref>.1: Density plots of cosine similarities between sentence pairs in STS-B. Pairs are divided into 5 groups based on ground truth ratings (higher means more similar) along the y-axis, and x-axis is the cosine similarity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(a) Unsupervised SimCSE predicts the input sentence itself from in-batch negatives, with different hidden dropout masks applied. (b) Supervised SimCSE leverages the NLI datasets and takes the entailment (premisehypothesis) pairs as positives, and contradiction pairs as well as other in-batch instances as negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>alignuniform plot of models based on BERT base . Color of points and numbers in brackets represent average STS performance (Spearman's correlation). Next3Sent: "next 3 sentences" from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>value distributions of sentence embedding matrix from sentences in STS-B. We normalize the singular values so that the largest one is 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>MLM k%: use BERT base to replace k% of words.</figDesc><table><row><cell>Data augmentation</cell><cell></cell><cell></cell><cell>STS-B</cell></row><row><cell>None (unsup. SimCSE)</cell><cell></cell><cell></cell><cell>82.5</cell></row><row><cell>Crop</cell><cell cols="2">10% 20%</cell><cell>30%</cell></row><row><cell></cell><cell cols="2">77.8 71.4</cell><cell>63.6</cell></row><row><cell>Word deletion</cell><cell cols="2">10% 20%</cell><cell>30%</cell></row><row><cell></cell><cell cols="2">75.9 72.2</cell><cell>68.2</cell></row><row><cell>Delete one word</cell><cell></cell><cell></cell><cell>75.9</cell></row><row><cell>w/o dropout</cell><cell></cell><cell></cell><cell>74.2</cell></row><row><cell>Synonym replacement</cell><cell></cell><cell></cell><cell>77.4</cell></row><row><cell>MLM 15%</cell><cell></cell><cell></cell><cell>62.2</cell></row><row><cell cols="4">Table 1: Comparison of data augmentations on STS-B</cell></row><row><cell cols="4">development set (Spearman's correlation). Crop k%:</cell></row><row><cell cols="4">keep 100-k% of the length; word deletion k%: delete</cell></row><row><cell cols="4">k% words; Synonym replacement: use nlpaug (Ma,</cell></row><row><cell cols="4">2019) to randomly replace one word with its synonym;</cell></row><row><cell>Training objective</cell><cell>f ?</cell><cell cols="2">(f ? 1 , f ? 2 )</cell></row><row><cell>Next sentence</cell><cell>67.1</cell><cell></cell><cell>68.9</cell></row><row><cell>Next 3 sentences</cell><cell>67.4</cell><cell></cell><cell>68.8</cell></row><row><cell>Delete one word</cell><cell>75.9</cell><cell></cell><cell>73.1</cell></row><row><cell cols="2">Unsupervised SimCSE 82.5</cell><cell></cell><cell>80.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Comparison of different unsupervised objec-</cell></row><row><cell>tives (STS-B development set, Spearman's correlation).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>compares our approach to common</cell></row><row><cell>data augmentation techniques such as crop, word</cell></row><row><cell>deletion and replacement, which can be viewed as</cell></row><row><cell>2 We randomly sample 10 6 sentences from English</cell></row><row><cell>Wikipedia and fine-tune BERTbase with learning rate = 3e-5,</cell></row><row><cell>N = 64. In all our experiments, no STS training sets are used.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Effects of different dropout probabilities p</cell></row><row><cell>on the STS-B development set (Spearman's correlation,</cell></row><row><cell>BERT base ). Fixed 0.1: default 0.1 dropout rate but ap-</cell></row><row><cell>ply the same dropout mask on both x i and x + i .</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of different supervised datasets as positive pairs. Results are Spearman's correlations on the STS-B development set using BERT base (we use the same hyperparameters as the final SimCSE model). Numbers in brackets denote the # of pairs. Sample: subsampling 134k positive pairs for a fair comparison among datasets; full: using the full dataset. In the last block, we use entailment pairs as positives and contradiction pairs as hard negatives (our final model).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>.2 and Table B.3. We call for unifying the setting in evaluating sentence embeddings for future research.Training details. We start from pre-trained checkpoints of BERT (Devlin et al., 2019) (uncased) or RoBERTa<ref type="bibr" target="#b12">(Liu et al., 2019)</ref> (cased) and take the [CLS] representation as the sentence embedding 9 (see ?6.3 for comparison between different pooling methods). We train unsupervised SimCSE on 10 6 randomly sampled sentences from English Wikipedia, and train supervised SimCSE on the combination of MNLI and SNLI datasets (314k). More training details can be found in Appendix A.</figDesc><table><row><cell>Model</cell><cell cols="7">STS12 STS13 STS14 STS15 STS16 STS-B SICK-R</cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell cols="3">Unsupervised models</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GloVe embeddings (avg.) ?</cell><cell>55.14</cell><cell>70.66</cell><cell>59.73</cell><cell>68.25</cell><cell>63.66</cell><cell>58.02</cell><cell>53.76</cell><cell>61.32</cell></row><row><cell>BERTbase (first-last avg.)</cell><cell>39.70</cell><cell>59.38</cell><cell>49.67</cell><cell>66.03</cell><cell>66.19</cell><cell>53.87</cell><cell>62.06</cell><cell>56.70</cell></row><row><cell>BERTbase-flow</cell><cell>58.40</cell><cell>67.10</cell><cell>60.85</cell><cell>75.16</cell><cell>71.22</cell><cell>68.66</cell><cell>64.47</cell><cell>66.55</cell></row><row><cell>BERTbase-whitening</cell><cell>57.83</cell><cell>66.90</cell><cell>60.90</cell><cell>75.08</cell><cell>71.31</cell><cell>68.24</cell><cell>63.73</cell><cell>66.28</cell></row><row><cell>IS-BERTbase ?</cell><cell>56.77</cell><cell>69.24</cell><cell>61.21</cell><cell>75.23</cell><cell>70.16</cell><cell>69.21</cell><cell>64.25</cell><cell>66.58</cell></row><row><cell>CT-BERTbase</cell><cell>61.63</cell><cell>76.80</cell><cell>68.47</cell><cell>77.50</cell><cell>76.48</cell><cell>74.31</cell><cell>69.19</cell><cell>72.05</cell></row><row><cell>*  SimCSE-BERTbase</cell><cell>68.40</cell><cell>82.41</cell><cell>74.38</cell><cell>80.91</cell><cell>78.56</cell><cell>76.85</cell><cell>72.23</cell><cell>76.25</cell></row><row><cell>RoBERTabase (first-last avg.)</cell><cell>40.88</cell><cell>58.74</cell><cell>49.07</cell><cell>65.63</cell><cell>61.48</cell><cell>58.55</cell><cell>61.63</cell><cell>56.57</cell></row><row><cell>RoBERTabase-whitening</cell><cell>46.99</cell><cell>63.24</cell><cell>57.23</cell><cell>71.36</cell><cell>68.99</cell><cell>61.36</cell><cell>62.91</cell><cell>61.73</cell></row><row><cell>DeCLUTR-RoBERTabase</cell><cell>52.41</cell><cell>75.19</cell><cell>65.52</cell><cell>77.12</cell><cell>78.63</cell><cell>72.41</cell><cell>68.62</cell><cell>69.99</cell></row><row><cell>*  SimCSE-RoBERTabase</cell><cell>70.16</cell><cell>81.77</cell><cell>73.24</cell><cell>81.36</cell><cell>80.65</cell><cell>80.22</cell><cell>68.56</cell><cell>76.57</cell></row><row><cell>*  SimCSE-RoBERTalarge</cell><cell>72.86</cell><cell>83.99</cell><cell>75.62</cell><cell>84.77</cell><cell>81.80</cell><cell>81.98</cell><cell>71.26</cell><cell>78.90</cell></row><row><cell></cell><cell></cell><cell cols="3">Supervised models</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>InferSent-GloVe ?</cell><cell>52.86</cell><cell>66.75</cell><cell>62.15</cell><cell>72.77</cell><cell>66.87</cell><cell>68.03</cell><cell>65.65</cell><cell>65.01</cell></row><row><cell>Universal Sentence Encoder ?</cell><cell>64.49</cell><cell>67.80</cell><cell>64.61</cell><cell>76.83</cell><cell>73.18</cell><cell>74.92</cell><cell>76.69</cell><cell>71.22</cell></row><row><cell>SBERTbase ?</cell><cell>70.97</cell><cell>76.53</cell><cell>73.19</cell><cell>79.09</cell><cell>74.30</cell><cell>77.03</cell><cell>72.91</cell><cell>74.89</cell></row><row><cell>SBERTbase-flow</cell><cell>69.78</cell><cell>77.27</cell><cell>74.35</cell><cell>82.01</cell><cell>77.46</cell><cell>79.12</cell><cell>76.21</cell><cell>76.60</cell></row><row><cell>SBERTbase-whitening</cell><cell>69.65</cell><cell>77.57</cell><cell>74.66</cell><cell>82.27</cell><cell>78.39</cell><cell>79.52</cell><cell>76.91</cell><cell>77.00</cell></row><row><cell>CT-SBERTbase</cell><cell>74.84</cell><cell>83.20</cell><cell>78.07</cell><cell>83.84</cell><cell>77.93</cell><cell>81.46</cell><cell>76.42</cell><cell>79.39</cell></row><row><cell>*  SimCSE-BERTbase</cell><cell>75.30</cell><cell>84.67</cell><cell>80.19</cell><cell>85.40</cell><cell>80.82</cell><cell>84.25</cell><cell>80.39</cell><cell>81.57</cell></row><row><cell>SRoBERTabase ?</cell><cell>71.54</cell><cell>72.49</cell><cell>70.80</cell><cell>78.74</cell><cell>73.69</cell><cell>77.77</cell><cell>74.46</cell><cell>74.21</cell></row><row><cell>SRoBERTabase-whitening</cell><cell>70.46</cell><cell>77.07</cell><cell>74.46</cell><cell>81.64</cell><cell>76.43</cell><cell>79.49</cell><cell>76.65</cell><cell>76.60</cell></row><row><cell>*  SimCSE-RoBERTabase</cell><cell>76.53</cell><cell>85.21</cell><cell>80.95</cell><cell>86.03</cell><cell>82.57</cell><cell>85.83</cell><cell>80.50</cell><cell>82.52</cell></row><row><cell>*  SimCSE-RoBERTalarge</cell><cell>77.46</cell><cell>87.27</cell><cell>82.36</cell><cell>86.66</cell><cell>83.93</cell><cell>86.70</cell><cell>81.95</cell><cell>83.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Sentence embedding performance on STS tasks (Spearman's correlation, "all" setting). We highlight the highest numbers among models with the same pre-trained encoder. ?: results from Reimers and Gurevych (2019); ?: results from Zhang et al. (2020); all other results are reproduced or reevaluated by ourselves. For BERT-flow</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Ablation studies of different pooling methods in unsupervised and supervised SimCSE. [CLS] w/ MLP (train): using MLP on [CLS] during training but removing it during testing. The results are based on the development set of STS-B using BERT base .</figDesc><table><row><cell cols="2">Hard neg N/A</cell><cell cols="3">Contradiction</cell><cell>Contra.+ Neutral</cell></row><row><cell>?</cell><cell>-</cell><cell>0.5</cell><cell>1.0</cell><cell>2.0</cell><cell>1.0</cell></row><row><cell>STS-B</cell><cell cols="4">84.9 86.1 86.2 86.2</cell><cell>85.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>STS-B development results with different hard negative policies. "N/A": no hard negative.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>shows the comparison</cell></row><row><cell>between different pooling methods in both unsuper-</cell></row><row><cell>vised and supervised SimCSE. For [CLS] repre-</cell></row><row><cell>sentation, the original BERT implementation takes</cell></row><row><cell>an extra MLP layer on top of it. Here, we consider</cell></row><row><cell>three different settings for [CLS]: 1) keeping the</cell></row><row><cell>MLP layer; 2) no MLP layer; 3) keeping MLP dur-</cell></row><row><cell>ing training but removing it at testing time. We find</cell></row><row><cell>that for unsupervised SimCSE, taking [CLS] rep-</cell></row><row><cell>resentation with MLP only during training works</cell></row><row><cell>the best; for supervised SimCSE, different pooling</cell></row><row><cell>methods do not matter much. By default, we take</cell></row><row><cell>[CLS]with MLP (train) for unsupervised SimCSE</cell></row></table><note>and [CLS]with MLP for supervised SimCSE.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Retrieved top-3 examples by SBERT and supervised SimCSE from Flickr30k (150k sentences).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table A .</head><label>A</label><figDesc>1: Batch sizes and learning rates for SimCSE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table C</head><label>C</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">.1: Comparison of using NLI or target data for postprocessing methods ("all", Spearman's correlation).</cell></row><row><cell>?</cell><cell>N/A 0.001 0.01 0.05 0.1</cell><cell>1</cell></row><row><cell cols="3">STS-B 85.9 84.9 85.4 86.2 82.0 64.0</cell></row><row><cell cols="3">Table D.1: STS-B development results (Spearman's</cell></row><row><cell cols="3">correlation) with different temperatures. "N/A": Dot</cell></row><row><cell cols="2">product instead of cosine similarity.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>Ablation studies of the MLM objective based on the development sets using BERT base .</figDesc><table><row><cell>Model</cell><cell cols="2">STS-B Avg. transfer</cell></row><row><cell>w/o MLM</cell><cell>86.2</cell><cell>85.8</cell></row><row><cell>w/ MLM</cell><cell></cell><cell></cell></row><row><cell>? = 0.01</cell><cell>85.7</cell><cell>86.1</cell></row><row><cell>? = 0.1</cell><cell>85.7</cell><cell>86.2</cell></row><row><cell>? = 1</cell><cell>85.1</cell><cell>85.8</cell></row><row><cell>Table D.2:</cell><cell></cell><cell></cell></row><row><cell>.1,</cell><cell></cell><cell></cell></row><row><cell>with a carefully tuned temperature ? = 0.05, co-</cell><cell></cell><cell></cell></row><row><cell>sine similarity is better than dot product.</cell><cell></cell><cell></cell></row><row><cell>15 https://github.com/bohanli/BERT-flow</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>Table E.1: Transfer task results of different sentence embedding models (measured as accuracy). ?: results from Reimers and Gurevych</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We take STS-B pairs with a score higher than 4 as ppos and all STS-B sentences as p data .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://www.quora.com/q/quoradata/ 5 ParaNMT is automatically constructed by machine translation systems. Strictly speaking, we should not call it "supervised". It underperforms our unsupervised SimCSE though.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Though our final model only takes entailment pairs as positive instances, here we also try taking neutral and contradiction pairs from the NLI datasets as positive pairs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">There is an MLP layer over [CLS] in BERT's original implementation and we keep it with random initialization.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12"><ref type="bibr" target="#b11">Li et al. (2020)</ref> and<ref type="bibr" target="#b29">Su et al. (2021)</ref> have consistent results, so we assume that they take the same evaluation and just take BERT-whitening in experiments here.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">https://github.com/huggingface/ transformers 14 https://www.sbert.net/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">https://github.com/facebookresearch/ SentEval</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Tao Lei, Jason Lee, Zhengyan Zhang, Jinhyuk Lee, Alexander Wettig, Zexuan Zhong, and the members of the Princeton NLP group for helpful discussion and valuable feedback. This research is supported by a Graduate Fellowship at Princeton University and a gift award from Apple.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Different Settings for STS Evaluation</head><p>We elaborate the differences in STS evaluation settings in previous work in terms of (a) whether to use additional regressors; (b) reported metrics; (c) different ways to aggregate results.</p><p>Additional regressors. The default SentEval implementation applies a linear regressor on top of  <ref type="bibr" target="#b26">Reimers and Gurevych (2019)</ref> Spearman all <ref type="bibr" target="#b42">Zhang et al. (2020)</ref> Spearman all <ref type="bibr" target="#b11">Li et al. (2020)</ref> Spearman wmean <ref type="bibr" target="#b29">Su et al. (2021)</ref> Spearman wmean <ref type="bibr" target="#b36">Wieting et al. (2020)</ref> Pearson mean <ref type="bibr" target="#b2">Giorgi et al. (2021)</ref> Spearman mean Ours Spearman all <ref type="table">Table B</ref>.1: STS evaluation protocols used in different papers. "Reg.": whether an additional regressor is used; "aggr.": methods to aggregate different subset results.</p><p>frozen sentence embeddings for STS-B and SICK-R, and train the regressor on the training sets of the two tasks, while most sentence representation papers take the raw embeddings and evaluate in an unsupervised way. In our experiments, we do not apply any additional regressors and directly take cosine similarities for all STS tasks.</p><p>Metrics. Both Pearson's and Spearman's correlation coefficients are used in the literature. <ref type="bibr" target="#b25">Reimers et al. (2016)</ref> argue that Spearman correlation, which measures the rankings instead of the actual scores, better suits the need of evaluating sentence embeddings. For all of our experiments, we report Spearman's rank correlation.</p><p>Aggregation methods. Given that each year's STS challenge contains several subsets, there are different choices to gather results from them: one way is to concatenate all the topics and report the overall Spearman's correlation (denoted as "all"), and the other is to calculate results for different subsets separately and average them (denoted as "mean" if it is simple average or "wmean" if weighted by the subset sizes). However, most papers do not claim the method they take, making it challenging for a fair comparison. We take some of the most recent work: SBERT <ref type="bibr" target="#b26">(Reimers and Gurevych, 2019)</ref>, BERT-flow <ref type="bibr" target="#b11">(Li et al., 2020)</ref> and BERT-whitening <ref type="bibr">(Su et al., 2021) 12</ref> as an example: In <ref type="table">Table B</ref>.2, we compare our reproduced results to reported results of SBERT and BERT-whitening, and find that <ref type="bibr" target="#b26">Reimers and Gurevych (2019)</ref> take the "all" setting but <ref type="bibr" target="#b11">Li et al. (2020)</ref>; <ref type="bibr" target="#b29">Su et al. (2021)</ref> take the "wmean" setting, even though <ref type="bibr" target="#b11">Li et al. (2020)</ref> claim that they take the same setting as Reimers</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning dense representations for entity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayali</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Lansing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Garcia-Olano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="528" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DeCLUTR: Deep contrastive learning for unsupervised textual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osvald</forename><surname>Nitski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bader</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.72</idno>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="879" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?szl?</forename><surname>Luk?cs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00652</idno>
		<title level="m">Balint Miklos, and Ray Kurzweil. 2017. Efficient natural language response suggestion for smart reply</title>
		<imprint>
			<date>Sanjiv Kumar</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1367" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Whiteningbert: An easy unsupervised sentence embedding approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01767</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.550</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-guided contrastive learning for BERT sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Goo</forename><surname>Kang Min Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.197</idno>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2528" to="2540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the sentence embeddings from pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9119" to="9130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Nlp augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Language Resources and Evaluation (LREC)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08473</idno>
		<title level="m">COCO-LM: Correcting and contrasting text sequences for language model pretraining</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the trace and the sum of elements of a matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Merikoski</forename><surname>Jorma Kaarlo</surname></persName>
		</author>
		<idno type="DOI">10.1016/0024-3795(84)90078-8</idno>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="177" to="185" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">All-but-thetop: Simple and effective postprocessing for word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramod</forename><surname>Viswanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial NLI: A new benchmark for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.441</idno>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4885" to="4901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised learning of sentence embeddings using compositional n-gram features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1049</idno>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="528" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Task-oriented intrinsic evaluation of semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics (COLING)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyiwen</forename><surname>Ou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15316</idno>
		<title level="m">Whitening sentence representations for better semantics and faster retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Building a question answering test collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="200" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving neural language generation with spectrum control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Annotating expressions of opinions and emotions in language. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="165" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1042</idno>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A bilingual generative transformer for semantic sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.122</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1581" to="1594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP): System Demonstrations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Clear: Contrastive learning for sentence representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuofeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15466</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ConSERT: A contrastive framework for self-supervised sentence representation transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanmeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.393</idno>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5065" to="5075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00166</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An unsupervised sentence embedding method by mutual information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Kwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bing</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.124</idno>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1601" to="1610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<title level="m">Model STS12 STS13 STS14 STS15 STS16 STS-B SICK-R Avg</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert-Whitening</surname></persName>
		</author>
		<imprint>
			<pubPlace>NLI, all</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert-Whitening</surname></persName>
		</author>
		<imprint>
			<pubPlace>NLI, wmean</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<title level="m">Model STS12 STS13 STS14 STS15 STS16 STS-B SICK-R Avg</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert-Flow</surname></persName>
		</author>
		<imprint>
			<publisher>NLI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
