<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hard-Aware Deeply Cascaded Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
							<email>yhyuan@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Machine Perception(MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
							<email>kuiyuanyang@deepmotion.ai</email>
							<affiliation key="aff1">
								<orgName type="department">DeepMotion</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
							<email>chzhang@cis.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Machine Perception(MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hard-Aware Deeply Cascaded Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Riding on the waves of deep neural networks, deep metric learning has achieved promising results in various tasks by using triplet network or Siamese network. Though the basic goal of making images from the same category closer than the ones from different categories is intuitive, it is hard to optimize the objective directly due to the quadratic or cubic sample size. Hard example mining is widely used to solve the problem, which spends the expensive computation on a subset of samples that are considered hard. However, hard is defined relative to a specific model. Then complex models will treat most samples as easy ones and vice versa for simple models, both of which are not good for training. It is difficult to define a model with the just right complexity and choose hard examples adequately as different samples are of diverse hard levels. This motivates us to propose the novel framework named Hard-Aware Deeply Cascaded Embedding(HDC) to ensemble a set of models with different complexities in cascaded manner to mine hard examples at multiple levels. A sample is judged by a series of models with increasing complexities and only updates models that consider the sample as a hard case. The HDC is evaluated on CARS196, CUB-200-2011, Stanford Online Products, VehicleID and DeepFashion datasets, and outperforms state-of-the-art methods by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep metric embedding has attracted increasing attention for various tasks , such as visual product search <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref>, face recognition <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b3">4]</ref>, local image descriptor learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24]</ref>, person/vehicle reidentification <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b16">17]</ref>, zero-shot image classification <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b4">5]</ref>, fine-grained image classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref> and object tracking <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33]</ref>. Although deep metric embedding is modified into different forms for various tasks, it shares the same objective to learn an embedding space <ref type="bibr">Figure 1</ref>. Illustration of samples with different hard levels: A query image is shown at the center, while other images from the same category (Nissan 240X Coupe 1998 from CARS196 <ref type="bibr" target="#b10">[11]</ref>) are used to form positive pairs with the query image. that pulls similar images closer and pushes dissimilar images far away. Typically, the target embedding space is learned with a convolutional neural network equipped with contrastive/triplet loss.</p><p>Different from the traditional classification based models, the models of deep metric embedding consider two images (a pair) or three images (a triplet) as a training sample. Thus N images can generate O(N 2 ) or O(N 3 ) samples. It becomes impossible to consider all samples even for a moderate number of images. Fortunately, not all samples are equally informative to train a model, which inspires many recent works to mine hard examples for training <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>However, the hard level of a sample is defined relative to a model. Then samples can be divided into different hard levels as illustrated in <ref type="figure">Figure 1</ref>. For a complex model, most samples will be treated as easy ones, and the model converges fast but is prone to overfitting. While for a simple model, most samples will be treated as hard ones and cannot  <ref type="figure">Figure 2</ref>. Hard-Aware Deeply Cascaded Embedding : We will train the first model with all the pairs, the second model with the semihard samples which are selected by the above model, the third model with the remained hard samples selected by the second model. Our framework support any K cascaded models. We plot the case=3 for convenience. G1, G2, G3, F1, F2, F3 are the computation blocks in Convolutional Neural Networks. fully benefit from hard example mining. It would be ideal to define a model with the just right complexity to mine hard examples adequately, which is an open problem itself.</p><p>To alleviate the above problem, we ensemble a set of models with different complexities in a cascaded manner and mine hard examples adaptively, which is schematically illustrated in <ref type="figure">Figure 2</ref>. The most simple model is implemented by a shallow network, while complex models are implemented by cascading more layers following the simple ones. During the training phase, a sample will be considered by a series of models with increasing depth. Specifically, a sample firstly makes its forward pass through the simple model, the pass will stop if the simple model considers the sample as an easy one, otherwise the forward pass continues until a model considers the sample as an easy one or the deepest model is reached. Then the errors will be back-propagated to models that consider the sample as a hard case. We empirically show that the HDC achieves state-of-the-art results on five benchmarks.</p><p>In summary, we make the following contributions:</p><p>? We propose the Hard-Aware Deeply Cascaded Embedding to solve the under-fitting and over-fitting problem when mining the hard samples during training. To the best of our knowledge, this is the first attempt to investigate and solve this problem. ? We conduct extensive experiments on five various datasets and all achieve state-of-the-art results. The promising results on different datasets demonstrate that the proposed method has good generalization capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep metric learning attracts great attention in recent years, and hard negative mining is becoming a common practice to effectively train deep metric networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Wang et al. <ref type="bibr" target="#b38">[39]</ref> sample triplets during the first 10 training epoches randomly, and mine hard triplets in each minibatch after 10 epoches. Cui et al. <ref type="bibr" target="#b7">[8]</ref> leverage human to label hard negative images from images assigned high confidence scores by the model during each round. Simo-Serra et al. <ref type="bibr" target="#b23">[24]</ref> analyze the influence of both of hard positive mining and hard negative mining, and find that the combination of aggressive mining for both positive and negative pairs improves the discrimination. However, these methods mine the hard images only based on a single model, which cannot adequately leverage samples with different hard levels.</p><p>Our method of ensembling a set of models of different complexities in a cascaded manner shares the same spirit as the acceleration technique used in object/face detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42</ref>]. In the detection task, an image may contain several positive patches and a large number of negative patches. To reduce the computational cost, the model is broken down into a set of cascaded computation blocks, where computation blocks at early stages reject most easy background patches, while computation blocks at latter stages focus more on object-like patches.</p><p>Our method also shares similar form with deeplysupervised network (DSN) proposed for image classification <ref type="bibr" target="#b14">[15]</ref>, of which loss functions are added to the output layers and several middle layers. DSN improves the directness and transparency of the hidden layer learning process and tries to alleviate the "gradient vanishing" problem. Similar idea is adopted in GoogLeNet <ref type="bibr" target="#b30">[31]</ref>. BranchyNet <ref type="bibr" target="#b33">[34]</ref> attempts to speed up image classification by taking advantage of the DSN framework during test phase, where an image will be predicted using features learned at an early layer if high confidence score can be achieved. During the training phase of DSN, all samples are used and intermediate losses are only used to assist the training of the deepest model. While in our framework, samples of different hard levels are assigned to models with adequate complexities, and all models are ensembled together as a whole model. To be noted, ensemble is also a useful technique that has been widely used in model design to boost performance. Hinton et al. <ref type="bibr" target="#b28">[29]</ref> add dropout into fully-connected layers, which implicitly ensembles an exponential number of subnetworks in a single network. He et al. <ref type="bibr" target="#b9">[10]</ref> propose ResNet by adding residual connections into a network and win the ILSVRC 2015 competition, which is latterly proved by Veit et al. <ref type="bibr" target="#b35">[36]</ref> that ResNet is actually exponential ensembles of relatively shallow networks.</p><p>In addition, there are several works focused on designing new loss functions for deep metric embedding recently. Rippel et al. <ref type="bibr" target="#b18">[19]</ref> design a Nearest Class Multiple Centroids (NCMC) like loss which encourages images from the same category to form sub-clusters in the embedding space. Huang et al. <ref type="bibr" target="#b5">[6]</ref> propose position-dependent deep metric to solve the problem that intra-class distance in a high-density region may be larger than the inter-class distance in low-density regions. Ustinova et al. <ref type="bibr" target="#b34">[35]</ref> propose a histogram loss, which aims to make the similarity distributions of positive and negative pairs less overlapping. Unlike other losses used for deep embedding, histogram loss comes with virtually no parameters that need to be tuned. K. Sohn et al. <ref type="bibr" target="#b25">[26]</ref> proposed multi-class N-pair loss by generalizing triplet loss by allowing joint comparison among more than one negative examples. Different from our work, these works improve deep embedding by designing new loss functions within a single model. They can benefit from our method by mining hard examples adaptively using multiple cascaded models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Hard-Aware Deeply Cascaded Embedding</head><p>Hard-Aware Deeply Cascaded embedding(HDC) is based on a straightforward intuition: handling samples of different hard levels with models of different complexities. Based on deep neural networks, models with different complexities can be naturally derived from sub-networks of different depths. For clarity, we will first formulate the general framework of HDC and then analyze the concrete case for the contrastive loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Formulation</head><p>Here are some notations that will be used to describe our method:</p><p>? P = {I + i , I + j } : all the positive image pairs constructed from training set, where I + i and I + j are supposed to be similar or share the same label.</p><formula xml:id="formula_0">? N = {I ? i , I ? j } :</formula><p>all the negative image pairs constructed from training set, where I ? i and I ? j are supposed to be irrelevant or from different labels. ? G k : the k th computation block including several convolutional layers, pooling layers, and other possible operations in a network. Suppose there are K blocks in total, G 1 takes an image as input, and G k , k &gt; 1 takes the outputs of its previous block as input, then all K blocks are cascaded together as a feed-forward network.</p><formula xml:id="formula_1">? {o + i,k , o + j,k } : the output of the k th computation block G k for the positive pairs {I + i , I + j }. ? {o ? i,k , o ? j,k } : the output of the k th computation block G k for the negative pairs {I ? i , I ? j }. ? F k : the k th transform function that transforms o k to a low dimensional feature vector f k for distance calcu- lation. ? {f + i,k , f + j,k } : the k th computed feature vector after F k for the positive pairs {I + i , I + j }. ? {f ? i,k , f ? j,k } : the k th computed feature vector after F k for the negative pairs {I ? i , I ? j }.</formula><p>Accordingly, there are K models corresponding to K subnetworks of different depths. The first model is the simplest one which uses the first block G 1 and generates features for the pairs {I i , I j } by:</p><formula xml:id="formula_2">{o i,1 , o j,1 } = G 1 ? {I i , I j } (1) {f i,1 , f j,1 } = F 1 ? {o i,1 , o j,1 } (2)</formula><p>If the pair is considered easy by the current model, it will not be passed to more complex models. Otherwise, the pair will continue its forward pass until the k th model considers it as an easy case or the final K th model is reached. We can calculate the features of k th model by:</p><formula xml:id="formula_3">{o i,k , o j,k } = G k ? {o i,k?1 , o j,k?1 } (3) {f i,k , f j,k } = F k ? {o i,k , o j,k }<label>(4)</label></formula><p>Then the loss of k th model is defined as:</p><formula xml:id="formula_4">L k = (i,j)?P k L + k (i, j) + (i,j)?N k L ? k (i, j)<label>(5)</label></formula><p>where P k denotes all positive pairs that are considered as hard examples by previous models and N k indicate negative ones. The definition of hard will be concretely given in Section 3.2. Therefore, the final loss of the HDC is defined as:</p><formula xml:id="formula_5">L = K k=1 ? k L k<label>(6)</label></formula><p>where ? k is the weight for model k.</p><p>The HDC is different from previous deep metric embedding, where only a single model (i.e., model K) is used to mine hard samples. As samples of a dataset are with diverse hard levels, it is difficult to find a single model with the just right complexity to mine hard samples. In contrast, the HDC framework cascades multiple models with increasing complexities and mines samples of different hard levels in a seamless way.</p><p>The model parameters are distributed in G k , F k , 1 ? k ? K. They can be optimized by the standard SGD, the gradient of G k is:</p><formula xml:id="formula_6">?L ?G k = K l=k ? l ?L l ?G k<label>(7)</label></formula><p>where the gradient of G k is calculated by all the models that are built on G k . The gradient of F k is:</p><formula xml:id="formula_7">?L ?F k = ? k ?L k ?F k<label>(8)</label></formula><p>where the gradient of F k is only calculated by model k since F k is only used by model k for feature transformation. The HDC is general for deep metric embedding with hard example mining. Here we take contrastive loss as an example to give the specific loss function. We first introduce the original contrastive loss which penalizes large distance between positive pairs and negative pairs with distance smaller than a margin, i.e.,</p><formula xml:id="formula_8">L + (i, j) = D(f + i , f + j ) (9) L ? (i, j) = max{0, M ? D(f ? i , f ? j )}<label>(10)</label></formula><p>where D(f i , f j ) is the Euclidean distance between the two L2-normalized feature vectors of f i and f j , M is the margin. By applying the contrastive loss to Eq.(5), we get the HDC based contrastive loss, i.e.,</p><formula xml:id="formula_9">L k = (i,j)?P k D(f + i,k , f + j,k )+ (i,j)?N k max{0, M ? D(f ? i,k , f ? j,k )}<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Definition of Hard Example</head><p>Given the defined loss function, we follow conventional hard example mining to define the samples of large loss values as hard examples except that multiple losses will be used to mine hard examples for each sample. Because the loss distributions are different for different models and keep changing during training, it is difficult to predefine thresholds for each model when mining hard samples. Instead, we simply rank losses of all positive pairs in a mini-batch in descending order and take top h k percent samples in the ranking list as hard positive set for the model k. Similar strategies are adopted for hard negative example mining. Then the selected hard samples are forwarded to the later cascaded models.</p><p>Here, we use a toy dataset with positive pairs as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>(a) and negative pairs as illustrated in <ref type="figure" target="#fig_1">Figure 3(b)</ref>, together with the model with K = 3 illustrated in <ref type="figure">Figure 2</ref> to schematically the process of hard example mining. Cascade Model-1 will forward all pairs in P 0 and N 0 , and try to push all positive points towards the anchor point while pushing all negative points away from the anchor point, and form P 1 , N 1 (points in the 2 nd and 3 rd tier) by selecting hard samples according to its loss. Similarly, P 2 and N 2 (points in the 3 rd tier) are formed by Cascade Model-2.</p><p>From the illustrated model in <ref type="figure">Figure 2</ref>, ensembling models in a cascaded manner brings an additional advantage of computational efficiency, since lots of computations are shared during forward pass which is efficient for both training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation Details</head><p>We use mini-batch SGD to optimize the loss function <ref type="bibr" target="#b5">(6)</ref>, and adopt multi-batch <ref type="bibr" target="#b31">[32]</ref> to use all the possible pairs in a mini-batch for stable estimation of the gradient. Algorithm 1 details the framework of our implementation for the HDC. Specifically, sampling strategy from <ref type="bibr" target="#b27">[28]</ref> is to construct a mini-batch of images as input, e.g., a mini-batch of 100 images are randomly sampled evenly from 10 different categories. To leverage more training samples, we further take multi-batch method <ref type="bibr" target="#b31">[32]</ref> to construct all image pairs in a mini-batch to calculate the training loss, e.g., 100 2 ? 100 = 9900 pairs are constructed for 100 images. With the cascaded models, an image is represented by concatenating features from all models.</p><p>Algorithm 1 Hard-Aware Deeply Cascaded Embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1: Given training images set</head><formula xml:id="formula_10">{I i } N i=1 . 2: for t = 1; t &lt; T ; t + + do 3:</formula><p>Sample a mini-batch of training images, following the method in <ref type="bibr" target="#b27">[28]</ref> and initialize the P 0 and N 0 within the mini-batch following the method in <ref type="bibr" target="#b31">[32]</ref>.  Compute the losses for the all pairs constructed in the mini-batch according to Eq.(9) and Eq.(10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Get the P k and N k by choosing the hard pairs following the method described in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Backward and update the gradients according to corresponding parts in Eq. <ref type="bibr" target="#b6">(7)</ref> and Eq.(8) for all the pairs in P k and N k . 9: end for 10: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>The proposed HDC is verified on image retrieval tasks and evaluated by two standard performance metrics, i.e., MAP and Recall@K. MAP <ref type="bibr" target="#b16">[17]</ref> is the mean of average precision scores for all query images over the all the returned images. Recall@K is the average recall scores over all the query images in testing set following the definition in <ref type="bibr" target="#b26">[27]</ref>. Specifically, for each query image, top K nearest images will be returned based on some algorithm, the recall score will be 1 if at least one positive image appears in the returned K images and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Five datasets that are commonly chosen in deep metric embedding are used in our experiments. For fair comparison with the existing methods, we follow the standard protocol of train/test split.</p><p>? CARS196 dataset <ref type="bibr" target="#b10">[11]</ref>, which has 196 classes of cars with 16,185 images, where the first 98 classes are for training <ref type="bibr">(8,</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment Setup</head><p>We choose GoogLeNet <ref type="bibr" target="#b30">[31]</ref> as our model for retrieval tasks. Since GoogLeNet has three output classifiers(two auxiliary classifiers from intermediate layers), HDC adopts them as three cascaded sub-networks corresponding to the three rows illustrated in <ref type="figure">Figure 2</ref>. i.e., G1 contains the layers from the input to "Inception(4a)" inclusively before the first classifier. We initialize the weights from the network pretrained on ImageNet ILSVRC-2012 <ref type="bibr" target="#b19">[20]</ref>. We use the same hyper parameters in all experiments without specifically tuning. Specifically, K is set to 3, ? 1 =? 2 =? 3 =1,  We can see that the overlap area between the 2 distributions decreases from left to right. You can check the LDA score of these methods on table 1 increases from left to right. {h 1 , h 2 , h 3 } = {100, 50, 20}, mini-batch size is 100, margin parameter M is set to 1, the initial learning rate starts from 0.01 and is divided by 10 every 3-5 epoches, and we train models for at most 15 epoches. The other settings follow the same protocol in <ref type="bibr" target="#b27">[28]</ref>. The embedding dimensions of all the cascade models in our HDC are 128, so the embedding dimension of the ensembled model is 384. The code is publicly available at https://github.com/PkuRainBow/ Hard-Aware-Deeply-Cascaded-Embedding_release</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Baseline</head><p>We name different methods with superscript and subscript to denote their specific settings, the number in superscript denotes the dimension used by the method, the subscript denotes bounding boxes are used during training and testing. Different from the original Contrastive 128 <ref type="bibr" target="#b2">[3]</ref>, we use the Contrastive ?128 to denote the contrastive loss computed with multi-batch <ref type="bibr" target="#b31">[32]</ref> method.</p><p>To directly verify the effectiveness of HDC, we first design several baseline methods including: (1) GoogLeNet/pool5 1024 uses the feature vector directly outputted from pool5 of the pre-trained GoogLeNet, <ref type="bibr" target="#b1">(2)</ref> Contrastive ?128 uses contrastive loss without hard example mining, (3) Hard + Contrastive ?128 combines the contrastive loss and hard example mining. In addition to report our method named as HDC + Contrastive ?384 , we also report the performance of sub models learned in our method, i.e., HDC + Contrastive-1 ?128 , HDC + Contrastive-2 ?128 and HDC + Contrastive-3 ?128 . Hard+Contrastive uses the same network architecture as HDC+Contrastive-3, i.e., {G1,G2,G3,F3}. Only top 50 percent examples with larger loss are chosen as hard examples to update the model. The results of these methods on CARS196 are summarized in <ref type="table">Table 1</ref>. Obviously, training on the target dataset brings significant performance improvement comparing with GoogLeNet/pool5 1024 , hard example mining further brings more performance gain, while the hard aware method achieves the best performance. HDC + Contrastive-3 ?128 is much better than the traditional Hard + Contrastive ?128 as the shallow modules of the model are also trained by hard samples mined by shallow models. Our method in the last row achieves the best result comparing with all baselines, which verifies the effectiveness of the hard-aware sample mining.  <ref type="figure" target="#fig_5">Figure 4</ref> shows the distance distributions of positive pairs and negative pairs following <ref type="bibr" target="#b34">[35]</ref>, where green area represents the distance distributions of positive pairs while red area for negative pairs. Our method has the smallest overlapping area, and better separates positive pairs and negative pairs in the embedding space. We also calculate the LDA score which measures the distance between two distributions to quantitatively compare the difference, i.e.,</p><formula xml:id="formula_11">score = |m ? ? m + | 2 v + + v ?<label>(12)</label></formula><p>where m + and m ? are the mean distance of positive pairs and negative pairs, v + and v ? are the variance of the distances of positive pairs and negative pairs. The results on CARS196 are reported in the right part of <ref type="table">Table 1</ref>. It can be observed that the retrieval performance measured by Recall@K positively correlates with LDA score, and our method achieves the highest LDA score 2.50. We also conduct experiment on CUB-200-2011 and report the results in <ref type="table">Table 2</ref>, where the conclusion is the same on CARS196.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with state-of-the-art</head><p>We compare our method with state-of-the-art methods on the five datasets. On the CARS196, CUB-200-2011 and Stanford Online Products datasets: (1) LiftedStruct 128 <ref type="bibr" target="#b26">[27]</ref> uses a novel structured prediction objective on the lifted dense pairwise distance matrix. (2) PDDM + Triplet 128 <ref type="bibr" target="#b5">[6]</ref> combines Position-Dependent Deep Metric units (PDDM) and Triplet Loss. (3) PDDM + Quadruplet 128 <ref type="bibr" target="#b5">[6]</ref> combines the PDDM with Quadruplet Losses proposed in <ref type="bibr" target="#b12">[13]</ref>. (4) Histogram Loss 512 <ref type="bibr" target="#b34">[35]</ref> is penalizing the overlap between distributions of distances of positive pairs and negative pairs.(5) Binomial Deviance 512 <ref type="bibr" target="#b34">[35]</ref> is used to evaluate the cost between similarities and labels, which is proved robust to outliers. (6)Npairs <ref type="bibr" target="#b25">[26]</ref> uses multi-class N-pair loss by generalizing triplet loss by allowing joint comparison among more than one negative examples. The subscript means using multiple crops when testing, while all the other methods use single crop except Npairs . All these methods use GoogLeNet as the base model, which is the same as our method.  <ref type="bibr" target="#b17">[18]</ref> simultaneously learns the landmarks and attributes of the images using VGG-16 <ref type="bibr" target="#b24">[25]</ref>. To test the generality of our method, we use the same hyper-parameters without specifically tuning on these datasets. <ref type="table" target="#tab_2">Table 3</ref> quantifies the advantages of our method on both CARS196 and CUB-200-2011. We conduct three groups of experiments to ensure fairness as different methods adopt different settings, i.e., with/without bounding boxes and with multiple crops when testing. PDDM + Triplet 128 and PDDM + Quadruplet 128 both use the images cropped with the annotated bounding boxes as training set and test set. With bounding boxes, cluttered backgrounds are removed and better performance is expected. HDC + Contrastive ?384 shows significant performance gain both on CARS196 and CUB-200-2011. On CARS196, we improve the Recall@1 score from 57.4% to 83.8%. CUB-200-2011 is more challenging than CARS196 as the car is rigid while birds have more variations. We get 2.4% absolute improvement on CUB-200-2011. Histogram Loss 512 and Binomial Deviance 512 are trained without bounding boxes, for fair comparison, we also validate our method without using bounding boxes. HDC + Contrastive ?384 outperforms all methods without using bounding boxes on both datasets, and has even better results than methods using bounding boexes on CARS196. Besides, we test our method when using multiple crops for test. HDC + Contrastive ?384 also achieves state-of-the-art performance compared with Npairs . <ref type="table" target="#tab_4">Table 5</ref> reports the results on Stanford Online Products.  Stanford Online Products suffers the problem of large number of categories and few images per category, which is very different from CARS196 and CUB-200-2011. Our method achieves 4% absolute improvements over previous state-ofthe-art methods measured by Recall@1. When testing with multiple crops, HDC + Contrastive ?384 further improves the Recall@1 from 67.7% to 70.1%. <ref type="figure" target="#fig_6">Figure 5</ref>(a) shows some retrieval results on Stanford Online Products with features learned by HDC + Contrastive ?384 . Similar to the Stanford Online Products, DeepFashion In-shop Clothes and VehicleID also suffer the problem of limited images in each class and large number of classes. <ref type="table" target="#tab_3">Table 4</ref> and 6 compare the results on the two datasets, where our method outperforms state-of-the-art methods by a large margin.</p><p>Through extensive empirical comparisons on various datasets under different settings, we show that our method is general and can achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose a novel Hard-Aware Deeply Cascaded Embedding to consider both hard levels of samples and the complexities of models. Different from training three separated models, our design ensembles a set of models with increasing complexities in a cascaded manner and shares most of the computation among models. Samples with different hard levels are mined accordingly using the models with adequate complexities. Controlled experimental results demonstrate the advantages by the hard-aware design, and extensive comparisons on five benchmarks further verify the effectiveness of the proposed method in learning deep metric embedding. Currently, the method is verified by three cascaded models with increasing complexities, in the future, we would further improve the method by cascading more models and increasing complexities in a smoother way. And we would also try to combine our method with other loss functions in the future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Data Distribution: (a) Positive Pairs Distribution: Based on the anchor point in the center, P0 contains all the points. P1 contains red, purple points. P2 only contains red points. (b) Negative Pairs Distribution: N0 contains all the points. N1 contains red, purple points. N2 only contains red points. Green arrows denote loss from Cascade-Model-1, red arrows denote loss from Cascade-Model-2 and yellow arrow denote loss from Cascade-Model-3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 : 5 :</head><label>45</label><figDesc>for k = 1; k ? K; k + + do Forward all the images in set P k?1 and N k?1 to k th model to compute the features according to Eq.(3) and Eq.(4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) LDA score = 0.54 (b) LDA score = 0.91 (c) LDA score = 1.99 (d) LDA score = 2.50</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Histograms for positive and negative distance distribution on the CARS196 test set: (a) GoogLeNet/pool5 1024 (b) Contrastive ?128 (c) Hard + Contrastive ?128 (d) HDC + Contrastive ?384 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Retrieval Results on CARS196 and Stanford Online Products: (a) CARS196. (b) Stanford Online Products.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparisons of the Statistics of Histograms and Recall@K on CARS196 test set. The mean and variance under the column named Positive Pairs correspond to m + and v + . The mean and variance under the column named Negative Pairs correspond to m ? and v ? . Comparisons of the Statistics of Histograms and Recall@K on CUB-200-2011 test set.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Recall@K(%)</cell><cell></cell><cell></cell><cell cols="2">Positive Pairs</cell><cell cols="2">Negative Pairs</cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>mean</cell><cell>variance</cell><cell>mean</cell><cell>variance LDA score</cell></row><row><cell>GoogLeNet/pool5 1024</cell><cell cols="6">40.5 53.0 65.0 76.3 86.0 93.1</cell><cell>0.804</cell><cell>0.019</cell><cell>0.941</cell><cell>0.016</cell><cell>0.54</cell></row><row><cell>Contrastive  ?128</cell><cell cols="6">56.0 67.6 77.0 84.8 90.5 94.5</cell><cell>1.110</cell><cell>0.052</cell><cell>1.350</cell><cell>0.011</cell><cell>0.91</cell></row><row><cell>Hard + Contrastive  ?128</cell><cell cols="6">67.6 77.9 85.6 91.2 95.0 97.3</cell><cell>0.786</cell><cell>0.029</cell><cell>1.140</cell><cell>0.034</cell><cell>1.99</cell></row><row><cell>HDC + Contrastive-1  ?128</cell><cell cols="6">41.9 55.5 67.6 78.3 86.9 93.2</cell><cell>0.741</cell><cell>0.045</cell><cell>1.200</cell><cell>0.074</cell><cell>1.77</cell></row><row><cell>HDC + Contrastive-2  ?128</cell><cell cols="6">58.0 70.4 80.2 87.5 92.9 96.1</cell><cell>0.660</cell><cell>0.023</cell><cell>1.050</cell><cell>0.046</cell><cell>2.20</cell></row><row><cell>HDC + Contrastive-3  ?128</cell><cell cols="5">71.4 81.8 88.5 93.4 96.6</cell><cell>98.2</cell><cell>0.792</cell><cell>0.014</cell><cell>1.070</cell><cell>0.020</cell><cell>2.27</cell></row><row><cell>HDC + Contrastive  ?384</cell><cell cols="5">73.7 83.2 89.5 93.8 96.5</cell><cell>98.4</cell><cell>0.756</cell><cell>0.015</cell><cell>1.080</cell><cell>0.027</cell><cell>2.50</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Recall@K(%)</cell><cell></cell><cell></cell><cell cols="2">Positive Pairs</cell><cell cols="2">Negative Pairs</cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>mean</cell><cell>variance</cell><cell>mean</cell><cell>variance LDA score</cell></row><row><cell>HDC + Contrastive-1  ?128</cell><cell cols="6">43.4 55.8 69.1 80.4 88.1 93.9</cell><cell>0.709</cell><cell>0.023</cell><cell>1.000</cell><cell>0.026</cell><cell>1.73</cell></row><row><cell>HDC + Contrastive-2  ?128</cell><cell cols="6">51.9 63.8 75.1 84.3 91.2 95.3</cell><cell>0.637</cell><cell>0.016</cell><cell>0.919</cell><cell>0.021</cell><cell>2.15</cell></row><row><cell>HDC + Contrastive-3  ?128</cell><cell cols="6">58.5 71.1 80.8 88.5 93.5 96.5</cell><cell>0.770</cell><cell>0.012</cell><cell>1.000</cell><cell>0.013</cell><cell>2.12</cell></row><row><cell>HDC + Contrastive  ?384</cell><cell cols="6">60.7 72.4 81.9 89.2 93.7 96.8</cell><cell>0.741</cell><cell>0.012</cell><cell>0.989</cell><cell>0.014</cell><cell>2.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Recall@K(%) on CARS196 and CUB-200-2011. 32.3 46.1 58.9 72.2 83.4 26.4 37.7 49.8 62.3 76.4 85.3 Triplet 128 [21, 38] 39.1 50.4 63.3 74.5 84.1 89.8 36.1 48.6 59.3 70.0 80.2 88.4 LiftedStruct 128 [27] 49.0 60.3 72.1 81.5 89.2 92.8 47.2 58.9 70.2 80.2 89.3 93.2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">CARS196</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CUB-200-2011</cell><cell></cell><cell></cell></row><row><cell>K</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell cols="2">Contrastive 128 [3] 21.7 Binomial Deviance 512 [35] -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="6">52.8 64.4 74.7 83.9 90.4 94.3</cell></row><row><cell>Histogram Loss 512 [35]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="6">50.3 61.9 72.6 82.4 88.8 93.7</cell></row><row><cell>HDC + Contrastive  ?384</cell><cell cols="6">73.7 83.2 89.5 93.8 96.7 98.4</cell><cell cols="6">53.6 65.7 77.0 85.6 91.5 95.5</cell></row><row><cell>PDDM + Triplet 128 [6]</cell><cell cols="6">46.4 58.2 70.3 80.1 88.6 92.6</cell><cell cols="6">50.9 62.1 73.2 82.5 91.1 94.4</cell></row><row><cell cols="7">PDDM + Quadruplet 128 [6] 57.4 68.6 80.1 89.4 92.3 94.9</cell><cell cols="6">58.3 69.2 79.0 88.4 93.1 95.7</cell></row><row><cell>HDC + Contrastive  ?384</cell><cell cols="6">83.8 89.8 93.6 96.2 97.8 98.9</cell><cell cols="6">60.7 72.4 81.9 89.2 93.7 96.8</cell></row><row><cell>Npairs [26]</cell><cell cols="4">71.1 79.7 86.5 91.6</cell><cell>-</cell><cell>-</cell><cell cols="4">50.9 63.3 74.3 83.2</cell><cell>-</cell><cell>-</cell></row><row><cell>HDC + Contrastive  ?384</cell><cell cols="6">75.0 83.9 90.3 94.3 96.8 98.4</cell><cell cols="6">54.6 66.8 77.6 85.9 91.7 95.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Recall@K(%) on In-shop Clothes Retrieval Dataset. On VehicleID and In-shop Clothes Retrieval datasets : (1) CCL + Mixed Diff 1024 [17] uses the Coupled Cluster Loss and Mixed Difference Network Structure. (2) Fash-ionNet</figDesc><table><row><cell>K</cell><cell>1</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell></row><row><cell>FashionNet + Joints 4096 [18]</cell><cell cols="6">41.0 64.0 68.0 71.0 73.0 73.5</cell></row><row><cell cols="7">FashionNet + Poselets 4096 [18] 42.0 65.0 70.0 72.0 72.0 75.0</cell></row><row><cell>FashionNet 4096 [18]</cell><cell cols="6">53.0 73.0 76.0 77.0 79.0 80.0</cell></row><row><cell>HDC + Contrastive  ?384</cell><cell cols="6">62.1 84.9 89.0 91.2 92.3 93.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Recall@K(%) on Stanford Online Products.</figDesc><table><row><cell>K</cell><cell>1</cell><cell>10</cell><cell>100</cell><cell>1000</cell></row><row><cell>Contrastive 128 [3]</cell><cell cols="3">42.0 58.2 73.8</cell><cell>89.1</cell></row><row><cell>Triplet 128 [21, 38]</cell><cell cols="3">42.1 63.5 82.5</cell><cell>94.8</cell></row><row><cell>LiftedStruct 128 [27]</cell><cell cols="3">60.8 79.2 91.0</cell><cell>97.3</cell></row><row><cell>LiftedStruct 512 [27]</cell><cell cols="3">62.1 79.8 91.3</cell><cell>97.4</cell></row><row><cell cols="4">Binomial Deviance 512 [35] 65.5 82.3 92.3</cell><cell>97.6</cell></row><row><cell>Histogram Loss 512 [35]</cell><cell cols="3">63.9 81.7 92.2</cell><cell>97.7</cell></row><row><cell>HDC + Contrastive  ?384</cell><cell cols="3">69.5 84.4 92.8</cell><cell>97.7</cell></row><row><cell>Npairs [26]</cell><cell cols="3">67.7 83.8 92.9</cell><cell>97.8</cell></row><row><cell>HDC + Contrastive  ?384</cell><cell cols="3">70.1 84.9 93.2</cell><cell>97.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>MAP of Vehicle Retrieval Task.</figDesc><table><row><cell>MAP</cell><cell cols="3">Small Medium Large</cell></row><row><cell>VGG + Triplet Loss 1024</cell><cell>0.444</cell><cell>0.391</cell><cell>0.373</cell></row><row><cell>VGG + CCL 1024 ( [17])</cell><cell>0.492</cell><cell>0.448</cell><cell>0.386</cell></row><row><cell>Mixed Diff + CCL 1024 ( [17])</cell><cell>0.546</cell><cell>0.481</cell><cell>0.455</cell></row><row><cell>GoogLeNet/pool5 1024</cell><cell>0.418</cell><cell>0.392</cell><cell>0.347</cell></row><row><cell>HDC + Contrastive  ?384</cell><cell>0.655</cell><cell>0.631</cell><cell>0.575</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Real-time pedestrian detection with deep network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferguson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pn-net: Conjoined triple deep network for learning local image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.05030</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning visual similarity for product design with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhattarai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02975</idno>
		<title level="m">Cp-mtml: Coupled projection multi-task metric learning for large scale face retrieval</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving semantic embedding consistency by metric learning for zero-shot classiffication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Local similarity-aware deep feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fine-grained categorization and dataset bootstrapping using deep metric learning with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05227</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Matchnet: Unifying feature and metric learning for patchbased matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F. Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning local image descriptors with deep siamese and triplet convolutional networks by minimising global loss functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar B G</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quadruplet-wise image similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07866</idno>
		<title level="m">Learning by tracking: Siamese cnn for robust target association</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep relative distance learning: Tell the difference between similar vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05939</idno>
		<title level="m">Metric learning with adaptive density discrimination</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Embedding deep metric for person re-identification: A study against large variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fashion Style in 128 Floats: Joint Ranking and Classification using Weak Data for Feature Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06452</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stefanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03776</idno>
		<title level="m">Pronet: Learning to propose object-specific boxes for cascaded neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning a metric embedding for face recognition using the multibatch method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tadmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rosenwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07270</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05863</idno>
		<title level="m">Siamese instance search for tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teerapittayanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcdanel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kung</surname></persName>
		</author>
		<title level="m">Branchynet: Fast inference via early exiting from deep neural networks. ICPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning deep embeddings with histogram loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Residual networks are exponential ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Mining discriminative triplets of patches for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.01130</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Top-push videobased person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.08683</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Embedding label structures for fine-grained feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Zero-shot learning via joint latent similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
