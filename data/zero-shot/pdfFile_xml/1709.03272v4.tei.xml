<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fused Text Segmentation Networks for Multi-oriented Scene Text Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Westone Cryptologic Research Center</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youxuan</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Xiamen No</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Qiu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information and Electrical Shanghai</orgName>
								<orgName type="institution">Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">High School</orgName>
								<address>
									<settlement>Fujian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fused Text Segmentation Networks for Multi-oriented Scene Text Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce a novel end-end framework for multi-oriented scene text detection from an instanceaware semantic segmentation perspective. We present Fused Text Segmentation Networks, which combine multi-level features during the feature extracting as text instance may rely on finer feature expression compared to general objects. It detects and segments the text instance jointly and simultaneously, leveraging merits from both semantic segmentation task and region proposal based object detection task. Not involving any extra pipelines, our approach surpasses the current state of the art on multioriented scene text detection benchmarks: ICDAR2015 Incidental Scene Text and MSRA-TD500 reaching Hmean 84.1% and 82.0% respectively. Morever, we report a baseline on totaltext containing curved text which suggests effectiveness of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recently, scene text detection has drawn great attention from computer vision and machine learning community. Driven by many content-based image applications such as photo translation and receipt content recognition, it has become a promising and challenging research area both in academia and industry. Detecting text in natural images is difficult, because both text and background may be complex in the wild and it often suffers from disturbance such as occlusion and uncontrollable lighting conditions <ref type="bibr" target="#b0">[1]</ref>.</p><p>Previous text detection methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> have achieved promising results on several benchmarks. The essential problem in text detection is to represent text region using discriminative features. Conventionally, hand-crafted features are designed <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> to capture the properties of text region such as texture and shape, while in the past few years, deep learning based approaches <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> directly learn hierarchical features from training data, demonstrating more accurate and efficient performance in various benchmarks such as ICDAR series contests <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>Existing methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref> have obtained decent performance for detecting horizontal or near-horizontal text. While horizontal text detection has constraints of axis-aligned bounding-box ground truth, the multi-oriented text is not restrictive to a particular orientation and usually uses quadrilaterals for annotations. Therefore, it reports relatively lower accuracies in ICDAR 2015 Competition Challenge 4 Incidental scene text localization <ref type="bibr" target="#b15">[16]</ref> compared to horizontal scene text detection benchmarks <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Recently, a few approaches <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> have been proposed to address the multi-oriented text detection. In general, there are currently four different types of methods. Region based methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b20">[21]</ref> leverage advanced object detection techniques such as Faster RCNN <ref type="bibr" target="#b22">[23]</ref> and SSD <ref type="bibr" target="#b23">[24]</ref>. Segmentation-based methods <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> mainly utilize fully convolutional neural networks (FCN) for generating text score maps, which often need several stages and components to achieve final detections. Direct regression based method <ref type="bibr" target="#b17">[18]</ref> regresses the position and size of an object from a given point. Finally, hybrid method <ref type="bibr" target="#b19">[20]</ref> combines text scores map and rotated/quadrangle bounding boxes generation to collaboratively obtain the efficient and accurate performance in multi-oriented text detection.</p><p>Inspired by recent advance of instance-aware semantic segmentation <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, we present a novel perspective to handle the task of multi-oriented text detection. In this work, we leverage the merits from accurate region proposal based methods <ref type="bibr" target="#b22">[23]</ref>, and flexible segmentation based methods which can easily generate arbitrary-shaped text mask <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> . It is an end-to-end trainable framework excluding redundant and low-efficient pipelines such as the use of text/nontext salient map <ref type="bibr" target="#b24">[25]</ref> and text-line generation <ref type="bibr" target="#b25">[26]</ref>. Based on region proposal network (RPN), our approach detects and segments text instance simultaneously, followed by non-maximum suppression (NMS) to suppress overlapping instances. Finally, a minimum quadrangle bounding box to fit each instance area is generated as the result of the whole detection process.</p><p>Our main contributions are summarized as follows:</p><p>? We present an end-end efficient and trainable solution for multi-oriented text detection from an instance aware segmentation perspective, excluding any redundant pipelines. ? During feature extraction, feature maps are composed in a fused fashion to adaptively satisfy the finer representation of text instance. ? Mask-NMS is introduced to improve the standard NMS when facing heavily inclined or line-level text instances. ? Without many bells and whistles, our approach outperforms state of the art on current multi-oriented text detection benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Detecting text in natural images has been widely studied in past few years, motivated by many text-related real-world applications such as photo OCR and blind navigation. One of the mainstream traditional methods for scene text detection are Connected Components (CCs) based methods <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> , which consider text as a group of individual components such as characters. Within these methods, stroke width transform (SWT) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b30">[31]</ref> and maximally stable extremal region (MSER) <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b6">[7]</ref> are usually used to seek character candidates. Finally, these candidates are combined to obtain text objects. Although these bottom up approaches may be accurate on some benchmarks <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, they often suffer from too many pipelines, which may cause inefficiency. Another mainstream traditional methods are sliding window based <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b9">[10]</ref>. These methods often use a fixed-size or multiscale window to slide through the image searching the region which most likely contains text. However, the process of sliding window may involve large computational cost which results in inefficiency. Generally, traditional methods often require several steps to obtain final detections, and handdesigned features are usually used to represent properties of text. Therefore, they may suffer from inefficiency and low generalization ability against complex situations such as nonuniform illumination <ref type="bibr" target="#b34">[35]</ref>.</p><p>Recent progress on deep learning based approaches for object detection and semantic segmentation has provided new techniques for reading text in the wild, which can be also seen as an instance of general object detection. Driven by the advance of object detection frameworks such as Faster RCNN <ref type="bibr" target="#b22">[23]</ref> and SSD <ref type="bibr" target="#b23">[24]</ref>, these methods achieved state of the art by either using a region proposal network to first classify some text region proposals <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b16">[17]</ref>, or directly regress text bounding boxes coordinates from a set of default boxes <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>. These methods are able to achieve leading performance on horizontal or multi-oriented scene text detection benchmarks. However, they may also be restricted to rectangular bounding box constraints even with appropriate rotation <ref type="bibr" target="#b20">[21]</ref>. Different from these methods, FCN based approaches generate text/non-text map which classifies text at the pixel level <ref type="bibr" target="#b24">[25]</ref>. Though it may be suited well for arbitrary shape of text in natural images, it often involves several pipelines which leads to inefficiency <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p><p>Inspired by recent advance on instance-aware semantic segmentation <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, we present an end-end trainable framework called Fused Text Segmentation Networks (FTSN) to handle arbitrary-shape text detection with no extra pipelines involved. It inherits merits from both object detection and semantic segmentation architecture which efficiently detects and segments an text instance simultaneously and accurately gives predictions in the pixel level. As text may rely on finer feature representation, a fused structure formed by multi-level feature maps is set to fit this property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head><p>The proposed framework for multi-oriented scene text detection is diagrammed in <ref type="figure" target="#fig_1">Fig.2</ref>. It is a deep CNN model which mainly consists of three parts. Feature representations of each image are extracted through resnet-101 backbone <ref type="bibr" target="#b35">[36]</ref>, then multi-level feature maps are fused as FusedMapA which is fed to the region proposed network (RPN) for text region of interest (ROI) generation and FusedMapB for later rois' PSROIPooling. Finally the rois are sent to the detection, segmentation and box regression branches to output text instances in pixel level along with their corresponding bounding boxes. The post-processing part includes NMS and minimal quadrilateral generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>The convolutional feature representation is designed in a fusion fashion. The text instance is not like the general object such as people and cars which have relatively strong semantics. On the contrary, texts often vary tremendously in intra-class geometries. Consequently, low-level features should be taken into consideration. Basically, resnet-101 consists of five stages. Before region proposing, stage3 and upsampled stage4 feature maps are combined to form FusedMapA through elementwise adding, then upsampled feature maps from stage5 are fused with FusedMapA to form FusedMapB. It is noted that downsampling is not involved during stage5. Instead, we use the hole algorithm <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> to keep the feature stride and maintain the receptive field. The reason for this is that both text properties and the segmentation task may require finer features and involving final downsampling may lose some useful information.</p><p>Because using feature stride of stage3 may cause millions of anchors in original RPN <ref type="bibr" target="#b22">[23]</ref> which makes model training hard, so we add a 3 ? 3 with stride 2 convolution to reduce such huge number of anchors.</p><p>Followed FCIS <ref type="bibr" target="#b26">[27]</ref>, we use Joint Mask Prediction and Classification to simultaneously classify and mask the text instance on 2 ? (1 + 1) inside/outside score maps generated through PSROIPooling on conv-cls-seg feature maps, and box regression branch utilizes 4 ? (1 + 1) feature maps from convbox after PSROIPooling ("1 + 1" means one class is for text and the other for background). We use k = 7 shown in <ref type="figure" target="#fig_1">Fig.2</ref> in our experiments by default. It is noted that after PSROIPooling, the resolution of feature maps becomes 21?21. Therefore, we use global average pooling <ref type="bibr" target="#b38">[39]</ref> for classification (after </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ground Truth and Loss Function</head><p>The whole multi-task loss L can be interpreted as</p><formula xml:id="formula_0">L = L rpn + L ins (1) L rpn = L rcls + ? r L rbox (2) L ins = L cls + ? m L mask + ? b L box<label>(3)</label></formula><p>The full loss L consists of two sub stage losses: RPN loss L rpn where L rcls is for region proposal classification and L rbox is for box regression, and text instance loss L ins based on each ROI, where L cls , L mask andL box represent losses for instance classification, mask and box regression task respectively. ? is the hyper-parameter to control the balance among each loss term. They are set as ? r = 0.2, ? m = 2, ? b = 0.2 in our experiments.</p><p>Classification and mask task both use cross-entropy as loss function, whereas we use smooth-L1 for box regression task formulated as</p><formula xml:id="formula_1">smooth L1 (x) = 0.5(?x) 2 if |x| &lt; 1/? 2 , |x| ? 0.5/? 2 otherwise.<label>(4)</label></formula><p>? is set to 3 in our experiments which makes the box regression loss less sensitive to outliers.</p><p>Ground truth of each text instance is presented by bounding boxes and masks shown in <ref type="figure" target="#fig_2">Fig.3</ref>. In most multi-oriented text detection dataset, annotations are given in quadrilaterals such as IC15 or can be converted to quadrilaterals such as TD500. For each instance, we directly generate mask from quadrilateral coordinates and use the minimal rectangle containing the mask as the bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Post Processing</head><p>Mask-NMS To obtain final detection results, we use Non-Maximum Suppression mechanism (NMS) to filter overlapped text instances and preserve those with highest scores. After NMS, we generate a minimum quadrilateral for each text instance covering the mask as shown in <ref type="figure" target="#fig_0">Fig.1</ref>. <ref type="figure">Fig. 4</ref>. Heavily inclined text filtering results. Left: standard NMS may filter the correct detection as shown in white dashed line. Middle: dashed lines in yellow show the bounding boxes used in IOU computation of standard NMS, and the yellow transparent area illustrates that the IOU between the two closed inclined instance is big enough to make the correct detection filtered. Right: Mask-NMS takes the mask area of each instance to compute MMI replacing IOU. Therefore, there is no intersection between each instance.</p><p>Standard NMS computes IOU among bounding boxes, which may be fine for word-level and near-horizontal results' filtering. However, it may filter some correct line-level detections when they are close and heavily inclined as shown in <ref type="figure">Fig.4</ref> or when words stay close in the same line as shown in <ref type="figure">Fig.5</ref>. Consequently, we propose a modified NMS called Mask-NMS to handle such situations. Mask-NMS mainly changes bounding box IOU computation to so-called maskmaximum-intersection (MMI) as formulated:</p><formula xml:id="formula_2">M M I = max(I/I A , I/I B )<label>(5)</label></formula><p>I A , I B are mask areas of two text instances to be computed, I is the intersection area between the masks. Maximum intersection over the mask areas are used to replace original IOU for the reason that detections may easily involve line-level and word-level text instances simultaneously at the same line as shown in <ref type="figure">Fig.5</ref>. The proposed Mask-NMS has significantly improved performance for multi-oriented scene text detection as shown in section.5. <ref type="figure">Fig. 5</ref>. line-level text NMS results. From left to right: ground truth, standard NMS and Mask-NMS results. Benchmarks may provide annotations at different levels such as word-level in IC15 and line-level in TD500. However, model may be confused about them and make predictions at different levels as shown in the middle. Using maximum intersection can greatly avoid this situation as the MMI over the two text instances in the middle image is 1 so one of them is certainly filtered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>To evaluate the proposed framework, we conduct quantitative experiments on three public benchmarks: ICDAR2015, MSRA-TD500 and Total-Text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>ICDAR 2015 Incidental Text (IC15) the Challenge 4 of ICDAR 2015 Robust Reading Competition <ref type="bibr" target="#b15">[16]</ref>. IC15 contains 1000 training and 500 testing incidental images taken by Google Glasses without paying attention to viewpoint and image quality. Therefore, large variations in text scale, orientation and resolution lead to difficulty for text detection. Annotations of the dataset are given in word-level quadrilaterals.</p><p>MSRA-TD500 (TD500) is early presented in <ref type="bibr" target="#b39">[40]</ref>. The dataset is multi-oriented and multi-lingual including both Chinese and English text which consists of 300 training and 200 testing images. Different from IC15, annotations of TD500 are at line level which are rotated rectangles.</p><p>Total-Text is presented in ICDAR2017 <ref type="bibr" target="#b40">[41]</ref>. It consists of 1555 images with more than 3 different text orientations: Horizontal, Multi-Oriented, and Curved.</p><p>SynthText in the Wild (SynthText) The dataset contains 800,000 synthetic images <ref type="bibr" target="#b10">[11]</ref>, text with random color, fonts, scale and orientation are rendered on natural images carefully to have a realistic look. Annotations are given in character, word and line level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Training We pretrain the proposed FTSN on a subset of SynthText containing 160,000 images, then finetune on IC15, TD500 and Total-Text. For optimization, standard SGD is used during training with learning rate 5 ? 10 ?3 for first 5 epochs and 5 ? 10 ?4 for the last epoch,and we also apply online hard example mining (OHEM) <ref type="bibr" target="#b41">[42]</ref> for balancing the positive and negative samples. Different from original RPN anchor ratios and scales setting for object detection, anchor scales of [32 2 , 64 2 , 128 2 , 256 2 ] and ratios of [1/3,1/2,1,2,3,5,7] are set because text often has a large aspect ratio and a small scale <ref type="bibr" target="#b12">[13]</ref>.</p><p>Data augmentation Multi-scale training, rotation and color jittering are applied during training. Scales are randomly chosen from [600,720,960,1100] and each number represents the short edge of input images. Rotation with 15 ? ,30 ? and 45 ? are applied with horizontal flip. Consequently, it enlarges 8x dataset size than the original one. Random brightness, contrast and saturation jittering are applied for input images.</p><p>Testing Input images are resized to 848?1500 when testing. After NMS, mask voting <ref type="bibr" target="#b42">[43]</ref> is used to obtain an ensemble text instance mask by averaging all reasonable detections.</p><p>Experiments are conducted on MXNet <ref type="bibr" target="#b43">[44]</ref> and run on a server with Intel i7 6700K CPU, 64GB RAM, GTX 1080 and Ubuntu 14.04 OS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results</head><p>Tabel.1 shows results of the proposed FTSN on IC15 compared with previous state of art published methods. SNMS and MNMS represent standard NMS and Mask-NMS respectively. Our FTSN with Mask-NMS outperforms former best result by 5.3% in Precision and 3.1% in Hmean. It is evaluated by the official submission server 1 .</p><p>Results on TD500 are shown in <ref type="table">Table.</ref>2 along with other state of art methods. It is shown that our methods outperform  the current state of art approaches by a large margin in Hmean and Recall, without adding extra real-world training images. Our method also shows great flexibility on the total-text dataset containing curved text. As the dataset is new to the community, experiments are seldom conducted on it which makes our results as a baseline shown in Table3. The evaluation metric uses IoU of 0.5 between each instance masks.</p><p>Outperforming the current state of the art, our approach runs about 4 FPS on 848 ? 1500 images and 2.5 FPS when using Mask-NMS, which presents efficiency and accuracy.</p><p>It is noted that the proposed Mask-NMS significantly improved Hmean by 0.7 and 0.3 percent on IC15 and TD500, which mainly target the situations in <ref type="figure">Fig.4</ref> and <ref type="figure">Fig.5</ref>. <ref type="figure" target="#fig_3">Fig.6</ref> shows example results of FTSN. From left to right, it illustrates results on IC15 ,TD500 and Total-Text dataset. The decent performance for word-level, line-level and curved text detection with large variation in resolution, view point, scale and linguistics suggests excellent generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We present FTSN, an end-end efficient and accurate multioriented scene text detection framework. It has outperformed previous state of the art approaches on word-level line-level annotated benchmarks and report a baseline on total-text demonstrating decent generalization ability and flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEGEMENTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>FTSN Work Flows. From left to right, input images, text instance segmentation results and final processed quadrilateral results are shown infigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The proposed framework consists of three parts: feature extraction, feature fusion along with region proposing and text instance prediction. The dashed line represents a convolution with 1x1 kernel size and 1024 output channels. The line in red is for upsampling operation and blue lines indicate on which feature maps PSROIPooling are performed using given ROIs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Ground truth. Left: original image. Right: corresponding ground truth in which dashed lines are for bounding boxes and quadrilaterals filled with different colors are for masks pixel-wise max) and box regression branches, and pixel-wise softmax on mask branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Results of ICDAR2015, MSRA-TD500 and Total-Text</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell cols="4">ICDAR 2015 INCIDENTAL DATASET</cell></row><row><cell>Method</cell><cell cols="4">Precision (%) Recall (%) Hmean (%)</cell></row><row><cell>HUST[16]</cell><cell></cell><cell>44.0</cell><cell>37.8</cell><cell>40.7</cell></row><row><cell>Zhang et al.[25]</cell><cell></cell><cell>71.0</cell><cell>43.0</cell><cell>54.0</cell></row><row><cell>DMPNet[21]</cell><cell></cell><cell>73.2</cell><cell>68.2</cell><cell>70.6</cell></row><row><cell>Qin et al.[17]</cell><cell></cell><cell>79.0</cell><cell>65.0</cell><cell>71.0</cell></row><row><cell>SegLink[19]</cell><cell></cell><cell>73.1</cell><cell>76.8</cell><cell>75.0</cell></row><row><cell>RRPN[22]</cell><cell></cell><cell>73.2</cell><cell>82.2</cell><cell>77.4</cell></row><row><cell>EAST[20]</cell><cell></cell><cell>83.3</cell><cell>78.3</cell><cell>80.7</cell></row><row><cell>He et al.[18]</cell><cell></cell><cell>82.0</cell><cell>80.0</cell><cell>81.0</cell></row><row><cell>Proposed FTSN+SNMS</cell><cell></cell><cell>87.1</cell><cell>80.0</cell><cell>83.4</cell></row><row><cell cols="2">Proposed FTSN+MNMS</cell><cell>88.6</cell><cell>80.0</cell><cell>84.1</cell></row><row><cell></cell><cell cols="2">TABLE II</cell><cell></cell></row><row><cell cols="3">MSRA-TD500 DATASET</cell><cell></cell></row><row><cell>Method</cell><cell cols="4">Precision (%) Recall (%) Hmean (%)</cell></row><row><cell>Yao et al.[40]</cell><cell></cell><cell>63.0</cell><cell>63.0</cell><cell>60.0</cell></row><row><cell>Zhang et al.[25]</cell><cell></cell><cell>83.0</cell><cell>67.0</cell><cell>74.0</cell></row><row><cell>RRPN[22]</cell><cell></cell><cell>82.0</cell><cell>68.0</cell><cell>74.0</cell></row><row><cell>He et al.[18]</cell><cell></cell><cell>77.0</cell><cell>70.0</cell><cell>74.0</cell></row><row><cell>EAST[20]</cell><cell></cell><cell>87.3</cell><cell>67.4</cell><cell>76.1</cell></row><row><cell>SegLink[19]</cell><cell></cell><cell>86.0</cell><cell>70.0</cell><cell>77.0</cell></row><row><cell>Proposed FTSN+SNMS</cell><cell></cell><cell>86.6</cell><cell>77.3</cell><cell>81.7</cell></row><row><cell cols="2">Proposed FTSN+MNMS</cell><cell>87.6</cell><cell>77.1</cell><cell>82.0</cell></row><row><cell></cell><cell cols="2">TABLE III</cell><cell></cell></row><row><cell cols="3">TOTAL-TEXT DATASET</cell><cell></cell></row><row><cell>Method</cell><cell cols="4">Precision (%) Recall (%) Hmean (%)</cell></row><row><cell>Proposed FTSN</cell><cell>84.7</cell><cell>78.0</cell><cell>81.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://rrc.cvc.uab.es/?ch=4</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scene text detection and recognition: recent advances and future trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="36" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detecting and reading text in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="366" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2963" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fastext: Efficient unconstrained scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1206" to="1214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Text flow: A unified text detection system in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-time scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Text Localization Based on Fast Feature Pyramids and Multi-Resolution Maximally Stable Extremal Regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamberletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Noce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gallo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Robust Scene Text Detection with Convolution Neural Network Induced MSER Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep features for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="512" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multioriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Textboxes: A fast text detector with a single deep neural network,&quot; in Association for the Advancement of Artificial Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Icdar 2011 robust reading competition challenge 2: Reading text in scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1491" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Icdar 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cascaded segmentation-detection networks for word-level text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep direct regression for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08289</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">East: An efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep matching prior network: Toward tighter multioriented text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01425</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01086</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1137</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multioriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4159" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Accurate text localization in natural image with cascaded convolutional text network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09423</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional instanceaware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Real-time lexicon-free scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1872" to="1885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="591" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Text localization in natural images using stroke feature transform and text covariance descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1241" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A method for text localization and recognition in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="770" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust wide-baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image &amp; Vision Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="761" to="767" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Text detection and localization in complex scene images using constrained adaboost algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hanif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prevost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition, 2009. ICDAR&apos;09. 10th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Character proposal network for robust text extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2633" to="2637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Total-text: A comprehensive dataset for scene text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IAPR International Conference on Document Analysis and Recognition ICDAR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3150" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
