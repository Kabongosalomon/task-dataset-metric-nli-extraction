<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SSP-Net: Scalable Sequential Pyramid Networks for Real-Time 3D Human Pose Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carbonera</forename><surname>Diogo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luvizon</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ETIS UMR 8051</orgName>
								<orgName type="institution" key="instit1">Paris Seine University</orgName>
								<orgName type="institution" key="instit2">ENSEA</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>F-95000</postCode>
									<settlement>Cergy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Tabia</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ETIS UMR 8051</orgName>
								<orgName type="institution" key="instit1">Paris Seine University</orgName>
								<orgName type="institution" key="instit2">ENSEA</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>F-95000</postCode>
									<settlement>Cergy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR 8049</orgName>
								<orgName type="institution" key="instit1">IBISC</orgName>
								<orgName type="institution" key="instit2">Univ. d?Evry Val d?Essonne</orgName>
								<orgName type="institution" key="instit3">Universit? Paris Saclay c LIGM</orgName>
								<address>
									<addrLine>?cole des Ponts</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">UPE</orgName>
								<address>
									<settlement>Champs-sur-Marne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ETIS UMR 8051</orgName>
								<orgName type="institution" key="instit1">Paris Seine University</orgName>
								<orgName type="institution" key="instit2">ENSEA</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>F-95000</postCode>
									<settlement>Cergy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SSP-Net: Scalable Sequential Pyramid Networks for Real-Time 3D Human Pose Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D human pose estimation</term>
					<term>Neural nets</term>
					<term>Computer vision 2018 MSC: 00-01</term>
					<term>99-00</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose a highly scalable convolutional neural network, endto-end trainable, for real-time 3D human pose regression from still RGB images. We call this approach the Scalable Sequential Pyramid Networks (SSP-Net) as it is trained with refined supervision at multiple scales in a sequential manner. Our network requires a single training procedure and is capable of producing its best predictions at 120 frames per second (FPS), or acceptable predictions at more than 200 FPS when cut at test time. We show that the proposed regression approach is invariant to the size of feature maps, allowing our method to perform multi-resolution intermediate supervisions and reaching results comparable to the state-of-the-art with very low resolution feature maps. We demonstrate the accuracy and the effectiveness of our method by providing extensive experiments on two of the most important publicly available datasets for 3D pose estimation, Human3.6M and MPI-INF-3DHP. Additionally, we provide relevant insights about our decisions on the network architecture and show its flexibility to meet the best precision-speed compromise.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Predicting 3D human poses from monocular images is an important task that benefits several applications, from human understanding and action recognition <ref type="bibr" target="#b0">[1]</ref> to human shape analysis and character control <ref type="bibr" target="#b1">[2]</ref>, among many others. As a consequence of its high relevance, 3D human pose estimation is a very active topic, also due to the several challenges involved, such as the complexity in the human body structure, the variations in the visual aspects from one person to another, and the possibility of one or more body parts being occluded in the images. To handle these challenging cases, multi-scale analysis is traditionally used to allow a multi-level scene understanding.</p><p>With the breakthrough of deep Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b2">[3]</ref> alongside consistent computational power increase, human pose estimation methods have shifted from classical approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> to deep architectures <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Most of current deep learning approaches for 3D human pose estimation are based on an extension of the stacked hourglass model <ref type="bibr" target="#b7">[8]</ref> where each body joint is associated with a volumetric heatmap (e.g., <ref type="bibr" target="#b8">[9]</ref>) that corresponds to the probability density of the joint in the 3D space. These volumetric heatmaps have two main issues. First, the accuracy of the pose estimation is very sensitive to the resolution of the volumetric heatmap, since the precision of the prediction is directly related to the volume of space encoded by a single voxel of the heatmap. Second, as large volumetric heatmaps are preferred, these methods require large amounts of memory to store the activations. The combination of these two issues results in neural architectures that do not scale well, that is, they are either accurate but very slow or quick but inaccurate. Furthermore, a new model has to be trained specifically for the chosen trade-off between speed and accuracy.</p><p>In the light of the limitations of current methods, we propose a new neural architecture that solves the scalability issues, by regressing the pose in multiple scales in a sequential coarse-to-fine approach. We call this approach the Scalable Sequential Pyramid Networks (SSP-Net). With a single training procedure, the SSP-Net produces a full model with several refined prediction outputs that can be cut a test time to select the best accuracy vs speed trade off. The contribution of this paper is an extremely fast 3D human body pose estimation architecture that obtains state of the art results at over 100 FPS. We also show that our method is robust to the resolution of the model, as it is able to obtain subpixel accuracy, leading to competitive results even for 4 ? 4 pixels feature maps.</p><p>The rest of this paper is divided as follows. In the next section, we present a review of the related work. The architecture of the proposed network and the proposed regression method are presented in Section 3. In Section 4, we show the experimental evaluation of our method on 3D human pose estimation, providing intuitions on the proposed method based on a detailed ablation study. We conclude the paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section, we review some of the recent methods most related to our work, which are divide into two groups: 3D human pose estimation and Multistage architectures for human pose estimation. We encourage readers to read the survey on 3D human pose estimation in <ref type="bibr" target="#b9">[10]</ref> for a more detailed bibliographic review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">3D human pose estimation</head><p>Estimating the human body joints in 3D coordinates from monocular RGB images is a challenging problem with a vast bibliography available in the litera-ture <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. Despite the fact that methods for 2D pose estimation are mainly based on detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>, 3D pose estimation is frequently handled as a regression problem <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. The main reason is due to the additional third dimension in 3D predictions, which significantly increases the required memory and computations, especially in detection based approaches, were the space is frequently represented by voxels <ref type="bibr" target="#b19">[20]</ref>. On the other hand, regression methods handle the problem more efficiently, usually resulting in precise estimations with lower resolution <ref type="bibr" target="#b0">[1]</ref>.</p><p>A common approach for 3D human pose regression is to lift 3D coordinates from 2D predictions <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. Despite being robust to visual variations, lifting 3D poses from 2D points is an ill-defined problem, which can result in ambiguity. In that case, incoherent predictions are common, which requires a matching strategy between the estimated 3D poses and a structural model <ref type="bibr" target="#b6">[7]</ref>. As an alternative, the body joints can be represented relative to their parent joints, requiring the prediction of the delta between two neighbour joints <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. This approach reduces the variance in the target space. However, it introduces an accumulative error propagated from the root joint to the body extremities.</p><p>Another problem related to 3D pose estimation is the lack of rich visual data. Since precise 3D annotations depend on expensive and complex Motion Capture (MoCap) systems, public datasets are usually collected in controlled environment with static and clean background, despite having few subjects. To alleviate this problem, Mehta et al. <ref type="bibr" target="#b24">[25]</ref> proposed to first train a 2D model on data collected "in-the-wild" with 2D manual annotations, and then to use transfer learning to build a neural network that predicts 3D joint positions with respect to the root joint. Transfer learning is an useful but tricky technique. For that reason, in our approach, we decided to have a single training procedure that uses manually 2D annotated data in-the-wild simultaneously with high precise MoCap data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-stage architectures for human pose estimation</head><p>Multi-stage architectures have been widely used for human pose estimation, specially for the more established problem of 2D pose estimation <ref type="bibr" target="#b25">[26]</ref>, usually as sequential predictions <ref type="bibr" target="#b26">[27]</ref> or by means of recurrent networks <ref type="bibr" target="#b27">[28]</ref>. A common practice in previous methods is to regress heatmap representations corresponding to a map of scores for a given body joint. The refinement of such heatmaps is crucial for achieving good precision, as noted in <ref type="bibr" target="#b28">[29]</ref>. Following this idea, Newell et al. <ref type="bibr" target="#b7">[8]</ref> proposed the stacked hourglass architecture, which is essentially a sequence of U-nets, each one producing a new set of heatmaps that are refined by further hourglasses. Approaches based on heatmap estimation have two drawbacks: first, predicted heatmaps require an elevated resolution for acceptable precision, since the body joint coordinates are extracted in a post-processing stage based on the argument of the maximum a posteriori probability (MAP or argmax). The second limitation is the requirement for artificially generated ground truth heatmaps during training, since argmax is not differentiable. Contrarily, in our method we can have precise predictions with very low resolution feature maps, in addition to not requiring artificially generated ground truth.</p><p>On 3D scenarios, Zhou et al. <ref type="bibr" target="#b29">[30]</ref> benefits from 2D heatmaps to guide 3D pose regression, introducing a weakly-supervised approach for lifting 3D predictions from 2D data, and Pavlakos et al. <ref type="bibr" target="#b19">[20]</ref> extended the Staked Hourglass network to volumetric heatmaps prediction, on which the z coordinate is encoded in the additional heatmap dimension. However, the method proposed in <ref type="bibr" target="#b19">[20]</ref> suffers from the significant increase in the number of parameters and in the required memory to store all the intermediate values, due to the highly expensive volumetric heatmaps. This problem can be alleviated by the differentiable version of argmax <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b0">1]</ref>, also called integral regression in <ref type="bibr" target="#b8">[9]</ref>, but it remains dependent on a costly voxilized representation of the 3D space.</p><p>The method presented in this work differs from all previous approaches in several aspects. First, it departs from requiring volumetric representations by predicting pairs of heatmaps and depth maps. Second, differently from the stacked hourglass architecture, our method has intermediate supervision at different scales, providing different levels of semantic and resolution, which are all aggregated in a densely connected way for better predictions refinement. Third, after a single training procedure, our scalable network can be cut at different positions, providing a vast trade off for precision vs. speed. All these advantages result from the proposed architecture, as detailed next. The proposed network architecture is depicted in <ref type="figure">Fig. 1</ref>. The input of our method is an RGB image I ? R H?W ?3 with resolution H ? W , which is feed to the entry flow network. The entry-flow produces convolutional features with resolution R H/4?W/4?384 , which are then fed to a sequence of pyramids. The outputs of the network are a set of predicted 3D poses, designated by p l k ? R N ?3 , and optionally a set of joint confidence scores, designated by c l k ? R N ?1 , where N is the number of body joints, k is the pyramid index, and l is the level index. All prediction blocks are supervised during training. The motivation for a new architecture design is to provide an explicit multilevel supervision, enforcing the model to be able to represent the output independently on the resolution of feature maps. This approach allows the model to effectively combine low resolution feature maps, rich in semantic information, with high resolution features, containing more detailed information. In order to allow incrementally refined estimations, all predictions from both low and high resolutions are re-injected into the network. As a consequence of this densely supervised architecture, the network can offer early predictions with reduced computational time, or refined predictions with improved precision. The details about the proposed network are presented as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Scalable Sequential Pyramid Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network architecture</head><p>The global architecture of the proposed network ( <ref type="figure">Fig. 1)</ref> is essentially composed of a combination of four modules: entry-flow, downscaling and upscaling units, and prediction blocks. The role of the entry-flow (detailed in <ref type="table" target="#tab_0">Table 1</ref>) is to provide deep convolutional features extraction, which are successively downscaled and upscaled, respectively by downscaling and upscaling pyramids. Each pyramid is composed of a sequence of downscaling or upscaling units (DU or UU, see <ref type="figure" target="#fig_1">Fig. 2</ref>), interleaved with prediction blocks (PB) at each level. Prediction blocks are indexed by the pyramid index k ? {1, 2, . . . , K}, where K is the number of pyramids, and by the index level l ? {0, 1, . . . , L}, where L is the number of downscaling or upscaling steps performed, considering k = 1 and l = 0 the CNN features from the entry-flow. Note that in this arrangement, an odd k index corresponds to a downscaling pyramid and an even k index corresponds to an upscaling pyramid.</p><p>The basic building block for the pyramid networks is the separable residual unit ( <ref type="figure" target="#fig_1">Fig. 2a</ref>), which consists of a depth wise separable convolution <ref type="bibr" target="#b31">[32]</ref> with a residual connection. Our choice for depth wise separable convolutions is mainly due to its benefits in efficiency <ref type="bibr" target="#b32">[33]</ref>. One important advantage from our approach is the combination of features from different pyramids and levels. This is performed in both DU/UU, since they combine features from lower/higher levels, as well as features from previous pyramids.</p><formula xml:id="formula_0">+ Output: (H s ?W s ? N fout ) Input: (H s ?W s ?N fin ) SC k?k, N f (a) Sep. residual block + Output: (H s /2?W s /2?N fout ) Input: (H s ?W s ?N fin ) R 5?5, N fout MaxPooling 2?2 R 5?5, N fout (b) Downscaling unit + R 5?5, N fout UpSampling 2?2 R 5?5, N fout Input: (H s /2?W s /2?N fin ) Output: (H s ?W s ?N fout ) (c) Upscaling unit</formula><p>Details of the prediction block (PB) are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. It takes as input a feature map X l k , considering pyramid k and level l, and produces a set of heatmaps h l k and depth maps d l k , which are used for 3D pose regression (explained in Section 3.2). heatmaps and depth maps generation is defined in the following equations:</p><formula xml:id="formula_1">Y l k = ReLU (BN (SC(X l k ))),<label>(1)</label></formula><formula xml:id="formula_2">h l k = W k,l h * Y l k ,<label>(2)</label></formula><formula xml:id="formula_3">d l k = W k,l d * Y l k ,<label>(3)</label></formula><p>where Y l k is an intermediate feature representation, SC is a separable convolution, W k,l h and W k,l d are weight matrices with shape R N f ?N , respectively for heatmaps and depth maps projection, and * is the convolution operation. Additionally, each prediction block also produces a new feature map F l k , which combines the input features with predicted heatmaps and depth maps, and is used by next blocks and units for further improvements. This step is defined in equation 4:</p><formula xml:id="formula_4">F l k = X l k + Y l k + W k,l r * h l k + W k,l s * d l k ,<label>(4)</label></formula><p>where W k,l r and W k,l s are called re-injection matrices. Differently from the stacked hourglass <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20]</ref> architectures, where only the higher resolution features are supervised, we use intermediate supervision at every level of the pyramids. Adding more supervisions does not significantly increase the computational cost of our method, since contrarily to the stacked hourglass we do not need to generate artificial ground truth heatmaps. On the other hand, with intermediate supervisions in multiple levels we enforce the robustness of our method to variations in the scale of feature maps, while efficiently increasing the receptive field of the global network. Furthermore, our architecture injects the predictions from these intermediate supervisions back into the network by merging them with the current features. This allows the subsequent blocks to perform refining operations instead of full predictions.</p><formula xml:id="formula_5">SC 5?5, N f Input ( ): (H s ?W s ?N f ) Output ( ): (H s ?W s ?N f ) C 1?1, N C 1?1, N f C 1?1, N C 1?1, N f</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">3D pose regression approach</head><p>As discussed in Section 2, traditional regression methods use fully connected layers to learn a regression mapping from features to predictions. However, this approach usually gives sub-optimal solution. While methods in the state of the art are frequently based on detection, which requires expensive volumetric heatmap representations, regression approaches have the advantage of directly providing 3D pose prediction as joint coordinates without additional post-processing steps.</p><p>In our approach, we split the problem as 2D regression and depth estimation, using two different mappings: heatmaps for (x, y) coordinates and depth maps for z. For 2D regression, we based our approach on the soft-argmax <ref type="bibr" target="#b30">[31]</ref>, and for depth estimation, we propose an new attention mechanism guided by 2D joint estimation. Our method does not require any parameter and is fully differentiable. The next sections explain each part of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Soft-argmax for 2D regression</head><p>Let us redefine the softmax operation on a single heatmap h ? R H?W as:</p><formula xml:id="formula_6">?(h) i,j = e hi,j H l=1 W c=1 e h l,c ,<label>(5)</label></formula><p>where h i,j is the value of h at location (i, j) and H ? W is the heatmap size. Contrary to the more common cross-channel softmax, we use here a spatial softmax to ensure that each heatmap is L1 normalized and positive. Then, we define the soft-argmax as:</p><formula xml:id="formula_7">? d (h) = H i=1 W j=1 W i,j,d ?(h) i,j ,<label>(6)</label></formula><p>where d is a given component x or y, and W is a H ? W ? 2 weight matrix for both components (x, y). The matrix W can be expressed by its components W x and W y , which are 2D discrete normalized ramps, defined as follows:</p><formula xml:id="formula_8">W i,j,x = 2j ? 1 2W , W i,j,y = 2i ? 1 2H .<label>(7)</label></formula><p>Finally, given a heatmap h, the regressed location in the image plane is given by:</p><formula xml:id="formula_9">p img = (? x (h), ? y (h)) T .<label>(8)</label></formula><p>The soft-argmax operation can be seen as the 2D expectation of the normalized heatmap, which is a good approximation of the argmax function, considering that the exponential normalization results in a pointy distribution.</p><p>In order to integrate the soft-argmax layer into a deep neural network, we need its derivative with respect to h:</p><formula xml:id="formula_10">?? d (h) ?h i,j = W i,j,d ?(h) i,j (1 ? ?(h) i,j ) ? H l=1 W c=1 W l,c,d ?(h) i,j ?(h) l,c | l =i;c =j (9)</formula><p>The soft-argmax function can thus be integrated in a trainable framework by using back propagation and the chain rule on Equation <ref type="bibr" target="#b8">9</ref>. Moreover, similarly to what happens on softmax, the gradient is exponentially increasing for higher values, resulting in very discriminative response at the joint position. The soft-argmax layer can be easily implemented in recent frameworks by concatenating a spatial softmax followed by one non-trainable convolutional layer with 2 filters of size H ?W , with fixed parameters according to <ref type="bibr">Equation 7</ref>.</p><p>Unlike traditional argmax, soft-argmax provides sub-pixel accuracy, allowing good precision even with very low resolution. Additionally, our approach allows learning very discriminative heatmaps directly from the (x, y) joint coordinates without explicitly computing artificial ground truth. Samples of heatmaps learned by our approach are shown in <ref type="figure" target="#fig_7">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Joint based attention for depth estimation</head><p>For each body joint, we estimate its relative depth? with respect to the root joint, which is usually designated by the pelvis. Specifically, we define an attention mechanism for predicted depth maps based on the appearance information encoded in heatmaps. Considering one heatmap h and the respective depth map d, both with size R H?W , the estimated relative depth is given by:</p><formula xml:id="formula_11">z = H i=1 W j=1 d i,j e hi,j H i=1 W j=1 e hi,j ,<label>(10)</label></formula><p>which can be interpreted as a selection of relevant regions from d based on the response from h. In our implementation, values in depth maps are normalized in the interval [0, 1], corresponding to a range of depth prediction. The 3D poses estimated by our approach are composed by the (x, y) coordinates in pixels (Equation 8) and by the z coordinate relative to the root joint. In order to recover the absolute 3D pose in world coordinates, we require the absolute depth of the root joint and the camera calibration parameters to convert pixels into millimeters. We believe that estimating the absolute 3D pose directly in world coordinates is not the most relevant problem, since the camera calibration can affect such a prediction drastically. On the other hand, the relative position of joints with respect to the root is of high relevance, and usually is the only measure used to compare different methods. We show in the experiments that absolute depth of the root joint can be estimated without major impact on accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Joint confidence score</head><p>Additionally to the joint locations, we estimate the joint confidence score? c n , which corresponds to the probability of the n th joint being visible (or present, even if occluded) in the image. Given a normalized heatmap, any window with 2 ? 2 pixels is enough to regress a coordinate value with sub-pixel accuracy in a smaller squared region defined by the centers of the 2 ? 2 pixels, as depicted in <ref type="figure" target="#fig_3">Fig. 4</ref>. Therefore, we apply a summation with a 2 ? 2 sliding window on each normalized heatmap by using a SumPooling with stride 1, and take the maximum response as the confidence score. If the normalized heatmap is very pointy, the score is close to 1. On the other hand, if the normalized heatmap is smooth or has more than one separated region with high response, the confidence score drops. Despite giving an additional piece of information, the joint confidence score does not depend on additional parameters and is computationally negligible, compared to the cost of the convolutional layers. Additionally, by supervising this output we can enforce the network to learn pointy responses for body parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the proposed method quantitatively on two challenging datasets for 3D human pose estimation: Human3.6M <ref type="bibr" target="#b33">[34]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b24">[25]</ref>. We also use the manually annotated MPII Human Pose dataset (2D only) <ref type="bibr" target="#b34">[35]</ref> to improve the quality of low level visual features of our network by mixing it with the other two datasets in a 50%/50% ratio on each training batch. Details about the 3D human pose datasets used in our experiments are provided as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Human3.6M Human3.6M <ref type="bibr" target="#b33">[34]</ref> is a 3D human pose dataset composed of videos with 11 subjects performing 17 different activities, recorded by 4 cameras simultaneously, resulting in 3.6 million image frames. For each person, 17 joints are used in our method. The camera parameters are available, so it is possible to project the 3D joints to the image plane, as well as the inverse projection from points in the image plane plus depth back to world coordinates, where the error in computed in millimeters. On this dataset, we evaluate our method by measuring the mean per joint position error (MPJPE), which is a common metric used for this dataset. We followed the most common evaluation protocol <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b6">7]</ref> by taking five subjects for training (S1, S5, S6, S7, S8) and evaluating on two subjects (S9, S11) on one every 64th frames. On evaluation, the ground truth and the predicted poses are aligned on the root joint, and the error is computed on the remaining 16 joints. As in many similar approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b19">20]</ref>, we use ground truth person bounding boxes for image cropping and the absolute Z of the root joint to do the inverse projection. Nonetheless, we demonstrate in the ablation studies (Section 4.4) that errors in the absolute Z of the root joint are much less relevant than relative joint errors, and we also report our results using estimated absolute position.</p><p>MPI-INF-3DHP MPI-INF-3DHP <ref type="bibr" target="#b24">[25]</ref> is, to the best of our knowledge, the most recent dataset for 3D human pose estimation. It was recorded with a markerless MoCap system, which allows videos to be recorded in outdoor environment e.g. , TS5 and TS6 from testing. A total of 8 actors were recorded performing 8 activities sets each. The activities involve some complex exercising poses, which makes this dataset more challenging than Human3.6M. The authors proposed three evaluation metrics: the mean per joint position error, in millimeters, the 3D Percentage of Correct Keypoints (PCK), and the Area Under the Curve (AUC) for different threshold on PCK. The standard threshold for PCK is 150mm. Differently from previous work, we use the real 3D poses to compute the error instead of the normalized 3D poses, since the last one cannot be easily computed from the image plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>The proposed network was trained simultaneously on 3D pose regression and on joint confidence scores. For pose regression, we used the elastic net loss function (L1 + L2) <ref type="bibr" target="#b35">[36]</ref>:</p><formula xml:id="formula_12">L p = 1 N J N J n=1 p n ?p n 1 + p n ?p n 2 2 ,<label>(11)</label></formula><p>where p n andp n are respectively the ground truth and the predicted n th joint coordinates. We use directly the joint coordinates normalized to the interval [0, 1], where the top-left image corner corresponds to (0, 0), and the bottomright image corner corresponds to <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b0">1)</ref>. For the depth (z coordinate), the root joint is assumed to have z = 0.5, and a range of 2 meters is used to represent the remaining joints, which means that z = 0 corresponds to a depth of ?1 meter with respect to the root. For the joint confidence scores, we use the binary cross entropy loss function:</p><formula xml:id="formula_13">L c = 1 N J N J n=1 [(c n ? 1) log (1 ?? n ) ? c n log? n ],<label>(12)</label></formula><p>where c n and? n are respectively the ground truth and the predicted confidence scores. We use c n = 1 if the n th joint is present in the image and c n = 0 otherwise. The network architecture used in our experiments is implemented according to <ref type="figure">Fig. 1</ref> and is composed of 8 pyramids, divided as 4 downscaling and 4 upscaling pyramids, each one with 4 scales (K = 8 and L = 3). We optimize the network using back propagation and RMSprop with batches of 24 images and initial learning rate of 0.001, which is divided by 10 when validation score plateaus. We used standard data augmentation on all datasets, including: random rotations (?30 ? ), random bounding box rescaling with a factor from 0.7 to 1.3, and random brightness gain on color channels from 0.9 to 1.1. <ref type="figure" target="#fig_4">Figure 5</ref> shows some qualitative results of our method for 3D pose estimation, including challenging poses and some outdoor scenes. A quantitative evaluation is presented as follows. <ref type="table" target="#tab_1">Table 2</ref> shows our results compared to recent methods, where we achieve 50.2 mm average MPJPE considering multi-crop and 51.6 mm single-crop at 120 frames per second (FPS). Our approach achieves results comparable to the state-of-the-art overall, and improves individual activities up to 12.4% on "Photo" and 7.7% on "Sit down", which is the most challenging case. In general, our method improves state-of-the-art on individual activities even on single-crop at full speed, running on a desktop GeForce GTX 1080Ti GPU, which is, to the best of our knowledge, better than any previous method. Additionally, with the proposed architecture, our approach can be even faster with a small decrease in performance, as shown in the ablation study. We also evaluate our method using the estimated Z (depth) of root joints, which corresponds to our results when nothing else is specified. For doing that, we use a MLP with three layers and 128-128-256 neurons, which takes as input the image bounding box normalized coordinates (2D only) and a vector of visual features (F 3 1 ), and outputs the estimated absolute Z of the root joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on 3D pose estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Human3.6M</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">MPI-INF-3DHP</head><p>Our results on this dataset are presented in <ref type="table" target="#tab_2">Table 3</ref>. We reached a comparable result to Luo et al. <ref type="bibr" target="#b23">[24]</ref>, Improving their result on the average PCK by 1.4%, while producing inferences much faster (120 FPS on a GTX 1080 Ti vs 20 FPS from <ref type="bibr" target="#b23">[24]</ref> on a Titan XP). Furthermore, we are the only method to not use the universal normalized poses from this dataset, since our method requires the full pose in its original coordinates to allows camera projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>In this section we provide some additional experiments that show the behaviour of our method with respect to the proposed network architecture.</p><p>In <ref type="figure" target="#fig_5">Fig. 6a</ref>, we consider each intermediate supervision of the network as a valid output and we show the improvement on accuracy (error decreasing) with respect to the number of pyramids in the network. Additionally, the error with respect to each pyramid scale is also shown. We can clearly see that all the scales are improved by the sequence of pyramids, in such a way that in the last pyramid all scales present very similar error. This evolution can be better seen in <ref type="table" target="#tab_4">Table 5</ref>, where the error of all intermediate predictions are shown. Note that the precision of our regression method is invariant to the scale of the feature maps, since we reached excellent results with heatmaps of 4 ? 4 pixels. The same is not true for detection based approach, like in <ref type="bibr" target="#b19">[20]</ref>, since in their method the predictions are quantized by the argmax function. The error introduced by this quantization can be observed in <ref type="table" target="#tab_3">Table 4</ref>, where we compare our regression approach with ground truth volumetric heatmaps and argmax. Results using ground truth limb lengths.</p><p>One important characteristic of our network is that it offers an excellent trade off between performance and speed. In <ref type="figure" target="#fig_5">Fig. 6b</ref> we show the per joint error for four pyramids with their respective scales compared to the inference speed. Note that we are able to reach 55.5 millimeters error, which is still a good result on Human3.6M, at a very fast inference rate of 200 FPS. Additionally, in <ref type="figure" target="#fig_7">Fig 7 we</ref> show the our approach is able to learn very low resolution heatmap representations, while still achieving competitive results.</p><p>Finally, we demonstrate on <ref type="figure" target="#fig_5">Fig. 6c</ref> the influence of a bad prediction of the absolute root depth by adding a Gaussian noise on the ground truth reference. By adding a noise of 100 millimeters (about the same magnitude of the precision of our method on MPI-INF-3DHP), we have an increase in error inferior to 2 millimeters. This clearly reinforces our idea that the error on relative joint positions is much more relevant than the absolute offset of the root joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have presented a new regression method and a new scalable network architecture for 3D human pose estimation from still RGB images. The method is based on the proposed Scalable Sequential Pyramid Networks, which is a highly scalable network that can be very precise at a small computational cost and extremely fast with a small decrease in accuracy, with a single training procedure. The proposed parameter free regression approach is invariant to the    Results using the universal (normalized) ground truth poses. resolution of feature maps thanks to the soft-argmax operation, while performing state-of-the-art scores on important benchmarks for 3D pose estimation. Additionally, we provided some intuitions about the behaviour of our method in our ablation study, which demonstrates its effectiveness, specially for efficient predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>This work was partially founded by CNPq (Brazil) -Grant 233342/2014-1. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 Figure 1 :</head><label>21</label><figDesc>Downscaling Unit {p,?}k l : Output prediction for pyramid k and level l Global architecture of SSP-Net. The entry-flow extracts a preliminary feature map from the input image. These features are then fed through a sequence of CNNs composed of prediction blocks (PB) connected by alternating downscaling and upscaling units (DU and UU). Each PB outputs a supervised pose prediction that is refined by further blocks and units. SeeFigures 2 and 3for the architectural details of DU, UU, and PB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Elementary blocks of the proposed network. In (a), the separable residual block which is used as the basic building block. In (b) and (c), the downscaling unit (DU) and upscaling unit (UU) take as secondary input the feature maps F l k?1 issued from the previous pyramid. SC: (depthwise) separable convolution; R: separable residual block; Hs ? Ws: features size; N f in/f out : number of input/output features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Network architecture of the prediction block. Input features X l k (for pyramid k and level l) are used to produce heatmaps h l k and depth maps d l k , from which 3D pose and confidence scores are estimated. Output features F l k are a combination of input features and re-injected predictions. C: convolution; SC: separable convolution; Hs ? Ws: features size; N f : number of features; N : number of body joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Estimation of joint confidence scores. The blue squares represent the pixels in the normalized heatmap with its center marked as a red dot. The red square is the region on which a coordinate can be regressed, considering responses only on the 2 ? 2 window from pixels (1, 1) to<ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b1">2)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Input image samples (top), and their respective predicted 3D poses (bottom) for the MPI-INF-3DHP dataset, including two outdoor scenes (from testing).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Ablation study of our method. In (a), we shown the error performed by each intermediate supervision. The trade off between precision (related to the number of pyramids) and speed is shown in (b), for all the pyramid levels. In (c) we present the increase in reconstruction error with respect to a Gaussian noise injected on absolute root joint position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Input image samples (a), and their respective heatmaps indirectly learned for selected joints at different pyramid levels (b, c, d, e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Entry-flow network.</figDesc><table><row><cell></cell><cell>Layer</cell><cell cols="2">Filters Size/strides</cell><cell>Output</cell></row><row><cell></cell><cell>Input</cell><cell>3</cell><cell></cell><cell>256 ? 256</cell></row><row><cell></cell><cell>Convolution</cell><cell>64</cell><cell>7 ? 7/2</cell><cell>128 ? 128</cell></row><row><cell></cell><cell>Convolution</cell><cell>64</cell><cell>1 ? 1</cell></row><row><cell></cell><cell>Convolution</cell><cell>128</cell><cell>3 ? 3</cell></row><row><cell></cell><cell>Residual</cell><cell></cell><cell></cell><cell>128 ? 128</cell></row><row><cell></cell><cell>MaxPooling</cell><cell></cell><cell>3 ? 3/2</cell><cell>64 ? 64</cell></row><row><cell></cell><cell>Convolution</cell><cell>128</cell><cell>1 ? 1</cell></row><row><cell>2?</cell><cell>Convolution</cell><cell>256</cell><cell>3 ? 3</cell></row><row><cell></cell><cell>Residual</cell><cell></cell><cell></cell><cell>64 ? 64</cell></row><row><cell></cell><cell>MaxPooling</cell><cell></cell><cell>2 ? 2/2</cell><cell>32 ? 32</cell></row><row><cell></cell><cell>Convolution</cell><cell>192</cell><cell>1 ? 1</cell></row><row><cell>2?</cell><cell>Convolution</cell><cell>384</cell><cell>3 ? 3</cell></row><row><cell></cell><cell>Residual</cell><cell></cell><cell></cell><cell>32 ? 32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison results with previous work on Human3.6M using the MPJPE (millimeters errors) evaluation on reconstructed poses. AZ: using the absolute z of the root joint. MC: multi-crop, using 5 different bounding boxes with horizontal flip.</figDesc><table><row><cell>Methods</cell><cell>Dir.</cell><cell>Disc.</cell><cell>Eat</cell><cell cols="2">Greet Phone</cell><cell>Posing</cell><cell>Purch.</cell><cell>Sit</cell></row><row><cell>Pavlakos et al. [20]</cell><cell>67.4</cell><cell>71.9</cell><cell>66.7</cell><cell>69.1</cell><cell>71.9</cell><cell>65.0</cell><cell>68.3</cell><cell>83.7</cell></row><row><cell>Mehta et al. [25]</cell><cell>52.5</cell><cell>63.8</cell><cell>55.4</cell><cell>62.3</cell><cell>71.8</cell><cell>52.6</cell><cell>72.2</cell><cell>86.2</cell></row><row><cell>Martinez et al. [22]</cell><cell>51.8</cell><cell>56.2</cell><cell>58.1</cell><cell>59.0</cell><cell>69.5</cell><cell>55.2</cell><cell>58.1</cell><cell>74.0</cell></row><row><cell>Luo et al. [24]  *</cell><cell>49.2</cell><cell>57.5</cell><cell>53.9</cell><cell>55.4</cell><cell>62.2</cell><cell>52.1</cell><cell>60.9</cell><cell>73.8</cell></row><row><cell>Sun et al. [23]</cell><cell>52.8</cell><cell>54.8</cell><cell>54.2</cell><cell>54.3</cell><cell>61.8</cell><cell>53.1</cell><cell>53.6</cell><cell>71.7</cell></row><row><cell>Luvizon et al. [1]</cell><cell>49.2</cell><cell>51.6</cell><cell>47.6</cell><cell>50.5</cell><cell>51.8</cell><cell>48.5</cell><cell>51.7</cell><cell>61.5</cell></row><row><cell>Sun et al. [9]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours 120 FPS</cell><cell>46.9</cell><cell>50.9</cell><cell>49.9</cell><cell>47.5</cell><cell>51.9</cell><cell>46.2</cell><cell>49.2</cell><cell>61.7</cell></row><row><cell>Ours +AZ</cell><cell>46.1</cell><cell>50.2</cell><cell>50.2</cell><cell>47.5</cell><cell>52.0</cell><cell>45.9</cell><cell>48.5</cell><cell>62.3</cell></row><row><cell>Ours +AZ+MC</cell><cell>45.1</cell><cell>49.1</cell><cell>49.0</cell><cell>46.5</cell><cell>50.6</cell><cell>44.8</cell><cell>47.7</cell><cell>60.6</cell></row><row><cell>Methods</cell><cell>SitD.</cell><cell cols="2">Smoke Photo</cell><cell>Wait</cell><cell>Walk</cell><cell cols="2">WalkD. WalkP.</cell><cell>Avg</cell></row><row><cell>Pavlakos et al. [20]</cell><cell>96.5</cell><cell>71.4</cell><cell>76.9</cell><cell>65.8</cell><cell>59.1</cell><cell>74.9</cell><cell>63.2</cell><cell>71.9</cell></row><row><cell>Mehta et al. [25]</cell><cell>120.0</cell><cell>66.0</cell><cell>79.8</cell><cell>63.9</cell><cell>48.9</cell><cell>76.8</cell><cell>53.7</cell><cell>68.6</cell></row><row><cell>Martinez et al. [22]</cell><cell>94.6</cell><cell>62.3</cell><cell>78.4</cell><cell>59.1</cell><cell>49.5</cell><cell>65.1</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell>Luo et al. [24]</cell><cell>96.5</cell><cell>60.4</cell><cell>73.9</cell><cell>55.6</cell><cell>46.6</cell><cell>69.5</cell><cell>52.4</cell><cell>61.3</cell></row><row><cell>Sun et al. [23]</cell><cell>86.7</cell><cell>61.5</cell><cell>67.2</cell><cell>53.4</cell><cell>47.1</cell><cell>61.6</cell><cell>53.4</cell><cell>59.1</cell></row><row><cell>Luvizon et al. [1]</cell><cell>70.9</cell><cell>53.7</cell><cell>60.3</cell><cell>48.9</cell><cell>44.4</cell><cell>57.9</cell><cell>48.9</cell><cell>53.2</cell></row><row><cell>Sun et al. [9]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.6</cell></row><row><cell>Ours 120 FPS</cell><cell>66.5</cell><cell>53.4</cell><cell>55.2</cell><cell>45.5</cell><cell>42.1</cell><cell>55.6</cell><cell>45.9</cell><cell>51.6</cell></row><row><cell>Ours +AZ</cell><cell>66.8</cell><cell>53.4</cell><cell>54.7</cell><cell>45.2</cell><cell>41.9</cell><cell>54.7</cell><cell>45.5</cell><cell>51.4</cell></row><row><cell>Ours +AZ+MC</cell><cell>65.4</cell><cell>52.0</cell><cell>52.8</cell><cell>44.2</cell><cell>40.6</cell><cell>54.1</cell><cell>44.4</cell><cell>50.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison results with previous work on MPI-INF-3DHP using the PCK and AUC metrics (higher is better) and the MPJPE metric (lower is better), on reconstructed poses. AZ: using the absolute z of the root joint.</figDesc><table><row><cell>Methods</cell><cell>Std. Walk</cell><cell>Exer.</cell><cell>Sit Chair</cell><cell>Croush Reach</cell><cell>OnThe Floor</cell><cell cols="2">Sport Misc.</cell><cell></cell><cell>Avg</cell><cell></cell></row><row><cell></cell><cell cols="4">PCK PCK PCK PCK</cell><cell>PCK</cell><cell cols="3">PCK PCK PCK</cell><cell cols="2">AUC MPJPE</cell></row><row><cell>Zhou et al. [30]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>69.2</cell><cell>32.5</cell><cell>-</cell></row><row><cell>Mehta et al. [25]</cell><cell>86.6</cell><cell>75.3</cell><cell>74.8</cell><cell>73.7</cell><cell>52.2</cell><cell>82.1</cell><cell>77.5</cell><cell>75.7</cell><cell>39.3</cell><cell>117.6</cell></row><row><cell>Mehta et al. [6]</cell><cell>87.7</cell><cell>77.4</cell><cell>74.7</cell><cell>72.9</cell><cell>51.3</cell><cell>83.3</cell><cell>80.1</cell><cell>76.6</cell><cell>40.4</cell><cell>124.7</cell></row><row><cell>Luo et al. [24]</cell><cell cols="2">90.4 79.1</cell><cell cols="2">88.5 81.6</cell><cell>66.3</cell><cell cols="2">91.9 92.2</cell><cell>81.8</cell><cell cols="2">45.2 89.4</cell></row><row><cell>Ours +AZ</cell><cell>87.1</cell><cell>85.4</cell><cell>85.9</cell><cell>81.6</cell><cell>68.5</cell><cell>88.2</cell><cell>83.0</cell><cell>83.2</cell><cell>44.3</cell><cell>96.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results on Human3.6M (millimeters error), comparing predictions using ground truth heatmaps and argmax vs. our regression approach.</figDesc><table><row><cell>Method / resolution</cell><cell>s = 4</cell><cell>s = 8</cell><cell>s = 16</cell><cell>s = 32</cell></row><row><cell>Volumetric GT heatmaps (s ? s ? s) + argmax</cell><cell>233.9</cell><cell>128.6</cell><cell>59.9</cell><cell>31.0</cell></row><row><cell>Our regression approach (soft-argmax)</cell><cell>53.0</cell><cell>51.8</cell><cell>51.4</cell><cell>51.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Mean per joint position error (MPJPE) in millimeters for all intermediate supervisions of the SSP-Net on Human3.6M. Odd pyramid numbers correspond to Downscaling Pyramids, and even numbers correspond to Upscaling Pyramids.</figDesc><table><row><cell cols="2">Scale Features res.</cell><cell></cell><cell></cell><cell cols="4">Pyramid number / MPJPE</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>L 0</cell><cell>32 ? 32</cell><cell>-</cell><cell>64.1</cell><cell>-</cell><cell>55.3</cell><cell>-</cell><cell>52.4</cell><cell>-</cell><cell>51.6</cell></row><row><cell>L 1</cell><cell>16 ? 16</cell><cell cols="8">85.5 65.5 60.1 55.5 55.3 52.1 51.8 51.4</cell></row><row><cell>L 2</cell><cell>8 ? 8</cell><cell cols="8">71.7 67.1 58.5 57.1 53.1 53.0 52.1 51.8</cell></row><row><cell>L 3</cell><cell>4 ? 4</cell><cell>68.7</cell><cell>-</cell><cell>58.9</cell><cell>-</cell><cell>54.2</cell><cell>-</cell><cell>53.0</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Building statistical shape spaces for 3d human modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wuhrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Helten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2017.02.018</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2017.02.018" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="276" to="286" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DeepPose: Human Pose Estimation via Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3487" to="3494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation using a joint pixel-wise and part-wise formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="DOI">10.1145/3072959.3073596</idno>
		<ptr target="http://gvv.mpi-inf.mpg.de/projects/VNect/" />
		<title level="m">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ramanan, 3d human pose estimation = 2d pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<title level="m">Stacked Hourglass Networks for Human Pose Estimation, European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Integral human pose regression</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kakadiaris, 3d human pose estimation: A review of the literature and analysis of covariates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2016.09.002</idno>
		<ptr target="https://doi.org/10.1016/j.cviu.2016.09.002" />
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A very simple framework for 3d human poses estimation using a single 2d image: Comparison of geometric moments descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Atrevi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vivet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Duculty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Emile</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2017.06.024</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2017.06.024" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="389" to="401" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2220" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Posebits for monocular human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.300</idno>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2345" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structured prediction of 3d human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference<address><addrLine>York, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09-19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Iterated second-order label sensitive pooling for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1661" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Andres, B. Schiele, Arttrack: Articulated multi-person tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.142</idno>
	</analytic>
	<monogr>
		<title level="m">30th IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1293" to="1301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recovering 3d human pose from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="58" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision ECCV 2016 Workshops</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08985abs/1701.08985</idno>
		<ptr target="http://arxiv.org/abs/1701.08985" />
		<title level="m">Deep multitask architecture for integrated 2d and 3d human sensing</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07432</idno>
		<title level="m">Compositional human pose regression</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Orinet: A fully convolutional network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<ptr target="http://gvv.mpi-inf.mpg.de/3dhp_dataset" />
	</analytic>
	<monogr>
		<title level="m">2017 Fifth International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Convolutional pose machines</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<title level="m">Chained Predictions Using Convolutional Neural Networks, European Conference on Computer Vision (ECCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down reasoning with hierarchical rectified gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human pose estimation via Convolutional Part Heatmap Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Human pose regression by combining indirect part detection and contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<idno type="arXiv">CoRRabs/1710.02322.arXiv:1710.02322</idno>
		<ptr target="http://arxiv.org/abs/1710.02322" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>CoRR abs/1704.04861</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<title level="m">2D Human Pose Estimation: New Benchmark and State of the Art Analysis</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
