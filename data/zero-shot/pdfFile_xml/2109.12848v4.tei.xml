<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A General Gaussian Heatmap Label Assignment for Arbitrary-Oriented Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanchao</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Wei</forename><forename type="middle">Li</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Xiang-Gen</forename><surname>Xia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Ran</forename><forename type="middle">Tao</forename></persName>
						</author>
						<title level="a" type="main">A General Gaussian Heatmap Label Assignment for Arbitrary-Oriented Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Arbitrary-oriented object</term>
					<term>convolutional neural network</term>
					<term>gaussian heatmap</term>
					<term>label assignment</term>
					<term>object detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, many arbitrary-oriented object detection (AOOD) methods have been proposed and attracted widespread attention in many fields. However, most of them are based on anchor-boxes or standard Gaussian heatmaps. Such label assignment strategy may not only fail to reflect the shape and direction characteristics of arbitrary-oriented objects, but also have high parameter-tuning efforts. In this paper, a novel AOOD method called General Gaussian Heatmap Label Assignment (GGHL) is proposed. Specifically, an anchor-free object-adaptation label assignment (OLA) strategy is presented to define the positive candidates based on two-dimensional (2-D) oriented Gaussian heatmaps, which reflect the shape and direction features of arbitrary-oriented objects. Based on OLA, an oriented-boundingbox (OBB) representation component (ORC) is developed to indicate OBBs and adjust the Gaussian center prior weights to fit the characteristics of different objects adaptively through neural network learning. Moreover, a joint-optimization loss (JOL) with area normalization and dynamic confidence weighting is designed to refine the misalign optimal results of different subtasks. Extensive experiments on public datasets demonstrate that the proposed GGHL improves the AOOD performance with low parameter-tuning and time costs. Furthermore, it is generally applicable to most AOOD methods to improve their performance including lightweight models on embedded platforms. ). <ref type="figure">p1 (x1,y1)   p3(x3,y3)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Down-Sample</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CNN Model</head><p>Plane <ref type="figure">Label: [x1,y1,x2,y2,x3,y3,x4,y4</ref>,class] Labels: (8+numclass) ? N arXiv:2109.12848v4 [cs.CV] 3 Jan 2022 vmin vmax ? HBB OBB HBB Anchor boxes OBB of anchor-based methods vmin vmax ? Positive location OBB Anchor boxes OBB BBox of FCOS vmin vmax OBB HBB ? t b r l OBB of dense-points methods</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In the past few years, the continued innovation of convolutional neural network (CNN) based object detection (OD) methods has emerged <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. As one of the more specialized tasks of OD, the AOOD task also follows the trend and develops rapidly. It detects objects more accurately through bounding boxes with directions in scenes of remote sensing <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b7">[7]</ref>, retail <ref type="bibr" target="#b8">[8]</ref>, text <ref type="bibr" target="#b9">[9]</ref>, etc.</p><p>Along with the intensive studies, the CNN structure of AOOD models has become more and more complicated to make the distribution of extracted features approximate to the distribution of ground truth. However, it is not the only way to improve the detection performance through extracting features using a CNN structure <ref type="bibr" target="#b10">[10]</ref> as we shall see below. As shown in step 2 of <ref type="figure" target="#fig_0">Fig. 1</ref>, more than one location in a feature map that is used to detect the object. In this regard, most CNN-based OD methods <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> assign a label to many candidate locations as ground truth during the CNN training to improve the robustness, which is called label assignment <ref type="bibr" target="#b10">[10]</ref>. From a more macro perspective, the CNN training is essentially a process of learning the one-to-many mapping from the predictions of many candidate locations to a labeled object. Different one-to-many label assignment strategies directly affect the detection performance by generating different ground truths (called sample spaces) for training. Therefore, to improve the detection performance, one way is to use a more complex CNN, i.e., a more complex approximation function. The other way is to design a label assignment strategy for constructing a better sample space that is more in line with the characteristics of the object's shape and direction. The latter is as important as the former in object detection tasks. Most of the AOOD methods, such as SCRDet <ref type="bibr" target="#b11">[11]</ref>, LO-Det <ref type="bibr" target="#b12">[12]</ref>, DAL <ref type="bibr" target="#b13">[13]</ref>, CenterMap <ref type="bibr" target="#b14">[14]</ref>, DCL <ref type="bibr" target="#b15">[15]</ref>, Oriented R-CNN <ref type="bibr" target="#b16">[16]</ref>, etc., use the anchor-based label assignment strategy, as shown in <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>. However, this strategy may lead to the mismatch of positive and negative (P&amp;N) locations when the default anchor boxes cannot cover a specific shape <ref type="bibr" target="#b10">[10]</ref>, especially in the complex scene. Besides, the anchor-based strategy requires many dataset-dependent hyperparameters <ref type="bibr" target="#b17">[17]</ref>, which costs a lot of efforts for tuning when the dataset is changed <ref type="bibr" target="#b18">[18]</ref>. Regarding the above issues, anchor-free methods like FCOS <ref type="bibr" target="#b2">[3]</ref> and CenterNet <ref type="bibr" target="#b19">[19]</ref> redefine P&amp;N locations <ref type="bibr" target="#b10">[10]</ref>  and get rid of the dependence on anchors' shape. Among them, dense-points methods, such as FCOS <ref type="bibr" target="#b2">[3]</ref>, IENet <ref type="bibr" target="#b20">[20]</ref>, AOPG <ref type="bibr" target="#b21">[21]</ref>, etc., relax the sample space constraints, as shown in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>, which may cause some negative locations to be misallocated as positive locations. While the key-point methods like CenterNet <ref type="bibr" target="#b19">[19]</ref>, BBAVectors <ref type="bibr" target="#b22">[22]</ref>, O 2 -DNet <ref type="bibr" target="#b23">[23]</ref>, etc., use a stricter positive location assignment strategy, as shown in <ref type="figure" target="#fig_1">Fig. 2 (c)</ref>, which relies on higher resolution feature maps and makes the number of P&amp;N locations more unbalanced. Furthermore, the above label assignment strategies do not fully consider the characteristics of the object's shape and direction when defining P&amp;N locations. Therefore, the expected label assignment strategy should construct sample space without anchor boxes and define P&amp;N locations that are more in line with the characteristics of objects in the AOOD task.</p><p>An expected training sample space also requires a proper objective function to guide the model to learn higher quality features, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Whether it is OD or AOOD task, the mainstream objective function paradigm is to optimize classification and OBB regression tasks independently and minimize the sum of each loss as the optimization goal <ref type="bibr" target="#b24">[24]</ref>. However, there may be one of the two situations: an accurately localized object has a lower classification score, or an accurately classified object has a lower OBB regression score. Therefore, as analyzed by PISA <ref type="bibr" target="#b24">[24]</ref>, Free-Anchor <ref type="bibr" target="#b17">[17]</ref>, AutoAssign <ref type="bibr" target="#b18">[18]</ref>, etc., jointly optimizing different subtasks is a more reasonable goal. Furthermore, it should be considered that different objects may have different numbers of positive locations, and different locations contribute unequally to the loss function.</p><p>In summary, mismatched, overly slack, or unduly strict label assignment strategies make it difficult for constructed training sample space to adapt to object characteristics and may have many hyperparameters. Moreover, the inconsistent loss function for different subtasks makes it more challenging to learn the optimal parameters of CNN in the sample space. Therefore, for a CNN-based AOOD method, compared with developing a dazzling network structure, it is even more important to construct an object-adaptation label assignment strategy and design a goal-consistent loss function. In this regard, a novel and practical AOOD method with higher performance and fewer hyperparameters called General Gaussian Heatmap Label Assignment (GGHL) is proposed. The contributions of this work are summarized as follows:</p><p>1) An object-adaptation label assignment (OLA) strategy without any prior anchor boxes is proposed based on twodimensional (2-D) oriented Gaussian heatmaps. It simplifies the P&amp;N location definition and makes the distribution of positive locations more flexible to fit the object's size and direction.</p><p>2) An oriented-bounding-box representation component (ORC) based on the distances from the positive point to OBB vertexes is developed, which indicates any OBBs without anchor boxes. Furthermore, an object-adaptive weight adjustment mechanism (OWAM) is designed to adaptively adjust the Gaussian center prior weights of different locations and used to weight the loss of different P&amp;N locations.</p><p>3) A joint-optimization loss (JOL) with area normalization and dynamic weighting is proposed. It refines the misaligned optimization goals between positive and negative locations, OBB regression and classification tasks by jointly optimizing their likelihood function (LF). Besides, it balances the model's learning preferences for objects of different categories with different sizes at different locations.</p><p>The remainder of this paper is organized as follows. Section II reviews and analyzes the related works. Section III presents a detailed description of the proposed GGHL. In Section IV, extensive experiments are conducted, and the results are discussed. Finally, conclusions are summarized in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Arbitrary-Oriented Object Detection</head><p>Benefiting from the open-source AOOD datasets annotated with OBBs in the scenes like remote sensing <ref type="bibr" target="#b4">[5]</ref>, the prediction of the OD model has become more refined, which helps to accurately locate the object in the image and reflect its shape and direction. In the AOOD task, whether the two-stage methods <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref> or the one-stage methods <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13]</ref>, most of them adopt the anchor-box-based framework due to its mature application in various OD tasks. However, since oriented anchors are more prone to mismatch problems and have more hyperparameters than horizontal anchors, many works have dealt with them. For example, Ding et al. <ref type="bibr" target="#b25">[25]</ref> transformed ROI to rotated ROI for avoiding a large number of oriented anchors in the two-stage detector. Xu et al. <ref type="bibr" target="#b26">[26]</ref> proposed a gliding vertex method to represent OBBs, the model of which is based on horizontal anchors without setting oriented anchors with multiple angles. DAL <ref type="bibr" target="#b13">[13]</ref> analyzed and proposed a dynamic matching and assignment strategy.</p><p>Oriented R-CNN <ref type="bibr" target="#b16">[16]</ref> proposed an oriented RPN to directly generate oriented proposals in a nearly cost-free manner and employed the midpoint offsets to represent OBBs based on Gliding Vertex <ref type="bibr" target="#b26">[26]</ref>. To remove anchor boxes, BBAVectors <ref type="bibr" target="#b22">[22]</ref>, DRN <ref type="bibr" target="#b8">[8]</ref>, O 2 -DNet <ref type="bibr" target="#b23">[23]</ref>, etc., employed the anchor-free framework and designed new OBB representation components. AOPG <ref type="bibr" target="#b21">[21]</ref> abandoned the horizontal boxes-related operations and generated oriented boxes by Coarse Location Module in an anchor-free manner. However, these anchor-free AOOD methods do not consider the characteristics of the object's shape and direction while just borrowing label assignment strategies from other OD tasks. In addition, few other methods like CSL <ref type="bibr" target="#b27">[27]</ref> predict oriented objects through angle classification. <ref type="figure" target="#fig_1">Fig. 2</ref> summarizes the mainstream label assignment and OBB representation strategies of the existing AOOD methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Label Assignment Strategy</head><p>The label assignment is a core issue that a CNN model based on the dense-positive detection paradigm needs to consider. Faster R-CNN <ref type="bibr" target="#b0">[1]</ref> introduces the anchor-based label assignment strategy to explicitly enumerate the prior information of different scales and aspect ratios. This strategy introduces many hyperparameters that depend on the datasets <ref type="bibr" target="#b17">[17]</ref>. It means that one needs to spend a lot of efforts adjusting the hyperparameters when the dataset is changed <ref type="bibr" target="#b18">[18]</ref>. Moreover, these easily overlooked recessive costs cannot be reduced by a lightweight CNN model <ref type="bibr" target="#b12">[12]</ref>. To solve the problem that the anchor-based strategy relies on many hyperparameters and may have mismatches, some OD methods, such as FCOS <ref type="bibr" target="#b2">[3]</ref> and CenterNet <ref type="bibr" target="#b19">[19]</ref>, designed different anchor-free assignment strategies, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. ATSS <ref type="bibr" target="#b10">[10]</ref> analyzed and suggested that the gap between the anchor-box-based strategy and the anchor-free strategy lies in the definition of P&amp;N locations. Borrowing from the learning-to-match strategy of FreeAnchor <ref type="bibr" target="#b17">[17]</ref>, AutoAssign <ref type="bibr" target="#b18">[18]</ref> further let the model learn to define P&amp;N locationcs and assign labels automatically. However, the existing label assignment strategies do not fully consider the characteristics of the object location, shape, and direction when defining P&amp;N locations. Therefore, how to design a more appropriate label allocation strategy for oriented objects remains to be explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss Function for AOOD</head><p>Most existing AOOD methods still follow the classic OD loss paradigm that optimizes the OBB regression and object classification tasks separately <ref type="bibr" target="#b28">[28]</ref><ref type="bibr" target="#b29">[29]</ref><ref type="bibr" target="#b30">[30]</ref>. The difference between them and ordinary OD loss is an additional loss related to the direction of OBBs. For instance, PIoU <ref type="bibr" target="#b28">[28]</ref> calculated the approximate IoU of the OBB and ground truth through pixel counting; RIL <ref type="bibr" target="#b29">[29]</ref> used the Hungarian algorithm to determine the optimal matching; GWD <ref type="bibr" target="#b30">[30]</ref> represented the OBB regression loss by the distance of the Gaussian distributions. Furthermore, KLD <ref type="bibr" target="#b31">[31]</ref> used the Kullback-Leibler divergence between the Gaussian distributions as the regression loss, which dynamically adjusted the parameter gradients according to the characteristics of the object. While DCL <ref type="bibr" target="#b15">[15]</ref> further optimizes the accuracy and efficiency of angle classification loss based on CSL <ref type="bibr" target="#b27">[27]</ref>. Although these methods are effective, they did not consider the inconsistency of OBB regression and object classification optimization goals and relied on a large number of anchor boxes. In ordinary OD tasks, although PISA <ref type="bibr" target="#b24">[24]</ref>, FreeAnchor <ref type="bibr" target="#b17">[17]</ref>, AutoAssign <ref type="bibr" target="#b18">[18]</ref>, etc., analyzed this problem, they did not consider the direction and shape of OBBs and were not used in the AOOD task. Moreover, they did not notice that the contributions to the loss function of different objects and different locations are different, which needs to be studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED GGHL FRAMEWORK</head><p>The framework of the proposed GGHL is shown in <ref type="figure">Fig. 3</ref>, which is mainly composed of three parts: the proposed label assignment strategy OLA, the CNN model with developed ORC, and the designed objective function JOL. First, each label is assigned one-to-many to the Gaussian candidate locations in the feature maps through the proposed OLA strategy. Second, a CNN model is constructed to extract features from the input images. Then, the proposed ORC encodes these features to predict the OBB and category at each positive location. Furthermore, the Gaussian prior weight of each positive candidate location is adjusted by the designed CNNlearnable OWAM to fit the object's shape adaptively. Third, the joint-optimization loss between the ground truth of the constructed training sample space and the prediction of the CNN model is calculated. Finally, the CNN model is trained until the loss converges to obtain the optimal parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. OLA Strategy</head><p>In the previous works, although anchor-based methods, e.g., CenterMap <ref type="bibr" target="#b14">[14]</ref>, introduced "Gaussian-like" or "Centernesslike" <ref type="bibr" target="#b2">[3]</ref> weighting mechanisms for positive candidates, they are still essentially based on maximum IoU matching to define P&amp;N samples. As mentioned before, such matching strategies suffer from mismatch risks especially in dense object scenarios and they rely on a large number of hyperparameters. Methods like GWD <ref type="bibr" target="#b30">[30]</ref> mainly employ 2-D Gaussians for loss calculation, and its label assignment is still based on anchor boxes. CenterNet <ref type="bibr" target="#b19">[19]</ref>, BBAvectors <ref type="bibr" target="#b22">[22]</ref>, DRN <ref type="bibr" target="#b8">[8]</ref>, etc., also use the 2-D Gaussian distribution to define positive candidate locations, as shown in <ref type="figure" target="#fig_2">Fig. 4 (b)</ref>. However, their Gaussian heatmap of each object is a circle (the standard Gaussian distribution), which may not well reflect the shape and direction of an object. Besides, they only take Gaussian peak points as positive locations, which need to detect objects on higher resolution feature maps (stride=4) with higher computational complexity and more unbalanced P&amp;N locations. In contrast, the proposed OLA uses an oriented elliptical Gaussian region to represent an object's positive candidate set intuitively. Furthermore, the objects are assigned to lowerresolution feature maps with different scales (stride=8, <ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b32">32)</ref> according to their sizes, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref> (d) (e) (f), which has lower computational complexity and is compatible with the mainstream Backbone-FPN <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b33">33]</ref> pipeline in OD tasks.</p><p>Different from the existing methods, the proposed OLA strategy directly uses the 2-D Gaussian for label assignment to make the assigned candidates more in line with objects' shapes and directions, and alleviates the mismatch problem of anchor-based methods in dense instance scenarios. More specifically, the proposed OLA more fully discusses the relationship between 2-D Gaussian and geometric transformations in oriented object label assignment from a theoretical perspective. Based on this, the technical details to be considered for 2-D Gaussian label assignment are explained, including multi-scale assignment, overlap problem in the assignment, discussion of Gaussian radius, etc.</p><p>1) First, a general 2D Gaussian distribution is used to represent the positive candidate area with rotation and scaling, and the locations of the entire Gaussian region are regarded as positive locations and given different weights according to the Gaussian density function, compared to just taking the point at Gaussian peak as the positive locations.</p><p>Specifically, the Gaussian probability density function (PDF) is represented as</p><formula xml:id="formula_0">f (X) = 1 ? 2?C ? e ? 1 2 (X?u) T C ?1 (X?u) ,<label>(1)</label></formula><p>where X = [x, y] T ? N (?, C) contains two random variables in the two dimensions. ? ? R 2 represents the mean vector, and the non-negative semi-definite real matrix C ? R 2?2 represents the covariance matrix of the two variables. The real symmetric matrix C is orthogonally diagonalized and decomposed into</p><formula xml:id="formula_1">C = AA T = Q?Q T = Q? 1/2 Q? 1/2 T .<label>(2)</label></formula><p>Algorithm 1: Generate the Gaussian Candidate Region Input: Labels, each of which contains four vertices ((x1, y1), (x2, y2), (x3, y3), (x4, y4)) of the OBB: represent an OBB); number of labels N l Output: General Gaussian heatmap F 1 for 1 to N l do <ref type="bibr" target="#b1">2</ref> Pre-process the label to get ?, Q, ? ; <ref type="bibr" target="#b2">3</ref> Calculate the threshold thr = f (x b , y b ) at the end point (x b , y b ) of the semi-axis according to ? and Eq. 4, which is explained in Section III-A-2) ; <ref type="bibr" target="#b3">4</ref> for min(x1, x2, x3, x4) to max(x1, x2, x3, x4) do <ref type="bibr" target="#b4">5</ref> for min(y1, y2, y3, y4) to max(y1, y2, y3, y4) do <ref type="bibr" target="#b5">6</ref> Calculate f (x, y) according to Eq. 4 ; Thus,</p><formula xml:id="formula_2">(X ? ?) T C ?1 (X ? ?) = Q? 1/2 T (X ? ?) T Q? 1/2 T (X ? ?) ,<label>(3)</label></formula><p>where Q is a real orthogonal matrix, and ? is a diagonal matrix composed of the eigenvalues of descending order. The Gaussian probability density function is transformed into</p><formula xml:id="formula_3">f (X) = 1 ? 2?Q?Q T ?e ? 1 2 Q? 1 2 T (X??) T Q? 1 2 T (X??) . (4)</formula><p>From the perspective of geometric transformation, the mean vector ? = [? 1 , ? 2 ] T controls the spatial translation. The real orthogonal matrix Q is a rotation in this case:</p><formula xml:id="formula_4">Q = cos ? ? sin ? sin ? cos ? ,<label>(5)</label></formula><p>where ? denotes the angle of rotation. Because ?Q and Q are the same in this case, ? ? [0, ?). The diagonal matrix ? composed of eigenvalues represents the scaling, that is</p><formula xml:id="formula_5">? = SS T = ? 1 ? 2 = s 2 1 s 2 2 ,<label>(6)</label></formula><p>where the eigenvalues ? 1 and ? 2 represent the square of the semi-major axis s 1 and the square of the semi-minor axis s 2 of the ellipse, respectively. Finally, the distribution becomes the standard Gaussian distribution of [0, 0] T mean vector and I 2?2 covariance matrix, where I 2?2 is the 2?2 identity matrix.</p><p>In summary, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref> (c), probability density of any 2-D Gaussian distribution f (X) is obtained by a linear transformation from a standard 2-D Gaussian distribution (two random variables are independent Gaussian random variables with normal distribution). According to Eq. 4, a P&amp;N location distribution map F is generated through Algorithm 1. Define the element at (x, y) of F as F x,y ; and define f (x, y) ? [0, 1] as the Gaussian value at F x,y calculated by Eq. 4, which is normalized in each generated Gaussian region respectively. If f (x, y) = 0, this location is defined as negative (background), F x,y = 0. If f (x, y) &gt; 0, this location is defined as positive (foreground), F x,y = f (x, y), and the value of f (x, y) represents the weight of this location in the Gaussian region it belongs to.</p><p>2) Second, the problem of possible overlap of Gaussian regions needs to be considered in the assignment process. Unlike FCOS <ref type="bibr" target="#b2">[3]</ref> or CenterMap <ref type="bibr" target="#b14">[14]</ref>, which assign overlapping region labels to instances with smaller areas, the proposed OLA allows more flexibility to assign labels for each candidate location. Specifically, if a location is contained in different Gaussian regions, it is assigned to the region that has the largest f (x, y). This location is selected as the candidate to predict the object belongs to this Gaussian region. Moreover, the calculation of weights using Gaussian PDFs does not suffer from interpolation approximation problems encountered during the rotation of "Centerness" maps <ref type="bibr" target="#b14">[14]</ref>. After determining the positive candidate locations, other parameters in the labels also need to be assigned to them, see Section III-B for details.</p><p>3) Third, the spatial and scale extents of the candidate regions using the above strategy need to be carefully studied. First, a bounding box centered at the Gaussian peak location (called C-BBox) is computed based on the assigned labels. Then, it is assumed that many bounding boxes of different sizes centered at the other Gaussian candidate locations are generated. At a location, if there exists a bounding box whose Intersection over Union (IoU) with the C-BBox is greater than the threshold T IoU , this location is selected as a positive location. As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, these positive locations form a subset of the original Gaussian candidate locations (appearing as a smaller ellipse that is co-centered with the original Gaussian ellipse), and its semi-axis length is</p><formula xml:id="formula_6">r c i = 1 ? T IoU 2 ? r i , i = 1, 2,<label>(7)</label></formula><p>where r i , i = 1, 2, represents the semi-axis lengths of the original Gaussian ellipse. Then, the Gaussian boundary threshold in Algorithm 1 is calculated from r c i . The purpose of the above reasoning is to explain the relationship between the Gaussian range and the IoU metric, and in practice it is not necessary to calculate the IoU during label assignment. Considering the versatility of multiple criteria, T IoU is set to 0.3, which is the same as many classic methods, like Faster R-CNN <ref type="bibr" target="#b0">[1]</ref> and YOLO <ref type="bibr" target="#b33">[33]</ref>.</p><p>In order to detect objects of different sizes on feature maps of different scales, objects' OBBs with different sizes are assigned to feature maps with different down-sampling rates stride m = 2 m+3 , m = 1, 2, 3, as ground truth. The generated F m from Algorithm 1 of different scales are visualized in <ref type="figure" target="#fig_2">Fig. 4</ref> (d) (e) (f). To ensure that more than one positive candidate is generated on a certain scale after assignment, we set max</p><formula xml:id="formula_7">i (r c i ) /stride m ? 1, that is, max i (2r i ) ? 2?stridem 1?T IoU</formula><p>for each m, m = 1, 2, 3. Define the lengths of the four sides of an OBB as d j , j = 1, 2, 3, 4, then max</p><formula xml:id="formula_8">j (d j ) = max i (2r i ) ? 2?stridem 1?T IoU</formula><p>because when calculating the diagonal matrix ? in Eq. 6 to generate the Gaussian ellipse, half the values of the length and width of the OBB are used as s 1 and s 2 . Thus, introduce a hand-crafted hyperparameter ? = 3 to get two boundary values of the three assignment ranges, they are</p><formula xml:id="formula_9">range 1 = ? ? 2 ? stride 1 1 ? T IoU , range 2 = ? ? 2 ? stride 3 1 ? T IoU .<label>(8)</label></formula><p>When max</p><formula xml:id="formula_10">j (d j ) ? (1, range 1 ], max j (d j ) ? (range 1 , range 2 ],</formula><p>and max j (d j ) ? range 2 , ? 2len img , the object is assigned to feature maps with down sampling rates stride 1 , stride 2 , and stride 3 , respectively. len img represents the length or width of the image input to CNN. The hyperparameter ? is the only hand-crafted hyperparameter in the proposed GGHL. In Section IV, the setting of ? will be discussed later.</p><p>B. ORC, OWAM, and CNN Model 1) Oriented-bounding-box representation component (ORC). The proposed ORC is used to encode the ground truth labels and CNN's predictions to represent objects in feature maps by their positive locations, OBBs, and categories. The existing OBB representation methods are divided into two main categories: angle-based and vertex-based. The anglebased methods, e.g., CenterMap <ref type="bibr" target="#b14">[14]</ref>, only represent rotated rectangular bounding boxes, and the problem of periodicity and mutation in angle regression has been analyzed in GWD <ref type="bibr" target="#b30">[30]</ref>. The vertex-based methods, such as Gliding Vertex <ref type="bibr" target="#b26">[26]</ref>, represent more other shapes of quadrilaterals, but do not account for the case where the vertices do not fall on the circumscribed HBB. Moreover, these OBB representations are based on anchor boxes, which are inflexible and depend on many anchor hyperparameters. The proposed ORC follows the simple principle of anchor-free 2-D Gaussian assignment, which directly represents the OBB using the horizontal and vertical components of the distances from each Gaussian candidate position to the four vertices of the OBB, as shown in <ref type="figure" target="#fig_5">Fig. 6</ref>. The proposed ORC is free from the dependence on the anchors in decoding the OBB and fits naturally with the proposed OLA. Moreover, the proposed ORC addresses the undiscussed case in Gliding Vertex <ref type="bibr" target="#b26">[26]</ref>, in which some vertices do not fall on the HBB.</p><p>The representation method of ORC is shown in <ref type="figure" target="#fig_5">Fig. 6</ref> and all the defined variables of ORC at the location (x, y) m are summarized in <ref type="table" target="#tab_1">Table I</ref>. To represent an object in the feature map, first, the positive locations to detect the object are assigned. In the proposed OLA, the locations in the general Gaussian region are defined as the positive locations, while the other locations are defined as the negative locations. Thus, matrices obj m , m= 1, 2, 3 are generated to represent the ground truth positive and negative locations, which are binary versions of the matrices F m . Let the component at location   If F x,y,m = 0, obj x,y,m = 0, and (x, y) m is a negative location. In CNN, obj m , m = 1, 2, 3, are generated to represent the estimation of obj m , whose component obj x,y,m at (x, y) m is in the range of (0, 1).</p><p>Second, when the positive locations are assigned, OBBs of different locations are represented for locating the objects more accurately. As shown in <ref type="figure" target="#fig_5">Fig.6</ref>, we use l x,y,m = [l 1 , l 2 , l 3 , l 4 ], and s x,y,m = [s 1 , s 2 , s 3 , s 4 ] to represent the OBB of an object at (x, y) m . l 1 , l 2 , l 3 , l 4 are the distances from the location (x, y) m to the top, right, bottom, and left edges of the circumscribing horizontal bounding box (HBB) calculated from the ground truth coordinates. s 1 , s 2 , s 3 , s 4 , are the distances from the vertices of the HBB to the corresponding vertices of the OBB. Note that s x,y,m is normalized to the range of [0, 1] by dividing by the corresponding side length of the HBB. Besides, as with Gliding Vertex <ref type="bibr" target="#b26">[26]</ref>, ar x,y,m ? [0, 1] are generated to represent the area ratio of the HBB and OBB. Thus, the OBB of the object at (x, y) m is represented by a 1 ? 9-dimensional vector obb x,y,m = [l x,y,m , s x,y,m , ar x,y,m ]. Correspondingly, the CNN's prediction of the OBB, obb x,y,m , at (x, y) m is represented as obb x,y,m = l x,y,m ,? x,y,m , ar x,y,m .</p><p>Third, the object's category is represented at each location. The ground truth classification at (x, y) m is represented x,y,m ? (0, 1) of which represents the probability that the object belongs to the cth category.</p><p>2) Refined approximation of OBBs. Furthermore, not all convex quadrilaterals are directly represented by the ideal ORC drawn as shown in <ref type="figure" target="#fig_7">Fig. 7</ref>. The cases that the vertices do not fall on the HBB and the implicit ordering of vertices in the ORC need to be discussed. These are not fully considered in vertex-based methods, such as Gliding Vertex <ref type="bibr" target="#b26">[26]</ref>. They cope with these cases by converting the quadrilateral to its minimal outer rectangle. But such large-scale one-sizefits-all approximate conversions introduce large errors. And vertex-based methods only represent rotated rectangles and not arbitrary convex quadrilaterals. In response, we generalize and discuss the problem more comprehensively by generalizing the ORC representation and vertex ordering of arbitrary convex quadrilaterals to 16 cases. For their interpretations, schematic diagrams are more intuitive and easier to understand than words, so a summary of these cases is illustrated in <ref type="figure" target="#fig_7">Fig. 7</ref>. According to this refined approximation (RA), it is only necessary to make different types of approximations with as small error as possible for few convex quadrilaterals to represent arbitrary convex quadrilaterals and to obtain implicitly ordered vertices. Based on the statistics of more than two million OBBs in the DOTA dataset <ref type="bibr" target="#b4">[5]</ref>, only 4.79% of the OBBs need to be approximated. The error introduced by using the minimum outer rectangle approximation for all the "difficult" convex quadrilaterals (counted by pixel areas) is more than twice as large as the one in the proposed method. Due to the space limitation, a more detailed algorithm is available in our opensource codes (https://github.com/Shank2358/GGHL).</p><p>After the above variables are obtained and represented, the CNN training process is to make the CNN's predictions approach the ground truth values, i.e., minimizing the loss in Eq. 22, which will be described later.</p><p>3) Object-adaptive weight adjustment mechanism (OWAM). Generally, after generating an elliptical Gaussian candidate region and assigning labels to all the locations in this region, as shown in <ref type="figure" target="#fig_5">Fig. 6</ref>, the value of f (x, y) is used to weight a location of the candidate region when calculating the location loss. However, some objects like harbors in the remote sensing datasets as shown in <ref type="figure">Fig. 8</ref> do not conform to the Gaussian center prior. Therefore, it is not appropriate to use Gaussian weight directly. This has not been considered by the existing Gaussian-center-prior methods, such as CenterNet <ref type="bibr" target="#b19">[19]</ref>, BBAVectors <ref type="bibr" target="#b22">[22]</ref>, DRN <ref type="bibr" target="#b8">[8]</ref>, O 2 -DNet <ref type="bibr" target="#b23">[23]</ref>, and loss functions like GWD <ref type="bibr" target="#b30">[30]</ref>. In the field of horizontal object detection, AutoAssign <ref type="bibr" target="#b18">[18]</ref> and IQDet <ref type="bibr" target="#b34">[34]</ref> employed the adaptive weight adjustment with success. OWAM borrows this idea and extends it to oriented object detection. Compared to the existing methods, benefitting from using general Gaussian PDFs defined in OLA as prior weights, the proposed OWAM represents translation, rotation, and scaling for the arbitraryoriented object. This Gaussian prior is for each individual while not for each category designed in AutoAssign <ref type="bibr" target="#b18">[18]</ref>. Besides, as mentioned before, using general Gaussian instead of "Rotated-Centerness-like" mechanisms to learn "Objectness" avoids the possible interpolation approximation problem. Based on this prior, the weights are dynamically adjusted by the orientation correlation variables s x,y,m and ar x,y,m designed in ORC during the CNN training. That is, OWAM combines the static prior in OLA with the dynamically learnable OBB representation in ORC to re-weight the assigned candidates.</p><p>In OWAM, the higher weights are assigned to the key positive locations of an object learned by CNN, while not always the center point of an object that is used by F m , m = 1, 2, 3. As shown in <ref type="figure">Fig. 8</ref>, weight adjustment matrices G m , m = 1, 2, 3 are introduced to adaptively adjust the weight of each Gaussian region of F m , according to object's shape. Note that matrices G m are calculated based on the proposed ORCs of CNN using the OBB regression loss described below, which reflects the OBB shape prediction scores at positive locations.  <ref type="figure">Fig. 8</ref>. The object-adaptive weight adjustment mechanism (OWAM) based on Gaussian center prior (GCP) weight and OBB shape regression score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head><p>The value of each component in G m is in the range of (0, 1). More specifically, based on the above OBB encoding process, the OBB regression loss at each positive location (x, y) m , m = 1, 2, 3, is</p><p>Loss obb x,y,m , obb x,y,m = 1?GIoU l x,y,m ,l x,y,m</p><formula xml:id="formula_11">+ 4 k=1 s (k) x,y,m ?? (k) x,y,m 2 + (ar x,y,m ? ar x,y,m ) 2 .<label>(9)</label></formula><p>The loss function in Eq. 9 is obtained from maximizing the likelihood function of the parameters to estimate. It is explained in Appendix A-1). GIoU (?) function <ref type="bibr" target="#b35">[35]</ref> is an improved IoU for training, the calculation of which is given in Appendix B.? (k)</p><p>x,y,m is the kth component of 1 ? 4dimensional vector? x,y,m , and s (k)</p><p>x,y,m is the kth component of 1 ? 4-dimensional vector s x,y,m . Therefore, the output of Loss ?bb x,y,m , obb x,y,m is a scalar greater than or equal to 0 at location (x, y) m . The smaller its value is, the more accurate the prediction of OBB is.</p><p>Thus, e ?Loss(obbx,y,m, obbx,y,m) is in the range of (0, 1]. Let the value of G m at location (x, y) m , m = 1, 2, 3, be G x,y,m = e ?Loss(obbx,y,m, obbx,y,m) .</p><p>The larger its value is, the more accurate the prediction of OBB is. Then, G m is adaptively adjusted according to the shape of the objects to be predicted by the CNN training. So, the weight of (x, y) m is changed from f m (x, y) to</p><formula xml:id="formula_13">weight obb x,y,m = 1 ? obj x,y,m + ?f m (x, y) + (1 ? ?) G x,y,m ? obj x,y,m ,<label>(11)</label></formula><p>where scalar weight obb x,y,m ? (0, 1] represents the objectadaptive weight at (x, y) m . ? ? (0, 1) denotes the weighting factor. ? = 1 means that the weights are completely dependent on the Gaussian prior, and ? = 0. If (x, y) m is a negative location, then weight obb x,y,m = 1; otherwise, weight obb x,y,m ? (0, 1). Finally, W m ? H m -dimensional CNN-learnable weight matrices weight obb m composed of weight obb x,y,m are generated, where W m = W /stride m and H m = H/stride m represent the width and height of the feature map of scale respectively. weight obb m , m = 1, 2, 3, weight different locations dynamically to fit different object shapes in different scales. Besides, in the above process, there is no need to change the label assignments designed in Section III-A. Compared with the scheme of using CNN to predict the weights directly, this scheme makes the adjustment converge faster and more stable based on F m , during the CNN training. 4) CNN model. Many methods use large and complicated CNN models to pursue high accuracy, which may be too complex in practical applications. For computational simplicity, the CNN model of the proposed GGHL chooses a straightforward and practical structure for its versatility and ease of use, which can more precisely reflect the effectiveness of the proposed GGHL. The designed CNN model is mainly composed of three parts: backbone network, feature pyramid network (FPN) <ref type="bibr" target="#b32">[32]</ref>, and detection head composed of ORCs, which are represented by red, blue and yellow in <ref type="figure" target="#fig_2">Fig. 4 (b)</ref>, respectively. Considering that the object scale varies greatly in remote sensing scenes, spatial pyramid pooling (SPP) <ref type="bibr" target="#b36">[36]</ref> is introduced in FPN to fuse multiscale features and expand the receptive field. In addition, the DropBlock <ref type="bibr" target="#b37">[37]</ref> is used to improve the generalization ability of CNN, which does not bring additional computational complexity. The detection head uses a very light two-layer convolution structure, unlike the heavy convolution layers of RetinaNet <ref type="bibr" target="#b38">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Joint-optimization Loss (JOL)</head><p>First, the joint PDF of the P&amp;N location detection, OBB regression, and object classification at each location of feature maps is provided. Second, an area normalization and loss reweighting mechanism is designed for adaptively adjusting the weight of loss at different locations. Finally, the maximum likelihood estimation (MLE) is used to obtain the total jointoptimization function. Besides, the CNN predictions in the inference stage are explained.</p><p>1) The joint PDF of the positive or negative location detection, OBB regression, and object classification. Use </p><p>The estimation obj x,y,m is in the range of (0, 1), which represents the classification score that the location (x, y) m is positive. The larger the obj x,y,m is, the more likely (x, y) m is to be a positive location. For the OBB regression, define nn obb (?) as a deterministic regression function related to the CNN, which uses the linear activation function <ref type="bibr" target="#b0">[1]</ref>. Note that in the CNN training stage, the ground truth of positive and negative location detection, i.e., obj x,y,m ? {0, 1}, is given. The estimation of OBB is only carried out at the positive locations <ref type="bibr" target="#b0">[1]</ref>:</p><formula xml:id="formula_15">obb x,y,m = obj x,y,m ? nn obb x obb x,y,m , ? obb x,y,m ,<label>(13)</label></formula><p>which is used in the joint PDF and loss function. While in the CNN inference stage, obj x,y,m is unknown, but obj x,y,m has been obtained after the training, which will be explained in detail in Sub-section 4). related to the CNN. Note that in the CNN training stage, obj x,y,m and obb x,y,m are given, and G x,y,m is calculated after obb x,y,m is predicted by the CNN. In this stage, the estimation of classification is only carried out at the positive locations, i.e. obj x,y,m = 1 <ref type="bibr" target="#b1">[2]</ref>. In the existing methods like <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b38">38]</ref>, the classification score is usually learned independently by CNN. While in the proposed GGHL, G x,y,m ? (0, 1] is multiplied to the classification score, which makes the classification score also affected by the OBB regression score. Thus, the estimation that object belongs to the cth category is </p><p>where G x,y,m is given in Eq. 10. The estimation cls x,y,m is, the more likely the object at (x, y) m is to belong to the cth category. Therefore, the classification subtask is affected by the OBB regression error. In the training process, in order to obtain a higher classification accuracy, the model parameters will be jointly adjusted to approach the optimal results of not only the classification sub-task but also the OBB regression task. Thus, when ? loc </p><p>which is derived in Appendix A. The PDF of the error of the OBB regression, which is assumed to obey an i.i.d. Gaussian distribution with a mean of 0 and variance ? 2 .</p><p>2) Area normalization and loss adaptive re-weighting. Because CNN prefers to learn the object with a larger Gaussian region generated by the proposed OLA, i.e., with more positive locations, an area normalization factor ? x,y,m at (x, y) m that decreases with increasing Gaussian candidate areas is considered. The statistics of OLA assignment for many AOOD datasets, such as DOTA <ref type="bibr" target="#b4">[5]</ref>, show that the number of objects and the area of the assigned candidate region (number of pixels) exhibit a decreasing trend from fast to slow. Therefore, the reciprocal form of the logarithm is chosen to design variables ? x,y,m so that the variation trend approximates the distribution described above with a lower bound. According to Eq. 8, the theoretical maximum value of candidate area is len img ? (1 ? T IoU ) 32 <ref type="bibr" target="#b1">2</ref> , and the variation of this value is still large, so its square root is taken. To ensure that the denominator is not 0, 1 is added to the log of the denominator.</p><p>To make the maximum value be 1, the numerator is log2. The designed area normalization variable is</p><formula xml:id="formula_18">? x,y,m = log 2 log 1 + ? area x,y,m ,<label>(16)</label></formula><p>where area x,y,m denotes the area of positive region and is always no less than 1. The normalization weight is in the range of (0, 1].</p><p>In JOL, to make the detection of positive and negative affected by the object's shape, weight obb x,y,m designed in Eq. 11 is used to adaptively weight the location loss according to the error of OBB regression, i.e., the error of object's shape prediction. Besides, to impose classification effects on regression, the weight weight cls x,y,m is designed to weight the OBB regression loss after G x,y,m is obtained in the total loss, </p><p>where the ground truth category is the cth category. Similar to Eq. 11, weight cls x,y,m is also in the range (0, 1]. 1 ? obj x,y,m is used to make the non-object part of the weights equal to 1. Here, weight obb</p><p>x,y,m and weight cls x,y,m do not perform the gradient backpropagation during training. In GGHL, taking ? = 0.5 obtains equal contributions from the prior weights and adjusted values, which may not be optimal but is simplest.</p><p>3) Total joint-optimization function. After caonsidering ? x,y,m and weight obb x,y,m , and introducing the Focal Loss <ref type="bibr" target="#b38">[38]</ref>, from the LF of Eq. 15 and using the MLE, the total loss of all the locations in feature maps is obatined, which is  </p><p>where F M m represents the feature maps in scales m, m = 1, 2, 3. ? is the hyperparameter of Focal Loss <ref type="bibr" target="#b38">[38]</ref>, which is set to 2 as <ref type="bibr" target="#b38">[38]</ref>. In JOL, the loss of P&amp;N location detection is separated from the classification loss so that the imbalance of P&amp;N samples will not affect the classification task. The OBB regression loss is Loss obb x,y,m , obb x,y,m ,</p><p>and the classification loss is </p><p>where the classification estimation cls (c)</p><p>x,y,m defined by Eq. 14 is associated with the OBB regression result G x,y,m . This is different from the ordinary loss scheme in which independent regression loss and classification loss are added together.</p><p>4) The CNN predictions in the inference stage. Note that obb x,y,m and cls x,y,m in the CNN training stage and inference stage are different. In the CNN inference stage, obj x,y,m is unknown, after obj x,y,m is obtained. If obj x,y,m is larger than the threshold, which is given by the benchmarks of different datasets, the location is predicted as a positive location, obb x,y,m = nn obb x obb x,y,m , ? obb</p><p>x,y,m * , </p><p>where ? obb</p><p>x,y,m * and ? cls</p><p>x,y,m * represent the optimal parameters obtained from the CNN training for OBB regression and object classification, respectively. If obj x,y,m is less than the threshold, the location is predicted as a negative location, and the OBB regression and object classification are not performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND DISCUSSIONS</head><p>In this section, experiments on public AOOD datasets are conducted to verify the effectiveness of the proposed GGHL. First, the experimental conditions are explained. Secondly, the ablation experiments are conducted, the effectiveness of each components is analyzed, and the results are discussed. Furthermore, the proposed GGHL is used to replace the label assignment strategy of other mainstream AOOD methods to evaluate its versatility. Besides, the lightweight AOOD model LO-Det <ref type="bibr" target="#b12">[12]</ref> is improved by the proposed GGHL, and its performance is evaluated on embedded platforms to verify the application friendliness. Third, comparative experiments on several public datasets of different scenes are evaluated to compare the performance of the proposed GGHL with the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Conditions</head><p>1) Experimental platforms. All the experiments were implemented on a computer with an AMD 5950X CPU, 128 GB of memory, and two NVIDIA GeForce RTX 3090 GPU (2?24GB). Besides, in order to evaluate the application friendliness of the proposed GGHL, the embedded devices NVIDIA Jetson AGX Xavier and NVIDIA Jetson TX2 were also used for application experiments.</p><p>2) Datasets. In order to evaluate the performance of the proposed GGHL fully, multiple public datasets of different scenes and different image types are employed. a) DOTA <ref type="bibr" target="#b4">[5]</ref> is currently the largest AOOD dataset containing 2806 aerial images from 800 ? 800 pixels to 4000 ? 4000 pixels, in which more than 188,000 objects falling into 15 categories are annotated. Due to the huge size, these images are usually <ref type="bibr" target="#b12">[12]</ref> cropped into sub-images of 800?800 pixels with an overlap of 200 pixels. In addition, the multiscale cropping (MSC) strategy is used like many recently proposed AOOD methods <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b39">39]</ref>. For MSC, the original images are scaled to [0.5; 1.0; 1.5] and then cropped into patches of size 800 ? 800. The categories of the objects in DOTA are: Plane (PL), Baseball diamond (BD), Bridge (BR), Ground field track (GFT), Small vehicle (SV), Large vehicle (LV), Ship (SH), Tennis court (TC), Basketball court (BC), Storage tank (ST), Soccer-ball field (SBF), Roundabout (RA), Harbor (HA), Swimming pool (SP), and Helicopter (HC). DOTAv2.0 <ref type="bibr" target="#b40">[40]</ref> further expands three categories of objects, i.e., container-crane, airport, and helipad, based on DOTAv1.0 dataset. DOTAv2.0 contains 11,268 images and 1,793,658 instances, which is currently the largest AOOD dataset. b) SKU110-R <ref type="bibr" target="#b8">[8]</ref> is a dense oriented commodity detection dataset. The images are collected from thousands of supermarket stores. It is an extension of the original SKU110K dataset containing 1,733,678 instances. The number of images in training set, validation set and test set are 57533, 4116, and 20552, respectively. c) SSDD+ <ref type="bibr" target="#b41">[41]</ref> is a polarized synthetic aperture radar (SAR) image dataset. It has 1,160 ship images including 2456 instances collected by RadarSat-2, TerraSARX, and Sentinel-1 sensors under different sea conditions. The polarization modes contain HH, HV, VV, and VH. The ratio of training set , validation set and testing set is 7:1:2.</p><p>3) Evaluation metrics. The mean Average Precision (mAP) with IoU threshold = 0.5, the widely used metric in OD tasks is adopted for evaluating the detection accuracy. The average precision of each category is AP. AP with an IoU threshold of 0.3 is represented as AP@0.3. The inference frames per second (fps) are used to evaluate the detection speed. The floating point of operations (FLOPs) is used to evaluate the computational complexity of the model. The memory occupied by parameters is used to evaluate the model size.</p><p>4) Implementation details. To compare the proposed GGHL with state-of-the-art methods fairly, training hyperparameters are set to be the same as the methods compared. The initial learning rate is set as 2?10 ?4 . The final learning rate is 1?10 ?6 , and the SGD strategy is adopted. Weight decay is 5?10 ?4 , and momentum is 0.9. The maximum training epoch is 36. The confidence threshold is 0.2, and the non-maximum suppression (NMS) threshold is 0.45. Data augmentation strategies including mixup, random cropping, and random flipping are used. 5) Baseline &amp; Comparative methods. An OD model usually consists of CNN parts and non-CNN parts. In order to evaluate the performance of each component proposed in GGHL, two models with the same CNN structure are constructed as baselines. Among these two Vanilla models, the one adopting the anchor-based label assignment strategy is called Vanilla-AB, while the other one using the anchor-free standard-Gaussian-based label assignment strategy is called Vanilla-AF. The two CNNs are only slightly different in the number of feature maps in the output layer. They both employ the OBB representation method of Gliding Vertex <ref type="bibr" target="#b26">[26]</ref> (called Vanilla-Head in the experiments) with static candidate region, and the loss function with additive paradigm.</p><p>In order to compare and analyze the performance of the proposed GGHL more comprehensively, many state-of-the-art AOOD methods are selected for comparison, such as SCRDet <ref type="bibr" target="#b11">[11]</ref>, Gliding Vertex <ref type="bibr" target="#b26">[26]</ref>, RIL <ref type="bibr" target="#b29">[29]</ref>, etc. Moreover, some popular anchor-free models like CenterNet <ref type="bibr" target="#b19">[19]</ref> and FCOS <ref type="bibr" target="#b2">[3]</ref> and the latest AOOD models like NPMMR-Det <ref type="bibr" target="#b42">[42]</ref> and LO-Det <ref type="bibr" target="#b12">[12]</ref> are also adopted as the baseline to evaluate the versatility of the proposed GGHL.  <ref type="table" target="#tab_1">Table II</ref>. The more detailed experimental results for each category are listed in <ref type="table" target="#tab_1">Table III</ref>. First, an anchor-based detector Vanilla-AB is constructed, and the effect of the widely-used MSC data augmentation strategy <ref type="bibr" target="#b39">[39]</ref> is evaluated. From the experimental results, it can be seen that the MSC strategy increases mAP by 2.09 on the DOTA dataset. It can be observed from <ref type="table" target="#tab_1">Table II</ref> that the average precision (AP) improvement of using MSC is more obvious for extreme scale objects, including large-scale objects, such as GFT and SF, and smallscale objects like SV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Experiments and Discussions</head><p>Second, when the external control factors like data augmentation strategy are consistent, the performance of anchor-based Vanilla-AB and anchor-free Vanilla-AF is compared. For this direct modification from anchor-based to anchor-free strategy, each layer of FPN changes from predicting anchor boxes of three scales to directly predicting the standard Gaussian candidates, of which the number of output feature maps becomes one-third. Although the computational complexity (FLOPs), model size (Model Parameters) and the number of hyperparameters have been reduced, and the detection speed has become faster, the mAP has been reduced by 2.31. On one hand, the performance is reduced due to the absence of the anchor prior and the reduction of model parameters. On the other hand, as analyzed above, the circular positive candidate defined by the standard Gaussian is not suitable for oriented objects, especially BR, SV and other objects with obvious directionality. For objects with approximately square OBBs, such as PL and BD, the performance loss is not obvious.      Third, based on the baseline, i.e., Vanilla-AF, the proposed OLA and ORC are used to make the positive candidate region conform to the shape and direction characteristics of the objects. This improvement makes the mAP increase by 2.56. The object candidates of ORC are further improved by OWAM, i.e., ORC-OWAM, and mAP is further improved by 0.54. For the non-Gaussian center prior objects analyzed previously, like the harbor (HA), the performance improves more. The visualized feature maps of the CNN output layer in <ref type="figure" target="#fig_21">Fig. 9</ref> verify this claim. Further using the proposed JOL, the mAP increases by 1.52. The visualized results before and after NMS without/with JOL are shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. When JOL is not used, the prediction result retained after NMS sorting may have a high classification score but a low location score, such as the soccer-ball field (SBF) in <ref type="figure" target="#fig_0">Fig. 10 (b)</ref>. Note that the location of the SBF in <ref type="figure" target="#fig_0">Fig. 10 (b)</ref> has a large deviation. Instead, when JOL is used, a more consistent result with the highest scores of OBB regression and classification is obtained, as shown in <ref type="figure" target="#fig_0">Fig. 10 (d)</ref>. The mutual promotion of the three components of GGHL is more significant, and the mAP has reached 76.95. It is an increase of 4.62 (6.39%) compared to the baseline. Compared with the anchor-based method, i.e., Vanilla-AB (MSC), it increases by 2.31 (3.09%), and has faster speed, lower computational complexity and model size, and Note: Bold indicates the best result. "minRect" denotes the approximate method using minimum circumscribed rectangle.</p><p>saves the recessive cost of hyperparameter adjustments. In summary, the ablation experiments and visualization feature maps support the claims analyzed in the introduction and verify the effectiveness of each component from quantitative and qualitative perspectives. In addition, experiments are performed on different values of T IoU , ? , ?, and ?, the results are listed in <ref type="table" target="#tab_1">Table IV</ref>. When T IoU = 0.3, ? = 3 and ? = 0.5 with area normalization factor ?, the proposed GGHL has the best performance on the DOTA dataset. Multi-scale assignment controlled by ? has a greater impact on model performances. Designing a scale assignment strategy for multi-scale objects is a direction worth continuing to study in the future. Using or not using OWAM has a greater impact on the results (mAP gap reaches 2.06), but the value of ? between 0.3-0.5 has little impact on mAP. Using area normalization significantly improves mAP, which confirms its effectiveness. Furthermore, <ref type="table" target="#tab_6">Table V</ref> analyzes the performance of ORC with/without refined approximation (RA) and Gliding Vertex <ref type="bibr" target="#b26">[26]</ref>. The ablation experiments demonstrate the effectiveness of RA and the effectiveness of the proposed label assignment strategy compared with the anchor-based strategy.</p><p>2) Ablation experiments on different baseline models. To further verify the effectiveness and versatility of the proposed GGHL, several state-of-the-art models are selected as the baselines for ablation experiments. The results of using GGHL on other models on the DOTA dataset are listed in <ref type="table" target="#tab_1">Table VI</ref>. First, the two popular anchor-free models, CenterNet <ref type="bibr" target="#b19">[19]</ref> and FCOS <ref type="bibr" target="#b2">[3]</ref>, are selected as baselines. Since these two models are designed for the ordinary OD task, they have been modified for the AOOD task. Among them, the modified CenterNet, i.e., R-CenterNet, uses Darknet53, which is simpler and the same as the Vanilla Model, instead of the original complex Hourglass-104 as the backbone. The modified FCOS, i.e., R-FCOS-P5, also uses the same backbone, using a 3-layer (P3-P5) FPN structure. The mAPs of the modified R-CenterNet and R-FCOS-P5 on the DOTA dataset are 72.08 and 73.48, respectively. The proposed GGHL is employed into these baselines to improve their label assignment strategy, and the mAP on the DOTA dataset is increased by 1.57 on R-CenterNet and 3.09 on R-FCOS-P5. Since these two baselines are anchorfree models originally, the model inference speed remains unchanged after the employment of GGHL. In addition, the performance of AutoAssign <ref type="bibr" target="#b18">[18]</ref>, which also uses an adaptive weighting strategy, has been tested based on R-FCOS <ref type="bibr" target="#b2">[3]</ref>.</p><p>Although it also improves the performance of baseline, its performance on the AOOD task is inferior to the proposed GGHL because the Gaussian prior of AutoAssign <ref type="bibr" target="#b18">[18]</ref> is not directional and is shared with each category. Second, NPMMR-Det <ref type="bibr" target="#b42">[42]</ref>, one of the latest methods in the typical AOOD task of remote sensing object detection, is selected as the baseline. It is an anchor-based model that balances accuracy and speed through CNN feature refinement design. The experiments indicate that using GGHL to improve it increases mAP by 2.07 and the speed by 3.46 fps. This result not only validates the effectiveness of GGHL for improving the anchor-based model, but also demonstrates that GGHL is also compatible with more complex CNN designs.</p><p>Third, the latest lightweight AOOD model LO-Det <ref type="bibr" target="#b12">[12]</ref> is selected as the baseline to verify the effectiveness of the proposed GGHL on the lightweight model. The experimental results show that after the employment of GGHL improvements, LO-Det's mAP on the DOTA dataset has increased by Note: The unit G is Giga, which represents 1?10 ?9 . The unit MB represents 1?10 ?6 bytes. Speed 1 is the speed on RTX 3090 GPU, speed 2 is the speed on NVIDIA Jetson TX2, Speed 3 is the speed on NVIDIA Jetson AGX Xavier. The inference speed (average of 10 tests) only includes the network inference speed without post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TX2</head><p>Xavier TX2 Xavier <ref type="figure" target="#fig_0">Fig. 11</ref>. The experiments on embedded devices. <ref type="bibr" target="#b4">5</ref>.09 (+7.69%), and the speed has increased by 2.06 fps. Furthermore, experiments have also been carried out on embedded devices, and the results are shown in <ref type="table" target="#tab_1">Table VII</ref> and <ref type="figure" target="#fig_0">Fig. 11</ref>. The speed of improved LO-Det on TX2 and Xavier embedded devices has increased by 0.69 fps and 1.60 fps, respectively. FLOPs are reduced by 0.12 G, and model parameters are reduced by 0.21 MB. The improved performance verifies the application friendliness of the proposed GGHL for lightweight models on embedded devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparative Experiments and Analysis</head><p>Comparative experiments are conducted extensively on several public AOOD datasets from different typical scenarios to compare the performance of the proposed GGHL and the stateof-the-art methods.</p><p>1) Comparative Experiments on the DOTA dataset. In the AOOD task, most methods use the DOTA aerial remote sensing data <ref type="bibr" target="#b43">[43]</ref><ref type="bibr" target="#b44">[44]</ref><ref type="bibr" target="#b45">[45]</ref><ref type="bibr" target="#b46">[46]</ref>, for performance comparison and analysis. In ablation experiments, this data set has been used to evaluate in detail the effectiveness, versatility, and performance of each component of the proposed GGHL. <ref type="table" target="#tab_1">Table VIII</ref> provides comparison of detection performance for each category and some detail information of experimental implementation is supplied at its bottom. It can be observed that the detection accuracy of the proposed GGHL method (mAP=76.95) surpasses most of the AOOD methods in the past three years and has a very fast detection speed (fps=42.39). And GGHL has a new improvement in accuracy or speed by combining with AOOD methods, such as NPMMR-Det or LO-Det. Although GGHL's mAP is slightly lower than that of the excellent AOOD methods, such as S 2 A-Net <ref type="bibr" target="#b45">[45]</ref> and R 3 Det-GWD <ref type="bibr" target="#b30">[30]</ref> using larger backbones like ResNet101 or ResNet152, it runs faster than them. Moreover, GGHL is an anchor-free method with the lower recessive cost, which does not need to set and adjust prior hyperparameters. The visualization  Note: Bold font indicates the best results. In order to make a fair comparison with the methods in the DOTAv2.0 benchmark <ref type="bibr" target="#b40">[40]</ref>, the experiments above do not use data augmentation and other tricks like these comparison methods. mAP@v1.0 denotes the results on the DOTAv1.0 dataset, mAP@v1.5 denotes the results on the DOTAv1.5 dataset, and mAP@v2.0 denotes the results on the DOTAv2.0 <ref type="bibr" target="#b40">[40]</ref> dataset. The speed of all methods are tested on a single NVIDIA Tesla V100 GPU.</p><p>results of GGHL on the DOTA dataset including optical RGB images and panchromatic images are shown in <ref type="figure" target="#fig_0">Fig. 12</ref>. It can also be observed that the proposed GGHL accurately detects densely arranged objects benefited from the definition of positive locations and the label assignment strategy that more fit the objects' shape and direction. Furthermore, comparative experiments are also conducted on the new versions of the DOTA datasets, i.e., DOTAv1.5 and DOTAv2.0 <ref type="bibr" target="#b40">[40]</ref>. The proposed GGHL is compared with the methods in the official latest benchmark <ref type="bibr" target="#b40">[40]</ref>, and the results are listed in <ref type="table" target="#tab_1">Table IX</ref>, which indicates that the proposed GGHL not only achieves state-of-the-art performance in mAP but also has a very fast detection speed.</p><p>2) Comparative Experiments on other AOOD datasets. Further comparative experiments are carried out on multiple Note: AF represents anchor-free methods, and AB represents anchor-based methods. Some methods' codes are not open-source, the unreported results of which is indicated by "-". "#" indicates the results we reproduced.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, a novel AOOD method, i.e., GGHL, was proposed. In GGHL, an anchor-free Gaussian OLA strategy reflecting objects' shape and direction was designed to define and assign the positive candidate locations. An ORC mechanism was developed to indicate OBBs and an OWAM was presented to adjust the Gaussian center prior sample space for fitting the characteristics of different objects adaptively through the CNN learning. For refining the misalign optimal results of different subtasks in the constructed sample space during training, a JOL with area normalization and dynamic weighting was designed.</p><p>The extensive experiments on several public datasets have demonstrated the following: 1) The proposed GGHL has achieved state-of-the-art performance both in accuracy and speed on the AOOD task. The effectiveness of each component has been verified, and the claims made for each component are consistent and verified. 2) The proposed GGHL is a general framework that can be used to improve other AOOD methods in different scenarios. It improves the accuracy without reducing the detection speed, and does not require many anchor hyperparameters.</p><p>3) The proposed GGHL is friendly to the landing of CNN-based AOOD application, which improves the performance of lightweight AOOD models on embedded devices and saves a lot of hidden costs of tuning parameters.</p><p>Despite the demonstrated benefits, the strategy of assigning labels to different scales adaptively and the abandonment of NMS to construct an end-to-end CNN model are still to be studied in the future.</p><p>The codes are available at https://github.com/Shank2358.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. APPENDIXES A. The PDF of CNN in GGHL</head><p>Since the above model is too complicated, let's start with the basic neuron model in a neural network to explain probability density function (PDF).</p><p>Without loss of generality, we may model the CNN model as follows:? = nn (x, ?) + b, (A-1)</p><p>where nn (?) is a deterministic function related to the CNN;</p><p>x is the input;? denotes the output, i.e., the predictions of CNN; ? denotes the vector composed of learnable parameters; b represents the bias vector, which is usually set to a zero vector in CNN. To simplify the derivation, this setting is also used here, i.e.,? = nn (x, ?). 1) PDF and loss function of OBB regression. Define the ground truth of the prediction as y. Define the error between the actual value and the predicted value as ? = y ??, which is assumed to obey an i.i.d. Gaussian distribution with a mean of 0 and variance ? 2 . Therefore, the PDF is which represents the probability density of y when x and ? are given. Then, for multiple y (i) , i = 1, 2, ? ? ? , m, in different locations of output layers, their joint PDF is p y (1) ? ? ? y (m) x (1) ? ? ? x (m) ; ?</p><formula xml:id="formula_25">= m i=1 p y (i) x (i) ; ? = m i=1 1 ? ? 2? e ? ( y (i) ?? (i) ) 2 2? 2 . (A-3)</formula><p>Then, their joint likelihood function (LF) for ? is L (?) = log p y (1) ? ? ? y (m) x (1) ? ? ? x (m) ; ?</p><formula xml:id="formula_26">= m log 1 ? ? 2? ? 1 2? 2 m i=1 y (i) ?? (i) 2 .</formula><p>(A-4)</p><p>Now let us reconsider the process of using CNN to predict the shape of OBBs, which is a regression. Since the error of the OBB regression is assumed to obey an i.i.d. Gaussian distribution with a mean of 0 and variance ? 2 , the PDF of , where the GIoU calculation can be found in Appendix B. We adopt this idea. Therefore, the loss function of OBB regression at location (x, y) m in Eq. 10 is obtained.</p><p>2) PDF of object classification. The object classification task in this case is composed of multiple i.i.d. binary classifications and each component of y is either 0 or 1. To estimate y, the non-linear activation function Sigmoid (?) is used on the basic neuron model in output layers. Thus, each component of y is in (0, 1) that represents the classification score. In CNN, this classification score is usually interpreted as "probability" of the binary classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Assume that, given x and ?, y follows Bernoulli (1,?), and the PDF is p (y |x ; ?) =? y (1 ??) 1?y .</p><p>(A-8)</p><p>Then, for multiple y (i) , i = 1, 2, ? ? ? , m, in different locations of output layers, their joint PDF is p y (1) ? ? ? y (m) x (1) ? ? ? x (m) ; ? where ? obj x,y,m , m = 1, 2, 3, represent the parameter at (x, y) m used to predict whether this location is positive or negative.</p><p>3) The joint PDF of the positive or negative location detection, OBB regression, and object classification. Combining Eq. A-5, Eq. A-10 and Eq. A-11, we obtain Eq. 18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The calculation of GIoU in ORC</head><p>The GIoU of Eq. 11 in ORC is calculated according to Algorithm 2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The primary process of training a CNN-based object detection model that adopts the dense detection paradigm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Schematic diagram of mainstream label assignment strategies and their sample spaces. (a) The anchor-based strategy. (b) The dense-points strategy. (c) The key-point strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>The principle of the proposed object-adaptation label assignment (OLA) strategy. (a) The original image. (b) The higher-resolution heatmaps (down-sampling stride=4) generated by the Gaussian key-points strategy of CenterNet [19], BBAVectors [22], DRN [8], etc. (c) The principle of generating 2-D Gaussian heatmaps. (d) (e) (f) represent the multiscale (downsampling stride=8, 16, 32) Gaussian positive candidates generated by the proposed OLA strategy. The color bars represent Gaussian probability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>7 if f (x, y) &lt; thr then 8 Fx 15</head><label>7815</label><figDesc>x, y) &gt; Fx,y then 11 Fx,y = f (x, y) Assign other parameters of the label (see Section III-B); Normalize f (x, y) in each Gaussian region. 16 end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>The radiuses of the candidate region. (a) The case that the predicted box is as large as the ground truth. (b) The case that the predicted box is larger than the ground truth. (c) The case that the predicted box is smaller than the ground truth. (d) The radiuses of the candidate region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>The OBB representation of the proposed ORC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(x, y) m of obj m be obj x,y,m . (For convenience, the subscripts x, y, m are used in the following to indicate the variables at the location (x, y) m .) If F x,y,m &gt; 0, obj x,y,m = 1, and (x, y) m is a positive location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Discussion of different cases of arbitrary convex quadrilateral represented by ORC. The colored letters p 1 , p 2 , p 3 , and p 4 indicate the ordered convex quadrilateral vertices after ORC preprocessing. as a 1 ? num cls -dimensional one-hot vector cls x,y,m = cls (1) x,y,m , ? ? ? , cls (num cls ) x,y,m , where num cls denotes the number of categories. Let the cth component of cls x,y,m be cls (c) x,y,m ? {0, 1}, c ? A = {1, 2, ? ? ? , num cls }. If the object at location (x, y) m belongs to the cth category, cls (c) x,y,m = 1; otherwise, cls (c) x,y,m = 0. Correspondingly, the CNN's prediction of cls x,y,m is represented as cls x,y,m = cls (1) x,y,m , ? ? ? , cls (num cls ) x,y,m , the component cls (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>loc x,y,m = [obj x,y,m , obb x,y,m , cls x,y,m ] to represent the ground truth of object detection at location (x, y) m . For the CNN model, let ? loc x,y,m = ? obj x,y,m , ? obb x,y,m , ? cls x,y,m be the CNN parameters used for object detection at the location (x, y) m . ? obj x,y,m , ? obb x,y,m , and ? cls x,y,m are the parameters used for the positive or negative location detection, OBB regression, and object classification, respectively. Similarly, let x loc x,y,m = x obj x,y,m , x obb x,y,m , x cls x,y,m be the input features of the prediction layers of CNN at (x, y) m , which are extracted by the hidden layers of CNN. Then, define the predictions of CNN as loc x,y,m = obj x,y,m , obb x,y,m , cls x,y,m , which is generated by ? loc x,y,m and x loc x,y,m . Specifically, for the positive or negative location detection, define nn obj (?) as a deterministic function with Sigmoid activation related to CNN, then, obj x,y,m = nn obj x obj x,y,m , ? obj x,y,m .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>x,y,m = ? cls x,y,m (1) , ? ? ? , ? cls x,y,m (num cls ) , and the input features x cls x,y,m = x cls x,y,m (1) , ? ? ? , x cls x,y,m (num cls ) . Define nn cls (?) as a deterministic function with Sigmoid activation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>,m = obj x,y,m ? G x,y,m ?nn cls x cls x,y,m (c) , ? cls x,y,m (c) ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>function is in the range of (0, 1). The larger the cls (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>( 1 )</head><label>1</label><figDesc>x,y,m and x loc x,y,m are given, the joint PDF of the positive or negative location detection, OBB regression, and object classification is p loc x,y,m x loc x,y,m , ? loc x,y,m = p obj x,y,m x obj x,y,m ; ? obj x,y,m ?p obb x,y,m obj x,y,m ; x obb x,y,m ; ? obb x,y,m ?p(cls (1) x,y,m ? ? ? cls (num cls ) x,y,m |obb x,y,m ; obj x,y,m ; , ? ? ? , ? cls x,y,m (num cls ) ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>weight cls x,y,m = 1 ? obj x,y,m + ?f m (x, y) + (1 ? ?) nn cls x cls x,y,m (c) , ? cls x,y,m (c) ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>=</head><label></label><figDesc>Loss obj, obj ? weight obb x,y,m ? ? x,y,m +Loss obb, obb ? weight cls x,y,m ? ? x,y,m +Loss cls, cls ? ? x,y,m .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>( 18 ) 3 ( 1 ? 3 (</head><label>18313</label><figDesc>In the total loss, the loss of P&amp;N location detection isLoss obj, obj = ? x,y?F Mm m=1,2,obj x,y,m ) ? log (obj x,y,m ) ? x,y?F Mm m=1,2,obj x,y,m ) ? log (1 ? obj x,y,m ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>cls x,y,m = nn cls x cls x,y,m , ? cls x,y,m * ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>1 )</head><label>1</label><figDesc>Ablation experiments of each component. Ablation results of each component on the DOTA dataset are listed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 9 .</head><label>9</label><figDesc>The visualized feature maps of Gaussian center prior and learnable OBB regression confidence. (a) Input image. (b) Gaussian center prior. c) Learnable OBB regression confidence. Some typical non-Gaussian areas are marked with white circles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 10 .</head><label>10</label><figDesc>A visual comparison of the results with and without JOL is on the validation dataset. a) Results before NMS using Vanilla Loss. b) Results after NMS using Vanilla Loss. c) Results before NMS using JOL. d) Results after NMS using JOL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Visualization Results of the proposed GGHL on the DOTA Dataset. Visualization Results of the proposed GGHL on (a) the SKU-110R Dataset, and (b) the SSDD+ Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>p</head><label></label><figDesc>(y |x ; ?) = p (?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>2 . (A- 5 ) 2 . (A- 6 ) 2 + 2 ,</head><label>252622</label><figDesc>obb x,y,m = [l x,y,m , s x,y,m , ar x,y,m ], when x obb x,y,m and ? obb x,y,m are given, is p obb x,y,m obj x,y,m ; x obb x,y,m ; ? obb x,y,m = 1 ? ? 2? e ? (obbx,y,m? obbx,y,m) 2 2? Note that the prediction of OBB is performed under the condition of determined positive and negative locations, so the obj x,y,m is also one of the conditions in Eq. A-5. The LF of parameters ? obb x,y,m , is L ? obb x,y,m = ? obb x,y,m ? obb x,y,m According to MLE, the loss function at location (x, y) m is Loss obb x,y,m ? obb x,y,m = (ar x,y,m ? ar x,y,m ) ,m is the kth component of 1?4-dimensional vector l x,y,m , and l (k) x,y,m is the kth component of 1 ? 4-dimensional vector l x,y,m .? (k) x,y,m is the kth component of 1?4-dimensional vector? x,y,m , and s (k)x,y,m is the kth component of 1 ? 4dimensional vector s x,y,m . Literature<ref type="bibr" target="#b35">[35]</ref> proposed the GIoU term 1 ? GIoU l x,y,m ,l x,y,m to replace the term of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>? 1 ?</head><label>1</label><figDesc>x cls x,y,m , ? cls x,y,m , and obb x,y,m are given, the PDF of object classification is p cls x,y,m obb x,y,m ; obj x,y,m ; x cls x,y,m ; ? obb x,y,m = p(cls (1) x,y,m ? ? ? cls (num cls ) x,y,m |obb x,y,m ; obj x,y,m ; x (1) x,y,m ,? ? ? x (num cls ) x,y,m ; ? cls x,y,m (1) ,? ? ? , ? cls x,y,m (num cls ) x obj x,y,m and ? obj x,y,m are given. The PDF of obj x,y,m is p obj x,y,m x obj x,y,m ; ? obj x,y,m = obj x,y,m objx,y,m obj x,y,m 1?objx,y,m , (A-11)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>CNN Model with ORC and OWAM Positive Sample Negative Sample Training Sample Space (a) Label Assignment: OLA [x1,y1,x2,y2,x3,y3,x4,y4,class] p1 (x1,y1) p3(x3,y3) Plane p4(x4,y4) p2(x2,y2) Image Label OBB's error A General Gaussian Positive Candidate Region Inference Train (c) Loss: JOL Positive Candidates Gaussian Center Prior OWAM P&amp;N Location OBB Regression Object Classification Predictions Ground Truth Joint PDF ORC Ws?Hs?8 Classification Ws?Hs?Classes</head><label></label><figDesc></figDesc><table><row><cell cols="4">ORC (b) Regression ORC ORC GGHL Head l2 l1 s2 s1</cell><cell>l3 s3</cell><cell>l4 s4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Confidence</cell><cell>vmin</cell><cell>p1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>p4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Area Ratio</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ws?Hs?2</cell><cell>p3</cell><cell>p2 vmax</cell></row><row><cell cols="2">Skip Connections</cell><cell></cell></row><row><cell>ConvSets</cell><cell>UpSample</cell><cell>Cat</cell></row><row><cell cols="2">Backbone Features</cell><cell>SPP</cell><cell>Drop Blocks</cell></row><row><cell>Backbone</cell><cell></cell><cell cols="2">FPN</cell></row><row><cell cols="4">Fig. 3. The GGHL framework comprises (a) the proposed OAL strategy, (b)</cell></row><row><cell cols="4">the CNN model with developed ORC and OWAM, (c) the designed objective</cell></row><row><cell>function JOL.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I SUMMARYshape and direction OBB Gaussian Positive Candidate Region Positive Sample Negative Sample p 1 p 2 p 3 p 4 v min v maxOBB HBB OBB of GGHL Stride=8,16,32 GGHL label assignment Training Sample Space:</head><label>I</label><figDesc>OF THE DEFINITION OF ORC VARIABLES AT (x, y) m</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Value of</cell></row><row><cell>Variable</cell><cell></cell><cell></cell><cell>Definition</cell><cell>Dimension</cell><cell>Each</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Component</cell></row><row><cell></cell><cell cols="4">Ground truth representing that the</cell></row><row><cell>objx,y,m</cell><cell cols="4">location (x, y) m is positive or negative</cell><cell>Scalar</cell><cell>1 or 0</cell></row><row><cell>obj x,y,m</cell><cell cols="4">Prediction score that the location objx,y,m is positive</cell><cell>(0, 1)</cell></row><row><cell>lx,y,m</cell><cell></cell><cell cols="3">Vector of the distances from objx,y,m to the</cell><cell>1 ? 4</cell><cell>[0, +?)</cell></row><row><cell>lx,y,m</cell><cell></cell><cell cols="3">HBB boundaries</cell></row><row><cell>sx,y,m</cell><cell cols="4">Vector of the distances from the HBB vertices to the corresponding</cell><cell>1 ? 4</cell><cell>[0, 1]</cell></row><row><cell>sx,y,m</cell><cell></cell><cell cols="3">OBB vertices at (x, y) m</cell></row><row><cell>arx,y,m arx,y,m</cell><cell></cell><cell cols="3">Area ratio of the HBB and OBB at (x, y) m</cell><cell>Scalar</cell><cell>[0, 1]</cell></row><row><cell>obbx,y,m obj x,y,m</cell><cell></cell><cell cols="3">Vector representing the OBB at (x, y) m</cell><cell>1 ? 9</cell><cell>[0, 1]</cell></row><row><cell>clsx,y,m</cell><cell></cell><cell cols="3">Ground truth vector of classification at (x, y) m</cell><cell>1 ? num cls</cell><cell>1 or 0</cell></row><row><cell>clsx,y,m</cell><cell cols="4">Prediction vector of classification at (x, y) m</cell><cell>[0, 1)</cell></row><row><cell></cell><cell>l1</cell><cell>l2</cell><cell>l3</cell><cell>l4</cell></row><row><cell></cell><cell>s1</cell><cell>s2</cell><cell>s3</cell><cell>s4</cell></row></table><note>Label: [x1,y1,x2,y2,x3,y3,x4,y4,class]Consider: size,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Image Positive Sample Negative Sample Gaussian Center Prior Harbor Harbor 0&lt;Fx1,y1,m&lt;1 Gaussian Positive Region Fm Fx2,y2,m=0 Adjustment Matrix Gx2,y2,m=0 0&lt;Gx1,y1,m&lt;1 Generate + Label objm Gm obj=1 obj=0 Mask of Fm ? OBB Regression Score Matrix CNN OECm Ground Truth Predictions</head><label></label><figDesc></figDesc><table /><note>OLA: Static Gaussian prior ORC: Learnable OBB representation OWAM: Dynamic weighting JOL: Loss Function</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II ABLATION</head><label>II</label><figDesc>EXPERIMENTS AND EVALUATIONS OF THE PROPOSED GGHL ON THE DOTA DATASET Note: Bold indicates the best result. The size of the input image is 800?800 pixels. The unit G is Giga, which represents 1?10 ?9 . The unit MB represents 1?10 ?6 bytes. The inference speed only includes the network inference speed without pre-processing &amp; post-processing. Vanilla-AB represents the anchor-based Vanilla model, and Vanilla-AF represents the anchor-free Vanilla model.</figDesc><table><row><cell></cell><cell>Methods</cell><cell>Data Augmentation MSC</cell><cell>Label Assignment Anchor-Box Standard Gaussian</cell><cell>OLA</cell><cell>OBB Representation Vanilla Head ORC ORC -OWAM</cell><cell>Objective Function Vanilla Loss JOL</cell><cell>mAP</cell><cell>Inference Speed (fps)</cell><cell>FLOPs (G)</cell><cell>Model Parameters (MB)</cell><cell>Number of Hyper-parameters</cell></row><row><cell>Anchor</cell><cell>Vanilla-AB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">72.55 38.77</cell><cell>130.93</cell><cell>68.87</cell><cell>19</cell></row><row><cell>-based</cell><cell>Vanilla-AB (MSC)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">74.64 38.77</cell><cell>130.93</cell><cell>68.87</cell><cell>19</cell></row><row><cell></cell><cell>Vanilla-AF (Baseline)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">72.33 42.39</cell><cell>121.84</cell><cell>62.59</cell><cell>3</cell></row><row><cell>Anchor</cell><cell>OLA + ORC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">74.89 42.39</cell><cell>121.84</cell><cell>62.59</cell><cell>3</cell></row><row><cell>-free</cell><cell>OLA + ORC-OWAM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">75.43 42.39</cell><cell>121.84</cell><cell>62.59</cell><cell>3</cell></row><row><cell></cell><cell>GGHL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">76.95 42.39</cell><cell>121.84</cell><cell>62.59</cell><cell>3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III MORE</head><label>III</label><figDesc>DETAILED MAP (%) RESULTS OF ABLATION EXPERIMENTS ON THE DOTA DATASET Baseline) 89.12 82.23 39.10 75.16 70.97 74.45 86.03 90.85 85.98 84.11 57.47 58.43 66.08 66.32 58.73 72.33 (Baseline) OLA + ORC 89.69 81.24 44.31 79.04 72.63 72.63 85.95 90.85 87.00 85.25 68.39 67.25 67.66 67.87 60.50 74.89 (+2.56) OLA + ORC-OWAM 89.25 82.56 44.47 77.21 73.41 80.00 83.67 90.81 87.63 84.03 68.93 65.38 69.37 67.26 67.55 75.43 (+3.10) GGHL 89.74 85.63 44.50 77.48 76.72 80.45 86.16 90.83 88.18 86.25 67.07 69.40 73.38 68.45 70.14 76.95 (+4.62)</figDesc><table><row><cell>Methods</cell><cell>PL</cell><cell>BD</cell><cell>BR</cell><cell>GTF</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA</cell><cell>HA</cell><cell>SP</cell><cell>HC</cell><cell>mAP</cell></row><row><cell>Vanilla-AB</cell><cell cols="16">89.66 81.39 41.09 66.99 69.22 72.36 86.76 90.89 84.79 85.20 56.12 65.41 69.29 67.64 61.48 72.55 (Baseline)</cell></row><row><cell>Vanilla-AB (MSC)</cell><cell cols="15">89.45 82.57 44.82 77.32 75.94 75.44 85.94 90.82 86.26 84.14 66.03 64.93 67.26 65.90 62.76</cell><cell>74.64 (+2.09)</cell></row><row><cell>Vanilla-AF (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Note: The size of the input image is 800?800 pixels. Bold indicates the best result. Vanilla-AB represents the anchor-based Vanilla model, and Vanilla-AF represents the anchor-free Vanilla model.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV EXPERIMENTS</head><label>IV</label><figDesc>WITH DIFFERENT VALUES OF T IoU , ? , ?, AND ? ON THE DOTA DATASET Bold indicates the best result. When evaluating one variable, the other variables are fixed to take the optimal value.</figDesc><table><row><cell>T IoU</cell><cell>mAP</cell><cell>?</cell><cell>mAP</cell><cell>?</cell><cell>mAP</cell><cell>?</cell><cell>mAP</cell></row><row><cell>0.3</cell><cell>76.95</cell><cell>2.0 2.5</cell><cell>74.83 75.71</cell><cell>0.1 0.3</cell><cell>75.42 76.67</cell><cell>without</cell><cell>76.13</cell></row><row><cell>0.4</cell><cell>76.18</cell><cell>3.0 3.5</cell><cell>76.95 74.99</cell><cell>0.5 1.0</cell><cell>76.95 74.89</cell><cell>with</cell><cell>76.95</cell></row><row><cell>Note:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V ABLATION</head><label>V</label><figDesc>EXPERIMENT OF ORC AND RA ON THE DOTA DATASET</figDesc><table><row><cell>Methods</cell><cell>minRect</cell><cell>RA</cell><cell>mAP</cell></row><row><cell>Gliding Vertex [26]</cell><cell></cell><cell></cell><cell>74.64 75.32</cell></row><row><cell>GGHL</cell><cell></cell><cell></cell><cell>76.28 76.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI ABLATION</head><label>VI</label><figDesc>EXPERIMENTS AND EVALUATIONS OF THE PROPOSED GGHL ON THE DOTA DATASET</figDesc><table><row><cell>Methods</cell><cell cols="2">Anchor Backbone</cell><cell>mAP</cell><cell>Inference Speed (fps)</cell></row><row><cell>R-CenterNet [19]</cell><cell>AF</cell><cell>DarkNet53</cell><cell>72.08</cell><cell>46.31</cell></row><row><cell>R-CenterNet [19] + GGHL1</cell><cell>AF</cell><cell cols="3">DarkNet53 73.63 (+1.57) 46.31 (+0)</cell></row><row><cell>R-FCOS-P5 [3]</cell><cell>AF</cell><cell>DarkNet53</cell><cell>73.48</cell><cell>42.39</cell></row><row><cell cols="2">R-FCOS-P5 + AutoAssign[18] AF</cell><cell cols="3">DarkNet53 75.34 (+1.86) 42.39 (+0)</cell></row><row><cell>R-FCOS-P5 [3] + GGHL2</cell><cell>AF</cell><cell cols="3">DarkNet53 76.57 (+3.09) 42.39 (+0)</cell></row><row><cell>NPMMR-Det [42]</cell><cell>AB</cell><cell>DarkNet53</cell><cell>75.67</cell><cell>32.52</cell></row><row><cell>NPMMR-Det [42] + GGHL</cell><cell>AF</cell><cell cols="3">DarkNet53 77.74 (+2.07) 35.98 (+3.46)</cell></row><row><cell>LO-Det 608 [12]</cell><cell cols="2">AB MobileNetv2</cell><cell>66.17</cell><cell>60.01</cell></row><row><cell>LO-Det [12] + GGHL 608</cell><cell cols="4">AF MobileNetv2 71.26 (+5.09) 62.07 (+2.06)</cell></row><row><cell cols="5">Note: The size of the default input image is 800?800 pixels. For the</cell></row><row><cell cols="5">lightweight detector LO-Det, the resolution of the CNN input layer is</cell></row><row><cell cols="5">608?608 pixels. AF represents anchor-free methods, and AB represents</cell></row><row><cell cols="5">anchor-based methods. The inference speed only includes the network</cell></row><row><cell cols="5">inference speed without post-processing. GGHL1: For embedding GGHL</cell></row><row><cell cols="5">into R-CenterNet, OLA and ORC are used but only the center point is taken</cell></row><row><cell cols="5">as a positive candidate like CenterNet; the original loss of CenterNet is</cell></row><row><cell cols="5">still used but weighted and regularized. GGHL2: OLA and ORC are used,</cell></row><row><cell cols="5">and loss is in the form of FCOS, but the Centerness is calculated by a</cell></row><row><cell cols="2">two-dimensional Gaussian function.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII ABLATION</head><label>VII</label><figDesc>EXPERIMENTS AND EVALUATIONS OF THE PROPOSED GGHL ON THE DOTA DATASET</figDesc><table><row><cell>Modules</cell><cell>mAP</cell><cell>Speed 1 (fps)</cell><cell>Speed 2 (fps)</cell><cell>Speed 3 (fps)</cell><cell>FLOPs (G)</cell><cell>Model Parameters (MB)</cell><cell>Number of Hyper -parameters</cell></row><row><cell cols="3">LO-Det 608 [12] 66.17 60.01</cell><cell>6.99</cell><cell cols="2">22.12 6.42</cell><cell>6.93</cell><cell>19</cell></row><row><cell>LO-Det [12]</cell><cell>71.26</cell><cell>62.07</cell><cell>7.68</cell><cell>23.72</cell><cell>6.30</cell><cell>6.72</cell><cell>3</cell></row><row><cell>+ GGHL 608</cell><cell>(+5.09)</cell><cell>(+2.06)</cell><cell>(+0.69)</cell><cell>(+1.60)</cell><cell>(-0.12)</cell><cell>(-0.21)</cell><cell>(-16)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII COMPARATIVE</head><label>VIII</label><figDesc>EXPERIMENTS ON THE DOTA DATASETMethodsYear Backbone Anchor PL BD BR GTF SV LV SH TC BC ST SBF RA HA SP HC mAP Speed (fps) ROI Trans.<ref type="bibr" target="#b25">[25]</ref> 2019 ResNet101 AB 88.53 77.91  37.63 74.08 66.53 62.97 66.57 90.50 79.46 76.75 59.04 56.73 62.54 61.29 55.56 67.74 7.80 RSDet [43] 2019 ResNet101 AB 89.80 82.90 48.60 65.20 69.50 70.10 70.20 90.50 85.60 83.40 62.50 63.90 65.60 67.20 68.00 72.20 -SCRDet [11] 2019 ResNet101 AB 89.98 80.65 52.09 68.36 68.36 60.32 72.41 90.85 87.94 86.86 65.02 66.68 66.25 68.24 65.21 72.61 9.51 R 3 Det [44] 2019 ResNet152 AB 89.80 83.77 48.11 66.77 78.76 83.27 87.84 90.82 85.38 85.51 65.67 62.68 67.53 78.56 72.62 76.47 10.53 Gliding Vertex [26] 2019 ResNet101 AB 89.64 85.00 52.26 77.34 73.01 73.14 86.82 90.74 79.02 86.81 59.55 70.91 72.94 70.86 57.32 75.02 13.10 O 2 -DNet [23] 2020 Hourglass-104 AF 89.31 82.14 47.33 61.21 71.32 74.03 78.62 90.76 82.23 81.36 60.93 60.17 58.21 66.98 61.03 71.04 -BBAVectors [22] 2020 ResNet101 AF 88.35 79.96 50.69 62.18 78.43 78.98 87.94 90.85 83.58 84.35 54.13 60.24 65.22 64.28 55.70 72.32 18.37 DRN [8] 2020 Hourglass-104 AF 89.71 82.34 47.22 64.10 76.22 74.43 85.84 90.57 86.18 84.89 57.65 61.93 69.30 69.63 58.48 73.23 -CSL [27] 2020 ResNet152 AB 90.25 85.53 54.64 75.31 70.44 73.51 77.62 90.84 86.15 86.69 69.60 68.04 73.83 71.10 68.93 76.17 -S 2 A-Net [45] 2020 ResNet50 AB 89.07 82.22 53.63 69.88 80.94 82.12 88.72 90.73 83.77 86.92 63.78 67.86 76.51 73.03 56.60 76.38 17.60 S 2 A-Net [45] 2020 ResNet101 AB 88.89 83.60 57.74 81.95 79.94 83.19 89.11 90.78 84.87 87.81 70.30 68.25 78.30 77.01 69.58 79.42 13.79 CFC-Net [46] 2021 ResNet50 AB 89.08 80.41 52.41 70.02 76.28 78.11 87.21 90.89 84.47 85.64 60.51 61.52 67.82 68.02 50.09 73.50 17.81 RIDet-O (RIL) [29] 2021 ResNet101 AB 88.94 78.45 46.87 72.63 77.63 80.68 88.18 90.55 81.33 83.61 64.85 63.72 73.09 73.13 56.87 74.70 13.36 S 2 A-Net + RIL [29] 2021 ResNet50 AB 89.31 80.77 54.07 76.38 79.81 81.99 89.13 90.72 83.58 87.22 64.42 67.56 78.08 79.17 62.07 77.62 17.25 RetinaNet-GWD [30] 2021 ResNet152 AB 86.14 81.59 55.33 75.57 74.20 67.34 81.75 87.48 82.80 85.46 69.47 67.20 70.97 70.91 74.07 75.35 11.65 R 3 Det-GWD [30] 2021 ResNet50 AB 88.89 83.58 55.54 80.46 76.86 83.07 86.85 89.09 83.09 86.17 71.38 64.93 76.21 73.23 64.39 77.58 16.22 R 3 Det-GWD [30] 2021 ResNet152 AB 88.99 82.26 56.62 81.40 77.04 83.90 86.56 88.97 83.63 86.48 70.45 65.58 76.41 77.30 69.21 78.32 10.50 GGHL 2021 DarkNet53 AF 89.74 85.63 44.50 77.48 76.72 80.45 86.16 90.83 88.18 86.25 67.07 69.40 73.38 68.45 70.14 76.95 42.30 NPMMR-Det-GGHL 2021 DarkNet53 AF 89.16 85.71 48.18 78.86 77.29 82.26 87.58 90.88 88.04 86.86 65.74 69.82 74.44 70.75 70.47 77.74 35.98 LO-Det-GGHL 2021 MobileNetv2 AF 89.66 83.02 38.55 77.09 72.57 71.86 82.47 90.78 78.05 83.56 47.74 67.83 64.21 67.83 54.16 71.26 62.07 Note: Bold font indicates the best results. AF represents anchor-free methods, and AB represents anchor-based methods. The inference speed only includes the network inference speed (batch size=1) without post-processing on an RTX 3090 GPU. When testing other methods, their open source codes are used. Since the deep learning frameworks are different, there may be slight relative errors in the test speed. Some methods'codes are not open-source, which is indicated by "-". Regarding some methods, we have tried our best but failed to reproduce the results shown in their original papers, so the best results reported by them are shown in theTable.To align the other tricks of GGHL and GWD, the result of combining tricks for GWD (DA+MS+MSC) is selected as a comparison.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IX</head><label>IX</label><figDesc></figDesc><table><row><cell cols="5">COMPARATIVE EXPERIMENTS ON DOTAV1.0, DOTAV1.5, AND</cell></row><row><cell cols="3">DOTAV2.0 [40] DATASETS</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">mAP@v1.0 mAP@v1.5 mAP@v2.0 Speed (fps)</cell></row><row><cell>RetinaNet OBB [38]</cell><cell>66.28</cell><cell>59.16</cell><cell>46.68</cell><cell>12.10</cell></row><row><cell>Mask R-CNN [40]</cell><cell>70.71</cell><cell>62.67</cell><cell>49.47</cell><cell>9.70</cell></row><row><cell>Cascade Mask R-CNN [40]</cell><cell>70.96</cell><cell>63.41</cell><cell>50.04</cell><cell>7.20</cell></row><row><cell>Hybrid Task Mask [40]</cell><cell>71.21</cell><cell>63.40</cell><cell>50.34</cell><cell>7.90</cell></row><row><cell>Faster R-CNN OBB [1]</cell><cell>69.36</cell><cell>62.00</cell><cell>47.31</cell><cell>14.10</cell></row><row><cell>Faster R-CNN OBB + Dpool [40]</cell><cell>70.14</cell><cell>62.20</cell><cell>48.77</cell><cell>12.10</cell></row><row><cell>Faster R-CNN H-OBB [40]</cell><cell>70.11</cell><cell>62.57</cell><cell>48.90</cell><cell>13.70</cell></row><row><cell>Faster R-CNN OBB + RT [40]</cell><cell>73.76</cell><cell>65.03</cell><cell>52.81</cell><cell>12.40</cell></row><row><cell>GGHL</cell><cell>73.98</cell><cell>68.92</cell><cell>57.17</cell><cell>41.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE X COMPARATIVE</head><label>X</label><figDesc>EXPERIMENTS AND EVALUATIONS OF THE PROPOSED GGHL ON THE SKU-110R DATASET</figDesc><table><row><cell>Methods</cell><cell cols="4">Backbone Anchor AP@0.75 Speed (fps)</cell></row><row><cell>YOLOv3-R [8]</cell><cell>DarkNet53</cell><cell>AB</cell><cell>51.10</cell><cell>44.07#</cell></row><row><cell>CenterNet-R [8]</cell><cell cols="2">Hourglass-104 AF</cell><cell>61.10</cell><cell>-</cell></row><row><cell>DRN [8]</cell><cell cols="2">Hourglass-104 AF</cell><cell>63.10</cell><cell>-</cell></row><row><cell cols="2">Vanilla-AF (Baseline) DarkNet53</cell><cell>AF</cell><cell>60.61</cell><cell>44.13</cell></row><row><cell>GGHL</cell><cell>DarkNet53</cell><cell>AF</cell><cell>63.73</cell><cell>44.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XI</head><label>XI</label><figDesc>AF represents anchor-free methods, and AB represents anchor-based methods. Some methods' codes are not open-source, the unreported results of which is indicated by "-". "#" indicates the results we reproduced.AOOD datasets including SUK110R<ref type="bibr" target="#b8">[8]</ref> and SSDD+<ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b48">48]</ref> to verify the effectiveness of the proposed GGHL comprehensively. Their image types contain optical RGB image and polarized SAR image. The challenges they face include dense instances, noise interference, and diverse object appearances. The experimental results are shown in Table X-XI andFig. 13. On the SKU-110R dataset, the mAP and speed of the proposed GGHL surpass those of the existing methods. Compared with the baseline, GGHL makes the AP@0.75 increase by 1.3. On the SSDD+ dataset, compared with the baseline, GGHL makes the AP@0.5 increase by 3.18 without reducing the speed. The lightweight model LO-Det+GGHL not only has a slightly higher accuracy than DRBox-v2, but also has a faster speed. In summary, extensive and in-depth experiments on multiple datasets have verified the effectiveness of the proposed GGHL and demonstrated the evaluation results of its performance.</figDesc><table><row><cell>plane large-vehicle plane large-vehicle plane large-vehicle soccer-ball-field soccer-ball-field soccer-ball-field</cell><cell>baseball-diamond ship baseball-diamond ship baseball-diamond ship roundabout roundabout roundabout</cell><cell>bridge tennis-court bridge tennis-court bridge tennis-court harbor harbor harbor</cell><cell>ground-track-field basketball-court ground-track-field basketball-court ground-track-field basketball-court swimming-pool swimming-pool swimming-pool</cell><cell>COMPARATIVE EXPERIMENTS AND EVALUATIONS OF THE PROPOSED GGHL ON THE SSDD+ DATASET Methods Backbone Anchor AP@0.3 AP@0.5 Speed (fps) DRBox-v1 [47] VGG16 AB 86.41 --SDOE [48] VGG16 AB -82.40 -DRBox-v2 [47] VGG16 AB 92.81 85.17# 49.09# Vanilla-AF (Baseline) DarkNet53 AF 95.09 87.04 43.87 GGHL DarkNet53 AF 95.10 90.22 43.87 LO-Det + GGHL 608 MobileNetv2 AF 94.18 85.90 62.66 Note: small-vehicle storage-tank small-vehicle storage-tank small-vehicle storage-tank helicopter helicopter helicopter</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FCOS: Fully convolutional onestage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision</title>
		<meeting><address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dense attention fluid network for salient object detection in optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1305" to="1317" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DOTA: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, Utah, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning rotation-invariant and fisher discriminative convolutional neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="265" to="278" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object detection in optical remote sensing images: A survey and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="296" to="307" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic refinement network for oriented and densely packed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="11" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TextBoxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3676" to="3690" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="9759" to="9768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SCRDet: Towards more robust detection for small, cluttered and rotated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="8232" to="8241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LO-Det: Lightweight oriented object detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dynamic anchor learning for arbitrary-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning center probability map for detecting objects in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="4307" to="4323" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dense label encoding for boundary discontinuity free rotation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Oriented r-cnn for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3520" to="3529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">AutoAssign: Differentiable label assignment for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03496</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">IENet: Interacting embranchment one stage anchor free detector for orientation aerial object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00969</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Anchorfree oriented proposal generator for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.01931</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Oriented object detection in aerial images with box boundary-aware vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020-12" />
			<biblScope unit="page" from="2150" to="2159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Oriented objects as pairs of middle lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="page" from="268" to="279" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Prime sample attention in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="11" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning RoI transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Rcognition</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented object detection with circular smooth label</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 European Conference on Computer Vision</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="page" from="677" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PIoU loss: Towards accurate oriented object detection in complex environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="page" from="195" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Optimization for oriented object detection via representation invariance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11636</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Rethinking rotated object detection with gaussian wasserstein distance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11952</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning high-precision bounding box for rotated object detection via kullback-leibler divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01883</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">YOLOv3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Iqdet: Instance-wise quality distribution sampling for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1717" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">DC-SPP-YOLO: Dense connection and spatial pyramid pooling based YOLO for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">522</biblScope>
			<biblScope unit="page" from="241" to="258" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DropBlock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="10" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/CVF International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">ReDet: A rotation-equivariant detector for aerial object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07733</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12219</idno>
		<title level="m">Object detection in aerial images: A large-scale benchmark and challenges</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ship detection in SAR images based on an improved faster R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 SAR in Big Data Era: Models, Methods and Applications</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-11" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A novel nonlocal-aware pyramid and multiscale multitask refinement detector for object detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning modulated loss for rotated object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08299</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">R3Det: Refined single-stage detector with feature refinement for rotating object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05612</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Align deep features for oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">CFC-Net: A critical feature capturing network for arbitrary-oriented object detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06849</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">DRBox-v2: An improved detector with rotatable boxes for target detection in SAR images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="8333" to="8349" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simultaneous ship detection and orientation estimation in SAR images based on attention module and angle regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9-2851</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
