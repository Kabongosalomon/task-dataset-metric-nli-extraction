<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disentangling Monocular 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mapillary Research</orgName>
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mapillary Research</orgName>
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mapillary Research</orgName>
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>L?pez-Antequera</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mapillary Research</orgName>
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mapillary Research</orgName>
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Disentangling Monocular 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Results obtained from our single image, monocular 3D object detection network MonoDIS on a KITTI3D test image with corresponding birds-eye view, showing its ability to estimate size and orientation of objects at different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this paper we propose an approach for monocular 3D object detection from a single RGB image, which leverages a novel disentangling transformation for 2D and 3D detection losses and a novel, self-supervised confidence score for 3D bounding boxes. Our proposed loss disentanglement has the twofold advantage of simplifying the training dynamics in the presence of losses with complex interactions of parameters, and sidestepping the issue of balancing independent regression terms. Our solution overcomes these issues by isolating the contribution made by groups of parameters to a given loss, without changing its nature. We further apply loss disentanglement to another novel, signed Intersection-over-Union criterion-driven loss for improving 2D detection results. Besides our methodological innovations, we critically review the AP metric used in KITTI3D, which emerged as the most important dataset for comparing 3D detection results. We identify and resolve a flaw in the 11-point interpolated AP metric, affecting all previously published detection results and particularly biases the results of monocular 3D detection. We provide extensive experimental evaluations and ablation studies on the KITTI3D and nuScenes datasets, setting new state-of-theart results on object category car by large margins.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent developments in object recognition <ref type="bibr" target="#b20">[21]</ref> have led to near-human performance on monocular 2D detection tasks. For applications with given, realistic accuracy requirements or constraints on computational budget, it is possible to choose general-purpose 2D object detectors from a large pool <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>The performance situation considerably changes in the 3D object detection case. Even though there are promising methods based on multi-sensor fusion (usually exploiting LIDAR information <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b35">36]</ref> next to RGB images), 3D detection results produced from a single, monocular RGB input image lag considerably behind. This can be attributed to the ill-posed nature of the problem, where a lack of explicit knowledge about the unobserved depth dimension introduces ambiguities in 3D-to-2D mappings and hence significantly increases the task complexity.</p><p>To still enable 3D object detection from monocular images, current works usually make assumptions about the scene geometry, camera setup or the application (e.g. that cars cannot fly <ref type="bibr" target="#b28">[29]</ref>). The implementation of such priors determines the encoding of extent and location/rotation of the 3D boxes, the corresponding 2D projections or their 3D box center depths. The magnitudes of these parameters have different units and therefore non-comparable meanings, which can negatively affect the optimization dynamics when error terms based on them are directly combined in a loss function. As a consequence, state-of-the-art, CNN-based monocular 3D detection methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref> report to train their networks in a stage-wise way. First the 2D detectors are trained until their performance stabilizes, before 3D reasoning modules can be integrated. While stage-wise training per se is not unusual in the context of deep learning, it could be an indication that currently used loss functions are yet sub-optimal.</p><p>A significant amount of recent works are focusing their experimental analyses on the KITTI3D dataset <ref type="bibr" target="#b7">[8]</ref>, and in particular its Car category <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42]</ref>. The availability of suitable benchmark datasets confines the scope of experimental analyses and when only few datasets are available, progress in the research field is strongly tied to the expressiveness of used evaluation metrics. KITTI3D adopted the 11-point Interpolated Average Precision metric <ref type="bibr" target="#b34">[35]</ref> used in the PASCAL VOC2007 <ref type="bibr" target="#b6">[7]</ref> challenge. We found a major flaw in the metric where using a single, confident detection result per difficulty category (KITTI3D distinguishes between easy, moderate and hard samples) suffices to obtain AP scores of ? 9% on a dataset level, which is up to 3? higher than the performance reported by recent works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>The contributions of our paper disentangle the task of monocular 3D object detection at several levels. Our major technical contribution disentangles dependencies of different parameters by isolating and handling parameter groups individually at a loss-level. This overcomes the issue of non-comparability for parameter magnitudes, while preserving the nature of the final loss. Our loss disentanglement significantly improves losses on both, 2D and 3D tasks. It also enables us to effectively train the entire CNN architecture (2D+3D) together and end-to-end, without the need of hyperparameter-sensitive, stage-wise training or warm-up phases. As additional contributions we i) leverage 2D detection performance through a novel loss based on a signed Intersection-over-Union criterion and ii) introduce a loss term for predicting detection confidence scores of 3D boxes, learned in a self-supervised way.</p><p>Another major contribution is a critical review of the 3D metrics used to judge progress in monocular 3D object detection, with particular focus on the predominantly used KITTI3D dataset. We observe that a flaw in the definition of the 11-point, interpolated AP metric significantly biases 3D detection results at the performance level of current stateof-the-art methods. Our applied correction, despite bringing all works evaluating on KITTI3D back down to earth, more adequately describes their true performance.</p><p>For all our contributions, we provide ablation studies on the KITTI3D and the novel nuScenes <ref type="bibr" target="#b1">[2]</ref> driving datasets. Fair comparisons indicate that our work considerably improves over current monocular 3D detection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We review the most recent, related works from 3D object detection and group them according to the data modalities used therein. After discussing RGB-only works just like ours, we list works exploiting also depth and/or synthetic data augmentation or 3D shape information, before finalizing with a high-level summary about LIDAR and/or stereo-based approaches.</p><p>RGB images only. Deep3DBox <ref type="bibr" target="#b23">[24]</ref> proposed to estimate full 3D pose and object dimensions from a 2D box by exploiting constraints from projective geometry. The core idea is that the perspective projection of a 3D bounding box should fit tightly to at least one side of its corresponding 2D box detection. In SSD-6D <ref type="bibr" target="#b11">[12]</ref> an initial 2D detection hypothesis is lifted to provide 6D pose of 3D objects by using structured discretizations of the full rotational space. 3D model information is learned by only training from synthetically augmented datasets. OFTNet <ref type="bibr" target="#b32">[33]</ref> introduces an orthographic feature transform, mapping features extracted from 2D to a 3D voxel map. The voxel map's features are eventually reduced to 2D (birds-eye view) by integration along the vertical dimension, and detection hypotheses are efficiently processed by exploiting integral-image representations. Mono3D <ref type="bibr" target="#b3">[4]</ref> emphasized on generation of 3D candidate boxes, scored by different features like class semantics, contour, shape and location priors. Even though at test time the results are produced based on single RGB images only, their method also requires semantic and instance segmentation results as input. The basic variant (w/o using depth) of ROI-10D <ref type="bibr" target="#b22">[23]</ref> proposes a novel loss to lift 2D detection, orientation and scale into 3D space that can be trained in an end-to-end fashion. FQNet <ref type="bibr" target="#b19">[20]</ref> infers a fitting quality criterion in terms of 3D IoU scores, allowing them to filter estimated 3D box proposals based on using only 2D object cues. MonoGRNet <ref type="bibr" target="#b28">[29]</ref> is the current state-of-the-art for RGB-only input, using a CNN comprised of four sub-networks for 2D detection, instance depth estimation, 3D location estimation and local corner regression, respectively. The three latter sub-networks emphasize on geometric reasoning, i.e. instance depth estimation predicts the central 3D depth of the nearest object instance, 3D location estimation seeks for the 3D bounding box center by exploiting 3D to 2D projections at given instance depth estimations, and local corner regression directly predicts the eight 3D bounding box corners in a local (or allocentric <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref> way). It is relevant to mention that <ref type="bibr" target="#b28">[29]</ref> reports that training was conducted stage-wise: First, the backbone is trained together with the 2D detector using Adam. Next, the geometric reasoning modules are trained (also with Adam). Finally, the whole network is trained end-to-end using stochastic gradient descent. The work in <ref type="bibr" target="#b0">[1]</ref> learns to estimate correspondences between detected 2D keypoints and 3D counterparts. However, this requires manual annotations on the surface of 3D CAD models and is limited in dealing with occluded objects. Including depth. An expansion stage of ROI-10D <ref type="bibr" target="#b22">[23]</ref> takes advantage of depth information provided by Su-perDepth <ref type="bibr" target="#b26">[27]</ref>, which itself is learned in a self-supervised manner. In <ref type="bibr" target="#b41">[42]</ref>, a multi-level fusion approach is proposed, exploiting disparity estimation results from a pre-trained module during both, the 2D box proposal generation stage as well as the 3D prediction part of their network.</p><p>Including 3D shape information. 3D-RCNN <ref type="bibr" target="#b12">[13]</ref> exploits the idea of using inverse graphics for instance-level, amodal 3D shape and pose estimation of all object instances per image. They propose a differentiable Render-and-Compare loss, exploiting available 2D annotations in existing datasets for guiding optimization of 3D object shape and pose. In <ref type="bibr" target="#b24">[25]</ref>, the recognition task is tackled by jointly reasoning about the 3D shape of multiple objects. Deep-MANTA <ref type="bibr" target="#b2">[3]</ref> uses 3D CAD models and annotated 3D parts in a coarseto-fine localization process. The work in <ref type="bibr" target="#b25">[26]</ref> encodes shape priors using keypoints for recovering the 3D pose and shape of a query object. In Mono3D++ <ref type="bibr" target="#b10">[11]</ref>, the 3D shape and pose for cars is provided by using a morphable wireframe, and it optimizes projection consistency between generated 3D hypotheses and corresponding, 2D pseudomeasurements.</p><p>LIDAR and/or stereo-based. 3DOP <ref type="bibr" target="#b4">[5]</ref> exploits stereo images and prior knowledge about the scene to directly reason in 3D. Stereo R-CNN <ref type="bibr" target="#b15">[16]</ref> tackles 3D object detection by exploiting stereo imagery and produces stereo boxes, keypoints, dimensions and viewpoint angles, summarized in a learned 3D box estimation module. In MV3D <ref type="bibr" target="#b5">[6]</ref>, a sensorfusion approach for LIDAR and RGB images is presented, approaching 3D object proposal generation and multi-view feature fusion via individual sub-networks. Conversely, Frustrum-PointNet <ref type="bibr" target="#b27">[28]</ref> directly operates on LIDAR point clouds and aligns candidate points provided from corresponding 2D detections for estimating the final, amodal 3D bounding boxes. PointRCNN <ref type="bibr" target="#b35">[36]</ref> describes a 2-stage framework where the first stage provides bottom-up 3D proposals and the second stage refines them in canonical coordinates. RoarNet <ref type="bibr" target="#b36">[37]</ref> applies a 2D detector to first estimate 3D poses of objects from a monocular image before processing corresponding 3D point clouds to obtain the final 3D bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task Description</head><p>We address the problem of monocular 3D object detection, where the input is a single RGB image and the output consists in a 3D bounding box, expressed in camera coordinates, for each object that is present in the image (see, <ref type="figure">Fig. 1</ref>). As opposed to other methods in the literature, we do not take additional information as input like depth obtained from LIDAR or other supervised or self-supervised monocular depth estimators. Also the training data consists solely of RGB images with corresponding annotated 3D bounding boxes. Nonetheless, we require a calibrated setting so we assume that per-image calibration parameters are available both at training and test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Architecture</head><p>We adopt a two-stage architecture that shares a similar structure with the state-of-the-art <ref type="bibr" target="#b22">[23]</ref>. It consists of a single-stage 2D detector (first stage) with an additional 3D detection head (second stage) constructed on top of features pooled from the detected 2D bounding boxes. Details of the architecture are given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Backbone</head><p>The backbone we use is a ResNet34 <ref type="bibr" target="#b9">[10]</ref> with a Feature Pyramid Network (FPN) <ref type="bibr" target="#b17">[18]</ref> built on top of it. The FPN network has the same structure as in <ref type="bibr" target="#b18">[19]</ref> with 3+2 scales, connected to the output of modules conv3, conv4 and conv5 of ResNet34, corresponding to downsampling factors of ?8, ?16 and ?32, respectively. Our ResNet34 differs from the standard one by replacing BatchNorm+ReLU layers with the synchronized version of InPlaceABN (iABN sync ) activated with LeakyReLU with negative slope 0.01 as proposed in <ref type="bibr" target="#b33">[34]</ref>. This modification does not affect the performance of the network, but allows to free up a significant amount of GPU memory, which can be exploited to scale up the batch size or input resolution. All FPN blocks depicted in <ref type="figure" target="#fig_0">Fig. 2</ref> correspond to 3 ? 3 convolutions with 256 channels, followed by iABN sync . Inputs. The input x to the backbone is a single RGB image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outputs.</head><p>The backbone provides 5 output tensors {f 1 , . . . , f 5 } corresponding to the 5 different scales of the FPN network, covering downsampling factors of ?8, ?16, ?32, ?64, and ?128, each with 256 feature channels (see,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">2D Detection Head</head><p>We consider the head of the single-stage 2D detector implemented in RetinaNet <ref type="bibr" target="#b18">[19]</ref>, which applies a detection module independently to each output f i of the backbone described above. The detection modules share the same parameters but work inherently at different scales, according to the scale of the features that they receive as input. As opposed to the standard RetinaNet, we employ iABN sync also in this head. The head, depicted in <ref type="figure" target="#fig_2">Fig. 3</ref>, is composed of two parallel stacks of 3 ? 3 convolutions, and is parametrized by n a reference bounding box sizes (anchors) per scale level.</p><p>Inputs. The inputs are the 5 outputs {f 1 , . . . , f 5 } of the backbone, where f i has a spatial resolution of h i ? w i .</p><p>Outputs. For each image, and each input tensor f i , the 2D detection head generates n a bounding box proposals (one per anchor) for each spatial cell g in the h i ? w i grid. Each proposal for a given anchor a with size (w a , h a ) is encoded as</p><formula xml:id="formula_0">a 5-tuple (? 2D , ? u , ? v , ? w , ? h ) such that ? p 2D = (1 + e ??2D ) ?1 gives the confidence of the 2D bounding box prediction, ? (u b , v b ) = (u g +? u w a , v g +? v h a )</formula><p>gives the center of the bounding box with (u g , v g ) being the image coordinates of cell g, and ? (w b , h b ) = (w a e ?w , h a e ? h ) gives the bounding box size. <ref type="figure" target="#fig_4">Fig. 5</ref> gives a visual description of the head's outputs.</p><p>Losses. We employ the focal loss <ref type="bibr" target="#b18">[19]</ref> to train the bounding box confidence score. This loss takes the following form, for a given cell g and anchor a with target confidence y ? {0, 1} and predicted confidence p ? [0, 1]:</p><formula xml:id="formula_1">L conf 2D (p 2D , y) = ??y(1?p 2D ) ? log p 2D ???p ? 2D log(1?p 2D ) ,</formula><p>where ? ? [0, 1] and ? &gt; 0 are hyperparameters that modulate the importance of errors and positives, respectively,? = 1 ? ? and? = 1 ? y. The confidence target y does not depend on the regressed bounding box, but only on the cell g and the anchor a. It takes value 1 if the reference bounding box centered in (u g , v g ) with size (w a , h a ) exhibits an Intersection-over-Union (IoU) with a ground-truth bounding box larger than a given threshold ? iou . For each cell g and anchor a that matches a groundtruth bounding boxb with predicted bounding box</p><formula xml:id="formula_2">b = (u b ? w b 2 , v b ? h b 2 , u b + w b 2 , v b + h b 2 )</formula><p>we consider the following detection loss:</p><formula xml:id="formula_3">L bb 2D (b,b) = 1 ? sIoU(b,b) ,<label>(1)</label></formula><p>where sIoU represents an extension of the common IoU function, which prevents gradients from vanishing in case of non-overlapping bounding boxes. We call it signed IoU function, as, intuitively, it creates negative intersections in case of disjoint bounding boxes (see, Appendix A). In Sec. 5, we discuss a disentangling transformation of the loss in Eq. (1) that allows to isolate the contribution of each network's output to the loss, while preserving the fundamental nature of the loss.</p><p>Output Filtering. The dense output of the 2D head is filtered as in <ref type="bibr" target="#b18">[19]</ref>: first, detections with scores lower than 0.05 are discarded, then Non-Maxima Suppression (NMS) with IoU threshold 0.5 is performed on the 5000 top-scoring among the remaining ones, and the best 100 are kept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">3D Detection Head</head><p>The 3D detection head ( <ref type="figure" target="#fig_3">Fig. 4</ref>) regresses a 3D bounding box for each 2D bounding box returned by the 2D detection head (surviving the filtering step). It starts by applying ROIAlign <ref type="bibr" target="#b8">[9]</ref> to pool features from FPN into a 14 ? 14 grid for each 2D bounding box, followed by 2 ? 2 average pooling, resulting in feature maps with shape 7 ? 7 ? 128. The choice of which FPN output is selected for each bounding box b follows the same logic as in <ref type="bibr" target="#b17">[18]</ref>, namely the features are pooled from the output f k , where k = min(5, max <ref type="bibr">(</ref></p><formula xml:id="formula_4">1, 2 + log 2 ( ? w b h b /224) ))</formula><p>. On top of this, two parallel branches of fully connected layers with 512 channels compute the outputs detailed below. Each fully connected layer but the last one per branch is followed by iABN (non-synchronized).</p><p>Input. The inputs are a 2D bounding box proposal b returned by the 2D detection head and features f k from the backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output.</head><p>The head returns for each 2D proposal b with center (u b , v b ) and dimensions (w b , h b ) a 3D bounding box encoded in terms of a 10-tuple ? = (?z, ? u , ? v , ? W , ? H , ? D , q r , q i , q j , q k ) and an additional output ? 3D such that ? p 3D|2D = (1 + e ??3D ) ?1 represents the confidence of the 3D bounding box prediction given the 2D proposal, ? z = ? z +? z ? z represents the depth of the center C of the predicted 3D bounding box, where ? z and ? z are given, dataset-wide depth statistics,</p><formula xml:id="formula_5">? c = (u b +? u , v b +? v ) gives the position of C projected on the image plane (in image coordinates), ? s = (W 0 e ? W , H 0 e ? H , D 0 e ? D ) is the size of the 3D</formula><p>bounding box, where (W 0 , H 0 , D 0 ) is a given, datasetwide reference size, and ? q = q r + q i i + q j j + q k k is the quaternion providing the pose of the bounding box with respect to an allocentric <ref type="bibr" target="#b12">[13]</ref>, local coordinate system. <ref type="figure" target="#fig_4">Fig. 5</ref> gives a visual description of the head's outputs. Losses. Let ? be the 10-tuple representing the regressed 3D bounding box and letB ? R 3?8 be the ground-truth 3D bounding box in camera coordinates. By applying the lifting transformation F introduced in <ref type="bibr" target="#b22">[23]</ref> and reviewed in Appendix B, we obtain the predicted 3D bounding box B given the network's output ?, i.e. B = F(?). The loss on the 3D bounding box regression is then given by</p><formula xml:id="formula_6">L bb 3D (B,B) = 1 8 B ?B H ,<label>(2)</label></formula><p>where ? H denotes the Huber loss with parameter ? H applied component-wise to each element of the argument matrix. The loss for the confidence p 3D|2D about the predicted 3D bounding box is self-supervised by the 3D bounding box loss remapped into a probability range via the transforma-</p><formula xml:id="formula_7">tionp 3D|2D = e ? 1 T L bb 3D (B,B)</formula><p>, where T &gt; 0 is a temperature parameter. The confidence loss for the 3D bounding box is then the standard binary cross entropy loss:</p><formula xml:id="formula_8">L conf 3D (p 3D|2D ,p 3D|2D ) = ?p log p ? (1 ?p) log(1 ? p)</formula><p>, where we have omitted the subscripts for the sake of readability. This loss allows to obtain a more informed confidence about the quality of the returned 3D bounding box than just using the 2D confidence. Akin to the 2D case, we employ also a different variant of Eq. (2) that disentangles the contribution of groups of parameters in order to improve the stability and effectiveness of the training. Yet, the confidence computation will be steered by Eq. (2). Output Filtering. The final output will be filtered based on a combination of the 2D and 3D confidences, following a Bayesian rule. The 3D confidence p 3D|2D is implicitly conditioned on having a valid 2D bounding box and the latter probability is reflected by p 2D . At the same time the confidence of a 3D bounding box given an invalid 2D bounding box defaults to 0. Hence, the unconditioned 3D confidence can be obtained by the law of total probability as</p><formula xml:id="formula_9">p 3D = p 3D|2D p 2D .</formula><p>This is the final confidence that our method associates to each 3D detection and that is used to filter the predictions via a threshold ? conf . We do not perform further NMS steps on the regressed 3D bounding boxes nor filtering based on 3D prior knowledge (e.g. one could reduce false positives by dropping "flying" cars). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Disentangling 2D and 3D Detection Losses</head><p>In this section we propose a transformation that can be applied to the 2D bounding box loss L bb 2D and the 3D counterpart L bb 3D , as well as a broader set of loss functions. We call it disentangling transformation because it isolates the contribution of groups of parameters to a given loss, while preserving its inherent nature. Each parameter group keeps its independent loss term, but they are all made comparable, thus sidestepping the difficulty of finding a proper weighting. While losses that combine parameters in a single term, such as those in Eq. (1) and Eq. <ref type="formula" target="#formula_6">(2)</ref>, are immune to the balancing issue, they might exhibit bad dynamics during the optimization as we will show with a toy experiment. The transformation we propose, instead, retains the best of both worlds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Disentangling Transformation</head><p>Let L : Y ? Y ? R + be a loss function defined on a space Y (e.g. the space of 3D bounding boxes) such that L(y,?) = 0 if? = y. Let ? ? R d be a set of possible network outputs that can be mapped to elements of Y via a function ? that we assume to be one-to-one. This property holds for 2D bounding boxes via the common 4D parametrization (center + dimensions), as well as for the 3D bounding boxes via the 10D representation described in Sec. 4.3. In the latter case, ? coincides with the lifting transformation F. Let? be a fixed output element (e.g. a groundtruth bounding box) and consider a partitioning of the d dimensions of ? into k groups. To give a concrete example, in case of 2D bounding boxes we can have 2 groups of parameters: one for the dimensions, and one for the center. In the case of 3D bounding boxes we consider 4 groups related intuitively to depth, projected center, rotation and dimensions. Given ? ? ? we denote by ? j the sub-vector corresponding to the jth group and by ? ?j the sub-vector corresponding to all but the jth group. Moreover, given ?, ? ? ?, we denote by ?(? j , ? ?j ) the mapping of a parametrization that takes the jth group from ? and the rest of the parameters from ? . The disentanglement of loss L given?, the mapping ? and  a decomposition of parameters into k groups is defined as:</p><formula xml:id="formula_10">L dis (y,?) = k j=1 L(?(? j ,? ?j ),?) ,</formula><p>where ? = ? ?1 (y) and? = ? ?1 (?). The idea behind the transformation is very intuitive besides the mathematical formalism. We simply replicate k times the loss L, each copy having only a group of parameters that can be optimized, the other being fixed to the ground-truth parametrization, which can be recovered via ? ?1 . We have applied the disentangling transformation to both the 2D loss in Eq. (1) and to the 3D loss in Eq. (2) and used them to conduct our experiments, unless otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Explanatory Toy Experiment</head><p>The toy experiment consists in comparing the optimization trajectories when we employ the (entangled) 3D object detection loss L bb 3D and the disentangled counterpart, which is obtained by applying the disentangling transformation described in Sec. 5. We took a ground-truth detection case from KITTI3D and picked an illustrative initialization for the 3D box for optimization (see, <ref type="figure" target="#fig_5">Fig. 6</ref> green and red boxes, respectively).</p><p>We perform the experiment using stochastic gradient descent with learning rate 0.001, momentum 0.9 and no weight decay. We run the experiment for 3000 iterations. We report in <ref type="figure" target="#fig_6">Fig. 7</ref> (first 4 plots from the left) the trajectories of the optimization process for each group of parameters when the entangled and disentangled losses are used. The parameter groups describe box dimensions, rotation quaternion, projected center of the 3D bounding box on the image, and the depth of the 3D bounding box center. The benefits deriving from the use of the disentangled loss can be clearly seen in the plots. Convergence is much faster and smoother. We can see that the trajectories induced by the entangled loss are suboptimal, since they explore multiple configurations of parameters before approaching the correct one, sometimes with considerable deviations (see, e.g. the dimensions of the bounding box). As an example, we report in <ref type="figure" target="#fig_5">Fig. 6</ref> (right) the point where the entangled version attains the largest deviation in terms of bounding box dimensions from the ground-truth, which happens at iteration 150, while at this stage the optimization dynamics using the disentangled loss fixed already all parameters but the depth. Despite the quaternion being aligned with the ground-truth rotation axis from the beginning, the optimization dynamics with the entangled loss starts diverging from it, producing unnatural poses and sizes that are not properly penalized by the entangled loss as can be seen by the loss values reported for the two configurations. Such unstable supervision delivered by the entangled loss harms the generalization ca-pabilities of the network. Interestingly, even though the optimization process that uses the disentangled loss does not directly optimize L bb 3D , it can minimize it more quickly than the counterpart directly optimizing it (see, <ref type="figure" target="#fig_6">Fig. 7 last)</ref>.</p><p>We provide also a video on our project website that shows the evolution of the optimization process described above. <ref type="figure" target="#fig_5">Fig. 6</ref> gives an overview of the first frame (left column). For each optimized loss (entangled on top and disentangled on the bottom) we provide the ground truth 3D bounding box in green and the currently predicted one in red. The faces with thick lines and showing a cross represent the front of the car and the bottom of the car, respectively, while the white line connects the respective centers. We also show the birds-eye view, where we projected the bottom face (the one with the cross) on the ground plane. There we also report the value of the entangled loss L bb 3D for both approaches for direct comparison and the iteration number. The video has been rendered with a logarithmic time scale in order to emphasize the initial part of the dynamics, which is also the most informative one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Critical Review on the KITTI3D AP Metric</head><p>The KITTI3D benchmark dataset <ref type="bibr" target="#b7">[8]</ref> significantly determines developments and general progress on 3D object detection, and has emerged as the most decisive benchmark for monocular 3D detection algorithms like ours. It contains a total of 7481 training and 7518 test images and has no official validation set. However, it is common practice to split the training data into 3712 training and 3769 validation images as proposed in <ref type="bibr" target="#b4">[5]</ref>, and then report validation results. On the official test split, there is no common agreement which of the training sets to use, but in case validation data is used for snapshot cherry-picking, it is imperative to provide test data scores from the same model. Each 3D ground truth detection box is assigned to one out of three difficulty classes (easy, moderate, hard), and the used 11-point Interpolated Average Precision metric is separately computed on each difficulty class. This metric was originally proposed in <ref type="bibr" target="#b34">[35]</ref>, and was used in the PAS-CAL VOC challenges <ref type="bibr" target="#b6">[7]</ref> between 2007 and 2010. It approximates the shape of the Precision/Recall curve as</p><formula xml:id="formula_11">AP| R = 1 |R| r?R ? interp (r) ,</formula><p>averaging the precision values provided by ? interp (r). In the current setting, KITTI3D applies exactly eleven equally spaced recall levels, i.e. where ?(r) gives the precision at recall r, meaning that instead of averaging over the actually observed precision values per point r, the maximum precision at recall value greater or equal than r is taken. The recall intervals start at 0, which means that a single, correctly matched prediction (according to the applied IoU level) is sufficient to obtain 100% precision at the bottom-most recall bin. In other words, if for each difficulty level a single, but correct prediction is provided to the evaluation, this produces an AP| R11 score of 1/11 ? 0.0909 for the entire dataset, which as shown in our experimental section already outperforms a number of recent methods while it clearly does not properly assess the quality of an algorithm. In light of KITTI3Ds importance, we propose a simple but effective fix that essentially exploits more of the information provided by the official evaluation server and evaluation scripts. Instead of sub-sampling 11 points from the provided 41 points, we approximate the area under the curve by simply replacing R 11 with R 40 = {1/40, 2/40, 3/40, . . . , 1} thus averaging precision results on 40 recall positions but not at 0. This eliminates the glitch encountered at the lowest recall bin, and allows to postprocess all currently provided test server results on 2D and 3D AP scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments on KITTI3D</head><p>We focus the validation of our method on the KITTI3D benchmark dataset that we described in Sec. 6, using the 0.7 IoU threshold for calculating AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Pre-processing</head><p>We provide some observations about the annotations that can be found in the dataset, and some simple filtering steps that we have applied to the annotations of the training split defined in <ref type="bibr" target="#b4">[5]</ref>. DontCare areas. Besides standard classes such as Car, Pedestrian and Cyclist, KITTI3D provides DontCare annotations. This class is used to label portions of the image that potentially include positive instances which have not been labeled under the proper class for reasons such as high distance. Accordingly, we avoid harvesting negatives in the 2D detection head if an anchor has IoU above 50% with those areas. DontCare overlap. Some positive bounding boxes, such as cars that were too near to the camera, have an IoU with a DontCare bounding box greater than 50%. We decided to set those bounding boxes as DontCare. This adjustment converted 729 cars (5.0%) to DontCare. Full occlusion. Some valid bounding boxes are actually fully occluded by a nearer object. Keeping those bounding boxes as positive instances might harm the learning process, so we decided to delete them. This adjustment deleted 218 (1.5%) cars.</p><p>From a total number of 14357 cars that were annotated, the valid number of Car bounding boxes was 13410 (93.4%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Implementation Details</head><p>We give more details about our implementation and instantiation of hyperparameters, in order to enable the reproducibility of our results. 2D Detection Head. For each FPN level f i and each spatial cell g we employ a total of 15 anchors spanning on five aspect ratios { 1 3 , 1 2 , 1, 2, 3} and three scales {4s i 2 j 3 : j ? {0, 1, 2}}, where s i is the downsampling factor of f i . Each anchor is considered positive if its IoU with a ground truth instance is greater than ? iou = 0.5. 3D Detection. We used a reference Car size of W 0 = 1.53m, H 0 = 1.63m, D 0 = 3.88m and depth statistics of ? z = 28.01m and ? z = 16.32m. We filtered the final 3D detections with a score threshold of ? conf = 0.05. Losses. We applied the same weighting policies in all our experiments. We set weight 1.0 to all losses in the 2D detection head and 0.5 to all losses in the 3D detection head. The Huber parameters is set to ? H = 3.0 and the 3D confidence temperature of T = 1. Optimization. Our training schedule is the same for all experiments, and it does not involve any multi-step or warmup procedures. We used SGD with a learning rate set at 0.01 and apply weight decay of 0.0001 to all parameters but scale and biases of iABN. We also freeze conv1 and conv2 of ResNet34 in the backbone. We trained with batch size of 96 on 4 NVIDIA V-100 GPUs for a total of 20k iterations, scaling the learning rate by a 0.1 factor at 12k and 16k iterations. Our input resolution is set according to <ref type="bibr" target="#b22">[23]</ref>. We applied horizontal flipping as the only form of training-data augmentation. No augmentation was performed for test/validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">2D Detection</head><p>In a first set of experiments, we study the signed IoU loss function (Sec. 4.2) in isolation. To do this, we train our backbone + 2D head to perform pure 2D detection of cars in KITTI3D, comparing between the original RetinaNet regression loss, signed IoU and signed IoU with disentanglement. For this simpler task we reduce the training schedule to 3.5k iterations, with learning rate steps after 2k and 3k, while keeping all other parameters as in Sec. 7.2. As shown in Tab. 1, using signed IoU leads to a modest performance increase, which improves considerably when adding disentanglement.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">3D Detection</head><p>In this section we focus on our main task and perform a detailed ablation of our contributions, comparing the results with most relevant state-of-the-art algorithms for monocular 3D detection. Keeping the network architecture and training schedule fixed, we evaluate different loss functions and detection scoring strategies. Following the discussion in Sec. 6, we report both, our revised AP| R40 metric (Tab. 3) and the original AP| R11 (Tab. 5). Ablation study. First, we turn our attention to the 3D BB loss in Eq. (2), comparing it to the direct Regression of the 10D parameters ? <ref type="bibr" target="#b22">[23]</ref> (first two lines of both tables). Confirming the findings in <ref type="bibr" target="#b22">[23]</ref>, we observe increased 3D detection scores when tying all parameters together in a single (entangled) loss function in metric space. Perhaps surprisingly, 3D BB also leads to better 2D detection performance: we suppose this could be due to more informative gradients propagating from the 3D head improving the backbone features. Adding our disentangled 2D detection loss based on the signed IoU (Eq. (1)) and the 3D confidence prediction (Sec. 4.3), consistently improves performance for both Regression and 3D BB (third and fourth lines in the tables). Similarly, applying disentangling to the 3D BB loss improves 3D detection performance, and has an even larger impact on the 2D side. Bringing all our contributions together leads to noticeable performance increases under all considered metrics (MonoDIS). In Tab. 2 we conduct an additional ablation study on the validation set in <ref type="bibr" target="#b4">[5]</ref> to assess the importance of the 3D confidence prediction. To this end, we take our best model trained and evaluated with the 3D confidence prediction (p 3D , AP| Rxx ) and compare against the same model when the 2D confidence is returned (p 2D , AP| Rxx ) and when it is randomly sampled (random, AP| Rxx ) . The ability of computing a reliable estimation of the confidence about the prediction is of utmost importance as can be inferred by the drastic drop of performance that we get when replacing p 3D with p 2D , or with a random confidence. This is a direct consequence of the important role that the returned confidence plays in the AP metric. Comparison with SOTA. In Tab. 3, 4 and 5 we report validation and test set results, respectively, of many recent monocular 3D detection approaches. When evaluating on the validation set, we consider the split defined in <ref type="bibr" target="#b4">[5]</ref>, as is done in all the baselines. Please note that the works in <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41]</ref> are using yet another training/validation split, rendering their results incomparable to ours while yielding numerically comparable ranges to e.g. <ref type="bibr" target="#b19">[20]</ref>. For the test set, we consider both the split in <ref type="bibr" target="#b4">[5]</ref>, which is shared with OFTNet <ref type="bibr" target="#b32">[33]</ref> and ROI-10D <ref type="bibr" target="#b22">[23]</ref>, and a larger training split 1 , since the setting used for MonoGRNet <ref type="bibr" target="#b28">[29]</ref> is not clear. In   <ref type="table">Table 2</ref>: Results on KITTI3D when using p 2D or p 3D = p 3D|2D p 2D as the final confidence score to rank predictions. In addition, we report the performance when the confidence is sampled from a uniform distribution.</p><p>Tab. 3 we show AP| R40 scores 2 for the test set results, and in Tb. 4 the corresponding AP| R11 scores. Nonetheless, we would like to stress that the AP| R11 is biased by the issue reported in Section 6 and we invite to rather consider AP| R40 as the reference metric for fair comparison. With a single exception, our approach beats all baselines on all 3D and bird's eye view metrics, often by a large margin, despite the fact that some of the outperformed methods rely on additional data, such as synthetic images (ROI-10D <ref type="bibr" target="#b22">[23]</ref>), or a pre-trained monocular depth prediction network (ROI-10D <ref type="bibr" target="#b22">[23]</ref>, Xu et al. <ref type="bibr" target="#b41">[42]</ref>). Interestingly, from the validation set results in Tab. 5, many existing approaches score lower than the "single correct hypothesis" baseline (see Sec. 6) on 3D detection AP| R11 , highlighting the need for an improved AP metric.</p><p>Results on additional KITTI3D classes. In Tab. 6 we provide the AP| R11 and AP| R40 scores (at IoU treshold 0.5, see official evaluation scripts) obtained on the validation set in <ref type="bibr" target="#b4">[5]</ref> for classes Pedestrian and Cyclist (trained independently). If compared to the results on class Car, it can be seen that performances on these two particular classes are in general lower. The performance degradation on classes Pedestrian and Cyclist compared to Car is due to i) the reduced number of annotations which is ? 6? and ? 20? lower than Car for class Pedestrian and Cyclist, respectively, and ii) the higher impact that errors on localization have on the AP scores since the object xz-extent is typically smaller. For these reasons, similarly to <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42]</ref>, we put a larger focus on class Car in the main paper.</p><p>Qualitative results. In <ref type="figure" target="#fig_13">Fig. 10</ref> we show qualitative results on a set of images taken from the validation set for the classes Car (top), Pedestrian (middle) and Cyclist (bottom). We also provide a video <ref type="bibr" target="#b2">3</ref> showing detection results obtained on a sequence from the validation set. The structure of the frames is similar to the one in <ref type="figure" target="#fig_13">Fig. 10</ref>, where detections are shown on the right side and the corresponding birds-eye view on the left. For simplicity, we decided to display all the detections with the same color. <ref type="bibr" target="#b1">2</ref> We calculated these from the precision-recall values published in the KITTI3D leaderboard page. <ref type="bibr" target="#b2">3</ref> https://research.mapillary.com/publication/ MonoDIS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Experiments on nuScenes</head><p>We conduct additional experiments on the novel nuScenes dataset <ref type="bibr" target="#b1">[2]</ref>.</p><p>About the dataset. The nuScenes dataset provides multimodal, street-level data collected with a car equipped with 6 cameras, 1 LiDAR, 5 Radars and IMU. It contains 15h of driving data (242 km at average speed of 16 km/h) covering parts of the areas of Boston (Seaport and South Boston) and Singapore (One North, Holland Village and Queenstown). These two cities have been chosen due to their known dense traffic and highly challenging driving situations and driving routes are selected to capture a diverse set of locations, times and weather conditions. The dataset provides 360 ? , synchronized sensor coverage, calibration of sensor intrinsics and extrinsics parameters, and objects annotations for 23 different classes from 1000 selected scenes of 20s duration each. Annotated objects in the scenes come with a semantic category, 3D bounding box, tracking information, and attributes (visibility, activity and pose) for each frame they occur in. Detection task. The nuScenes detection tasks requires detecting 10 object classes in terms of full 3D bounding boxes, attributes and velocities. In this work, we will focus on detecting the full 3D bounding box of object belonging to class car, because the only available baselines at the time of writing are OFTNet (monocular RGB image-based) and PointPillar <ref type="bibr" target="#b13">[14]</ref> (LiDAR-based). Fair comparison can only be made to OFTNet, where results are reported only for category car (see, <ref type="bibr" target="#b1">[2]</ref>). Evaluation metric. The authors of nuScenes propose an alternative metric called nuScenes detection score (NDS) that combines a measure of the detection performance with quality terms of box location (ATE, average translation error), size (ASE, average scale error), orientation (AOE, average orientation error), attributes (AAE, average attribute error) and velocity (AVE, average velocity error). The detection performance is measured in terms of Average Precision (AP), but with matches determined based on 2D center distance on the ground plane. Also the AP score is calculated as the normalized area under the precision/recall curve by excluding the [0 ? 10%] range. The final score averages   AP over matching thresholds of D = {0.5, 1, 2, 4} meters and the set of classes C:</p><formula xml:id="formula_12">mAP = 1 |C||D| c?C d?D AP c,d ,</formula><p>where AP c,d is the AP score on class c with matching threshold d. Obtained results. We present in <ref type="figure" target="#fig_9">Fig. 8</ref> the results obtained on the car class in terms of Precision/Recall curves (for all distance thresholds in D), as well as error curves for translation, scale and orientation true positive metrics (at distance threshold 2m), produced by the official nuScenes evaluation scripts. For direct comparison to available OFTNet and PointPillar results from <ref type="bibr" target="#b1">[2]</ref>, we also provide Tab. 7. It is important to stress that direct comparison is only fair to OFTNet which is also purely image-based, unlike PointPillar, which is LiDAR-based. We are not reporting the NDS score as it also requires predictions for attributes and velocities. Since that would imply modifications of the network design it would also render results inconsistent with those obtained on KITTI3D in Sec. 7.</p><p>The results in Tab. 7 show that our approach improves by 42% over OFTNet (in absolute terms), considering the primary AP metric at a distance threshold of 2m. In addition, MonoDIS improves on all available True Positive metrics over OFTNet and even on 2/3 metrics when compared to PointPillar (LiDAR-based). Despite obtaining bet- ter (lower) TP metrics ASE and AOE compared to PointPillar, the main advantage of LiDAR-based methods are shown in their lower translation errors (and therefore also in the corresponding AP scores at various distances). We provide some qualitative results in <ref type="figure" target="#fig_7">Fig. 11</ref>, demonstrating promising 3D recognition performance without using LiDAR and therefore actively sensed depth information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusions</head><p>We proposed a new loss disentangling transformation that allowed us to effectively train a 3D object detection network end-to-end without the need of stage-wise training or warm-up phases. Our solution isolates the contribution    made by groups of parameters to a given loss into separate terms that retain the same nature of the original loss, thus being compatible without the need of further, cumbersome loss balancing steps. We proposed two further loss functions where i) is based on a novel signed Intersectionover-Union criterion to improve 2D detection results and ii) is used to predict a detection confidence for the 3D bounding box predictions, learned in a self-supervised way. Besides the methodological contributions, we reveal a flaw in the primary detection metric used in KITTI3D, where a single, correctly predicted bounding box yields overall AP scores of 9.09% on validation or test splits. Our simple fix corrects performance results of previously published methods in general, and shows how significantly it was biasing monocular 3D object detection results in particular.</p><p>In our extensive experimental results and ablation studies we demonstrated the effectiveness of our proposed model, and significantly improved over previous state-of-the-art on both, KITTI3D and the novel nuScenes dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Signed Intersection over Union</head><p>Letb = (? 1 ,v 1 ,? 2 ,v 2 ) and b = (u 1 , v 1 , u 2 , v 2 ) be two bounding boxes, where (u 1 , v 1 ) denotes the top-left corner and (u 2 , v 2 ) denotes the bottom-right corner. We define the signed intersection-over-union as follows:  </p><formula xml:id="formula_13">sIoU(b,b) = |b b | ? |b| + |b| ? |b b | ? ,<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Lifting Transformation</head><p>We review the lifting transformation used in <ref type="bibr" target="#b22">[23]</ref>. Let ? be the 10D network's output from which we compute the depth z of the 3D bounding box's center, its projection on the image place c = (u c , v c ), the dimensions of the 3D bounding box s = (W, H, D) and the unit quaternion q as described in Sec. 4.3 of the main paper. Let K be the 3 ? 3 matrix of intrinsics with entries:   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Backbone architecture. Rectangles in the "FPN" block represent convolutions followed by iABN sync .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>2D detection module. Rectangles represent convolutions. All convolutions but the last per row are followed by iABN sync .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>3D detection head. "FC" rectangles represent fully connected layers. All FCs except the last of each row are followed by iABN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of the semantics of the outputs of the 2D and 3D detection heads. Left: 2D bounding box regression on image plane. Center: 3D bounding box regression. Right: allocentric angle from bird-eye view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Sample frames from the toy experiment's video on both, entangled (top) and disentangled (bottom) runs. Optimization process at iteration 0 (left) and 150 (right). Green is the ground-truth target. Red is the current prediction. The face with thick lines represents the front of the car. The face with a cross represents the bottom of the car. The birds-eye view on the left shows the projection of the crossed face.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Trajectories of the optimization process for each group of parameters (dimensions, rotation quaternion, projected center, depth), when using the entangled (magenta) and disentangled (blue) 3D detection losses. Left-to-right: trajectories of dimensions, rotation quaternion (last 3 coordinates), projection of the 3D bounding box center on the image and depth of the 3D bounding box center. The last plot shows the evolution of the entangled L bb 3D loss for both cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>R 11 =</head><label>11</label><figDesc>{0, 0.1, 0.2, . . . , 1}. The interpolation function is defined as ? interp (r) = max r :r ?r ?(r ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>2D</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Performance plots for class Car in nuScenes. Left: Precision/Recall curves for AP metric at multiple distance thresholds in D. Right: Error/Recall curves for relevant TP errors metrics on translation (ATE), scale (ASE) and orientation (AOE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>intersection operation between bounding boxes, |b| gives the area of bounding box b and |b| ? = +|b| if u 2 &gt; u 1 and v 2 &gt; v 1 , ?|b| otherwise, gives the signed area of b, which corresponds to the standard area with positive sign only if the first corner of b is the top-left one, while the second corner is the bottom-right one. To give a better intuition we provide some examples in Fig. 9, where green and red colors encode positive and negative areas, respectively: Left-to-right, the first two examples boil down to standard IoU yielding positive values, while the last ones are examples yielding negative values. The sIoU score is bounded in [?1, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Five examples of computation of the proposed signed IoU. Top: Colored areas represent the numerator of the sIoU formula, where green denotes positive area, red denotes negative area; numbers represent the corner ordering. Bottom: Areas represent the denominator, which is always positive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>F(?) = 1 2 R+ sin ? 2 j</head><label>22</label><figDesc>x , C y , C z ) be the position of the center of the 3D bounding box. The lifting transformation is defined as: q c S B 0 + C where B 0 holds the corners of the unit cube [?1, 1] 3 , S is the diagonal matrix with entries s, and R q c is the 3 ? 3 rotation matrix corresponding to quaternion q c = q cos ? 2with ? = tan ?1 ( Cx Cz ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Example results for classes Car (top), Pedestrian (middle) and Cyclist(bottom) with corresponding birds-eye view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Example results for class Car on nuScenes dataset for images taken at different weather and illumination conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Ablation results on KITTI3D with 2D detection networks, AP| R40 scores.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>AP| R40 scores on KITTI3D: ablation results (white background), test set results of SOTA (grey background) and ours (green background).</figDesc><table><row><cell></cell><cell></cell><cell>2D detection</cell><cell></cell><cell></cell><cell>3D detection</cell><cell></cell><cell></cell><cell>Bird's eye view</cell><cell></cell></row><row><cell>Method</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell>OFTNet [33]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.28</cell><cell>2.50</cell><cell>2.27</cell><cell>9.50</cell><cell>7.99</cell><cell>7.51</cell></row><row><cell>FQNet [20]</cell><cell>90.45</cell><cell>88.83</cell><cell>77.55</cell><cell>3.48</cell><cell>2.42</cell><cell>1.96</cell><cell>6.51</cell><cell>4.62</cell><cell>3.99</cell></row><row><cell>ROI-10D w/ Depth, Synthetic [23]</cell><cell>75.33</cell><cell>69.64</cell><cell>61.18</cell><cell>12.30</cell><cell>10.30</cell><cell>9.39</cell><cell>16.77</cell><cell>12.40</cell><cell>11.39</cell></row><row><cell>MonoGRNet [29]</cell><cell>87.23</cell><cell>77.46</cell><cell>61.12</cell><cell>11.29</cell><cell>12.90</cell><cell>11.34</cell><cell>20.55</cell><cell>16.37</cell><cell>15.16</cell></row><row><cell>MonoDIS</cell><cell>89.61</cell><cell>83.80</cell><cell>70.84</cell><cell>8.26</cell><cell>6.15</cell><cell>6.06</cell><cell>13.10</cell><cell>11.12</cell><cell>9.35</cell></row><row><cell>MonoDIS, larger training split</cell><cell>90.31</cell><cell>87.58</cell><cell>76.85</cell><cell>11.81</cell><cell>15.12</cell><cell>12.71</cell><cell>18.88</cell><cell>19.08</cell><cell>17.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>AP| R11 scores on KITTI3D: test set results of SOTA (grey background) and ours (green background).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>AP| R11 scores on KITTI3D (0.7 IoU threshold): Ablation results (white background), val set results of SOTA (grey background).</figDesc><table><row><cell></cell><cell></cell><cell>2D detection</cell><cell></cell><cell></cell><cell>3D detection</cell><cell></cell><cell></cell><cell>Bird's eye view</cell><cell></cell></row><row><cell>Method, metric, class</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell cols="2">Moderate Hard</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell>MonoDIS, AP| R 11 , pedestrian</cell><cell>72.16</cell><cell>64.93</cell><cell>56.89</cell><cell>10.79</cell><cell>10.39</cell><cell>9.22</cell><cell>11.04</cell><cell>10.94</cell><cell>10.59</cell></row><row><cell>MonoDIS, AP| R 40 , pedestrian</cell><cell>72.78</cell><cell>65.56</cell><cell>56.50</cell><cell>3.20</cell><cell>2.28</cell><cell>1.71</cell><cell>4.04</cell><cell>3.19</cell><cell>2.45</cell></row><row><cell>MonoDIS, AP| R 11 , cyclist</cell><cell>67.81</cell><cell>49.15</cell><cell>47.26</cell><cell>5.27</cell><cell>4.55</cell><cell>4.55</cell><cell>5.52</cell><cell>4.66</cell><cell>4.55</cell></row><row><cell>MonoDIS, AP| R 40 , cyclist</cell><cell>68.12</cell><cell>47.45</cell><cell>45.60</cell><cell>1.52</cell><cell>0.73</cell><cell>0.71</cell><cell>1.87</cell><cell>1.00</cell><cell>0.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Results on the classes Pedestrian and Cyclist on the KITTI3D validation set (0.5 IoU threshold).</figDesc><table><row><cell>Method</cell><cell cols="7">AP Car ? [%] 0.5m 1.0m 2.0m 4.0m ATE [m] ASE [1-IoU] AOE [rad] TP Car ?</cell></row><row><cell cols="3">PointPillar 55.5 71.8</cell><cell>76.1</cell><cell>78.6</cell><cell>0.27</cell><cell>0.17</cell><cell>0.19</cell></row><row><cell>OFTNet</cell><cell>-</cell><cell>-</cell><cell>27.0</cell><cell>-</cell><cell>0.65</cell><cell>0.16</cell><cell>0.18</cell></row><row><cell>MonoDIS</cell><cell cols="2">10.7 37.5</cell><cell>69.0</cell><cell>85.7</cell><cell>0.61</cell><cell>0.15</cell><cell>0.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Performance comparison for results on category car in nuScenes dataset<ref type="bibr" target="#b1">[2]</ref>. Top row: LiDAR-based Point-Pillar results (listed for completeness). Bottom: Available OFTNet results vs. MonoDIS.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/MarvinTeichmann/KittiBox</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection via geometric reasoning on keypoints. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Barabanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Artemov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burnaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murashkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">nuScenes: A multimodal dataset for autonomous driving. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teuliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Pascal visual object classes (VOC) challenge. (IJCV)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="j">Mask R-CNN</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mono3d++: Monocular 3d vehicle detection with two-scale 3d hypotheses and task priors. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1901" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3D-RCNN: Instancelevel 3d object reconstruction via render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<idno>2019. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multitask multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>2019. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>abs/1612.03144</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno>abs/1708.02002</idno>
		<imprint>
			<date type="published" when="2004" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep fitting degree scoring network for monocular 3d object detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep learning for generic object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietik?inen</surname></persName>
		</author>
		<idno>abs/1809.02165</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Are cars just 3d boxes? jointly estimating the 3d shape of multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Muhammad Zeeshan Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<idno>2014. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Reconstructing vehicles from a single image: Shape priors for road scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V S</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
		<idno>2017. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Superdepth: Selfsupervised, super-resolved monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Monogrnet: A geometric reasoning network for 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>2017. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>2015. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1811.08188</idno>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of DNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR)</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Introduction to Modern Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>McGraw-Hill, Inc</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Roarnet: A robust 3d object detection based on region approximation refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<idno>abs/1811.03818</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Frustum convnet: Sliding frustums to aggregate local point-wise features for amodal 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<idno>abs/1903.01864</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Squeezedet: Unified, small, low power fully convolutional neural networks for real-time object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">(CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Subcategoryaware convolutional neural networks for object proposals and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
