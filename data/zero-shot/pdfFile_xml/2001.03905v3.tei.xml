<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-shot Action Recognition with Permutation-invariant Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Data61/CSIRO</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">The University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Data61/CSIRO</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Few-shot Action Recognition with Permutation-invariant Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Self-supervision assumes generating cheap-to-obtain data (e.g., augmentation by rotations) from the original data and imposing an auxiliary task whose goal is to predict the label of an augmentation with the goal of robust representation learning <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b5">4,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b20">19]</ref>. We are first to apply the self-supervision by alignment paradigm to the problem of robust attention training (we devise an augmentation-guided attention).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Many few-shot learning models focus on recognising images. In contrast, we tackle a challenging task of few-shot action recognition from videos. We build on a C3D encoder for spatio-temporal video blocks to capture short-range action patterns. Such encoded blocks are aggregated by permutation-invariant pooling to make our approach robust to varying action lengths and long-range temporal dependencies whose patterns are unlikely to repeat even in clips of the same class. Subsequently, the pooled representations are combined into simple relation descriptors which encode so-called query and support clips. Finally, relation descriptors are fed to the comparator with the goal of similarity learning between query and support clips. Importantly, to re-weight block contributions during pooling, we exploit spatial and temporal attention modules and self-supervision. In naturalistic clips (of the same class) there exist a temporal distribution shift-the locations of discriminative temporal action hotspots vary. Thus, we permute blocks of a clip and align the resulting attention regions with similarly permuted attention regions of non-permuted clip to train the attention mechanism invariant to block (and thus long-term hotspot) permutations. Our method outperforms the state of the art on the HMDB51, UCF101, miniMIT datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Few-shot learning is an open problem with the goal to design algorithms that learn in the low-sample regime. Examples include meta-learning <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b43">42]</ref>, robust feature representations by relation learning <ref type="bibr" target="#b45">[44,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b55">54]</ref>, gradient-based <ref type="bibr" target="#b60">[59,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b44">43]</ref> and hallucination strategies for insufficient data <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b56">55]</ref>.</p><p>However, very few papers address video-based few-shot learning. As annotating large video datasets is prohibitive, this makes the problem we study particularly valuable. While results are far from satisfactory on Kinetics <ref type="bibr" target="#b4">[3]</ref>, the largest action recognition dataset, its size of 300,000 video clips with hundreds of frames each exceeds the size of large-scale image ImageNet <ref type="bibr" target="#b39">[38]</ref> and Places205 <ref type="bibr" target="#b57">[56]</ref> datasets.</p><p>There exist few limited works on few-shot learning for action recognition <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b53">52,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b59">58]</ref>. However, they focus on modeling 3D body joints with graphs <ref type="bibr" target="#b18">[17]</ref>, <ref type="figure">Fig. 1</ref>. Augmentation-guided attention by alignment. <ref type="figure">Fig. 1a</ref> shows that discriminative action blocks (in red, top left) may be misaligned with discriminative action blocks of test clip (in blue, bottom left). If the attention unit observes different distributions of locations of discriminative blocks at training and testing time, it fails. With the right approach, one may overcome the distribution shift (top and bottom right panels). <ref type="figure">Fig.  1b</ref> shows how the augmentation-guided attention by alignment works for permutationbased augmentations: (i) we shuffle training blocks of a clip to train an attention on permuted blocks, (ii) we shuffle in the same way coefficients of the attention vector from the non-permuted blocks. Both attention vectors are then encouraged to align by a dedicated loss term during training. <ref type="figure">Fig. 1c</ref> shows histograms of alignment-errors on test data. The top histogram shows larger errors (no alignment loss used in training) while the bottom histogram shows small errors (alignment during testing improves).</p><p>attribute-based learning in generative models <ref type="bibr" target="#b36">[35]</ref>, network design for low-sample classification <ref type="bibr" target="#b53">[52]</ref> and salient memory approach <ref type="bibr" target="#b59">[58]</ref>. In contrast, we focus on robust relation/similarity, spatial and temporal modeling of short-and long-term range action patterns via permutation-invariant pooling and attention.</p><p>To obtain a robust few-shot action recognition approach, we investigate how to: (i) represent discriminative short-and non-repetitive long-term action patterns for relation/similarity learning, (ii) localize temporally discriminative action blocks with limited number of training samples, and (iii) deal with long-term temporal distribution shift of such discriminative patterns (these patterns never re-appear at the same temporal locations even for clips of the same class).</p><p>To address the first point, our early experiments indicated that short-term discriminative action patterns can be captured by an encoder with C3D convolutional blocks. Thus, resulting features from a clip undergo permutation-invariant pooling which discards long-term non-repetitive dependencies. Finally, pooled query/support representations form relation descriptors are fed into a comparator.</p><p>Regarding the second point, aggregating spatio-temporal blocks with equal weights is suboptimal. Thus, we devise spatial/temporal attention units to emphasize discriminative blocks. Under the low-sample regime, self-supervision by jigsaw and rotation 1 helps train a more robust encoder, comparator and attention.</p><p>However, vanilla attention (and/or self-supervision) cannot fully promote the invariance to temporal (or spatial) permutations as described next.</p><p>To address the third point, we note that long-term dependencies in clips are non-repetitive e.g., videos of the same class often contain relevant action blocks at different temporal locations. <ref type="figure">Figure 1a</ref> shows that discriminative blocks of training and testing clips of dance class do not align (top left vs. bottom left corner). By permuting the blocks of training (top right), one can make them align with the most discriminative test samples (bottom right). <ref type="figure">Figure 1b</ref> shows that for a given clip, we (i) shuffle its blocks and feed them to the attention mechanism (shuffling pass), (ii) we shuffle accordingly the attention coefficients from a nonshuffled pass through attention, and (iii) we force attention coefficients from both passes to align. Such an attention by alignment deals with the distribution shift of discriminative temporal (and spatial) patterns via jigsaw augmentation (but applies also to rotation, zoom, etc.) To summarize, our contributions include: i. A robust pipeline with a C3D-based encoder capturing short-term dependencies which yields block representations subsequently aggregated by permutationinvariant pooling into fixed-length representations which form relation descriptors for relational/similarity learning in an episodic setting <ref type="bibr" target="#b48">[47]</ref>. ii. Spatial and temporal attention units which re-weight block contributions during the aggregation step. To improve training of the encoder, comparator and the attention unit under the low-sample regime, we introduce spatial and temporal self-supervision by rotations, and spatial and temporal jigsaws.</p><p>iii. An improved self-supervised attention unit by applying augmentation patterns such as jigsaws and/or rotations on the input of the attention unit and aligning the output with augmented the same way attention vector coefficients from non-augmented data passed by the attention unit 1 . Thus, the attention unit becomes invariant to a given augmentation action by design. iv. We propose new data splits for a systematic comparison of few-shot action recognition algorithms and we make them available as existing approaches use each different pipeline concepts, data modality, data splits and protocols 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Below, we discuss zero-, one-and few-shot learning models followed by a discussion on self-supervised learning and second-order pooling. One-and few-shot learning models have been widely studied in both the shallow <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b11">10,</ref><ref type="bibr" target="#b3">2,</ref><ref type="bibr" target="#b9">8]</ref> and deep learning pipelines <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b48">47]</ref>. Motivated by the human ability to learn new concepts from few samples, early works <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b32">31]</ref> employ generative models with an iterative inference. Siamese Network <ref type="bibr" target="#b23">[22]</ref> is a two-stream convolutional neural network which generates image descriptors 2 Section 2.1 explains that existing works do not specify class/validation splits which yields ?6% variations in accuracy rendering their protocols highly inaccurate. Section 2.2 explains this issue and how we compare our method to existing works. and learns the similarity between them. Matching Network <ref type="bibr" target="#b50">[49]</ref> proposes querysupport episodic learning and L-way Z-shot learning protocols 3 . The similarity between a query and support images is learnt for each episode. At the testing time, each test query (of novel class) is compared against a handful of annotated test support images for rapid recognition. Prototypical Networks <ref type="bibr" target="#b45">[44]</ref> compute distances between a datapoint and class-wise prototypes. Model-Agnostic Meta-Learning (MAML) <ref type="bibr" target="#b12">[11]</ref> is trained on multiple learning tasks. Relation Net <ref type="bibr" target="#b48">[47]</ref> learns relations between query and support images, and it leverages a similarity learning neural network to compare query-support pairs. SalNet <ref type="bibr" target="#b56">[55]</ref> uses saliencyguided end-to-end sample hallucination to grow the training set. Graph Neural Networks (GNN) have also been used in few-shot learning <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b16">15]</ref>. Self-supervised learning leverages free supervision signals residing in images and videos to promote robust representation learning in image recognition <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b5">4,</ref><ref type="bibr" target="#b17">16]</ref>, video recognition <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b41">40,</ref><ref type="bibr" target="#b13">12]</ref>, video object segmentation <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b58">57]</ref> and few-shot image classification <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b47">46]</ref>. Approaches <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b5">4,</ref><ref type="bibr" target="#b6">5]</ref> learn to predict random image rotations, relative pixel positions, and surrogate classes, respectively. Finally, <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b47">46]</ref> improve few-shot results by predicting image rotations/jigsaw patterns.</p><p>Second-order statistics are used by us for permutation-invariant pooling.</p><p>They are also used for texture recognition <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b38">37]</ref> by so-called Region Covariance Descriptors (RCD), object and action recognition <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b51">50,</ref><ref type="bibr" target="#b26">25]</ref>. Second-order pooling has also been used in fine-grained image classification <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b8">7,</ref><ref type="bibr" target="#b51">50]</ref>, domain adaptation <ref type="bibr" target="#b25">[24]</ref> and the fine-grained few-shot learning <ref type="bibr" target="#b55">[54,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b28">27]</ref>. Few-shot action recognition approaches <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b53">52</ref>] use a generative model, graph matching on 3D coordinates and a dilated networks with class-wise classifiers, respectively. Approach <ref type="bibr" target="#b59">[58]</ref> proposes a so-called compound memory network using key-value memory associations. ProtoGAN <ref type="bibr" target="#b7">[6]</ref> proposes a GAN model to generate action prototypes to address few-shot action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Contrast with existing works</head><p>Unlike <ref type="bibr" target="#b18">[17]</ref>, we use video clips rather than 3D skeletal coordinates. In contrast to <ref type="bibr" target="#b53">[52]</ref>, we use relation/similarity learning and our training/testing class concepts are disjoint. While <ref type="bibr" target="#b59">[58]</ref> memorizes key values/frames, we model short-and longterm dependencies. While <ref type="bibr" target="#b7">[6]</ref> forms action prototypes by GAN, we focus on self-supervised attention learning and permutation-invariant aggregation.</p><p>In contrast to self-supervision by rotations and jigsaw <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b47">46]</ref>, we use a sophisticated self-supervision on the attention unit for which a dedicated loss performs the alignment between the attention vector of augmented attention unit and the augmented in the same way attention vector from the non-augmented attention unit. Thus, we train a permutation-invariant attention to deal with the distribution shift of discriminative action locations.</p><p>Finally, we use second-order pooling <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b55">54]</ref> for a permutation-invariant aggregation of temporal blocks while <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b55">54]</ref> work with images. We develop a theory explaining why Power Normalization helps episodic learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Issues with fair comparisons</head><p>Each few-shot action recognition method from Section 2 uses different datasets and evaluation protocols making fair comparisons impossible. Class-wise splits/validation sets are unavailable ie., model <ref type="bibr" target="#b36">[35]</ref> uses a random split. Figures 5c and 5d of Section 4 show that the random choice of the split set yields up to ?6% deviation in accuracy rendering such a protocol problematic. Thus, we propose a new protocol with class splits and validation sets made publicly available.  <ref type="bibr" target="#b48">[47,</ref><ref type="bibr" target="#b55">54,</ref><ref type="bibr" target="#b56">55]</ref>, we adopt a C3Dbased Conv-4-64 backbone to extract spatio-temporal features which capture short-range dependencies. Next, we apply second-order pooling on 3D action features re-weighted by attention to obtain second-order statistics which are  <ref type="figure" target="#fig_2">Fig. 3a</ref>. A naive approach is to directly extract the temporal and spatial attention, whose size is 1 ? T ? H ? W . However, this is computationally expensive and results in overparametrization. Thus, we split the attention block into separate spatial and temporal branches whose impact is adjusted by ?s and ?t. <ref type="figure" target="#fig_2">Fig. 3b</ref> is the ? ratio w.r.t. the Z-shot value (see Eq. <ref type="formula" target="#formula_5">(6)</ref>). The dashed curve shows that as Z grows (0 denotes the regular classification), the memorization burden of co-occurrence (i, j) on the comparator grows ? times for second-order pooling without Power Normalization (as opposed to Power Normalization). The solid line shows that as we use larger N (video clips vs. images), not using PN is even more detrimental.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pipeline</head><p>permutation-invariant <ref type="bibr" target="#b29">[28]</ref> w.r.t. the spatio-temporal order of features. To paraphrase, we discard the long-range order of temporal (and spatial) blocks captured by the encoder. Finally, second-order matrices form relation descriptors from query/support clips fed into a 2D relation network to capture relations.</p><p>Let V denote a video (ie., with ?20 frames) and ? ? R C?T ?H?W be features extracted from V by f :</p><formula xml:id="formula_0">? = f (V; F ).</formula><p>(1)</p><p>To aggregate ? per clip into ? , we apply a pooling operator g over the support and query features, resp. For g, we use pooling operators from Sec. 3.2:</p><formula xml:id="formula_1">? = g(?).<label>(2)</label></formula><p>Once ? are computed for query/support clips, they form relation descriptors (via operator ?) passed to the relation network r to obtain the relation score ? sq :</p><formula xml:id="formula_2">? sq = r(?(? s , ? q ); R),<label>(3)</label></formula><p>where R are parameters of network r, and ? forms relation descriptors e.g., we use the concatenation along the channel mode. We use the Mean Square Error (MSE) loss over support and query pairs:</p><formula xml:id="formula_3">L = s?S q?Q (? sq ? ?(l s ? l q )) 2 ,<label>(4)</label></formula><p>where ?(l s ?l q ) = 1 if l s = l q , ?(l s ?l q ) = 0 otherwise. Class labels of support and query action clips are denoted as l s and l q . <ref type="figure">Fig. 4</ref>. Augmentation-guided attention by alignment. We firstly collect the encoded representations of original and augmented data, then we extract the temporal or spatial attention vectors from them. We apply the same augmentation(s) on the temporal or spatial attention vectors of the original data resulting in the augmented attention vectors which we align with attention vectors of the augmented data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pooling of encoded representations</head><p>For permutation-invariant pooling of temporal (and spatial) blocks, we investigate three pooling mechanisms discussed below. Average and max pooling are two widely-used pooling functions which can be used for aggregation of N = T ?W ?H fibers (channel-wise vectors) of feature map ? defined in Eq. (1). The average pooling is given as ? = 1 N N n=1 ? n where ? ? R C , and ? n ? R C are N fibers. Similarly, max pooling is given by ? c = max n=1,...,N ? cn , c = 1, ..., C, and ? = [? 1 , ..., ? C ] T . Average and max pooling are commutative w.r.t. the input fibers, thus being permutation-invariant. However, first-order pooling is less informative than second-order [23] discussed next. Second-order pooling captures correlations (or co-occurrences) between pairs of features in N fibres of feature map ?, which is reshaped such that ? ? R C?N , N = T ? H ? W . Such an operator proved robust in classification <ref type="bibr" target="#b24">[23]</ref> and few-shot learning <ref type="bibr" target="#b55">[54,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b28">27]</ref>. Specifically, we define:</p><formula xml:id="formula_4">? = ? 1 N N n=1 ? n ? T n = ? 1 N ?? T where ?(X) = 1 ? exp(?X) 1 + exp(?X) .<label>(5)</label></formula><p>Matrix ? ? R C?C is a Power Normalized autocorrelation matrix capturing correlations of fiber features ? n of feature map ? from Eq. (1) while ? applies Power Normalization (PN): we use the zero-centered element-wise Sigmoid <ref type="bibr" target="#b24">[23,</ref><ref type="bibr" target="#b55">54]</ref> on X, and ? controls the slope of PN. For a given pair of features i and j in matrix ? , that is ? ij , the role of PN is to detect the likelihood if at least one co-occurrence of features i and j has been detected <ref type="bibr" target="#b24">[23]</ref>. According to Eq. <ref type="formula" target="#formula_4">(5)</ref>, second-order pooling is permutation-invariant w.r.t. the order of input fibers as the summation in Eq. <ref type="formula" target="#formula_4">(5)</ref> is commutative w.r.t. the order of ? 1 , ..., ? N . Thus, second-order pooling factors out the spatial and temporal modes of ? and aggregates clips with varying numbers of temporal blocks (discards the order of long-range spatial/temporal dependencies) into a fixed length representation ? ? R C?C . Below we explain further why second-order pooling with Power Normalization is well suited for episodic few-shot learning.</p><p>Relation descriptors between query/support pooled matrices ? q and ? s are formed by operation ?(? q , ? s ) which, in our case, simply performs concatenation of ? q with ? s along the channel mode by cat(? q , ? s ) ? R 2?C?C , and ? s is obtained by the mean (or maximum) along the channel mode between ? 1 s , ..., ? Z s belonging to the same episode and class (Z &gt; 1 for the few-shot case). It is known from <ref type="bibr" target="#b29">[28]</ref> that the Power Normalization in Eq. (5) performs a co-occurrence detection rather than counting (correlation). For classification problems, assume a probability mass function p Xij (x) = 1/(N +1) if x = 0, ..., N , p Xij (x) = 0 otherwise, that tells the probability that co-occurrence between ? in and ? jn happened x = 0, ..., N times (given some clip). Note that classification often depends on detecting a co-occurrence (e.g., is there a flower co-occurring with a pot?) rather than counts (e.g., how many flowers and pots co-occur?). Using second-order pooling without PN requires a classifier to observe N + 1 training samples of flower and pot co-occurring in quantities 0, ..., N to memorise all possible co-occurrence count configurations. For relation learning, our ? stacks pairs of samples to compare, thus a comparator now has to deal with a probability mass function of R ij = X ij +Y ij depicting flowers and pots whose support(p Rij ) = 2N +1 &gt; support(p Xij ) = N +1 if random variable X = Y (same class). The same is shown by variances var(p Rij ) &gt; var(p Xij ). For Z-shot learning, the growth of variance and support equal (Z+1)N+1 indicates that the comparator has to memorize more configurations of co-occurrence (i, j) as Z grows.</p><p>However, this situation is alleviated by Power Normalization (operator ?) whose probability mass function can be modeled as p X ? ij (x) = 1/2 if x = {0, 1}, p X ? ij (x) = 0 otherwise, as PN detects a co-occurrence (or its lack). For Z-shot learning, support(p R ? ij ) = Z +2 support(p Rij ) = (Z +1)N +1. The ratio given as</p><formula xml:id="formula_5">? = support(p Rij ) support(p R ? ij ) = (Z +1)N +1 Z +2<label>(6)</label></formula><p>shows that the comparator has to memorize many more count configurations of co-occurrence (i, j) for naive pooling compared to PN as Z and/or N grow (N depends on the number of temporal and spatial blocks T , H and W ). <ref type="figure" target="#fig_2">Figure 3</ref> shows how ? varies w.r.t. Z and N . Our modeling assumptions are simple e.g., the assumption on mass functions with uniform probabilities, the use of the support of mass functions rather than variances to describe variability of co-occurrence (i, j). Yet, substituting these modeling choices with more sophisticated ones does not affect theoretical conclusions that: (i) PN benefits few-shot learning (Z ? 1) more than the regular classification (Z = 0) in terms of reducing possible count configurations of (i, j), and (ii) for videos (large N ) PN reduces the number of count configurations of (i, j) more rapidly than for images (smaller N ). While classifiers and comparators do not learn exhaustively all count configurations of co-occurrence (i, j) as they have some generalization ability, they learn quicker and better if the number of count configurations of (i, j) is limited. <ref type="figure" target="#fig_2">Figure 3a</ref> introduces decoupled spatial/temporal attention units consisting of three 3D Convolutional blocks and a Sigmoid output layer. Let t and s denote the temporal and spatial attention modules, and the attention be applied ahead of second-order pooling. We obtain temporal and spatial attention maps T ? R 1?T ?1?1 and S ? R 1?1??H?W , and attentive action features ? * by:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Temporal and spatial attention</head><formula xml:id="formula_6">T = t(?; T ), S = s(?; S),<label>(7)</label></formula><formula xml:id="formula_7">? * = (? t + T) ? (? s + S) ? ?,<label>(8)</label></formula><p>where T and S are network parameters of temporal/spatial attention units while ? t and ? s control the impact of attention vectors. Using attention helps spot discriminative temporal/spatial blocks, and suppress uninformative regions. However, the attention should be robust to varying distributions of locations of discriminative blocks in clips as proposed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Temporal and spatial self-supervision</head><p>Self-supervised Learning (SsL) helps learn representations without using manually-labeled annotations. We impose self-supervision both on encoders and attention units. For temporal self-supervision, we augment clips by shuffling the order of temporal blocks, which primes our network to become robust to long-term non-repetitive temporal dependencies in clips. Self-supervision also helps overcome the low-sample by encouraging network to learn auxiliary tasks. In contrast, previous works shuffled frames which breaks the highly discriminative short-term temporal dependencies. We use the following self-supervision strategies:</p><p>i. Temporal jigsaw. Jigsaw, a popular self-supervisory task breaks the object location bias and teaches the network to recognize shuffling. As in <ref type="bibr" target="#b54">[53]</ref>, we split clips into non-overlapping fixed-length temporal blocks and shuffle them. ii. Spatial jigsaw. We split frames into four non-overlapping regions, then randomly permute them. iii. Rotation. As the most popular self-supervisory task are rotations, we uniformly rotate all frames per clip by a random angle (0 ? , 90 ? , 180 ? , 270 ? ). <ref type="figure" target="#fig_0">Figure 2</ref> (blue frame) shows how we apply and recognize the self-supervision patterns e.g., shuffling and rotation angles. Below, we illustrate self-supervision via rotations. Consider the objective function L rot for self-supervised learning with a self-supervision discriminator d, where D are parameters of d. Thus:</p><formula xml:id="formula_8">? i = f ( rot(V i , ?) ; F ),<label>(9)</label></formula><formula xml:id="formula_9">p roti = d(? i ; D),<label>(10)</label></formula><formula xml:id="formula_10">L rot = ? i log exp(p roti [l ?i ]) s exp(p roti [l s ]) ,<label>(11)</label></formula><p>where V i is a randomly sampled clip, ? ? {0 ? , 90 ? , 180 ? , 270 ? } is a randomly selected rotation angle of a frame, l ?i ? {0, 1, 2, 3} is the rot. label for sample i.</p><p>Combining the original loss function L with such a self-supervision term L rot results in a self-supervised few-shot action recognition pipeline. However, this objective does not make the attention to be invariant to augmentations per se. <ref type="figure">Figure 4</ref> presents a strategy in which we extract the attention vector for an augmented clip, then we apply the same augmentation to the attention vector obtained from the original non-augmented clip, and we encourage such a pair of augmentation vectors to align by a dedicated MSE loss. This encourages the attention unit to be invariant w.r.t. a given augmentation type. <ref type="figure">Fig. 1a</ref> explains why the temporal permutation strategy benefits few-shot learning while <ref type="figure">Figure 4</ref> shows how to apply permutations and rotations. As an example, for a rotation-guided spatial-attention we define the alignment loss L att :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Augmentation-guided attention by alignment</head><formula xml:id="formula_11">? i = f ( rot(V i , ?) ; F ),<label>(12)</label></formula><formula xml:id="formula_12">S i = s(? i ; S),? i = s(? i ; S),<label>(13)</label></formula><formula xml:id="formula_13">L att = i || | rot(S i , ?) ?? i | ? ? || 2 F .<label>(14)</label></formula><p>where ? controls the strictness of alignment. The final objective then becomes:</p><p>arg min F ,D,T ,S L + ?L ss + ?L att <ref type="bibr" target="#b16">(15)</ref> where ? and ? are the hyper-parameters adjusted by cross-validation, L ss is a chosen type of self-supervision e.g., via rotations as introduced in Eq. (11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>Below, we describe our setup and evaluations in detail. To exclude complicated data pre-processing and frame sampling steps typically used in action recognition, we sample uniformly 20 frames along the temporal mode for each dataset.  <ref type="table">Table 2</ref>. Ablations of different modules of ARN given our proposed HMDB51 protocol (given 5-way acc.) We used spatial-jigsaw for self-supervision.</p><p>Baseline Spatial Attention Self-supervision Alignment  <ref type="bibr" target="#b59">[58]</ref> to select a subset for few-shot learning, consists of 64, 12 and 24 training, validation and testing classes. We use it for comparisons. Training, validation and testing splits on the first three datasets are detailed in our supplementary material while authors of <ref type="bibr" target="#b59">[58]</ref> provide the split on Kinetics. The frames of action clips from all datasets are resized to 128 ? 128. All models are trained on training splits. Validation set is only used for cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with previous works</head><p>Section 2.2 explains the issues with existing methods, protocols, and the lack of publicly available codes. For a fair comparison, we use firstly the protocol of <ref type="bibr" target="#b36">[35]</ref> (HMDB51 and UCF101 datasets) but we chose 5 splits at random according to their protocol to average results over multiple runs: we report an average-case result not the best case or a single run result (in contrast to <ref type="bibr" target="#b36">[35]</ref>). We also use the Kinetics split of <ref type="bibr" target="#b59">[58]</ref>, and compare our approach with <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b59">58,</ref><ref type="bibr" target="#b7">6]</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows that our ARN (best variant) outperforms GenApp <ref type="bibr" target="#b36">[35]</ref>, Proto-GAN <ref type="bibr" target="#b7">[6]</ref>, CMN <ref type="bibr" target="#b59">[58]</ref> by a large margin of 3% to 10% on the three protocols. Our standard errors are low as they result from 5 runs on 5 splits (average case) while a large deviation of ProtoGAN <ref type="bibr" target="#b7">[6]</ref> was obtained w.r.t. episodes on a single split. The weakness of protocol <ref type="bibr" target="#b36">[35]</ref>. Evaluation protocols in <ref type="bibr" target="#b36">[35]</ref> rely on randomly selecting training/testing classes with 50-50 ratio from all classes to form train- ing/testing splits on HMDB51 and UCF101. The performance of that protocol varies heavily due to the randomness. Moreover, results of <ref type="bibr" target="#b36">[35]</ref> are reported on a single run. <ref type="figure">Figures 5c and 5d</ref> show up to 6% variations due to the randomness, making a fair comparison between models difficult on such a protocol. The lack of validation set makes cross-validation also a random process affecting results. We rectify all this by providing standardized training, validation, and testing splits on HMDB51, mini MIT and UCF101. In what follows, we use our new splits with our few-shot ARN. We equip the Prototypical Net <ref type="bibr" target="#b45">[44]</ref>, Relation Net <ref type="bibr" target="#b48">[47]</ref> and SoSN <ref type="bibr" target="#b55">[54]</ref> with a 3D conv. feature encoder (C3D) for baselines used below. ARN modules (ablations). We start by studying ARN modules on HMDB51. <ref type="table">Table 2</ref> shows that combining attention with the baseline C3D SoSN pipeline brings ?1% gain. Switching to the attention by alignment brings 1.2-1.9% gain over the naive attention unit. Combining self-supervision with (i) the baseline and (ii) the baseline with attention brings ?3% and ?3.5% gain, resp. Combining all units together (attention, self-supervision and alignment) yields ?5% gain. The computational cost is similar to running either self-supervision or alignment.</p><p>Thus, in what follows we will report results for the most distinct four variants: (i) baseline (C3D SoSN), (ii) Temporal/Spatial Attention only (TA &amp; SA), Temporal/Spatial Self-supervision w/o attention (TS &amp; SS), and Temporal/Spatial Self-supervision with attention by alignment (TSA &amp; SSA). Pooling (ablations). Section 3.2 discusses pooling variants from <ref type="table" target="#tab_4">Table 5</ref>. Secondorder pooling (with PN) outperforms second-order pooling (w/o PN) followed by <ref type="table">Table 4</ref>. Evaluations on miniMIT and UCF101 datasets (given 5-way acc.) See the legend at the top of <ref type="table" target="#tab_2">Table 3</ref> for the description of abbreviations. average and max pooling. Combining average pooling with PN boosts its results which is consistent with the theoretical analysis in <ref type="figure" target="#fig_2">Figure 3</ref>. In what follows, we use the best pooling variant only, that is second-order pooling with PN. Main evaluations. <ref type="table" target="#tab_2">Tables 3 and 4</ref> present main evaluations on the proposed by us protocols. Notably, our approaches outperform all baselines (known approaches enhanced by us with the C3D-based encoder). Below, we detail the results. Attention. <ref type="table" target="#tab_2">Tables 3 and 4</ref> investigate the Temporal and Spatial Attention denoted as (TA) and (SA) on our few-shot ARN. TA on the 1-shot and 5-shot protocols improves the accuracy by 1.0% and 2.5% while SA boosts the 1-and 5-shot accuracy by 0.5% and 1.0%, respectively. For the combined Temporal and Spatial Attention (TA+SA), the Eq. <ref type="formula" target="#formula_7">(8)</ref> is used with ? s = 1.0 and ? t = 0.5 (HMDB51) and ? s = 1.5 and ? t = 1.0 (UCF101) chosen on the validation split. <ref type="table" target="#tab_2">Tables 3 and 4</ref> show that SA+TA achieves a further improvement of up to 1.1% for 1-shot learning but for 5-shot learning it may suffer an 0.8% drop in accuracy compared to the best score of TA and SA while still achieving between an 0.8 and 4.2% gain over the baseline C3D SoSN. This is consistent with our argument that vanilla attention units can be further improved for a better performance.  <ref type="figure">Fig. 5</ref>. In <ref type="figure">Fig. 5a</ref> are the loss curves for Eq. <ref type="bibr" target="#b16">(15)</ref>. <ref type="figure">Fig. 5b</ref> shows the validation score w.r.t. ? (1-shot prot.) Applying Spatial Self-super. &amp; Attention by alignment (SSA) ? &gt; 0 outperforms the Spatial Self-super. &amp; Attention only (? = 0). <ref type="figure">Fig. 5c and 5d</ref> show the performance variation on random splits of HMDB51 and UCF101 proposed by <ref type="bibr" target="#b36">[35]</ref>.</p><p>Temporal/spatial self-supervision. In this experiment, we disable attention units. <ref type="table" target="#tab_2">Tables 3 and 4</ref> show that self-supervision w.r.t. either temporal or spatial mode boosts performance of 1-shot and 5-shot learning over the C3D SoSN baseline on HMDB51 up to 3.2%. On miniMIT, we observe gains between 1.2 and 4.6%. On UCF101, we see gains between 0.6 and 1.4%. However, for UCF101 dataset, self-supervision by the spatial jigsaw and rotation lead to a marginal performance drop on 5-shot learning compared to C3D SoSN. Temporal/spatial self-supervision &amp; attention by alignment. According to <ref type="table" target="#tab_2">Tables 3 and 4</ref>, the gains are in 2-5% range compared to the baseline C3D SoSN. <ref type="figure">Figure 5a</ref> shows the training loss w.r.t. epoch (HMDB51) (temp. jigsaw). <ref type="figure">Figure 5b</ref> shows the validation accuracy (HMDB51) w.r.t. ? for SS (rot.) As can be seen, Self-supervision combined with Attention by alignment (any curve for ? &gt; 0) scores higher than Self-supervision with Attention only (? = 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have proposed a new few-shot Action Recognition Network (ARN) which comprises an encoder, comparator and an attention mechanism to model shortand long-range temporal patterns. We have investigated the role of self-supervision via spatial and temporal augmentations/auxiliary tasks. Moreover, we have proposed a novel mechanism dubbed attention by alignment which tackles the so-called distribution shift of temporal positions of discriminative long-range blocks. By combining losses of self-supervision and attention by alignment, we see gains of up to 6% accuracy. We make our dataset splits publicly available to facilitate fair comparisons of few-shot action recognition pipelines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Our few-shot Action Relation Network (ARN) contains: feature encoder with 4-layer 3D conv. blocks, relation network with 2D conv. blocks, and spatial and temporal attention units which refine the aggregation step. Specifically, we apply second-order pooling (operator g) over encoder outputs (re-weighted by attention vectors) per clip to obtain a Power Normalized Autocorrelation Matrix (AM). Query and support AMs per episode form relation descriptors (by operator ?) from which the relation network learns to capture relations. The block (blue dashed line) is the self-supervised learning module which encourages our pipeline to learn auxiliary tasks e.g., jigsaws, rotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>shows our Action Relation Network (ARN). In contrast to the Conv-4-64 backbone in few-shot image classification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Spatial and temporal units are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparisons between our ARN model and existing works on HMDA51 and UCF101 splits proposed in<ref type="bibr" target="#b36">[35]</ref> and a Kinetics split from<ref type="bibr" target="#b59">[58]</ref> (given 5-way acc.)</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">HMDB51 [35] 1-shot 5-shot</cell><cell cols="2">UCF101 [35] 1-shot 5-shot</cell><cell cols="2">Kinetics [6] 1-shot 5-shot</cell></row><row><cell>GenApp</cell><cell>[35]</cell><cell>?</cell><cell>52.5 ? 3.10</cell><cell>?</cell><cell>78.6 ? 2.1</cell><cell>-</cell><cell>-</cell></row><row><cell cols="6">ProtoGAN [6] 34.7 ? 9.20 54.0 ? 3.90 57.8 ? 3.0 80.2 ? 1.3</cell><cell>-</cell><cell>-</cell></row><row><cell>CMN</cell><cell>[58]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>60.5</cell><cell>78.9</cell></row><row><cell>Ours</cell><cell></cell><cell cols="5">44.6 ? 0.9 59.1 ? 0.8 62.1 ? 1.0 84.8 ? 0.8 63.7</cell><cell>82.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>HMDB51<ref type="bibr" target="#b30">[29]</ref> contains 6849 clips divided into 51 action categories, each with at least 101 clips, 31, 10 and 10 classes selected for training, validation and testing. Mini Moments in Time (miniMIT)<ref type="bibr" target="#b37">[36]</ref> contains 200 classes and 550 videos per class. We select 120, 40 and 40 classes for training, validation and testing. UCF101<ref type="bibr" target="#b46">[45]</ref>, action videos from Youtube, has 13320 video clips and 101 action classes. We randomly select 70 training, 10 validation and 21 testing classes.</figDesc><table><row><cell>1-shot 5-shot</cell></row><row><cell>40.83 55.18</cell></row><row><cell>41.27 56.12</cell></row><row><cell>44.19 58.50</cell></row><row><cell>44.61 59.71</cell></row><row><cell>43.11 57.35</cell></row><row><cell>45.17 60.56</cell></row></table><note>Kinetics, used by</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Evaluations on HMDB51 (5-way acc.) Attention: Temporal (TA), Spatial (SA). Self-super.: Temp. (TS), Spat. (SS), Self-Super. &amp; Alignment: Temp. (TSA), Spat. (SSA).</figDesc><table><row><cell>Model</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell></cell><cell></cell><cell>Baseline</cell></row><row><cell>C3D Prototypical Net [44]</cell><cell>38.05 ? 0.89</cell><cell>53.15 ? 0.90</cell></row><row><cell>C3D RelationNet [47]</cell><cell>38.23 ? 0.97</cell><cell>53.17 ? 0.86</cell></row><row><cell>C3D SoSN [54]</cell><cell>40.83 ? 0.96</cell><cell>55.18 ? 0.86</cell></row><row><cell cols="3">Temporal/Spatial Attention only (TA vs. SA)</cell></row><row><cell>ARN+TA</cell><cell>41.97 ? 0.97</cell><cell>57.67 ? 0.88</cell></row><row><cell>ARN+SA</cell><cell>41.27 ? 0.98</cell><cell>56.12 ? 0.89</cell></row><row><cell>ARN+SA+TA</cell><cell>42.41 ? 0.99</cell><cell>56.81 ? 0.87</cell></row><row><cell cols="3">Temporal/Spatial Self-supervision only (TS vs. SS)</cell></row><row><cell>ARN+TS (temp. jigsaw)</cell><cell>43.79 ? 0.96</cell><cell>58.13 ? 0.88</cell></row><row><cell>ARN+SS (spat. jigsaw)</cell><cell>44.19 ? 0.96</cell><cell>58.50 ? 0.86</cell></row><row><cell>ARN+SS (rotation)</cell><cell>43.90 ? 0.92</cell><cell>57.20 ? 0.90</cell></row><row><cell cols="3">Temp./Spat. Self-super. &amp; Att. by alignment (TSA vs. SSA)</cell></row><row><cell>ARN+TSA (temp. jigsaw)</cell><cell>45.20 ? 0.98</cell><cell>59.11 ? 0.86</cell></row><row><cell>ARN+SSA (spat. jigsaw)</cell><cell>45.15 ? 0.96</cell><cell>60.56 ? 0.86</cell></row><row><cell>ARN+SSA (rotation)</cell><cell>45.52 ? 0.96</cell><cell>58.96 ? 0.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>C3D Prototypical Net [44] 33.65 ? 1.01 45.11 ? 0.90 57.05 ? 1.02 78.25 ? 0.73 C3D RelationNet [47] 35.71 ? 1.02 47.32 ? 0.91 58.21 ? 1.02 78.35 ? 0.72 C3D SoSN [54] 40.83 ? 0.99 52.16 ? 0.95 62.57 ? 1.03 81.51 ? 0.74 Temporal/Spatial Attention only (TA vs. SA) ARN+TA 41.65 ? 0.97 56.75 ? 0.93 63.35 ? 1.03 80.59 ? 0.77 ARN+SA 41.27 ? 0.98 55.69 ? 0.92 63.73 ? 1.08 82.19 ? 0.70 ARN+TA+SA 41.85 ? 0.99 56.43 ? 0.87 64.48 ? 1.06 82.37 ? 0.72 Temporal/Spatial Self-supervision only (TS vs. SS) ARN+TS (temp. jigsaw) 42.45 ? 0.96 54.67 ? 0.87 63.79 ? 1.02 82.14 ? 0.77 ARN+SS (spat. jigsaw) 42.68 ? 0.95 54.46 ? 0.88 63.75 ? 0.98 80.92 ? 0.72 ARN+SS (rotation) 42.01 ? 0.94 56.83 ? 0.86 63.95 ? 1.03 81.09 ? 0.76 Temp./Spat. Self-super. &amp; Att. by alignment (TSA vs. SSA) ARN+TSA (temp. jigsaw) 42.65 ? 0.94 57.35 ? 0.85 65.46 ? 1.05 82.97 ? 0.71 ARN+SSA (spat. jigsaw) 42.92 ? 0.95 56.21 ? 0.85 66.04 ? 1.01 82.68 ? 0.72 ARN+SSA (rotation) 43.05 ? 0.97 56.71 ? 0.87 66.32 ? 0.99 83.12 ? 0.70</figDesc><table><row><cell></cell><cell cols="2">miniMIT</cell><cell cols="2">UCF101</cell></row><row><cell>Model</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison of pooling functions on our HMDB51 split (5-way 1-shot).</figDesc><table><row><cell cols="6">No Pooling Average Average+PN Max Second-order (w/o PN) Second-order (with PN)</cell></row><row><cell>35.71</cell><cell>39.51</cell><cell>40.02</cell><cell>38.95</cell><cell>39.97</cell><cell>40.83</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Kindly see<ref type="bibr" target="#b50">[49,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b48">47]</ref> for the concept of query, support and episodic learning, and the evaluation protocols which differ from traditional recognition and low-shot learning.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research is supported in part by the Australian Research Council through Australian Centre for Robotic Vision (CE140100016), Australian Research Council grants (DE140100180), the China Scholarship Council (CSC Student ID 201603170283). Hongdong Li is funded in part by ARC-DP (190102261) and ARC-LE (190100080). We thank CSIRO Scientific Computing, NVIDIA (GPU grant) and the National University of Defense Technology.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Below we demonstrate the detailed training/validation/testing splits used in our paper. Though the limited number of previous works propose some evaluation splits on several action recognition datasets, they differ in every paper thus making it very difficult to produce fair comparisons with other works. Additionally, most of works use random train/test splits and have no validation set, thus making the results suffer from high variance and potentially overfitting to the test data. As we aim to fix these problems, we formally introduce three new evaluation protocols as the standard benchmarks, which can help compare models more accurately in a fair setting.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">brush hairs, catch, chew, clap, climb, climb stairs, dive, draw sword, dribble, drink, fall floor, flic flac, handstand, hug, jump, kiss, pullup, punch, push, ride bike, ride horse, shake hands, shoot bow, situp, stand, sword, sword exercies, throw, turn, walk, wave. Actions of Validation Split (10): cartwheel, eat, golf, hit, laugh, shoot ball, shoot gun, smile, somersault, swing baseketball. Actions of Validation Split (10): fencing, kick</title>
	</analytic>
	<monogr>
		<title level="j">Actions of Train Split</title>
		<imprint>
			<biblScope unit="issue">31</biblScope>
		</imprint>
	</monogr>
	<note>kick ball, pick, pour, pushup, run, sit, smoke, talk</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">camping, carrying, celebrating, chopping, clapping, cleaning, clinging, closing, combing, competing, covering, crawling, crying, cutting, descending, destroying, digging, dining, drawing, drenching, drilling, drinking, dripping, driving, dropping, drying, dunking, emptying, entering, erupting, falling, filling, flipping, floating, flying, folding, frying, handwriting, hanging, hitting, juggling, kicking, knitting, landing, laughing, leaping, lecturing, lifting, mopping, opening, parading, photographing, picking, placing, pouring, pressing, protesting, pulling, pushing, rafting, raining, reading, removing, repairing, riding, rising, rowing, running, sawing, scratching, sewing, shaking, shaving, shopping, shouting, shredding, singing, skating, sleeping, slicing, sliding, smiling, smoking, snowing, speaking, spraying, spreading, sprinting, stacking, stirring, stitching, stretching, stroking, studying, swimming, swinging, tapping, tattooing, turning, twisting, typing, vacuuming, walking, washing, whistling, wrapping. Actions of Validation Split (40): ascending, boiling, bubbling, chasing, combusting, constructing, cracking, crashing, crushing, diving, drumming, eating, exercising, gardening, grilling, grooming, hammering, hugging, inflating, licking, painting, peeling, pitching, planting, playing, playing sports, rolling, sanding, shoveling, smashing, spinning, steering, surfing, sweeping, tapping, throwing, unloading, watering, waving, wrestling. Actions of Test Split (40): boxing, carving, catching, cheering, chewing, climbing, colliding, cooking, crafting, dancing, feeding, fishing, flooding, frowning, gripping, hiking, howling, jumping, launching, mowing, overflowing, pedaling</title>
	</analytic>
	<monogr>
		<title level="m">Actions of Train Split (120): arresting, assembling, attacking, baking, barbecuing, barking, bending, bicycling, biting, boating, bouncing, brushing, bulldozing, burning</title>
		<imprint/>
	</monogr>
	<note>performing, piloting, playing music, racing, raising, resting, rubbing, sailing, slapping, sneezing, sniffing, splashing, storming, tying, waking, waxing, welding, yawning</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Archery</forename><surname>Applyeyemakeup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babycrawling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bandmarching</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baseballpitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basketball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basketballdunk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bench-Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Biking</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Billiards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blowdryhair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bodyweightsquats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boxing-Punchingbag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boxingspeedbag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breaststroke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brushingteeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cricketbowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drumming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fencing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fieldhockeypenalty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frisbeecatch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frontcrawl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haircut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Headmassage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hulahoop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Javelinthrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jugglingballs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jumping-Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kayaking</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Knitting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longjump</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lunges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Militaryparade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nunchucks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parallelbars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pizzatossing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Playingcello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Playingdhol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Playingflute</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Playingpiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Playingsitar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Playingtabla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Playingviolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pole-Vault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pullups</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pushups</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rafting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ropeclimbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rowing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shavingbeard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Skijet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soccerjuggling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soccerpenalty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sumowrestling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabletennisshot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taichi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Throwdiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trampolinejumpling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Typing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Unevenbars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walkingwithdog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wall-Pushups</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoyo</forename><surname>Writingonboard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ApplyLipstick, CricketShot, HammerThrow, HandstandPushups, HighJump, HorseRiding, PlayingDaf, PlayingGuitar, Shotput, SkateBoarding. Actions of Test Split (21): BlowingCandles, CleanAndJerk, CliffDiving, Cut-tingInKitchen, Diving, FloorGymnastics, GolfSwing, HandstandWalking, HorseRace</title>
		<meeting><address><addrLine>IceDancing, JumpRope, PommelHorse, Punch, RockClimbingIndoor, Sal-saSpin, Skiing, SkyDiving, StillRings, Surfing, TennisSwing, VolleyballSpiking</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Actions of Train Split. References 1. Antoniou, A., Edwards, H., Storkey, A.: How to train your maml. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cross-generalization: Learning novel classes from a single example by feature replacement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Protogan: Towards few shot learning for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deepkspd: Learning kernel-matrix-based SPD representation for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01216-8_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01216-8_38" />
	</analytic>
	<monogr>
		<title level="j">In: ECCV</title>
		<imprint>
			<biblScope unit="volume">11206</biblScope>
			<biblScope unit="page" from="629" to="645" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Object classification from a single example utilizing class relevance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Geometry guided convolutional neural networks for self-supervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Boosting few-shot visual learning with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Generating classification weights with gnn denoising autoencoders for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neural graph matching networks for fewshot 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Representation learning with multiple lipschitzconstrained alignments on partially-labeled cross-domain data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="4320" to="4327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Evolutionarily learning multi-aspect interactions and influences from network structure and node content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Edge-labeling graph neural network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Tensor representations via kernel linearization for action recognition from 3d skeletons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Domain adaptation by mixture of alignments of second-or higher-order scatter tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<title level="m">Tensor representations for action recognition. TPAMI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Higher-order occurrence pooling for bags-of-words: Visual concept detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Power normalizations in fine-grained image, few-shot image and graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A deeper look at power normalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Mast: A memory-augmented self-supervised tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CogSci</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rapid natural scene categorization in the near absence of attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vanrullen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning from one example through shared densities on transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Matsakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A generative approach to zero-shot and few-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arulkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Moments in time dataset: one million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutfruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Enhanced local binary covariance matrices (ELBCM) for texture analysis and object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Ter?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gouiff?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lacassagne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIRAGE</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Time-contrastive networks: Self-supervised learning from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">ICRA</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep subspace networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adaptive subspaces for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">On modulating the gradient for meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Boosting supervision with self-supervision for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Region covariance: A fast descriptor for detection and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Beyond covariance: Feature representation with nonlinear kernel matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.519</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.519" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4570" to="4578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Few-shot learning with localization in realistic settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wertheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Dense dilated network for few shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICMR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Power normalizing second-order similarity network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Few-shot learning via saliency-guided hallucination of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Self-supervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Compound memory networks for few-shot video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Fast context adaptation via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shiarli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
