<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CNN+RNN Depth and Skeleton based Dynamic Hand Gesture Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Lai</surname></persName>
							<email>kelai@ucalgary.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Biometric Technologies Laboratory</orgName>
								<orgName type="institution">University of Calgary</orgName>
								<address>
									<settlement>Alberta</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><forename type="middle">N</forename><surname>Yanushkevich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Biometric Technologies Laboratory</orgName>
								<orgName type="institution">University of Calgary</orgName>
								<address>
									<settlement>Alberta</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CNN+RNN Depth and Skeleton based Dynamic Hand Gesture Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ICPR.2018.8545718</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Biometrics</term>
					<term>gesture recognition</term>
					<term>convolutional neural networks</term>
					<term>recurrent neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human activity and gesture recognition is an important component of rapidly growing domain of ambient intelligence, in particular in assisting living and smart homes. In this paper, we propose to combine the power of two deep learning techniques, the convolutional neural networks (CNN) and the recurrent neural networks (RNN), for automated hand gesture recognition using both depth and skeleton data. Each of these types of data can be used separately to train neural networks to recognize hand gestures. While RNN were reported previously to perform well in recognition of sequences of movement for each skeleton joint given the skeleton information only, this study aims at utilizing depth data and apply CNN to extract important spatial information from the depth images. Together, the tandem CNN+RNN is capable of recognizing a sequence of gestures more accurately. As well, various types of fusion are studied to combine both the skeleton and depth information in order to extract temporal-spatial information. An overall accuracy of 85.46% is achieved on the dynamic hand gesture-14/28 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>This paper concerns with biometric-based ambient computational intelligence techniques. Specifically, we focus on human activity and gesture recognition in the context of ambient monitoring in smart homes, assisting living or healthcare facilities. In a smart home, sensors can be programmed to learn about a resident's normal daily routines which can then be used for performing automated ambient health monitoring and assessment <ref type="bibr" target="#b0">[1]</ref>. For example, Pavel et al. <ref type="bibr" target="#b1">[2]</ref> suggested that there is a relationship between mobility patterns and cognitive ability. The theory was examined by observing the changes in mobility and found evidence to support the relationship between mobility and cognitive ability. In the current aging population, "the challenges of maintaining mobility and cognitive function make it increasingly difficult to remain living alone therefore forcing many people to seek residence in clinical institutions" <ref type="bibr" target="#b2">[3]</ref>. <ref type="bibr">Lee</ref> and Dey <ref type="bibr" target="#b3">[4]</ref> designed an embedded sensing system to determine if the resident gains more awareness about their functional abilities when given information regarding their movements. The ability to perform automated assessment of task quality and cognitive health has greatly improved accuracy <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. These techniques indicate that specific information can be extracted from a sensor and used in labelling the performed activity. For example, some activities such as washing dishes, taking medicine, and using the phone are characterized by the interaction with unique objects.</p><p>The main objective of this paper is to implement a framework for activity recognition, including gesture recognition. Traditional activity recognition uses mainly RGB images for analysis. Current methods incorporate different types of information including depth, infrared, and time <ref type="bibr" target="#b6">[7]</ref>. The proposed method focuses on creating a framework that uses both depth and skeleton information in the task of hand gesture/activity recognition. To prove such point, we have selected the specific task of recognizing dynamic hand gestures using depth and and skeleton information. We apply the state-of-the-art deep learning techniques such as convolutional neural network (CNN) and recurrent neural networks (RNN).</p><p>The paper is structured as follows: related works are introduced in Section II, framework of the proposed method in Section III, design of experiments and experimental results in Section IV, and the conclusions in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Activity and gesture recognition is an actively researched domain, especially in the light of recent development of new and advanced types of sensors that collect multiple, more precise data. Different spectra of data have been examined in the task of recognizing gestures and activities, including color, depth, GPS, acceleration, infrared, etc.</p><p>Prior to popularization of inexpensive depth sensors, it was mainly color (pixel intensity), or RGB, data available for gesture recognition. One of the most common method was to wear a color markers that indicate different regions of the hand. Iwai et al. used a colored glove in combination with a decision tree to perform gesture recognition <ref type="bibr" target="#b7">[8]</ref>. Another color-based approach by Bretzner et al. utilized multi-scale color features <ref type="bibr" target="#b8">[9]</ref>.</p><p>With the development of Microsoft Kinect, LeapMotion, and Intel RealSense, depth and 3D information is now more easily acquired for analysis. An approach based on extracted depth features that allow to create depth silhouette is proposed in <ref type="bibr" target="#b9">[10]</ref>. Another method that uses depth images to train a multi-layered random forest is suggested in <ref type="bibr" target="#b10">[11]</ref>.</p><p>Motivated by the relationship between hand gestures and sign language recognition, <ref type="bibr" target="#b11">[12]</ref> proposed a method of recognizing hand shapes using random forests applied to both depth and color images. Another approach uses depth and color images and various types of spatiotemporal descriptors, combined with different kernel choices for support vector machine to find the optimal recognition combination <ref type="bibr" target="#b12">[13]</ref>.</p><p>In recent years, deep learning techniques have revolutionized the pattern recognition in general. A 3D CNN <ref type="bibr" target="#b13">[14]</ref> combines spatiotemporal data augmentation in order to perform gesture recognition on both depth and color images. It has achieved a 77.5% classification rate on the VIVA challenge dataset. A method proposed by Nagi et al. <ref type="bibr" target="#b14">[15]</ref>, suggest the use of "state-of-the-art big and deep neural network combining convolution and max-pooling" for feature extraction and classification. In <ref type="bibr" target="#b15">[16]</ref>, RNN is used to model the temporal information in a sequence of skeleton joint movement to perform gesture recognition. Similarly, <ref type="bibr" target="#b16">[17]</ref> combines CNNs with long short term memory (LSTM) to recognize dynamic hand gesture using only skeleton-based information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FRAMEWORK</head><p>This section describes the proposed approach of using both depth and skeleton points in recognizing a hand gesture. The system consists of two main components: a depth-based CNN+RNN <ref type="figure" target="#fig_3">(Fig. 3)</ref>, and a skeleton-based RNN ( <ref type="figure" target="#fig_1">Fig. 1</ref>)</p><p>The first component of the system, CNN, is designed to extract features from depth image of a subject performing a hand gesture. However, since the processing of dynamic gesture recognition is dependent on an ordered sequence of images, a RNN is proposed to supplement the CNN to extract temporal patterns. The overall architecture of the CNN and the Long Short Term Memory (LSTM) network is shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. The structure consists of three components: the depth based feature extraction through CNN, the time series processing through RNN, and classification using a multilayer perceptron The second component of the system includes RNN which extracts temporal patterns from the movements of skeleton points within a sequence. The RNN structure is shown in <ref type="figure" target="#fig_1">Fig.  1</ref> and is similar to the one described in the first component, except for an additional FC layer and several LSTM units within each layer.</p><p>Overall, the proposed framework consists of two networks components that uses skeleton and depth information for gesture recognition independently. Since each network is capable of predicting a selected gesture based on the selected type of information, a fusion of both network is expected to yield a higher performance that is capable of selecting the best features from both the skeleton, the depth-based spatial information and the inherent temporal patterns between a collection of frames.</p><p>There are different ways to perform fusion as illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. The three main techniques considered in this study include the feature-level fusion, the score-level fusion, and the decision-level fusion.</p><p>The feature-level fusion can be performed at any layer before the MLP (which consists of the fully-connected, softmax, and classification layers). In general, the convolution layers in the CNN, and the LSTM layers in the RNN, are the portion of the network designed to extract features from the input data. A feature-level fusion can be performed by a fusion of the results after the input data has passed through a series of convolution/LSTM layers. Following the fusion, a classifier such as MLP or support vector machine can be attached in order to create a feature-level fusion network. Similarly to the feature-level fusion, the score-level fusion can be performed after or between the fully-connected and soft-max layers. In this process, we assume that features have been successfully extracted from the input data and passed through a selected classifier resulting in some score or probability. Since the score is heavily correlated to the network's prediction, the combination of multiple scores from different networks will provide a more reliable prediction.</p><p>The decision-level fusion is comparable to a score-level fusion except that the fusion is performed after the network's prediction. This type of fusion is based entirely on the network's predicted output and is not associated with the score/probability used for the decision. A network's predicted output is defined based on the probability/score output from the soft-max layer. The two most general method for a network to generate a prediction is either based on a decision threshold or the ranking order. The decision threshold method is based on selecting an arbitrary value for each network. Any scores above the selected value is considered accepted otherwise rejected. As for the ranking order method, all the scores are grouped and ordered such that the higher the list the more likely acceptance. A general usage is the rank-1 recognition which only considers the top prediction as the network's predicted output.</p><p>In this paper, we have explored both the feature-level and score-level fusion but excluded the decision-level fusion. Decision-level fusion is not examined in this paper because it is correlated to the score-level fusion, and there are only two network decisions to be combined.</p><p>In addition to the two level of fusion, there are various types of fusion of which we consider concatenation, averaging, and maximum. Fusion using concatenation was performed in this study at the feature-level because it generates a new set of features that considers both the extracted depth and skeleton information. Fusion using averaging and maximum is only applied to the score-level fusion because each score represents a network's strength in prediction. The averaging method is expected to generate a more reliable score, since it relies on two types of information and networks. Finally, the maximum technique shall place more emphasis on each network's ability to predict specific gestures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>The experiment is conducted on each components of the proposed framework shown in <ref type="figure" target="#fig_1">Fig. 3 and 1</ref> independently. We follow the same experimental setup as indicated in <ref type="bibr" target="#b17">[18]</ref> that used a leave-one subject-out cross-validation strategy. Based on this strategy, each proposed network is trained on 19 subjects and tested on the remaining subject, thus resulting in a 20-iteration cross validation. The depth-based CNN network is trained for 20 epochs using a min-batch size of 32 with the Adadelta optimizer <ref type="bibr" target="#b18">[19]</ref> with the default parameters of lr = 1.0, ? = 0.95, and = 1e ?08 . The input of the network is based on the cropped hand images from the DHG-14/28 dataset re-sized to 227x227. A low amount of epochs was chosen because the weights are designed to initialize the weights in the CNN+LSTM network therefore is not required to find the best optimal solution.</p><p>Similarly, the depth-based CNN-LSTM is using the Adadelta optimizer with default parameters with input image size of 227x227. The network is trained with a mini-batch size of 16, timestep of 32, and in 100 epochs. A timestep of 32 was selected because the number of key images for a gesture varies between 7 and 149 with an average of 34.59 key frames.</p><p>For the skeleton-based LSTM network, the Adam optimizer <ref type="bibr" target="#b19">[20]</ref> with default parameters of lr = 0.001, ? 1 = 0.9, ? 2 = 0.999, and = 1e ?08 is used to train the network. A timestep of 32 and a mini-batch size of 32 were selected becase it corresponds to the number of average key frames for each gesture. Note that the input data is represented by the 2D coordinate points indicating the skeleton joints of a hand available from the DHG-14/28 dataset.</p><p>For the score-level and feature-level fusion networks, the Adadelta optimizer with default parameters is used. The input data consists of sequences of depth images (227x227) and sequences of 2D skeleton joint locations (44x1). The training is ran for 100 epochs with a timestep of 32 and a mini-batch size of 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The dynamic hand gesture 14/28 (DHG-14/28) <ref type="bibr" target="#b17">[18]</ref> was chosen as the database, and this is one of the few databases containing data collected using a depth camera (Intel Re-alSense F200) sensor. Both depth and skeleton information for various hand gestures is available. In the DHG-14/28 <ref type="bibr" target="#b17">[18]</ref> dataset, there are 20 unique individuals performing 5 iterations of 14 gestures using two types of finger configurations, thus forming 28 sets of gestures, to a total of 2800 sequences. The depth information is saved in the form of images with resolution of 480x640 in 16-bits. The skeleton information contains 22 joint locations of a hand described in both 2D and 3D coordinates saved in 44x1 and 66x1 vector format, respectively.</p><p>For the DHG-14/28 dataset, each gesture is individually classified into two main categories: fine-grained and coarsegrained gestures. <ref type="table" target="#tab_0">Table I</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Preprocessing</head><p>The data inputs for the CNN+LSTM network are the depth images. The depth images originally contain 16-bit information per pixel, and were normalized so that pixel value ranges from 0 to 1. In addition, the hand images are cropped from the entire frame based on region of interest provided by the dataset. Lastly, for each sequence, only the frames between the starting and ending motion of the gesture are used for recognition.</p><p>In case of the LSTM network, only 2D skeleton points are used for processing. For a selected gesture, every skeleton point in a sequence is normalized by subtracting every point by the palm location from the initial frame. In addition, only the sequences marked between the start and the end of the gesture are used for recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Results</head><p>In order to compare the performance of the proposed method, we included the recognition rates of other methods that have also used the same database for experiments. <ref type="table" target="#tab_0">Table  II</ref> illustrates 14 unique gestures and the recognition rates of selected methods examined using the DHG-14/28 dataset. As mentioned in <ref type="bibr" target="#b15">[16]</ref>, it is not sufficient to provide only the averaged classification rates. Therefore for comparison, the best, worst, and the average classification rates for each gesture grain categories are provided in <ref type="table" target="#tab_0">Table II</ref>. In addition, the results in <ref type="table" target="#tab_0">Table II</ref> are further examined based on the gesture's grain (fine, coarse, and/or both types of grain) as categorized previously in <ref type="table" target="#tab_0">Table I</ref>.</p><p>The conducted experimental study shows that both the proposed method of using the depth-based CNN+LSTM and the skeleton-based LSTM network demonstrate relatively similar performance. Rows 4-6 in <ref type="table" target="#tab_0">Table II</ref> represent the performance of the proposed fusion networks. FL-fusion-Concat shows the recognition rates achieved by the feature-level fusion through concatenating the features extracted from the skeleton-based LSTM network and the depth-based CNN+LSTM network. SL-fusion-Average reports the performance obtained by the score-level fusion through averaging the results of the soft-max layers of each of the skeleton and the depth-based networks. SL-fusion-Maximum represents the score-level fusion that predicts the output based on finding the maximum scores between the skeleton and the depth network.</p><p>Of the three fusion networks, SL-fusion-Average performs the best, while FL-fusion-Concat performs the worst. From <ref type="table" target="#tab_0">Table II</ref>, FL-fusion-Concat (row 4) performs worse than the default Skeleton LSTM network, which indicate that the process of fusion of depth and skeleton information at a feature level degrades the overall performance. It should be noted that even though the overall performance is reduced, the recognition rate for the depth-based fine-grained gesture is 3.60% higher than the skeleton-based fine-grained gesture. SLfusion-Average (row 5) and SL-fusion-Maximum (row 6) show similar performance with SL-fusion-Average performing 0.1% better for both types of grained gestures. The performance of score-level fusion shows that this level of fusion results in better performance when compared to the independent skeleton and depth networks. Even though skeleton network provides higher performance independently, the combined score between both networks yields the best performance because the depth provides information that may be lost in the process of skeleton joints extraction.</p><p>To further examine the performance of the gesture recognition in terms of the averaged classification rates, a confusion matrix was created ( <ref type="figure" target="#fig_6">Fig. 6 and 5</ref>). It illustrates the accuracy of the classification performed by the proposed method when using score-level fusion of skeleton and depth information for 14 and 28 gestures, respectively. For each figure, the x-axis represents the network's prediction whereas the y-axis represents the true gesture. For example, the gesture R-CW (5th row) indicates that the network is able to predict 79.5% of all the R-CW gestures correctly, while misidentifying the other 20.5% as other gestures. <ref type="figure" target="#fig_7">Fig. 6</ref> shows that the biggest failure of the fusion network occurs at the prediction of the grabbing gesture (1st row) which is misidentified 67.0% of the time of which 42.0% is classified as the pinching gesture. This specific grabbing gesture has been noted by <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b17">[18]</ref> to be difficult to distinguish from the pinching motion. Similar observation can also be seen in <ref type="figure" target="#fig_6">Fig. 5</ref>.</p><p>Based on the gesture recognition rates from <ref type="figure" target="#fig_7">Fig. 6</ref> and 5, the Loss of Accuracy when Removing the Finger Differentiation (suggested in <ref type="bibr" target="#b17">[18]</ref>) is calculated to be 0.07286 which is seven times higher than 0.0114 obtained in <ref type="bibr" target="#b17">[18]</ref>. On average, the fusion provided a recognition rate of 85.46% and 74.19% for 14 and 28 gestures, respectively. The transition between 14 to 28 gestures results in a 11.27% decrease of which 7.29% comes from intra-gestures confusion. This indicates that the fusion network is not yet optimized for the 28 gestures, and by using the network weights pre-trained on 14 gestures, the  overall network produces greater error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>This paper contributes to the study of ambient, sensorbased monitoring of human activity and gesture recognition. Specifically, it focuses on the task for dynamic hand gesture recognition, using both skeleton and depth data acquired by depth-RGB sensors, and the powerful deep learning techniques. The results of the conducted experimental study are summarized as follows: 1) Usage of depth data, in addition to visual spectrum, RGB data, has potential to greatly improve the performance of hand gesture recognition algorithms that rely on temporal patterns. While usage of only depth data with CNN leads to the average recognition rate of 32.24%. However, utilizing the CNN for feature extraction, and passing those features to the RNN, the overall performance is shown to increase up to 75.79%. This results indicate that CNN alone is insufficient in extracting enough information from a singular depth image to correctly predict the desired gesture. By combining the CNN with the RNN, the new network is able to recognize patterns from the features extracted from a CNN throughout multiple frames. Another aspect influencing the performance is the chosen timestep parameter which depending on the input data may be severely truncated when given long sequences or padded with blank information for short sequences.</p><p>2) The performance of using skeleton data has shown a recognition rate of 82.18% which is higher compared to the depth-based networks. However, when examining the recognition rates for each grain gesture, the depth-based approach produces a higher recognition rate for the fine- grained gesture. Based on this observation, the fusion of both depth-based and skeleton-based networks should yield a higher overall recognition rate. Experiments show that by using a score-level fusion, a recognition rate of 85.46% is achieved, which is higher than the rates of 82.18% and 76.50% for the independent skeleton and the depth based approaches, respectively. In addition to an overall higher recognition rates, the rates for each type of grain gesture also increased. For the fine grained gestures, while the rate for separately obtained depthbased and skeleton-based approach are 73.50% and 67.20%, respectively, the combined rate is 76.00%. For the coarse grained gestures, the depth-based (78.17%) and skeleton-based (89.00%), when combined, show the rate of 90.72%. 3) The applied fusion allowed to achieve very good performance for the dataset consisting of 14 gestures. When the same approach is applied to the 28 gesture version, the performance degrades to 74.19% which is an ?11% decrease. The main reason for this decrease is that the networks' weights are optimized for recognizing 14 gestures. Therefore, a better performance can achieved by training each network independently for recognizing 28 gestures prior to fusion. As a future work, we anticipate to expand the proposed framework toward human activity recognition which would be applied to various assisting living, healthcare and humanmachine interaction scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(MLP). The first component, CNN, includes six 3x3 convolutional layers with a 2x2 max pooling layer between every other convolutional layer. The second component, RNN, includes two LSTM layers, each consisting of 256 LSTM units. The final component, MLP, contains three fully connected (FC) layers consisting of 256, 256, and 14 units, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>The structure of the skeleton-based LSTM network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The three levels of fusion for the LSTM and CNN+LSTM networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The structure of the depth-based CNN+LSTM network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>provides a list of all the gestures and the corresponding grain categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Illustrations of the grab gesture from the DHG-14/28 [18]. (a) Frame 1, (b) Frame 40, (c) Frame 50, (d) Frame 60.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>The confusion matrix showing accuracies of gesture recognition when using the score level fusion (average) of both skeleton and depth information on 28 gestures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>The confusion matrix showing accuracies of gesture recognition when using the score level fusion (average) of both skeleton and depth information on 14 gestures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I LIST</head><label>I</label><figDesc>OF GESTURES IN THE DHG-14/28 DATASET</figDesc><table><row><cell>Gesture</cell><cell>Grain</cell><cell>Tag Name</cell></row><row><cell>Grab</cell><cell>Fine</cell><cell>G</cell></row><row><cell>Tap</cell><cell>Coarse</cell><cell>T</cell></row><row><cell>Expand</cell><cell>Fine</cell><cell>E</cell></row><row><cell>Pinch</cell><cell>Fine</cell><cell>P</cell></row><row><cell>Rotation Clockwise</cell><cell>Fine</cell><cell>R-CW</cell></row><row><cell>Rotation Counter-clockwise</cell><cell>Fine</cell><cell>R-CCW</cell></row><row><cell>Swipe Right</cell><cell>Coarse</cell><cell>S-R</cell></row><row><cell>Swipe Left</cell><cell>Coarse</cell><cell>S-L</cell></row><row><cell>Swipe Up</cell><cell>Coarse</cell><cell>S-U</cell></row><row><cell>Swipe Down</cell><cell>Coarse</cell><cell>S-D</cell></row><row><cell>Swipe X</cell><cell>Coarse</cell><cell>S-X</cell></row><row><cell>Swipe V</cell><cell>Coarse</cell><cell>S-V</cell></row><row><cell>Swipe +</cell><cell>Coarse</cell><cell>S-+</cell></row><row><cell>Shake</cell><cell>Coarse</cell><cell>Sh</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="6">RECOGNITION RATES (%) OF THE DHG-14 DATASET</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Best</cell><cell>Fine Worst</cell><cell>Avg ? Std</cell><cell>Best</cell><cell cols="2">Coarse Worst</cell><cell>Avg ? Std</cell><cell>Best</cell><cell>Both Worst</cell><cell>Avg ? Std</cell></row><row><cell>Depth CNN</cell><cell>52.89</cell><cell>24.83</cell><cell>37.05? 7.89</cell><cell>38.92</cell><cell>20.61</cell><cell></cell><cell>29.57?4.51</cell><cell>40.19</cell><cell>25.05</cell><cell>32.24 ? 4.64</cell></row><row><cell>Depth CNN+LSTM</cell><cell>90.00</cell><cell>52.00</cell><cell>73.50 ? 10.93</cell><cell>92.22</cell><cell>58.89</cell><cell cols="2">77.06 ? 8.73</cell><cell>85.00</cell><cell>58.57</cell><cell>75.79 ? 7.23</cell></row><row><cell>Skeleton LSTM</cell><cell>82.00</cell><cell>54.00</cell><cell>69.90 ? 9.91</cell><cell>96.67</cell><cell>76.67</cell><cell cols="2">89.00 ? 5.40</cell><cell>91.43</cell><cell>71.43</cell><cell>82.18 ? 5.32</cell></row><row><cell>FL-fusion-Concat</cell><cell>90.00</cell><cell>48.00</cell><cell>72.90 ? 10.30</cell><cell>98.89</cell><cell>78.89</cell><cell cols="2">86.83 ? 4.68</cell><cell>87.86</cell><cell>67.86</cell><cell>81.86 ? 5.38</cell></row><row><cell>SL-fusion-Average</cell><cell>92.00</cell><cell>52.00</cell><cell>76.00 ? 10.51</cell><cell>97.78</cell><cell>81.11</cell><cell cols="2">90.72 ? 4.64</cell><cell>95.00</cell><cell>72.86</cell><cell>85.46 ? 5.16</cell></row><row><cell>SL-fusion-Maximum</cell><cell>94.00</cell><cell>54.00</cell><cell>75.30 ? 10.89</cell><cell>98.89</cell><cell>78.89</cell><cell cols="2">90.94 ? 4.36</cell><cell>91.43</cell><cell>71.43</cell><cell>85.36 ?5.06</cell></row><row><cell>Skeleton [16]</cell><cell>86.00</cell><cell>42.00</cell><cell>61.20 ? 12.37</cell><cell>97.78</cell><cell>74.44</cell><cell></cell><cell>86.44? 7.94</cell><cell>93.57</cell><cell>67.86</cell><cell>77.43 ? 6.82</cell></row><row><cell>Motion Feature [16]</cell><cell>84.00</cell><cell>46.00</cell><cell>71.50 ? 11.44</cell><cell>96.67</cell><cell>64.44</cell><cell cols="2">81.94 ? 8.17</cell><cell>90.00</cell><cell>58.57</cell><cell>78.21 ? 7.49</cell></row><row><cell>Skeleton + Motion Feature [16]</cell><cell>90.00</cell><cell>56.00</cell><cell>76.90 ? 9.19</cell><cell>97.78</cell><cell>72.22</cell><cell cols="2">89.00 ? 7.55</cell><cell>94.29</cell><cell>67.86</cell><cell>84.68 ? 6.67</cell></row><row><cell>3D Skeleton CNN+LSTM [17]</cell><cell>N/A</cell><cell>N/A</cell><cell>78.00</cell><cell>N/A</cell><cell>N/A</cell><cell></cell><cell>89.83</cell><cell>N/A</cell><cell>N/A</cell><cell>85.61</cell></row><row><cell>Skeleton-based, De Smedt [18]</cell><cell>N/A</cell><cell>N/A</cell><cell>73.60</cell><cell>N/A</cell><cell>N/A</cell><cell></cell><cell>88.33</cell><cell>N/A</cell><cell>N/A</cell><cell>83.07</cell></row><row><cell>Depth-based, De Smedt [18]</cell><cell>N/A</cell><cell>N/A</cell><cell>66.90</cell><cell>N/A</cell><cell>N/A</cell><cell></cell><cell>85.94</cell><cell>N/A</cell><cell>N/A</cell><cell>79.14</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This project was partially supported by Natural Sciences and Engineering Research Council of Canada (NSERC) through Discovery Grant "Biometric Intelligent Interfaces"; the Province of Alberta Queen Elizabeth II Scholarship and the University of Calgary W21C (VPR's grant "Innovations in Home Health Care to Support an Aging Population").</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated cognitive health assessment using smart home monitoring of complex tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Dawadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitter-Edgecombe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1302" to="1313" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mobility assessment using event-related responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jimison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transdisciplinary Conf. on Distributed Diagnosis and Home Healthcare</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="71" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Integration of smart home technologies in a health monitoring system for the elderly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arcelus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goubran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Knoefel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Advanced Information Networking and Applications Workshops</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="820" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Embedded assessment of aging adults: a concept validation with stakeholders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Pervasive Computing Technologies for Healthcare</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning setting-generalized activity models for smart spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human activity recognition and pattern discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Helal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pervasive Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="53" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning human activities and object affordances from rgb-d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="951" to="970" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gesture recognition by using colored gloves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iwai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yachida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="76" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hand gesture recognition using multi-scale colour features, hierarchical models and particle filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bretzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="423" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Depth silhouettes for gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mu?oz-Salinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Medina-Carnicer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Madrid-Cuevas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carmona-Poyato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="319" to="329" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-time sign language recognition using a consumer depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="83" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spelling it out: Real-time asl fingerspelling recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pugeault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1114" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hand gesture recognition in real time for automotive interfaces: A multimodal vision-based approach and evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2368" to="2377" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hand gesture recognition with 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Max-pooling convolutional neural networks for vision-based hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ducatelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Caro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cire?an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Signal and Image Processing Applications</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="342" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Motion feature augmented recurrent neural network for skeleton-based dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1708.03278</idno>
		<ptr target="http://arxiv.org/abs/1708.03278" />
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural networks and long short-term memory for skeleton-based human activity and hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>N??ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cabido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Pantrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Montemayor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>V?lez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="80" to="94" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Skeleton-based dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">De</forename><surname>Smedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Vandeborre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1206" to="1214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
