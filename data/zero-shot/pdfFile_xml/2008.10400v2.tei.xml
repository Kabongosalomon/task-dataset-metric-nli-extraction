<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AN ENSEMBLE OF SIMPLE CONVOLUTIONAL NEURAL NETWORK MODELS FOR MNIST DIGIT RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyeon</forename><surname>An</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Sogang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjun</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Sogang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanglee</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Sogang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heerin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Sogang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungmin</forename><surname>So</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Sogang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AN ENSEMBLE OF SIMPLE CONVOLUTIONAL NEURAL NETWORK MODELS FOR MNIST DIGIT RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>image classification ? MNIST</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We report that a very high accuracy on the MNIST test set can be achieved by using simple convolutional neural network (CNN) models. We use three different models with 3?3, 5?5, and 7?7 kernel size in the convolution layers. Each model consists of a set of convolution layers followed by a single fully connected layer. Every convolution layer uses batch normalization and ReLU activation, while pooling is not used. Rotation and translation is used to augment training data, which is a technique frequently used in most image classification tasks. A majority voting using the three models independently trained on the training set can achieve up to 99.87% accuracy on the test set, which is one of the state-of-the-art results. A two-layer ensemble, a heterogeneous ensemble of three homogeneous ensemble networks, can achieve up to 99.91% test accuracy. The results can be reproduced by using the code at https://github.com/ansh941/MnistSimpleCNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>MNIST handwritten digit recognition data set ( <ref type="figure" target="#fig_0">Figure 1</ref>, <ref type="bibr" target="#b0">[1]</ref>) is one of the most basic data sets used to test performance of neural network models and learning techniques. Using 60,000 images as the training set, a 97%-98% accuracy could easily be achieved on the test set of 10,000 images, with learning methods such as k-nearest neighbors (KNN), random forests, support vector machines (SVM) and simple neural network models. Convolutional neural networks (CNN) improve this accuracy to over 99% with less than 100 misclassified images in the test set. The final 100 images are more difficult to classify correctly. In order to improve accuracy after 99%, we need more complex models, careful tuning of hyperparameters such as learning rate and batch size, regularization techniques such as batch normalization and dropout, and augmentation of training data. The highest accuracy achieved on the MNIST test set are approximately 99.7% to 99.84%, as reported in the papers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>In this paper, we report a model that can achieve a very high accuracy on the MNIST test set without complex structural aspects or learning techniques. The model uses a set of convolution layers followed by a fully connected layer at the end, which is one of the commonly used model architectures. We use basic data augmentation schemes, translation and rotation. We train three models with similar architectures, and use majority voting between the models to obtain the arXiv:2008.10400v2 [cs.CV] 5 Oct 2020 final prediction. The three models have similar architectures, but have different kernel sizes in the convolution layers. Experiments show that combining models with different kernel sizes achieves better accuracy than combining models with the same kernel size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Network Design and Training</head><p>Our network models consist of multiple convolution layers and a fully connected layer at the end. In each convolution layer, a 2D convolution is performed, followed by a 2D batch normalization and ReLU activation. Max pooling or average pooling is not used after convolution. Instead, the size of feature map is reduced after each convolution because padding is not used. For example, if we use a 3?3 kernel, the width and height of the image is reduced by two after each convolution layer. Similar approach is taken in other networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b1">2]</ref>. The number of channels is increased after each layer in order to account for reduction in feature map size. Once the feature map size becomes small enough, a fully-connected layer connects the feature map to the final output. A 1D batch normalization is used at the fully-connected layer, while dropout is not used.</p><p>We use three different networks and combine the results from these networks. The networks differ only in the kernel sizes of the convolution layers: 3?3, 5?5, and 7?7. Because different kernel size lead to different size reduction in feature maps, the number of layers is different for each network. The first network, M 3 , uses 10 convolution layers with 16(i + 1) channels in ith convolution layer. The feature map becomes 8?8 with 176 channels after the 10th layer. The second network, M 5 , uses 5 convolution layers with 32i channels in ith convolution layer. The feature map becomes 8?8 with 160 channels after the 5th layer. The third network, M 7 , uses 4 convolution layers with 48i channels in ith convolution layer. The feature map becomes 4?4 with 192 channels after the 4th layer. The structure of the three networks are shown in <ref type="figure" target="#fig_2">Figure 2</ref>.   The network parameters are initialized using default initialization methods in PyTorch <ref type="bibr" target="#b6">[7]</ref>. For parameter optimization, we use the Adam optimizer with cross-entropy loss function. Learning rate starts at 0.001, and exponentially decays with decaying factor ?=0.98. The batch size is 120, and so 500 parameter updates occur in an epoch. We use exponential moving average of weights for evaluation, which may lead to better generalization <ref type="bibr" target="#b7">[8]</ref>. The exponential decay used for computing the moving average is 0.999.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results for Individual Networks and Ensembles</head><p>For each type of network, we have trained 30 networks with different initial parameters. Each network was trained for 150 epochs, since the test accuracy hardly improved after that point. <ref type="figure" target="#fig_4">Figure 4</ref> shows the change in the training accuracy and the test accuracy while training. In terms of test accuracy, networks with larger kernels show some instability at early epochs, but the patterns of all networks become similar after 50 epochs. <ref type="table" target="#tab_0">Table 1</ref> shows the minimum, average, maximum accuracy of 30 networks between 50 and 150 epochs, in the 95% confidence range. The accuracy of M 3 is slightly higher followed by M 5 and M 7 , but the difference is not too significant (less than 0.02%). Between 50 and 150 epochs of 30 networks, the highest test accuracy observed from M 3 , M 5 , M 7 was 99.82, 99.80, and 99.79 respectively.  It is known that using ensemble of networks can improve generalization and achieve higher test accuracy <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>.</p><formula xml:id="formula_0">(a) M3 (b) M5 (c) M7</formula><p>To test the performance of ensemble networks on the MNIST data set, we trained 30 networks each of M 3 , M 5 , and M 7 , and tested four different ensemble strategies. In the first three strategies, we randomly select three networks from the same type of networks (M 3 , M 5 , or M 7 ). In the fourth strategy, we select one network from each type. The final result is obtained by using majority voting. That is, if two networks agree that an image belongs to a particular class, that class is selected. If the three networks vote on different class, one class is randomly selected among the three. For each strategy, we tested 1000 ensemble networks and plotted the histogram for the test accuracy. <ref type="figure" target="#fig_5">Figure 5</ref> shows the benefit of using ensemble of homogeneous networks. For M 3 , M 5 , and M 7 , higher test accuracy could be achieved by combining results from three networks. (The line moves to the right.) <ref type="figure" target="#fig_6">Figure 6</ref> shows the test accuracy of the four ensemble methods discussed above, and <ref type="table" target="#tab_1">Table 2</ref> shows the 95% confidence range of test accuracy for the four methods. It can be observed that while the average test accuracy of homogeneous ensemble methods are similar, the ensemble method where one network is selected from each type of networks achieves higher accuracy.  From <ref type="figure" target="#fig_5">Figure 5</ref> we can see that using an ensemble of three homogeneous networks could improve the test accuracy. Also, it is shown in <ref type="figure" target="#fig_6">Figure 6</ref> that combining results from heterogeneous networks could help boost the accuracy as well. We have tested a two-level ensemble method, where we first combine results from three homogeneous networks, and then combine results from three homogeneous ensemble networks. For this study, we trained 3 groups of 10 networks for each type of network, M 3 , M 5 , and M 7 . For each network, we trained for 150 epochs and saved the best model in terms of test accuracy. Then, we randomly chose 3 networks from M 3 and combined their results using majority voting. Similarly, we combined results of three networks for M 5 and M 7 . After that, we used majority voting for the three ensemble networks. <ref type="figure" target="#fig_7">Figure 7</ref> shows the distribution of test accuracy for 1000 ensemble of individual networks (M 3 +M 5 +M 7 ) and 1000 ensemble of ensemble networks ((M 3 +M 3 +M 3 )+(M 5 +M 5 +M 5 )+(M 7 +M 7 +M 7 )). The graph shows that using ensemble of ensemble networks improves the test accuracy in average. <ref type="table" target="#tab_2">Table 3</ref> shows the 95% confidence range and the best accuracy observed for ensemble of individual and ensemble of ensemble networks. In addition to random selection, we also show the best case in order to see what is the best accuracy we can achieve. For the best case, we picked 10 homogeneous ensemble networks from M 3 , M 5 , and M 7 that shows the best test accuracy.</p><p>Then, we chose one network from each type and combined their results. The best accuracy achieved was 99.91%.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Impact of network architecture</head><p>When building a CNN, a common practice is to use pooling, such as max pooling or average pooling <ref type="bibr" target="#b12">[13]</ref>. Pooling is used to obtain translation invariance and also reduce dimension of the feature maps. A commonly used CNN model consists of a set of convolution layers where each convolution layer is followed by a pooling layer, and one or multiple fully connected layers at the end. Some networks have two convolution layers before the pooling layer. <ref type="figure" target="#fig_8">Figure 8</ref> show some of the commonly used CNN structures, and we name the three networks C 1 , C 2 , and C 3 . <ref type="figure" target="#fig_9">Figure 9</ref> shows the change in training and test accuracy during training. It can be observed that for networks using max pooling, the test accuracy goes through oscillations in the early stage of training. On the other hand, the test accuracy of M 5 increases in a more stable manner. <ref type="table" target="#tab_3">Table 4</ref> shows the test accuracy of 30 networks between 50 epoch and 150 The average test accuracy of C 3 and M 5 is better than that of C 1 and C 2 , which means using more convolution layers could result in better feature learning. Having more fully connected layers at the end did not help, as can be seen from the accuracy of C 1 and C 2 . Between C 3 and M 5 , M 5 achieves higher accuracy in general, and also can reach higher accuracy in the best case.   <ref type="figure" target="#fig_0">Figure 10</ref> shows the distribution plot of 30 networks for C 1 , C 2 , C 3 , and M 5 . For this graph, each network is trained for 150 epochs, and the network with the highest test accuracy is saved. It can be shown that M 5 achieves better test accuracy than other networks in general. <ref type="table" target="#tab_4">Table 5</ref> shows the 95% confidence range of test accuracy for each network models. <ref type="figure" target="#fig_0">Figure 10</ref>: Distribution of test accuracy for networks with different architectures. </p><formula xml:id="formula_1">(a) C1 (b) C2 (c) C3</formula><formula xml:id="formula_2">(a) C1 (b) C2 (c) C3 (d) M5</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Impact of data augmentation</head><p>Data augmentation is a technique to increase the diversity of training data without actually collecting data and labeling them. It is an essential technique for supervised learning in which a large data set is required for the network model to achieve high performance <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. When training the proposed network, we used two schemes for data generation: random rotation and random translation. There are many other schemes such as cropping, flipping, and resizing, and the best augmentation schemes depend on the data. In this section, we study whether data augmentation actually helps improving the network performance. We compared performance of four M 5 networks with different combinations of augmentation schemes applied. <ref type="figure" target="#fig_0">Figure 11</ref> shows the distribution plot of 30 networks for four different augmentation strategies. It can be observed that data augmentation is helpful in general. For the MNIST data set, applying random rotation has slightly higher contribution than random translation, but both schemes are needed to achieve the best accuracy. <ref type="table" target="#tab_5">Table 6</ref> shows the 95% confidence range of test accuracy for the four augmentation strategies.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Impact of batch normalization</head><p>Batch normalization is a well known technique to improve performance of the network as well as stability and speed of training <ref type="bibr" target="#b18">[19]</ref>. It has been reported that most neural network models benefit from using batch normalization <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. In this section we study the impact of batch normalization on the performance of the network model M 5 . We compared three configurations: the first model uses no batch normalization at all, the second model uses batch normalization only at the fully connected layer, and the third model uses batch normalization at all layers. <ref type="figure" target="#fig_0">Figure 12</ref> shows the distribution plot of 30 networks for each configuration, and <ref type="table" target="#tab_6">Table 7</ref> shows the 95% confidence range of test accuracy for each configuration. It is evident that using batch normalization helps improve the performance of neural network models. The best performance is achieved when batch normalization is used at each convolution and fully connected layer. <ref type="figure" target="#fig_0">Figure 12</ref>: Distribution of test accuracy for networks trained with different batch normalization schemes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The MNIST handwritten digit data set is often used as an entry-level data set for training and testing neural networks. While achieving 99% accuracy on the test set is rather easy, correctly classifying the last 1% of the images is challenging. People have tried many different network models and techniques to increase test accuracy, and the best accuracy reported reaches approximately 99.8%. In this paper we showed that a simple CNN model with batch normalization and data augmentation could reach the best accuracy. Using an ensemble of homogeneous and heterogeneous network models could boost the performance, up to 99.91% test accuracy which is one of the state-of-the-art performance. Studies with various different configurations show that the high performance is not achieved by a single technique or model architecture, but is contributed by multiple techniques such as batch normalization, data augmentation, and ensemble methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Images from the MNIST training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Network models used for MNIST digit classification. When training, we apply transformation on data that consist of random translation and random rotation. For random translation, an image is randomly shifted horizontally and vertically, up to 20% of the image size in each direction. For random rotation, the image is rotated up to 20 degrees in either clockwise or counterclockwise direction. The amount of transformation varies for each image and each epoch, so the network gets to see various versions of an image in the training set (Figure 3). For training and evaluation, the input vectors which are typically integers in [0, 255] are converted to floating point values in [-1.0, 1.0].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Random translation and random rotation applied to a training image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Train accuracy and test accuracy of M 3 , M 5 , and M 7 during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Distribution of test accuracy for individual networks and homogeneous ensemble networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Distribution of test accuracy for homogeneous and heterogeneous ensemble networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Distribution of test accuracy for ensemble of individual networks and ensemble of ensemble networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Commonly used CNN structures with max pooling. epoch of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Train accuracy and test accuracy of C 1 , C 2 , C 3 , and M 5 during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Distribution of test accuracy for networks trained with different augmentation schemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy of networks measured between 50 epoch and 150 epoch in training.</figDesc><table><row><cell>model</cell><cell></cell><cell>test accuracy</cell><cell></cell></row><row><cell></cell><cell>min</cell><cell>avg</cell><cell>max</cell><cell>best</cell></row><row><cell>M 3</cell><cell cols="3">99.5930 ? 0.0136 99.6949 ? 0.0058 99.7667 ? 0.0084</cell><cell>99.82</cell></row><row><cell>M 5</cell><cell cols="3">99.5863 ? 0.0115 99.6835 ? 0.0074 99.7583 ? 0.0081</cell><cell>99.80</cell></row><row><cell>M 7</cell><cell cols="3">99.5470 ? 0.0288 99.6711 ? 0.0089 99.7450 ? 0.0093</cell><cell>99.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>95% confidence range of test accuracy for homogeneous and heterogeneous ensemble networks.</figDesc><table><row><cell>configuration</cell><cell>test accuracy</cell><cell></cell></row><row><cell></cell><cell>95% confidence range</cell><cell>best accuracy</cell></row><row><cell>M 3 + M 3 + M 3</cell><cell>99.7901 ? 0.0014</cell><cell>99.86</cell></row><row><cell>M 5 + M 5 + M 5</cell><cell>99.7925 ? 0.0014</cell><cell>99.86</cell></row><row><cell>M 7 + M 7 + M 7</cell><cell>99.7874 ? 0.0014</cell><cell>99.85</cell></row><row><cell>M 3 + M 5 + M 7</cell><cell>99.8014 ? 0.0015</cell><cell>99.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>95% confidence range and best-case test accuracy for ensemble of individual networks and ensemble of ensemble networks.</figDesc><table><row><cell>configuration</cell><cell>test accuracy</cell><cell></cell></row><row><cell></cell><cell>95% confidence range</cell><cell>best accuracy</cell></row><row><cell>ensemble of individual networks</cell><cell>99.8014 ? 0.0015</cell><cell>99.87</cell></row><row><cell>ensemble of ensembles (random)</cell><cell>99.8118 ? 0.0002</cell><cell>99.89</cell></row><row><cell>ensemble of ensembles (best)</cell><cell>99.8646 ? 0.0008</cell><cell>99.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Test accuracy of networks measured between 50 epoch and 150 epoch in training.</figDesc><table><row><cell>model</cell><cell></cell><cell>test accuracy</cell><cell></cell></row><row><cell></cell><cell>min</cell><cell>avg</cell><cell>max</cell><cell>best</cell></row><row><cell>C 1</cell><cell cols="3">99.3052 ? 0.0865 99.5293 ? 0.0105 99.6419 ? 0.0059</cell><cell>99.70</cell></row><row><cell>C 2</cell><cell cols="3">99.3594 ? 0.0442 99.5316 ? 0.0090 99.6337 ? 0.0051</cell><cell>99.68</cell></row><row><cell>C 3</cell><cell cols="3">99.4720 ? 0.04268 99.6448 ? 0.0078 99.7372 ? 0.0033</cell><cell>99.78</cell></row><row><cell>M 5</cell><cell cols="3">99.5863 ? 0.0115 99.6835 ? 0.0074 99.7583 ? 0.0081</cell><cell>99.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>95% confidence range of test accuracy for networks with different architectures.</figDesc><table><row><cell>model</cell><cell>test accuracy</cell></row><row><cell>C 1</cell><cell>99.6466 ? 0.0121</cell></row><row><cell>C 2</cell><cell>99.6406 ? 0.0108</cell></row><row><cell>C 3</cell><cell>99.7440 ? 0.0080</cell></row><row><cell>M 5</cell><cell>99.7600 ? 0.0089</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>95% confidence range of test accuracy for networks trained with different augmentation schemes.</figDesc><table><row><cell cols="2">augmentation scheme</cell><cell>test accuracy</cell></row><row><cell>translation</cell><cell>rotation</cell></row><row><cell></cell><cell></cell><cell>99.6783 ? 0.0086</cell></row><row><cell></cell><cell></cell><cell>99.7203 ? 0.0074</cell></row><row><cell></cell><cell></cell><cell>99.7327 ? 0.0077</cell></row><row><cell></cell><cell></cell><cell>99.7600 ? 0.0089</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>95% confidence range of test accuracy for networks trained with different batch normalization schemes.</figDesc><table><row><cell>configuration</cell><cell>test accuracy</cell></row><row><cell>no batch normalization</cell><cell>99.6337 ? 0.0131</cell></row><row><cell>batch normalization at the final layer</cell><cell>99.7050 ? 0.0092</cell></row><row><cell>batch normalization at all layers</cell><cell>99.7600 ? 0.0089</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
	</analytic>
	<monogr>
		<title level="m">ATT Labs</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A branching and merging convolutional network with homogeneous filter capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Byerly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kalganova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dear</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09136</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">RMDL: Random multimodel deep learning for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heidarysafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Meimandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Conf. Inf. Syst. Data Mining (ICISDM)</title>
		<meeting>2nd Int. Conf. Inf. Syst. Data Mining (ICISDM)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Regularization of Neural Networks using DropConnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Averaging Weights Leads to Wider Optima and Better Generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bagging Predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discussion of additive logistic regression: A statistical view of boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Shapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="337" to="374" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">LightGBM: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3149" to="3157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">AutoAugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fast autoaugment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00397</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00910</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">across views</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding Batch Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How does batch normalization help optimization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
