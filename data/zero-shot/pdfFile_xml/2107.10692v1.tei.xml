<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Selective Pseudo-Label Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Mahon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Selective Pseudo-Label Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks (DNNs) offer a means of addressing the challenging task of clustering high-dimensional data. DNNs can extract useful features, and so produce a lower dimensional representation, which is more amenable to clustering techniques. As clustering is typically performed in a purely unsupervised setting, where no training labels are available, the question then arises as to how the DNN feature extractor can be trained. The most accurate existing approaches combine the training of the DNN with the clustering objective, so that information from the clustering process can be used to update the DNN to produce better features for clustering. One problem with this approach is that these "pseudo-labels" produced by the clustering algorithm are noisy, and any errors that they contain will hurt the training of the DNN. In this paper, we propose selective pseudo-label clustering, which uses only the most confident pseudo-labels for training the DNN. We formally prove the performance gains under certain conditions. Applied to the task of image clustering, the new approach achieves a state-of-the-art performance on three popular image datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Clustering is the task of partitioning a dataset into clusters such that data points within the same cluster are similar to each other, and data points from different clusters are different to each other. It is applicable to any set of data for which there is a notion of similarity between data points. It requires no prior knowledge, neither the explicit labels of supervised learning nor the knowledge of expected symmetries and invariances leveraged in self-supervised learning.</p><p>The result of a successful clustering is a means of describing data in terms of the cluster that they belong to. This is a ubiquitous feature of human cognition. For example, we hear a sound and think of it as an utterance of the word "water", or we see a video of a biomechanical motion and think of it as a jump. This can be further refined among experts, so that a musician could describe a musical phrase as an English cadence in A major, or a dancer could describe a snippet of ballet as a right-leg fouette into arabesque. When clustering high-dimensional data, the curse of dimensionality <ref type="bibr" target="#b1">[2]</ref> means that many classic algorithms, such as k-means <ref type="bibr" target="#b28">[29]</ref> or expectation maximization <ref type="bibr" target="#b9">[10]</ref>, perform poorly. The Euclidean distance, which is the basis for the notion of similarity in the Euclidean space, becomes weaker in higher dimensions <ref type="bibr" target="#b50">[51]</ref>. Several solutions to this problem have been proposed. In this paper, we consider those termed deep clustering.</p><p>Deep clustering is a set of techniques that use a DNN to encode the highdimensional data into a lower-dimensional feature space, and then perform clustering in this feature space. A major challenge is the training of the encoder. Much of the success of DNNs as image feature extractors (including <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b45">46]</ref>) has been in supervised settings, but if we already had labels for our data, then there would be no need to cluster in the first place. There are two common approaches to training the encoder. The first is to use the reconstruction loss from a corresponding decoder, i.e., to train it as an autoencoder <ref type="bibr" target="#b46">[47]</ref>. The second is to design a clustering loss, so that the encoding and the clustering are optimized jointly. Both are discussed further in Section 2.</p><p>Our model, selective pseudo-label clustering (SPC ), combines reconstruction and clustering loss. It uses an ensemble to select different loss functions for different data points, depending on how confident we are in their predicted clusters.</p><p>Ensemble learning is a function approximation where multiple approximating models are trained, and then the results are combined. Some variance across the ensemble is required. If all individual approximators were identical, there would be no gain in combining them. For ensembles composed of DNNs, variance is ensured by the random initializations of the weights and stochasticity of the training dynamics. In the simplest case, the output of the ensemble is the average of each individual output (mean for regression and mode for classification) <ref type="bibr" target="#b35">[36]</ref>.</p><p>When applying an ensemble to clustering problems (referred to as consensus clustering; see <ref type="bibr" target="#b2">[3]</ref> for a comprehensive discussion), the sets of cluster labels must be aligned across the ensemble. This can be performed efficiently using the Hungarian algorithm. SPC considers a clustered data point to be confident if it received the same cluster label (after alignment) in each member of the ensemble. The intuition is that, due to random initializations and stochasticity of training, there is some non-zero degree of independence between the different sets of cluster labels, so the probability that all cluster labels are incorrect for a particular point is less than the probability that a single cluster label is incorrect.</p><p>Our main contributions are briefly summarized as follows.</p><p>-We describe a generally applicable deep clustering method (SPC), which treats cluster assignments as pseudo-labels, and introduces a novel technique to increase the accuracy of the pseudo-labels used for training. This produces a better feature extractor, and hence a more accurate clustering. -We formally prove the advantages of SPC, given some simplifying assumptions.</p><p>Specifically, we prove that our method does indeed increase the accuracy of the targets used for pseudo-label training, and this increase in accuracy does indeed lead to a better clustering performance. -We implement SPC for image clustering, with a state-of-the-art performance on three popular image clustering datasets, and we present ablation studies on its main components.</p><p>The rest of this paper is organized as follows. Section 2 gives an overview of related work. Sections 3 and 4 give a detailed description of SPC and a proof of correctness, respectively. Section 5 presents and discusses our experimental results, including a comparison to existing image clustering models and ablation studies on main components of SPC. Finally, Section 6 summarizes our results and gives an outlook on future work. Full proofs and further details are in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>One of the first deep image clustering models was <ref type="bibr" target="#b18">[19]</ref>. It trains an autoencoder (AE) on reconstruction loss (rloss), and then clusters in the latent space, using loss terms to make the latent space more amenable to clustering.</p><p>In <ref type="bibr" target="#b43">[44]</ref>, the training of the encoder is integrated with the clustering. A second loss function is defined as the distance of each encoding to its assigned centroid. It then alternates between updating the encoder and clustering by k-means. A different differentiable loss is proposed in <ref type="bibr" target="#b42">[43]</ref>, based on a soft cluster assignment using Student's t-distribution. The method pretrains an AE on rloss, then, like <ref type="bibr" target="#b43">[44]</ref>, alternates between assigning clusters and training the encoder on cluster loss. Two slight modifications were made in later works: use of rloss after pretraining in <ref type="bibr" target="#b15">[16]</ref> and regularization to encourage equally-sized clusters in <ref type="bibr" target="#b13">[14]</ref>.</p><p>This alternating optimization is replaced in <ref type="bibr" target="#b12">[13]</ref> by a clustering loss that allows cluster centroids to be optimized directly by gradient descent.</p><p>Pseudo-label training is introduced by <ref type="bibr" target="#b5">[6]</ref>. Cluster assignments are interpreted as pseudo-labels, which are then used to train a multilayer perceptron on top of the encoding DNN, training alternates between clustering encodings, and treating these clusters as labels to train the encoder.</p><p>Generative adversarial networks <ref type="bibr" target="#b14">[15]</ref> (GANs) have produced impressive results in image synthesis <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5]</ref>. At the time of writing, the most accurate GANbased image clustering models <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b10">11]</ref> design a generator to sample from a latent space that is the concatenation of a multivariate normal vector and a categorical one-hot encoding vector, then recover latent vectors for the input images as in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref>, and cluster the latent vectors. A similar idea is employed in <ref type="bibr" target="#b20">[21]</ref>, though not in an adversarial setting. For more details on GAN-based clustering, see <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40]</ref> and the references therein.</p><p>Adversarial training is used for regularization in <ref type="bibr" target="#b32">[33]</ref>. In <ref type="bibr" target="#b33">[34]</ref>, the method is developed. Conflicted data points are identified as those whose maximum probability across all clusters is less than some threshold, or whose max and next-to-max are within some threshold of each other. Pseudo-label training is then performed on the unconflicted points only. A similar threshold-based filtering method is employed by <ref type="bibr" target="#b6">[7]</ref>.</p><p>A final model to consider is <ref type="bibr" target="#b29">[30]</ref>, which uses a second round (i.e., after the DNN) of dimensionality reduction via UMAP <ref type="bibr" target="#b31">[32]</ref>, before clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Pseudo-label training is an effective deep clustering method, but training on only partially accurate pseudo-labels can hurt the encoder's ability to extract 1. Train K autoencoders in parallel. 2. Cluster in the latent space of each, to obtain K sets of pseudo-labels. 3. Select for pseudo-label training, those points are those that received the same label in all K sets of pseudo-labels, after the labellings have been aligned using the Hungarian algorithm. 4. Train on the selected pseudo-labels. Go back to <ref type="bibr" target="#b1">(2)</ref>.</p><p>Training ends when the number of agreed points stops increasing. Then, each data point is assigned its most common cluster label across the (aligned) ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formal Description</head><p>Given a dataset X ? R n of size N with C true clusters, let (f j ) 1?j?K , f j : R n ? R m , and (g j ) 1?j?K , g j : R m ? R n be the K encoders and decoders, respectively. Let ? : R N ?m ? {0, . . . , C ? 1} N be the clustering function, which takes the N encoded data points as input, and returns a cluster label for each. We refer to the output of ? as a labelling. Let ? : {0, . . . , C ?1} K?N ? {0, . . . , C ?1} N ?{0, 1} N be the consensus function, which aggregates K different labellings of X into a single labelling, and also returns a Boolean vector indicating agreement. Then,</p><formula xml:id="formula_0">(c 1 , . . . , c N ), (a 1 , . . . , a N ) = ?(?(f 1 (X )) ? ? ? ? ? ?(f K (X ))),<label>(1)</label></formula><p>Algorithm 1 Training algorithm for SPC for j = 1, . . . , K do Update parameters of fj and gj using autoencoder reconstruction end for while number of agreed points increases do compute (c1, . . . , cN ), (a1, . . . , aN ) as in <ref type="formula" target="#formula_0">(1)</ref> for j = 1, . . . , K do Update parameters of fj and hj to minimize (2) end for end while where (c 1 , . . . , c N ) are the consensus labels, and a i = 1 if the i-th data point received the same cluster label (after alignment) in all labellings, and 0 otherwise. The consensus function is the ensemble mode average, c i is the cluster label that was most commonly assigned to the i-th data point.</p><p>Define</p><formula xml:id="formula_1">K pseudo-classifiers (h j ) 1?j?K , h j : R m ? R C , and let L = 1 N N i=1 K j=1 CE(h j (f j (x i )), c i ) a i = 1 ||g j (f j (x i )) ? x i || otherwise,<label>(2)</label></formula><p>where CE denotes categorical cross-entropy:</p><formula xml:id="formula_2">CE : R C ? {0, . . . , C ? 1} ? R CE(x, n) = ? log(x[n]) .</formula><p>First, we pretrain the autoencoders, then compute (c 1 , . . . , c N ), (a 1 , . . . , a N ) and minimize L, recompute, and iterate until the number of agreed points stops increasing. The method is summarized in Algorithm 1. <ref type="figure" target="#fig_1">Figure 2</ref> shows the training dynamics. Agreed points are those that receive the same cluster label in all members of the ensemble. As expected, the agreed points' accuracy is higher than the accuracy on all points. Initially, the agreed points will not include those that are difficult to cluster correctly, such as an MNIST digit 3 that looks like a digit 5. Some ensemble members will cluster it as a 3 and others as a 5. The training process aims to make these difficult points into agreed points, thus increasing the fraction of agreed points, without decreasing the agreed points' accuracy. <ref type="figure" target="#fig_1">Figure 2</ref> shows that this aim is achieved. As more points become agreed (black dotted line), the total accuracy approaches the agreed accuracy. The agreed accuracy remains high, decreasing only very slightly (blue line). The result is that the total accuracy increases (orange line). We end training when the number of agreed points plateaus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>Encoders are stacks of convolutional and batch norm layers; decoders of transpose convolutional layers. Decoders have a tanh activation on their output layer, all other layers use leaky ReLU. The MLP pseudo-classifier has a hidden layer of size 25. The latent space of the autoencoders has the size 50 for MNIST and FashionMNIST, and 20 for smaller USPS. We inject noise from a multivariate normal into the latent space as a simple form of regularization. As suggested in <ref type="bibr" target="#b47">[48]</ref>, the reconstruction loss is 1 . The architectures are the same across the ensemble, diversity comes from random initialization and training dynamics.</p><p>The clustering function (? above) is a composition of UMAP <ref type="bibr" target="#b31">[32]</ref> and either HDBSCAN <ref type="bibr" target="#b30">[31]</ref> or a Gaussian mixture model (GMM). As in previous works, we set the number of clusters to the ground truth. UMAP uses the parameters suggested in the clustering documentation clustering, n_neighbours is 30 for MNIST and scaled in proportion to the dataset size for the others. HDBSCAN uses all default parameters. We cut the linkage tree at a level that gives the correct number of clusters. On the rare occasions when no such cut can be found, the clustering is excluded from the ensemble. The GMM uses all default parameters.</p><p>Consensus labels are taken as the most common across the ensemble, after alignment with the Hungarian algorithm (called the "direct" method in <ref type="bibr" target="#b2">[3]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proof of Correctness</head><p>Proving correctness requires proving that the expected accuracy of the agreed pseudo-labels is higher than that of all pseudo-labels, and that training with more accurate pseudo-labels makes the latent vectors easier to cluster correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Agreed Pseudo-Labels are More Accurate</head><p>Given that each member of the ensemble is initialized independently at random, and undergoes different stochastic training dynamics, we can assume that each cluster assignment contains some unique information. Formally, there is strictly positive conditional mutual information between any one assignment in the ensemble and the true cluster labels, conditioned on all the other assignments in the ensemble. From this assumption, the reasoning proceeds as follows.</p><p>Choose an arbitrary data point x 0 and cluster c 0 . Let X be a random variable (r.v.), indicating the true cluster of x 0 , given that n members of the ensemble have assigned it to c 0 , and other assignments are unknown, n ? 0. Thus, the event X = c 0 is the event that x 0 is correctly clustered. Let Y be a Boolean r.v. indicating that the (n + 1)-th member of the ensemble also assigns it to c 0 . Assume that, if n ensemble members have assigned x 0 to c 0 , and other assignments are unknown, then x 0 belongs to c 0 with probability at least 1/C and belongs to all other clusters with equal probability, i.e.,</p><formula xml:id="formula_3">p(X = c 0 ) = t ?c = c 0 , p(X = c) = (1 ? t)/(C ? 1) , for some 1/C ? t ? 1.</formula><p>It follows that the entropy H(X) is a strictly decreasing function of t (see appendix for proof). Thus, the above assumption on conditional mutual information, written I(X; Y ) &gt; 0, is equivalent to p(X = c 0 |Y ) &gt; p(X = c 0 ). This establishes that the accuracy of the agreed labels is an increasing function of ensemble size. Standard pseudo-label training uses n = 1, whereas SPC uses n &gt; 1 and so results in more accurate pseudo-labels for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Increased Pseudo-Label Accuracy Improves Clustering</head><p>Problem Formulation. Let D be a dataset of i.i.d. points from a distribution over S ? R n , where S contains C true clusters c 1 , . . . , c C . Let T be the r.v.defined by the identity function on S and f : S ? R m , an encoding function parametrized by ?, whose output is an r.v. X. The task is to recover the true clusters conditional on X, and we are interested in choosing ? such that this task is as easy as possible. Pseudo-label training applies a second function h : R m ? {0, . . . , C ? 1} and trains the composition h ? f : R n ? {0, . . . , C ? 1} using gradient descent (g.d.), with cluster assignments as pseudo-labels. The claim is that an increased pseudo-label accuracy facilitates a better choice of ?.</p><p>To formalize "easy", recall the definition of clustering as a partition that minimizes intra-cluster variance and maximizes inter-cluster variance. We want the same property to hold of the r.v. X. Let y : D ? {0, . . . , C ? 1} be the true cluster assignment function and Y the corresponding random variable, then ease of recovering the true clusters is captured by a high value of d, where</p><formula xml:id="formula_4">d = Var(E[X|Y ]) ? E[Var(X|Y )] .</formula><p>High d means that a large fraction of the variance of X is accounted for by cluster assignment, as, by Eve's law, we can decompose:</p><formula xml:id="formula_5">Var (X) = E[Var(X|Y )] + Var(E[X|Y ]) ,<label>(3)</label></formula><p>In the following, we assume that f and g are linear, C = 2, h ? f (D) ? (0, 1), and E[T ] = 0. The proof proceeds by expressing the value of d in terms of expected distances between encoded points after a training step with correct labels and with incorrect labels, and hence proving that the value is greater in the former case. We show that the expectation is greater in each coordinate, from which the claim follows by linearity (see appendix for details). Lemma 1. Let x, x ? D be two data points, and consider the expected squared distance between their encodings under f . Let u same and u diff denote the value of this difference after a g.d. update in which both labels are the same and after a step in which both labels are different, respectively. Then, u same &lt; u diff .</p><p>If w ? R m and w ? R are, respectively, the vector of weights mapping the input to the i-th coordinate of the latent space, and the scalar mapping the i-th coordinate of the latent space to the output, then the expected squared distance in the i-th coordinate of the latent vectors before the g.d. update is</p><formula xml:id="formula_6">E x,x ?T [(w T x ? w T x ) 2 ] = E x,x ?T [(w T (x ? x )) 2 ] .</formula><p>When the two labels are the same, assume w.l.o.g. that y = y = 0. Then, with step size ? , the update for w and following expected squared difference u same is</p><formula xml:id="formula_7">w ? w ? ?(w (x + x )) u same = E x,x ?T [((w ? ?w (x + x )) T (x ? x )) 2 ] = E x,x ?T [(w T (x ? x ) ? ?w (||x|| 2 ?||x || 2 )) 2 ] .</formula><p>When the two labels are different, assume w.l.o.g. that y = 0, y = 1, giving</p><formula xml:id="formula_8">w ? w ? ?(w (x ? x )) u diff = E x,x ?T [((w ? ?(w (x ? x )) T )(x ? x )]) 2 ] = E x,x ?T [(w T (x ? x ) ? ?w ||x ? x || 2 ) 2 ] .</formula><p>It can then be shown (see appendix) that u same &lt; u diff . Lemma 2. Let z be a third data point, z ? D, z = x, x , and consider the expected squared distance of the encodings, under f , of x and z. Let v same and v diff denote, respectively, the value of this difference after a g.d. update with two of the same labels, and with two different labels. Then, v same = v diff . Lemma 3. Let s and r denote, respectively, the expected squared distance between the encodings, under f , of two points in the same cluster and between two points in different clusters. Then, there exist ? 1 , ? 2 &gt; 0 whose values do not depend on the parameters of f , such that d = ? 1 r ? ? 2 s.</p><p>For simplicity, assume that the clusters are equally sized. The argument can easily be generalized to clusters of arbitrary sizes. We then obtain</p><formula xml:id="formula_9">d = C ? 1 2C r ? 2C ? 1 2C s ,</formula><p>where C is the number of clusters (see appendix for proof). The fraction of pairwise correct pairs is one measure of accuracy (Rand Index). Thus, training with more accurate pseudo-labels facilitates better clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>Following previous works, we measure accuracy and normalized mutual information (NMI). Accuracy is computed by aligning the predicted cluster labels with the ground-truth labels using the Hungarian algorithm <ref type="bibr" target="#b24">[25]</ref> and then calculating as in the supervised case. NMI, as in <ref type="bibr" target="#b40">[41]</ref>, is defined as 2I(? ; Y )/(H(? ) + H(Y )), where? , Y , I(?, ?), and H(?) are, respectively, the cluster labels, ground truth labels, mutual information, and Shannon entropy. We report on two handwritten digits datasets, MNIST (size 70000) <ref type="bibr" target="#b25">[26]</ref> and USPS (size 9298) <ref type="bibr" target="#b19">[20]</ref>, and Fashion-MNIST (size 70000) <ref type="bibr" target="#b41">[42]</ref> of clothing items. <ref type="table">Table 1</ref> shows the central tendency for five runs and the best single run. We show results for two different clustering algorithms: Gaussian mixture model and the more advanced HDBSCAN <ref type="bibr" target="#b30">[31]</ref>. Both perform similarly, showing robustness to clustering algorithm choice. SPC-GMM performs slightly worse on USPS and FashionMNIST (though within margin of error), suggesting that HDBSCAN may cope better with the more complex images in FashionMNIST and the smaller dataset in USPS. In <ref type="table">Table 1</ref>, 'SPC' uses HDBSCAN.</p><p>SPC (using either clustering algorithm) outperforms all existing approaches for both metrics on MNIST and USPS, and for NMI on FashionMNIST. The disparity between the two metrics, and between HDBSCAN and GMM, on FashionMNIST is due to the variance in cluster size. Many of the errors are lumped into one large cluster, and this hurts accuracy more than NMI, because being in this large cluster still conveys some information about what the ground truth cluster label is (see appendix for full details).</p><p>The most accurate existing methods use data augmentation. This is to be expected, given the well-established success of data augmentation in supervised learning <ref type="bibr" target="#b17">[18]</ref>. More specifically, <ref type="bibr" target="#b16">[17]</ref>   <ref type="table">Table 1</ref>: Accuracy and NMI of SPC compared to other top-performing image clustering models. The best results are in bold, and the second-best are emphasized. We report the mean and standard deviation (in parentheses) for five runs.</p><p>example, on MNIST, n2d <ref type="bibr" target="#b29">[30]</ref> (which does not use data augmentation) is only 0.6 and 1.9 behind DDC <ref type="bibr" target="#b38">[39]</ref>, which does on ACC and NMI, respectively, but is 1.2 and 5.3 behind on USPS. SPC could easily be extended to include data augmentation, and even without using it, outperforms models that do. <ref type="table" target="#tab_2">Table 2</ref> shows the effect of removing each component of our model. All settings use HDBSCAN. Particularly relevant are rows 2 and 3. As described in Section 3, we produce multiple labellings of the dataset and select for pseudo-label training only those data points that received the same label in all labellings. We perform two different ablations on this method: A1 and A2. Both use all data points for training, but A1 trains each ensemble on all data points using the labels computed in that ensemble member, and A2 uses the consensus labels. At inference, both use consensus labels. The significant drop in accuracy in both settings demonstrates that the strong performance of SPC is not just due to the application of an ensemble to existing methods, but rather to the novel method of label selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Studies</head><p>It is interesting to observe that A1 performs worse than A2 on MNIST and USPS. Combining approximations in an ensemble has long been observed to give higher expected accuracy ( <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b3">4]</ref>), so the training targets would be more accurate in A1 than in A2. We hypothesize that the reason that this fails to translate to improved clustering is a reduction in ensemble variance. On MNIST   <ref type="table">Table 3</ref>: Ablation studies on the size of the ensemble. and USPS, high accuracy across the ensemble means high agreement. Giving the same training signal for every data point reduces variance further. Especially, compared with A2, the reduction is greatest on incorrectly clustered data points, because most incorrectly clustered data points are non-agreed points, and as argued in <ref type="bibr" target="#b22">[23]</ref>, high ensemble variance in the errors is important for performance.</p><p>A4 clusters in the latent space of one untrained encoder and then pseudo-label trains (essentially the method in <ref type="bibr" target="#b5">[6]</ref>). It performs significantly worse than SPC, showing the value of the decoder, and of SPC's label selection technique.</p><p>A3 omits the ensemble entirely. Comparing with A2 again shows that the ensemble itself only produces a small improvement. Alongside SPC's label selection method, the improvement is much greater.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ensemble Size</head><p>The number of autoencoders in the ensemble, K in the terminology of Section 3.1, is a hyperparameter. We add the concatenation of all latent spaces as an additional element. <ref type="table">Table 3</ref> shows the performance for smaller ensemble sizes. In MNIST and USPS, where the variance is reasonably small, there is a discernible trend of the performance increasing with K, then plateauing and starting to decrease. For FashionMNIST, where the variance is higher, the pattern is less clear. For all three datasets, however, we can see a significant difference between an ensemble of size two and an ensemble of size one (i.e., no ensemble). We hypothesize that the decrease for K = 20, 25 is due to a decrease in the number of agreed points, and so fewer pseudo-labels to train the encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper has presented a deep clustering model, called selective pseudo-label clustering (SPC). SPC employs pseudo-label training, which alternates between clustering features extracted by a DNN, and treating these clusters as labels to train the DNN. We have improved this framework with a novel technique for preventing the DNN from learning noise. The method is formally sound and achieves a state-of-the-art performance on three popular image clustering datasets. Ablation studies have demonstrated that the high accuracy is not merely the result of applying an ensemble to existing techniques, but rather is due to SPC's novel filtering method. Future work includes the application to other clustering domains, different from images, and an investigation of how SPC combines with existing deep clustering techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Appendix A: Full Proofs</head><p>This appendix contains the full proofs of the results in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">More Accurate Pseudo-Labels Supplement</head><p>The only part omitted from the argument in the main paper is a proof for the claim about the entropy of the random variable X. This is supplied by the following proposition. Proposition 1. Given a categorical random variable X of the form</p><formula xml:id="formula_10">p(X = c 0 ) = t ?c = c 0 , p(X = c) = 1 ? t C ? 1 ,</formula><p>for some 1/C ? t ? 1, the entropy H(X) is a strictly decreasing function of t.</p><p>Proof.</p><formula xml:id="formula_11">H(X) = ? t log t ? (1 ? t) log 1 ? t C ? 1 d(H(X)) dt = ? log t ? 1 ? 1 1 ? t + log 1 C ? 1 + + t 1 ? t + log 1 ? t = ? 2 ? log t ? log C ? 1 + log t ? 1 = ? 2 ? log t 1 ? t (C ? 1) .</formula><p>The argument to the log is clearly an increasing function of t for t &gt; 1. Therefore, for 1/C ? t &lt; 1, it is lower-bounded by setting t = 1/C. This gives</p><formula xml:id="formula_12">d(H(X)) dt ? ?2 ? log 1/C 1 ? 1/C (C ? 1) &lt; ? log 1/C 1 ? 1/C (C ? 1) = ? log 1 = 0 .</formula><p>The derivative is always strictly negative with respect to t, so, as a function of t, H(X) is always strictly decreasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Lemma 1 Supplement</head><p>The following is a proof for the claim that u same &lt; u diff , as stated in Section 4.</p><p>Decomposing u same according to the definition of variance (as the expectation of the square minus the square of the expectation) gives</p><formula xml:id="formula_13">E x,x ?T [w T (x ? x ) ? ?w (||x|| 2 ?||x || 2 )] 2 + Var(w T (x ? x ) ? ?w (||x|| 2 ?||x || 2 )) .</formula><p>The expectation term equals 0, as</p><formula xml:id="formula_14">w T E x,x ?T [(x ? x )] ? ?w E x,x ?T [(||x|| 2 ?||x || 2 )] = (wE[T ] ? E[T ]) ? ?w (E[||T || 2 ] ? E[||T || 2 ]) = 0 .</formula><p>By symmetry, we can replace covariances involving x with the same involving x. The remaining term can then be rearranged to give</p><formula xml:id="formula_15">u same = 2Var(w T x ? ?w ||x|| 2 ) = 2w T Cov(T )w + 2?w V ar(||x|| 2 ) ? 4Cov(w T x, ?w ||x|| 2 ) . Now rewrite u diff . Decomposing as above gives E x,x ?T [w T (x ? x ) ? ?w (||x ? x || 2 )] 2 + Var(w T (x ? x ) ? ?w (||x ? x || 2 )) ,</formula><p>and here the expectation term does not equal 0:</p><formula xml:id="formula_16">(w T E x,x ?T [(x ? x )] ? ?w E x,x ?T [(||x ? x || 2 )]) 2 = (?w ) 2 E x,x ?T [||x ? x || 2 ] 2 .</formula><p>The variance term can be expanded to give:</p><formula xml:id="formula_17">Var(w T (x ? x ) ? ?w (||x ? x || 2 )) = 2w T Cov(T )w + 2?w Var(||x ? x || 2 )? 4Cov(w T x, ?w ||x ? x || 2 ) .</formula><p>By comparing terms, we can see that this expression is at least as large as u same . First, consider the covariance terms.</p><formula xml:id="formula_18">Claim. Cov(w T x, ?w ||x ? x || 2 ) = Cov(w T x, ?w ||x|| 2 ). Cov(w T x, ?w ||x ? x || 2 ) = E[w T x?w ||x ? x || 2 ] ? E[w T x]E[?w ||x ? x || 2 ] = ?w E[w T x||x ? x || 2 ] ? 0E[?w ||x ? x || 2 ] = ?w E[w T x||x ? x || 2 ] = ?w E[w T x k x 2 ? 2xx + x 2 ] = ?w k E[w T xx 2 k ] ? 2E[w T xx k ]E[x ] + E[w T x]E[x 2 ] = ?w k E[w T xx 2 k ] ? 2E[w T xx k ] 0 + 0E[x 2 ] = ?w k E[w T xx 2 k ] = ?w E[w T x k x 2 k ] = ?w E[w T x||x|| 2 ] = ?w E[w T x||x|| 2 ] ? 0E[?w ||x|| 2 ] = E[w T x?w ||x|| 2 ] ? E[w T x]E[?w ||x|| 2 ] = Cov(w T x, ?w ||x|| 2 ) .</formula><p>So, we see the covariance terms are equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Next, compare the second variance terms</head><p>Claim. Var(||x ? x || 2 ) ? Var(||x|| 2 ). Assuming that the data are not all identical, this implies that u diff is strictly greater than u same .</p><formula xml:id="formula_19">Var(||x ? x || 2 ) =Var nz k=0 (x) 2 k + (x ) 2 k ? 2(x) k (x ) k = Var</formula><formula xml:id="formula_20">u diff ? u same = (?w ) 2 E x,x ?T [||x ? x || 2 ] 2 + 2w T Cov(T )w+ + 2?w Var(||x ? x || 2 ) ? 4 Cov(w T x, ?w ||x ? x || 2 )? ? ((2w T Cov(T )w + 2?w V ar(||x|| 2 ) ? 4 Cov(w T x, ?w ||x|| 2 ))) = (?w ) 2 E x,x ?T [||x ? x || 2 ] 2 + + 2?w Var(||x ? x || 2 ) ? Var(||x|| 2 ) ? ? 4 Cov(w T x, ?w ||x ? x || 2 ) ? Cov(w T x, ?w ||x|| 2 ) = (?w ) 2 E x,x ?T [||x ? x || 2 ] 2 + + 2?w Var(||x ? x || 2 ) ? Var(||x|| 2 ) ? (?w ) 2 E x,x ?T [||x ? x || 2 ] 2 &gt; 0 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Lemma 2 Supplement</head><p>The following is the complete proof of Lemma 2, which was omitted from the main paper.</p><formula xml:id="formula_21">Proof. v diff ? v same = E[(w T (x ? z) ? w (x ? x )(x ? z)) 2 ]? ? E[(w T (x ? z) ? w (x + x )(x ? z)) 2 ] = E[(w T (x ? z) ? w (x ? x ) T (x ? z)) 2 ? ? (w T (x ? z) ? w (x + x ) T (x ? z)) 2 ] = E[(w T (x ? z) ? w (x ? x ) T (x ? z)+ + w T (x ? z) ? w (x + x ) T (x ? z)) (w T (x ? z) ? w (x ? x ) T (x ? z)? ? w T (x ? z) ? w (x + x ) T (x ? z))] = E[(2w T (x ? z) ? w (x ? z) T (x ? x + x + x )) (?w (x ? z) T (x ? x ? x ? x ))] = E[(2w T (x ? z) ? 2w (x ? z) T (x))(2w (x ? z) T (x ))] = 2E[(w T (x ? z) ? w (x ? z) T (x))w (x ? z) T ]E[x ] = 2E[(w T (x ? z) ? w (x ? z)(x))w (x ? z) T ] 0 = 0 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Lemma 3 Supplement</head><p>The following is the complete proof of Lemma 3, which was omitted from the main paper.</p><p>Proof. </p><formula xml:id="formula_22">Var(T ) = 1 2 E x,x ?T [(x ? x ) 2 ] = 1 2 ( E x,</formula><formula xml:id="formula_23">1 C + r C ? 1 C ? s = C ? 1 2C r ? 2C ? 1 2C s .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Theorem 5 Supplement</head><p>The following is a more detailed version of the argument given in the main paper. If y(x) = y(x ), then Lemma 2 means that the expected distance of the encodings of x and x to any data point from another cluster is unchanged by whether the update was from points with the same or with different labels. Similarly, the distance between any two other points is unchanged by whether the update was from points with the same or with different labels. This establishes that r T = r F . As for the intra-cluster variance, it is smaller after the update with the same labels than with different labels. Lemma 1 shows that the expected distance between the encodings of the two points themselves is smaller if the labels were the same, and the same argument as above shows that all other expected distances within clusters are unchanged.</p><p>If y(x) = y(x ), then Lemma 2 means that the expected distance of the encodings of x and any data point from the same cluster is unchanged by whether the update was from points with the same or with different labels (and the same for x ). Similarly, the distance between any two other points is unchanged by whether the update was from points with the same or with different labels. This establishes that s T = s F . As for the inter -cluster variance, it is larger after the update with the same labels than with different labels. Lemma 1 shows that the    expected distance between the encodings of the two points themselves is larger if the labels were different, and the same argument as above shows that all other expected distances within clusters are unchanged.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The complete SPC method. (1) Pretrain autoencoders. (2) Perform multiple clusterings independently. (3) Identify agreed points as those that receive the same label in all ensemble members. (4) Perform pseudo-label training on agreed points and autoencoder training on unagreed points. Steps (2)-(4) are looped until the number of agreed points stops increasing. relevant features. Selective pseudo-label clustering (SPC) addresses this problem by selecting only the most confident pseudo-labels for training, using the four steps shown in Fig. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Iterations of (2)-(4) in Figure 1 on MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 1 .Theorem 1 .</head><label>11</label><figDesc>Let? : D ? {0, . . . , C ? 1} be the pseudo-label assignment function. For d i , d j ? D, the pseudo-labels are pairwise correct iff y(x i ) = y(x j ) and?(x i ) =?(x j ), or y(x i ) = y(x j ) and?(x i ) =?(x j ). Let d T and d F denote, respectively, the value of d after a g.d. step from two pairwise correct labels and from two pairwise incorrect labels, and let x, x ? D as before. Then, d T &gt; d F .Proof. Let r T , s T , and r F , s F be, respectively, the values of r and s after a g.d. step from two pairwise correct labels and from two pairwise incorrect labels. Consider two cases. If y(x) = y(x ), then r T = r F , by Lemma 2, and s T &lt; s F , by Lemmas 1 and 2, so by Lemma 3, d T &gt; d F . If y(x) = y(x ), then s T = s F , by Lemma 2, and r T &gt; r F , by Lemmas 1 and 2, so again d T &gt; d F , by Lemma 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>= 2 (</head><label>2</label><figDesc>Var(||x|| 2 ) + Var(x T x )) ? Var(||x|| 2 ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>x ?T [(x ? x ) 2 |y(x) = y(x )]P (y(x) = y(x ))+ E x,x ?T [(x ? x ) 2 |y(x) = y(x )]P (y(x) = y(x )) = 1 2 (sP (y(x) = y(x )) + rP (y(x) = y(x ))Noting that s = 2E[Var(T |C)], and using Eve's law, we have d = Var(T ) ? s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>have shown empirically that adding data augmentation to deep image clustering models improves performance in virtually all cases. Here, its effect is especially evident on the smaller dataset, USPS. For</figDesc><table><row><cell></cell><cell></cell><cell>MNIST</cell><cell></cell><cell>USPS</cell><cell cols="2">FashionMNIST</cell></row><row><cell></cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell></row><row><cell>SPC-best</cell><cell>99.21</cell><cell>97.49</cell><cell>98.44</cell><cell>95.44</cell><cell>67.94</cell><cell>73.48</cell></row><row><cell>SPC</cell><cell cols="6">99.03 (.1) 97.04 (.25) 98.40 (.94) 95.42 (.15) 65.58 (2.09) 72.09 (1.28)</cell></row><row><cell>SPC-GMM</cell><cell cols="6">99.05 (.2) 97.10 (.47) 98.18 (.14) 94.93 (.32) 65.03 (1.54) 69.51 (1.21)</cell></row><row><cell>DynAE [34] ?</cell><cell>98.7</cell><cell>96.4</cell><cell>98.1</cell><cell>94.8</cell><cell>59.1</cell><cell>64.2</cell></row><row><cell>ADC [33] ?</cell><cell>98.6</cell><cell>96.1</cell><cell>98.1</cell><cell>94.8</cell><cell>58.6</cell><cell>66.2</cell></row><row><cell>DDC [39] ?</cell><cell>98.5</cell><cell>96.1</cell><cell>97.0</cell><cell>95.3</cell><cell>57.0</cell><cell>63.2</cell></row><row><cell>n2d [30]</cell><cell>97.9</cell><cell>94.2</cell><cell>95.8</cell><cell>90.0</cell><cell>67.2</cell><cell>68.4</cell></row><row><cell>DLS [11]</cell><cell>97.5</cell><cell>93.6</cell><cell>-</cell><cell>-</cell><cell>69.3</cell><cell>66.9</cell></row><row><cell>JULE [45]</cell><cell>96.4</cell><cell>91.3</cell><cell>95.0</cell><cell>91.3</cell><cell>56.3*</cell><cell>60.8*</cell></row><row><cell>DEPICT [14]</cell><cell>96.5</cell><cell>91.7</cell><cell>96.4</cell><cell>92.7</cell><cell>39.2*</cell><cell>39.2*</cell></row><row><cell>DMSC [1] ?</cell><cell>95.15</cell><cell>92.09</cell><cell>95.15</cell><cell>92.09</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ClusterGAN [35] 95</cell><cell>89</cell><cell>-</cell><cell>-</cell><cell>63</cell><cell>64</cell></row><row><cell>VADE [21]</cell><cell>94.5</cell><cell>87.6*</cell><cell>56.6*</cell><cell>51.2*</cell><cell>57.8*</cell><cell>63.0*</cell></row><row><cell>IDEC [16]</cell><cell>88.06</cell><cell>86.72</cell><cell>76.05</cell><cell>78.46</cell><cell>52.9*</cell><cell>55.7*</cell></row><row><cell>CKM [13]</cell><cell>85.4</cell><cell>81.4</cell><cell>72.1</cell><cell>70.7</cell><cell>-</cell><cell>-</cell></row><row><cell>DEC [43]</cell><cell>84.3</cell><cell>83.4*</cell><cell>76.2*</cell><cell>76.7*</cell><cell>51.8*</cell><cell>54.6*</cell></row><row><cell>DCN [44]</cell><cell>83</cell><cell>81</cell><cell>68.8*</cell><cell>68.3*</cell><cell>50.1*</cell><cell>55.8*</cell></row><row><cell></cell><cell cols="3">? =uses data augmentation</cell><cell cols="2">*=results taken from [34]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation results, central tendency for three runs.</figDesc><table><row><cell>A1=w/o label filter-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Zero One Two Three Four Five Six Seven Eight Nine</figDesc><table><row><cell>HDBSCAN</cell><cell>6923 7878 6979 7095 6802 6290 6911 7384 6776 6962</cell></row><row><cell>GMM</cell><cell>6942 6958 6791 7885 6976 7096 7350 6294 6906 6802</cell></row><row><cell cols="2">Ground Truth 7000 7000 7000 7000 7000 7000 7000 7000 7000 7000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Sizes of predicted clusters for MNIST.</figDesc><table><row><cell></cell><cell>Zero One Two Three Four Five Six Seven Eight Nine</cell></row><row><cell>HDBSCAN</cell><cell>1565 1272 933 819 856 706 833 787 693 834</cell></row><row><cell>GMM</cell><cell>1271 834 785 833 690 835 862 930 699 1559</cell></row><row><cell cols="2">Ground Truth 1553 1269 929 824 852 716 834 792 708 821</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Sizes of predicted clusters for USPS.</figDesc><table><row><cell></cell><cell cols="4">Top Trouser Pullover Dress Coat Sandal Shirt Sneaker Bag Boot</cell></row><row><cell>HDBSCAN</cell><cell>7411 6755</cell><cell>56</cell><cell>6591 21333 6046 3173 5666</cell><cell>3711 9258</cell></row><row><cell>GMM</cell><cell>6700 3111</cell><cell cols="2">16379 6807 6753 9127 7389 4482</cell><cell>8814 438</cell></row><row><cell cols="2">Ground Truth 7000 7000</cell><cell>7000</cell><cell>7000 7000 7000 7000 7000</cell><cell>7000 7000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Sizes of predicted clusters for FashionMNIST.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported by the Alan Turing Institute under the UK EPSRC grant EP/N510129/1 and by the AXA Research Fund. We also acknowledge the use of the EPSRC-funded Tier 2 facility JADE (EP/P020275/1) and GPU computing support by Scan Computers International Ltd.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Appendix C: Extended Results</head><p>The results in the main paper report the central tendency of five different training runs for each dataset. Tables 4, 5, and 6 show the sizes of the clusters predicted by SPC for one randomly selected run out of these five. On MNIST and USPS, where the accuracy of SPC is &gt; 98%, the predicted sizes are close to the true sizes. On FashionMNIST, where the accuracy is ? 65%, there is a much greater variance. This accounts for the discrepancy in ACC and NMI for FashionMNIST. Most of the errors are put into one large cluster, specifically the cluster that was aligned to 'coat' is over three times larger than it should be. This hurts accuracy more than NMI, because the incorrect data points in the 'coat' cluster count for zero when calculating the accuracy, but they are not randomly distributed among the other classes, so the conditional entropy of a data point that was mis-clustered as a coat is &lt; log(10). Actually, most of the mistakes in the 'coat' cluster are pullovers or shirts, and almost none of them are, for examples, boots or tops. Comparing the cluster sizes for SPC-HDBSCAN and SPC-GMM also accounts for the differences across ACC and NMI between these two settings on FashionMNIST: SPC-GMM produces more uniformly-sized clusters, so the difference between ACC and NMI is smaller.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep multimodal subspace clustering networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abavisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1601" to="1614" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">3731</biblScope>
			<biblScope unit="page" from="34" to="37" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cluster ensembles: A survey of approaches with recent extensions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boongoen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Iam-On</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science Review</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep adaptive image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5879" to="5887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Combining forecasts: A review and annotated bibliography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Clemen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="559" to="583" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inverting the generator of a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Bharath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1967" to="1974" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc.: Series B (Methodol.)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Clustering by directly disentangling latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05210</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazzone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07068</idno>
		<title level="m">CAN: Creative adversarial networks, generating art by learning about styles and deviating from style norms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep clustering with concrete k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4252" to="4256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep clustering via joint convolutional autoencoder embedding and relative entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ghasedi Dizaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved deep embedded clustering with local structure preservation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1753" to="1759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep embedded clustering with data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conference on Machine Learning</title>
		<meeting>Asian Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="550" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep embedding network for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05148</idno>
		<title level="m">Variational deep embedding: An unsupervised and generative approach to clustering</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On combining classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sub-GAN: An unsupervised generative model via subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Precise recovery of latent vectors from generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04782</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Least square quantization in PCM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">N2d:(Not too) deep clustering via clustering the local manifold of an autoencoded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcconville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Santos-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Piechocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Craddock</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05968</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">HDBSCAN: Hierarchical density based clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Astels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">205</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">UMAP: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mrabah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bouguessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ksantini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11832</idno>
		<title level="m">Adversarial deep embedded clustering: on a better trade-off between feature randomness and feature drift</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep clustering with a dynamic autoencoder: From reconstruction towards centroids construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mrabah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ksantini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lachiri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07752</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Asnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03627</idno>
		<title level="m">ClusterGAN: Latent space clustering in generative adversarial networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An empirical evaluation of bagging and boosting for artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Maclin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICNN</title>
		<meeting>ICNN</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1401" to="1405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Chaitin-Kolmogorov complexity and generalization in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pearlmutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="925" to="931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improving regression estimation: Averaging methods for variance reduction with extensions to general convex measure optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Perrone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep density-based image clustering. Knowledge-Based Systems p</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">105841</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">WeGAN: Deep image hashing with weighted generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimed</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Data mining: practical machine learning tools and techniques with Java implementations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigmod Record</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="77" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<title level="m">Fashion-MNIST: A novel image dataset for benchmarking machine learning algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards k-means-friendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3861" to="3870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Developing population codes by minimizing description length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08861</idno>
		<title level="m">Loss functions for neural networks for image processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00335</idno>
		<title level="m">GAN-EM: GAN based EM learning framework</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep adversarial subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A survey on unsupervised outlier detection in high-dimensional numerical data. Statistical Analysis and Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The ASA Data Science Journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="363" to="387" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
