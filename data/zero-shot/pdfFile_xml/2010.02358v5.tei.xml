<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VisualWordGrid: Information Extraction From Scanned Documents Using A Multimodal Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-07-04">4 Jul 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">DataLab Groupe</orgName>
								<orgName type="institution">Credit Agricole S.A</orgName>
								<address>
									<settlement>Montrouge</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VisualWordGrid: Information Extraction From Scanned Documents Using A Multimodal Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-04">4 Jul 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Information Extraction ? Multimodal ? Scanned document analysis ? WordGrid ? Chargrid</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>KERROUMI Mohamed [0000?0002?2513?8837] , SAYEM Othmane [0000?0003?0311?9384] , and SHABOU Aymen [0000?0001?8933?7053]</p><p>Abstract. We introduce a novel approach for scanned document representation to perform field extraction. It allows the simultaneous encoding of the textual, visual and layout information in a 3-axis tensor used as an input to a segmentation model. We improve the recent Chargrid and Wordgrid [10] models in several ways, first by taking into account the visual modality, then by boosting its robustness in regards to small datasets while keeping the inference time low. Our approach is tested on public and private document-image datasets, showing higher performances compared to the recent state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In a vast majority of business workflows, information extraction from templatic documents such as invoices, receipts, tax notices, etc. is largely a manual task. Automating this process has become a necessity as the number of client documents increases exponentially. Most industrial automated systems today have a rule-based approach to documents with a certain structure, and can be associated with a finite number of templates. However, documents often have a variety of layouts and structures. In order to understand the semantic content of these documents, the human brain uses the document's layout, as well as the textual and visual information available in its contents.</p><p>The challenge is to overcome rule-based systems, and to design end-to-end models that automatically understand both the visual structure of the document and the textual information it contains. For instance, in a document like an invoice, the total amount to pay is associated with a numerical value that appears frequently near terms such as total, total to pay and net to pay, and also after fields like total before taxes, taxes, cost, etc. Thus, as Katti et al. showed with Chargrid <ref type="bibr" target="#b10">[10]</ref>, combining both positional and textual information, was proven to be efficient for this task.</p><p>On the other hand, the visual content of a document was proven to improve model accuracy for document classification when combined with textual information <ref type="bibr" target="#b0">[1]</ref>.</p><p>In this article, we prove that adding visual information to textual and positional features improves the performance of the information extraction task. The improvement is more significant when dealing with documents with rich visual characteristics such as tables, logos, signatures, etc. We extend the work of Katti et al. <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b5">6]</ref> with a new approach (called VisualWordGrid) that combines the aforementioned modalities with two different strategies to achieve the best results in the task of information extraction from image documents.</p><p>The present paper is organized as follows: Section 2 presents related work for information extraction. Section 3 describes the datasets we used for evaluation. Section 4 introduces the proposed approach. Section 5 discusses the obtained results. Finally, Section 6 provides our conclusions regarding the new method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Interest in solving the information extraction task has grown in fields where machine learning is used, from Natural Language Processing (NLP) to Computer Vision (CV) domains. Depending on the representation of the document, different methods are applied to achieve the best possible performance.</p><p>For instance, NLP methods transform each document to a 1D sequence of tokens, before applying named entity recognition models to recognize the class of each word <ref type="bibr" target="#b11">[11]</ref>. These methods can be successful when applied to documents with simple layout, such as books or articles. However, for documents like invoices, receipts or tax notices, where visual objects such as tables and grids are more common, these textual methods are less efficient. In such cases, structural and visual information are essential to achieve good performance.</p><p>Alternatively, computer vision methods can also be very efficient for this task, specifically for documents like Identity Cards which are very rich with visual features. In these approaches, only the image of the scanned document is given as an input. Object detection and semantic segmentation are among the most used techniques for field extraction <ref type="bibr" target="#b7">[8]</ref> from these documents. The OCR engine is applied at the end of the pipeline on image crops to extract the text of detected fields. These approaches can be very useful when dealing with documents with normalized templates. For documents with various templates and layouts, these models do not perform well.</p><p>Most recent studies try to exploit the textual and the layout aspects of the document by combining both NLP and CV methods in the extraction task. In the Chargrid <ref type="bibr" target="#b10">[10]</ref> or BertGrid <ref type="bibr" target="#b5">[6]</ref> papers, a document is presented as a 2D grid of characters (or words) embeddings. The idea behind this representation is to preserve structural and positional information, while exploiting textual information contained in the document. Both papers reported significant increase in the performance of information extraction task compared to purely textual approaches. In a more general approach, Zhang et al. <ref type="bibr" target="#b20">[20]</ref> recently proposed TRIE, an end-to-end text reading and information extraction approach, where a module of text reading is introduced. The model mixes textual and visual features of text reading and information extraction to reinforce both tasks mutually, in a multitask approach.</p><p>More recently, Yiheng et al. proposed the LayoutLM <ref type="bibr" target="#b16">[16]</ref>, a new method to leverage the visual information of a document in the learning process. Instead of having the text embedding of each token as the sole input, relative position of tokens in the image and the corresponding feature map of the image crop within the document were added too. Inspired by the BERT model <ref type="bibr" target="#b6">[7]</ref>, Yiheng et al. used scanned document classification task as a supervised pre-training step for LayoutLM to learn the interactions between text and layout information. Then they enforced this learning by a semi-supervised pre-training using Masked Visual-Lanquage Model (MVLM) as a multi-task learning. The dataset used for pre-training contains 11M documents and the pre-training took 170 hours on 8 GPUs. Hence this approach needs large computational resources.</p><p>Other works, focused on solving the document semantic segmentation task, introduced the idea of simultaneously encoding visual, textual and structural modalities into a tensor representation. For instance, Yang et al. <ref type="bibr" target="#b19">[19]</ref> proposed a multimodal approach to extract the semantic structure of documents using a fully convolutional neural network. Barman et al. <ref type="bibr" target="#b1">[2]</ref> proposed a multimodal segmentation model that combines both visual and textual features to extract semantic structure of historical documents.</p><p>Compared to these related works, we propose in this paper two multimodal document representation strategies suited to the information extraction task. The first one is simpler, yet highly effective compared to state-of-the-art multimodal approaches (while improving the extraction scores). The second one is similar to the related works <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b1">2]</ref> but slightly improved and adapted to the field extraction task (i.e. extracting small text regions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>In order to evaluate our work, we will show our experiments on two datasets showing interesting visual structures that help improving the information extraction task using multimodal strategies. <ref type="bibr" target="#b13">[13]</ref> It is a public dataset that was released to help improve and evaluate layout analysis techniques on scanned invoice documents. It contains 520 invoice images ( <ref type="figure" target="#fig_0">Fig.1)</ref> with their corresponding OCR files containing the extracted text, along with XML files containing the ground-truth bounding boxes of all the semantic fields. Each word in a given document of the dataset is classified into a semantic region described by a box. Among the 6 available fields, we will focus on extracting 4 of them: Receiver, Supplier, Invoice info, Total. Tax Notice Dataset It is an in-house private dataset. It contains 3455 tax notices since 2015 <ref type="figure" target="#fig_1">(Fig.2</ref>). The documents are in French and their templates changed over the years. Hence template matching could not be used as an approach for information extraction. The dataset was annotated by manually putting bounding boxes around fields of interest. There are mainly 6 entities to extract from each document: Year, Name, Address, Type of Notice, Reference Tax Income, Family Quotient. The dataset contains first and second pages from tax notices, as some fields can appear on both pages, depending on the issue date. Moreover, a single page doesn't necessarily contain all fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RVL-CDIP Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>In this section, we introduce the VisualWordGrid approach, a new 2D representation of documents that extends the Chargrid philosophy by adding the visual aspect of the document to the textual and layout ones. We define two main models that differ on document representation and model architecture : VisualWordGrid-pad and VisualWordGrid-2encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Document representation</head><p>Our main idea is adding the visual information of the image to the textual and structural data used in the WordGrid representation. The most direct way for doing so is by adding the corresponding RGB channels to each pixel embedding. While this concatenation has no impact on background pixels, it adds a large amount of noise to pre-trained word embeddings. The main challenge here is to adapt the concatenation method to preserve textual embeddings, while adding the background visual information. Our representations of documents extend <ref type="bibr" target="#b10">[10]</ref> using two strategies as follows.</p><p>Using an OCR, each document can be represented as a set of words and their corresponding bounding boxes. The textual and layout information of each document can be represented in D = {(t k , b k )|k = 1, ..., n}, with t k the k-th token in the text of the document and b k = (x k , y k , w k , h k ) its corresponding bounding box in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VisualWordGrid-pad</head><p>Our first model representation of the document is defined as follows :</p><formula xml:id="formula_0">W ij = (e d (t k ), 0, 0, 0 if ?k such as (i, j) ? b k (0 d , R ij , G ij , B ij otherwise (1) (i, j) ? b k ?? x k ? i ? x k + w k ? y k ? j ? y k + h k</formula><p>where d is the embedding dimension, e d is the word's embedding function, 0 d denotes an all-zero vector of size d, and R ij , G ij , B ij the RGB channels of the (i, j) th pixel in the raw document's image. In other words, for each point in the document's image, if this point is included in a word's bounding box b k = (x k , y k , w k , h k ), the vector representing this point is the word's embedding padded by 0 3 . Thus, by setting the RGB channels to 0 3 , we drop the visual information related to this point. However if the point is not included in any word's bounding box, the vector representing this point is the concatenation of 0 d and the RGB channels of this point. In this case, we keep the visual information. Hence, the visual, textual and layout information of the document are encoded simultaneously in a 3-axis tensor of shape (H, W, d + 3) as shown in <ref type="figure" target="#fig_2">Fig.3</ref>, while preserving their original information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VisualWordGrid-2encoders</head><p>Our second model representation is similar to the CharGrid-Hybrid approach presented in <ref type="bibr" target="#b10">[10]</ref>. Instead of encoding the document on the character level using a one-hot encoding, we encode the document on the word level using Word2Vec <ref type="bibr" target="#b12">[12]</ref> or Fasttext <ref type="bibr" target="#b3">[4]</ref> embeddings. Hence, for each document we have two inputs:</p><p>-WordGrid encoding: This input encodes the textual and layout information of the document. This approach of encoding is similar to WordGrid presented in <ref type="bibr" target="#b10">[10]</ref>. For words encoding, we use Word2Vec or Fasttext embeddings.</p><formula xml:id="formula_1">W ij = e d (r k ) if ?k such as (i, j) ? b k 0 d otherwise<label>(2)</label></formula><p>-Image: The raw image of the document resized to match the WordGrid encoding dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Architectures</head><p>In this section, we discuss model architectures related to both strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VisualWordGrid-pad</head><p>Once the 2D representation of the document is encoded, we use it to train a neural segmentation model. Unlike chargrid and wordgrid papers, we dropped the bounding box regression block to keep the semantic segmentation block only, since there can be at most one instance of each class in the datasets.</p><p>We use the Unet <ref type="bibr" target="#b14">[14]</ref> as a segmentation model and the ResNet34 [9] as a backbone for the encoder. The weights of the backbone are initialized using transfer learning from a model pre-trained on the ImageNet classification task. These weights are available in the open-source package Segmentation Models <ref type="bibr" target="#b17">[17]</ref>. The UNet component extracts and encodes advanced features of the input grid in a small feature map, and the decoder expands this feature map to recover segmentation maps of the same size as the input grid, and thus generates the predicted label masks. We used a softmax activation function for the final layer of the decoder. The shape of the decoder's output is (H, W, K + 1), where K is the number of fields of interest, and 1 is the background class.</p><p>In the inference step, we iterate over the bounding box of each word in the OCR output, then attribute a single class to the most dominant category pixelwise, to get the final prediction value for the corresponding field. (see <ref type="figure">Fig.4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VisualWordGrid-2encoders</head><p>This model is composed of 2 encoders. The first one is the classic WordGrid encoder <ref type="bibr" target="#b1">(2)</ref>, and the second one is the raw image encoder. We keep one decoder, and for each block in the decoder, we concatenate on the skip connections from both encoders (see <ref type="figure">Fig.4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>In this section, we provide implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word embedding function</head><p>We use different word embedding functions depending on the dataset. For RVL-CDIP, we propose Word2Vec pretrained embeddings on the Wikipedia corpus, publicly available thanks to Wiki2Vec <ref type="bibr" target="#b18">[18]</ref>. The choice of Wiki2Vec is due to the good quality of the OCR files, since words are correctly recognized and most of them have their related embeddings. Unlike RVL-CDIP dataset, OCR outputs of our Tax Notice dataset are noisy, due to the quality of customer documents scans. We observed frequent misspelling errors in the Tesseract 4.1 <ref type="bibr" target="#b15">[15]</ref> ouputs. Our experiments show that a custom FastText embedding trained on the corpus of the Tax Notice dataset is the best words embedding function to handle the noise. As explained in <ref type="bibr" target="#b3">[4]</ref>, Word embedding using this approach is the sum of n-grams subword embeddings. Hence, even in case of a misspelled or dropped character in the token, its embedding wouldn't differ too much from the embedding of the original word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss function</head><p>The loss function we use for training is the sum of the cross entropy loss for segmentation (L seg ) and the Intersection over Union loss (L IoU ).</p><formula xml:id="formula_2">Loss = L seg + L IoU<label>(3)</label></formula><p>-Cross Entropy Loss: The cross entropy loss penalizes pixel mis-classification. The goal is to assign each pixel to its ground truth field.</p><formula xml:id="formula_3">L seg = x?? ?log pl (x) (x)<label>(4)</label></formula><p>withl(x) the ground truth label of the point x.</p><p>-Intersection over Union Loss: This loss function is often used to train neural networks for a segmentation task. It's a differentiable approximation of the IoU metric and is the most indicative of success for segmentation tasks as explained in <ref type="bibr" target="#b2">[3]</ref>. In our case, it significantly increases performances of the model compared to a model trained only with the cross entropy. The IoU metric is defined as :</p><formula xml:id="formula_4">IoU = I U = |T ? P | |T ? P | = |T P | |T + P ? (T P )| (5) L IoU = 1 ? IoU<label>(6)</label></formula><p>where T is the true labels of the image pixels and P is their prediction labels. We also use the IoU metric to monitor the training of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head><p>To evaluate the performance of the different models, we used two metrics:</p><p>-Word Accuracy Rate (WAR): It's the same metric as the one used in <ref type="bibr" target="#b10">[10]</ref>. It's similar to the Levenshtein distance computed on the token level instead of the character level. It counts the number of substitutions, insertions and deletions between the ground-truth and the predicted instances. This metric is usually used to evaluate speech-to-text models. The WAR formula is as follows:</p><formula xml:id="formula_5">W AR = 1 ? #[insertions] + #[deletions] + #[substitutions] N<label>(7)</label></formula><p>where, N is the total number of tokens in the ground truth instance for a specific field. The WAR of a document is the average on all fields.</p><p>-Field Accuracy Rate (FAR): This metric evaluates the performance of the model in extracting complete and exact field information. A field is the set of words of a same entity. This metric counts the number of exact match between the ground-truth and the predicted instances. It is useful in industrial applications, as we need to evaluate the number of cases where the model succeeds to extract the whole field correctly, for control purposes for example. The FAR formula is as follows:</p><formula xml:id="formula_6">F AR = #[Fields exact Match] N f ields<label>(8)</label></formula><p>where, #[Fields exact Match] is the number of fields correctly extracted from the document with an exact match between the ground-truth and the predicted words values, and N f ields is the total number of fields. We note that for any processed document in the evaluation set, with no target field in the ground truth, we attribute an empty string to the value of each field, so false positives are penalized too.</p><p>In the next section, we will report for each model the average WAR and FAR metrics on the documents in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we compare our approaches (VisualWordGrid-pad, VisualWordGrid-2encoders) to two others, on both datasets. We report their average scores (F AR, W AR) and inference time (Inf erenceT ime) on CPU for a single document, and their number of trainable parameters. The two competing approaches are the following ones:</p><p>-Layout Approach: This approach is a layout encoding only. Instead of using word embedding or pixel RGB channels to encode a specific document as in <ref type="formula">(1)</ref>, it uses a simpler 2D encoding suited to a segmentation task, i.e.:</p><formula xml:id="formula_7">W ij = (1, 1, 1 if ?k such as (i, j) ? b k (0, 0, 0 otherwise<label>(9)</label></formula><p>Then, we use this type of document encoding <ref type="figure">(Fig. 5)</ref> as input to train an information extraction model using the proposed architecture, loss and model hyper-parameters. -WordGrid: This approach is very similar to BertGrid <ref type="bibr" target="#b5">[6]</ref>. Instead of using a Bert <ref type="bibr" target="#b6">[7]</ref> model to generate contextual embeddings, we use a Word2Vec pretrained embedding for RVL-CDIP dataset, and a custom Fasttext embedding for the Tax Notice dataset. Equation (2) introduces the document encoding formula.</p><p>We keep the same model architecture, loss function and model hyper-parameters as proposed in the VisualWordGrid model. <ref type="figure">Fig. 5</ref>. Invoice sample and its encoding using the Layout approach</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>Since the RVL-CDIP dataset volume is very small, we don't use a classic split of the dataset into training set and validation sets. Instead, we use a k-fold split of the dataset with k = 5. For each experiment, we do 5 tests, each one with a training on 80% of the dataset and the remaining 20% is split equally into validation and test sets. We report the average of the metrics on the 5 tests. This way, the values of the metrics don't depend on the seed of the split, and metrics are a more reliable representation of real model performance.</p><p>For the Tax Notice dataset, we assign 80% of the dataset to training, 15% to validation and 5% to test, on which we report our results. The OCR task to extract textual information was performed using the open source OCR engine Tesseract 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>For all experiments , we use Adam optimizer with lr=0.001 and batch size=8. We use a GPU NVIDIA Quadro RTX 6000 with 24GB GPU memory and the Keras framework <ref type="bibr" target="#b4">[5]</ref> to build and train models. The inference time is measured on Intel Xeon W-2133 CPU (3.60 GHz). The table 1 shows the scores (W AR, F AR) of the different approaches on the RVL-CDIP dataset.</p><p>We clearly see in the table 1 that VisualWordGrid-pad gives the best FAR and WAR scores. Our proposed encoding system improves the WordGrid FAR and WAR by 1 and 7.9 respectively. Moreover, it exploits all the visual, textual and structural content of documents while keeping the inference time and the number of parameters close to the WordGrid ones. Unlike Katti et al. <ref type="bibr" target="#b10">[10]</ref>, we notice an increase in the WAR score when using the two encoders approach (VisualWordGrid-2encoders) to capture the visual and textual aspect of document. It boosts the WordGrid performance, since the WAR goes up by 6.2 . The reasons for this improvement are the modifications we added to make the model more robust in the information extraction task. We used a ResNet34 backbone for the encoder and took advantage of transfer learning to speed up the training of the model. We also changed the cross entropy loss used in <ref type="bibr" target="#b10">[10]</ref> by adding the IoU loss to it. Notice that we used the IoU as a metric for the callback.</p><p>Similarly, we tested the different approaches on the Tax Notice dataset. We reported the results in table 2. The VisualWordGrid-padding approach slightly improves the WordGrid scores, while the VisualWordGrid-2encoders gives the best performance but at the expense of a slightly higher inference time.</p><p>As in several industrial applications, using information extraction requires the smallest inference time. VisualWordGrid-pad would be the best choice. It leverages the visual/textual/layout information of a document while keeping the number of trainable parameters roughly the same as WordGrid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>VisualWordGrid is a simple, yet effective 2D representation of documents that encodes the textual, layout and visual information simultaneously. The gridbased representation includes token embeddings and the image's RGB channels. We can take advantage of these multimodal inputs to perform several document understanding tasks. For the information extraction task, VisualWordGrid shows better results than those of state of the art models on two datasets (the public RVL-CDIP dataset and the private Tax Notice dataset), while keeping model parameters and inference time roughly the same (especially when using the padding strategy). In many fields, this approach is suitable for production.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Invoice example from RVL-CDIP with fields of interest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Tax Notice fake example with fields of interest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>VisualWordGrid encoding of an invoice sample. On the right, the proposed concatenation of a Wordgrid representation and the image of the document. On the left, a zoom of the previous figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Models performances on the RVL-CDIP dataset.</figDesc><table><row><cell>Approach</cell><cell cols="3">F AR W AR Inf erenceT ime #Parameters</cell></row><row><cell>Layout Only</cell><cell>23.0 % 5.4 %</cell><cell>2.14 s</cell><cell>24 439 384</cell></row><row><cell>WordGrid</cell><cell>27.7 % 10.8 %</cell><cell>2.22 s</cell><cell>24 743 673</cell></row><row><cell cols="2">VisualWordGrid-pad 28.7 % 18.7 %</cell><cell>3.77 s</cell><cell>24 753 084</cell></row><row><cell cols="2">VisualWordGrid-2encoders 26.9 % 17.0 %</cell><cell>6.08 s</cell><cell>48 003 004</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Models performances on the Tax Notice dataset.</figDesc><table><row><cell>Approach</cell><cell cols="3">F AR W AR Inf erenceT ime #Parameters</cell></row><row><cell>Layout Only</cell><cell>83.3 % 92.3 %</cell><cell>5.29 s</cell><cell>24 439 674</cell></row><row><cell>WordGrid</cell><cell>83.6 % 92.4 %</cell><cell>5.70 s</cell><cell>24 743 963</cell></row><row><cell>VisualWordGrid-pad</cell><cell>83.9 % 92.9 %</cell><cell>5.92 s</cell><cell>24 753 374</cell></row><row><cell cols="2">VisualWordGrid-2encoders 85.8 % 93.6 %</cell><cell>6.19 s</cell><cell>48 003 294</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Kerroumi et al.Fig. 4. VisualWordGrid pipelines.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multimodal deep networks for text and image-based document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Herold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Slimani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="427" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Combining visual and textual features for semantic segmentation of historical newspapers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Clematide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kaplan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep neural networks with intersection over union loss for binary image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Van Beers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lindstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Okafor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiering</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICPRAM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
	</analytic>
	<monogr>
		<title level="j">Keras</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bertgrid: Contextualized embedding for 2d document representation and understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reisswig</surname></persName>
		</author>
		<idno>abs/1909.04948</idno>
		<ptr target="http://arxiv.org/abs/1909.04948" />
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A table detection method for pdf documents based on convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IAPR Workshop on Document Analysis Systems (DAS)</title>
		<imprint>
			<date type="published" when="2016-04" />
			<biblScope unit="page" from="287" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/DAS.2016.23</idno>
		<ptr target="https://doi.org/10.1109/DAS.2016.23" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Chargrid: Towards understanding 2d documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Katti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reisswig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brarda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>H?hne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Faddoul</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1476/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11-04" />
			<biblScope unit="page" from="4459" to="4469" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
		<ptr target="https://www.aclweb.org/anthology/N16-1030" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Table detection in invoice documents by graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Goldmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Forn?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Llad?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="122" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An overview of the tesseract ocr engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Ninth Int. Conference on Document Analysis and Recognition (ICDAR)</title>
		<meeting>Ninth Int. Conference on Document Analysis and Recognition (ICDAR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="629" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Layoutlm: Pre-training of text and layout for document image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403172</idno>
		<ptr target="http://dx.doi.org/10.1145/3394486.3403172" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yakubovskiy</surname></persName>
		</author>
		<title level="m">Segmentation models</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sakuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Takefuji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
		<title level="m">Wikipedia2vec: An efficient toolkit for learning and visualizing the embeddings of words and entities from wikipedia</title>
		<imprint/>
	</monogr>
	<note>arXiv preprint 1812.06280v3 (2020</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning to extract semantic structure from documents using multimodal fully convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Asente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kraley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Trie: End-to-end text reading and information extraction for document understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
