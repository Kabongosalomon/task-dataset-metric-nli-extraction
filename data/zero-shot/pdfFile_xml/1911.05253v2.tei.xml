<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Where to Focus for Efficient Video Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
							<email>zhengkai.jiang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 4 Horizon Robotics</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 4 Horizon Robotics</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 4 Horizon Robotics</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 4 Horizon Robotics</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiming</forename><surname>Xiang</surname></persName>
							<email>smxiang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhong</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Where to Focus for Efficient Video Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>flow-warping</term>
					<term>learnable spatio-temporal sampling</term>
					<term>spatial correspondences</term>
					<term>temporal relations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transferring existing image-based detectors to the video is non-trivial since the quality of frames is always deteriorated by part occlusion, rare pose, and motion blur. Previous approaches exploit to propagate and aggregate features across video frames by using optical flow-warping. However, directly applying image-level optical flow onto the high-level features might not establish accurate spatial correspondences. Therefore, a novel module called Learnable Spatio-Temporal Sampling (LSTS) has been proposed to learn semantic-level correspondences among adjacent frame features accurately. The sampled locations are first randomly initialized, then updated iteratively to find better spatial correspondences guided by detection supervision progressively. Besides, Sparsely Recursive Feature Updating (SRFU) module and Dense Feature Aggregation (DFA) module are also introduced to model temporal relations and enhance per-frame features, respectively. Without bells and whistles, the proposed method achieves state-of-the-art performance on the ImageNet VID dataset with less computational complexity and real-time speed. Code will be made available at LSTS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection is a fundamental problem in computer vision and enables various applications, e.g., robot vision and autonomous driving. Recently, deep convolution neural networks have achieved significant process on object detection <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>. However, directly applying image object detectors frame-byframe cannot obtain satisfactory results since frames in a video are always deteriorated by part occlusion, rare pose, and motion blur. The inherent temporal information in the video, as the rich cues of motion, can boost the performance of video object detection. Previous efforts on leveraging the temporal information can mainly be divided into two categories. The first one relies on temporal information for postprocessing to ensure the object detection results more coherent <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. These methods usually apply image-detector to obtain detection results, then associate the results via the box-level matching, e.g., tracker or off-the-shelf optical flow. Such post-processing tends to slow down the processing of detection. Another category <ref type="bibr">[17-19, 31, 35, 37, 40-42]</ref> exploits the temporal information on feature representation. Specifically, they mainly improve features by aggregating that of adjacent frames to boost the detection accuracy, or propagating features to avoid dense feature extraction to improve the speed.</p><p>Nevertheless, when propagating the features across frames, optical flow based warping operation is always required. Such operation would introduce the following disadvantages: <ref type="bibr" target="#b0">(1)</ref> Optical flow tends to increase the number of model parameters by a large margin, which makes the applications on embedded devices unfriendly. Take the image detector ResNet101+RFCN <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref> as an example, the parameter size would increase from 60.0M to 96.7M even with the pruned FlowNet <ref type="bibr" target="#b41">[42]</ref>. <ref type="bibr" target="#b1">(2)</ref> Optical flow cannot represent the correspondences among highlevel features accurately. Due to the increase of the receptive field of networks, the small motion drift in the high-level feature always corresponds to large motion movements in the image-level. (3) Optical flow extraction is time-consuming. For example, FlowNet <ref type="bibr" target="#b6">[7]</ref> runs at only 10 frames per second (FPS) on KITTI dataset <ref type="bibr" target="#b10">[11]</ref>, which will hinder the practical application of video detectors.</p><p>To address the above issues, Learnable Spatio-Temporal Sampling (LSTS) module has been proposed to propagate the high-level feature across frames. Such module could learn spatial correspondences across frames itself among the whole datasets. Besides, without the extra optical flow, it allows to speed up the propagation process significantly. Given two frames I t and I t+k and the extracted features F t and F t+k , our proposed LSTS module will firstly samples specific locations. Then, the similarity between the sampled locations on feature F t and feature F t+k would be calculated. Next, the calculated weights together with feature F t are aggregated to produce propagated feature F t+k . At last, the sampling locations would be iteratively updated guided by the final detection supervision, which allows to propagate and align the high-level features across frames more accurately. Based on LSTS, an efficient video object detection framework is also introduced. The features of keyframes and non-keyframes would be extracted by expensive and light-weight network, respectively. To further leverage the temporal relation across whole videos, Sparsely Recursive Feature Updating (SRFU) and Dense Feature Aggregation (DFA) are then proposed to boost the dense low-level features, and enhance the feature representation separately.</p><p>Experiments are conducted on the public video object detection benchmark i.e., ImageNet VID datasets. Without bells and whistles, the proposed framework achieves state-of-the-art performance at the real-time speed and brings much fewer model parameters simultaneously. In addition, elaborate ablative studies show the advance of learnable sampling locations over the hand-crafted design.</p><p>We summarize the major contributions as follows: 1) LSTS module is proposed to propagate the high-level feature across frames, which could calculate the spatial-temporal correspondences accurately. Different from previous approaches, LSTS treat the offsets of sampling locations as parameters and the optimal offsets would be learned through back-propagation guided by bounding box and category supervision. 2) SRFU and DFA module are proposed to model temporal relation and enhance feature representation, respectively. 3) Experiments on VID dataset demonstrate that the proposed framework achieves state-of-the-art trade-off performance between speed and model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image Object Detection. Recently, state-of-the-art methods for image-based detectors are mainly based on the deep convolutional neural networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>. Generally, the image object detectors can be divided into two paradigms, i.e., the single-stage and the two-stage detectors. Two-stage detectors usually first generate region proposals, which are then refined by classification and regression process through the RCNN stage. ROI pooling <ref type="bibr" target="#b14">[15]</ref> was proposed to speed up R-CNN <ref type="bibr" target="#b11">[12]</ref>. Faster RCNN <ref type="bibr" target="#b26">[27]</ref> utilized anchor mechanism to replace Selective Search <ref type="bibr" target="#b32">[33]</ref> proposal generation process, achieving great performance promotion as well as faster speed. FPN <ref type="bibr" target="#b21">[22]</ref> introduced an inherent multi-scale, pyramidal hierarchy of deep convolution networks to build feature pyramids with marginal extra cost and significant improvements. The single-stage detector pipeline is more efficient but achieves less accurate performance. SSD <ref type="bibr" target="#b23">[24]</ref> directly generates results from anchor boxes on a pyramid of feature maps. RetinaNet <ref type="bibr" target="#b22">[23]</ref> handled extreme foreground and background imbalance issue by a novel loss named focal loss. Usually, the image object detector provides the baseline results for video object detection through frame-by-frame detection.</p><p>Video Object Detection. Compared with image object detection, temporal information provides the cue for video object detection, which can be utilized to boost accuracy or efficiency. To improve detection efficiency, a few works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42]</ref> exploited to propagate features across frames to avoid dense expensive feature extraction, which mainly relied on the flow-warping <ref type="bibr" target="#b41">[42]</ref> operation. DFF <ref type="bibr" target="#b41">[42]</ref> was proposed with an efficient framework which only runs expensive CNN feature extraction on sparse and regularly selected keyframes, achieving more than 10x speedup than using an image detector for per-frame detection. Towards High Performance <ref type="bibr" target="#b39">[40]</ref> proposed spare recursive feature aggregation and spatiallyadaptive feature updating strategies, which helps run real-time speed with significant performance. On the one hand, the slow flow extraction process is still the bottleneck for higher speed. On the other hand, the image-level flow which is used to propagate high-level feature may hinder the propagation accuracy, resulting in inferior accuracy.</p><p>To improve detection accuracy, different methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref> have been proposed to aggregate features across frames. They either rely on optical flow to propagate the neighbouring frames' features, or establish spatial-temporal relation to propagate the adjacent frames' features. Then the propagated features from the adjacent frames and current frame feature are aggregated to improve the feature. FGFA <ref type="bibr" target="#b40">[41]</ref> was proposed to aggregate nearby features for each frame. It achieves better accuracy at the cost of slow speed, which only runs on 3 FPS due to dense prediction and heavy flow extraction process. EDN <ref type="bibr" target="#b5">[6]</ref> was proposed to aggregate and propagate object relation to augment object features. SELSA <ref type="bibr" target="#b36">[37]</ref> and LRTR <ref type="bibr" target="#b30">[31]</ref> were proposed to aggregate feature by modeling temporal proposals. Besides, OGEM <ref type="bibr" target="#b3">[4]</ref> utilized object guided external memory network to model the relationship among temporal proposals. However, these methods cannot run real-time due to the multi-frames feature aggregation. Compared with the above works, our proposed method can be much efficient and run at real-time speed.</p><p>Flow-Warping for Feature Propagation. Optical flow <ref type="bibr" target="#b6">[7]</ref> has been widely used to model motion relation across frames in many video-based applications, such as video action recognition <ref type="bibr" target="#b31">[32]</ref> and video object detection <ref type="bibr" target="#b27">[28]</ref>. DFF <ref type="bibr" target="#b41">[42]</ref> is the first work to propagate deep keyframe feature to non-keyframe using flowwarping operation based on tailored optical-flow extraction network, resulting in 10x speedup but inferior performance. However, optical flow extraction is time-consuming, which means that we are also expected to manually design lightweight optical flow extraction network for higher speed, which can be in the price of losing precision. Whats more, it is less robust for feature-level warping using image-level optical flow. Compared with flow-warping based feature Low-Level Feature Extractor</p><formula xml:id="formula_0">0 1 1 + Fig. 2.</formula><p>Framework for inference. For simplicity, frames at t 0 , t 1 (keyframes) and t 1 +k (non-keyframe) would be fed into a high-level and a low-level feature extractor respectively. Based on the high-level features, the memory feature F memory is maintained by SRFU to capture the temporal relation, and updated iteratively at keyframes time step. Meanwhile, DFA propagates the memory feature F memory of keyframes to enhance and enrich the low-level features of non-keyframes. LSTS is embedded in SRFU and DFA to propagate and align features across frames accurately. Both the output of SFRU and DFA is produced by the task network to make the final prediction propagate, our proposed method is much lightweight and can model the correspondences across frames in the feature-level accurately.</p><p>Self-Attention for Feature Propagation. Attention mechanism <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref> is widely studied in computer vision and natural language processing. Self-attention <ref type="bibr" target="#b33">[34]</ref> and non-local <ref type="bibr" target="#b35">[36]</ref> are proposed to model the relation of language sequences and to capture long-range dependencies, respectively. An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all feature maps. Due to the formulation of such attention operation, it can naively be used to model the relation of the features across frames. However, motion across frames is usually limited in a near window, not the whole feature size. Thus MatchTrans <ref type="bibr" target="#b37">[38]</ref> was proposed to propagate the features across frames as a local Non-Local <ref type="bibr" target="#b35">[36]</ref> manner. Even so, the neighbourhood size is still needed to be carefully designed to match the motion distribution of whole datasets. Compared with MatchTrans <ref type="bibr" target="#b37">[38]</ref> module, our proposed LSTS module can adaptively learn the sampling locations, which allows to estimate spatial correspondences across frames more accurately. At the same time, Liu <ref type="bibr" target="#b24">[25]</ref> proposed to learn sampling locations for convolution kernel, which shares core idea with us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework Overview</head><p>In terms of the frame-by-frame detector, frames are divided into two categories i.e., keyframe and non-keyframe. In order to decrease the computational complexity, the feature extractors vary from different types of frames. Specifically, the features of the keyframe and the non-keyframe would derive from the heavy (H) and light (L) feature extraction networks, respectively. In Sec. 3.2, LSTS is proposed to align and propagate the featues across frames. In order to leverage the relation among frames, a memory feature F memory is maintained on keyframes, which is gradually updated by the proposed SRFU module (in Sec. 3.3). Besides, with the lightweight feature extractor network, the low-level features on the non-keyframes are usually not capable to obtain good detection results. Thus, DFA module (in Sec. 3.4) is proposed to improve the low-level features on the non-keyframes by utilizing the memory feature F memory on the keyframes. The pipeline of our framework is illustrated in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learnable Spatio-Temporal Sampling</head><p>After collecting the features F t , F t+k of corresponding frames I t , I t+k , LSTS module allows to calculate the similarity weights of correspondences. As <ref type="figure" target="#fig_2">Fig. 3</ref> shows, the procedure of our LSTS module consists of four steps: 1) It first samples some locations on the feature F t .</p><p>2) The correspondence similarity weights are then calculated on the embedded features f (F t ) and g(F t+k ) by using the sampled locations, where f (?) and g(?) are embedding functions, which aims to reduce the channel of features F t and F t+k to save computational cost. 3) Next, the calculated weights together with feature F t are aggregated to obtain propagated feature F t+k . 4) At last, the sampled locations can be iteratively updated according to final detection loss during training process. Sampling. To propagate features from F t to F t+k accurately, we need accurate spatial correspondences across two frames. Motivated by <ref type="figure" target="#fig_6">Fig. 5</ref>, the correspondences can be limited to the neighbourhood. Thus we first initialize some sampled locations on the neighborhood, which provides coarse correspondences. Besides, with the ability of learning, LSTS can shift and scale the distribution of sampled locations progressively to establish spatial correspondences more accurately. Uniform and Gaussian distribution are applied as two kinds of initialization methods, which will be discussed in detail in the experimental section.</p><p>Comparison. With the correspondence locations, the similarity of them would be calculated. To save the computational cost, features F t and F t+k are embedded to f (F t ) and g(F t+k ), respectively, where f and g are the embedding function. Then the correspondence similarity weights are calculated based on the embedded features f (F t ) and g(F t+k ). As <ref type="figure" target="#fig_2">Fig. 3</ref> shows, p 0 denotes the specific grid location (yellow square) on the feature map g( g(F t+k ) p0 denote the features at the location p n from f (F t ) and at location p 0 from g(F t+k ), respectively. We aims to calculate the similarity weight between each f (F t ) pn and g(F t+k ) p0 .</p><p>Considering p n may be in the arbitrary location on the feature map, f (F t ) pn firstly requires the bilinear interpolation operation following</p><formula xml:id="formula_1">f (F t ) pn = q G(p n , q) ? f (F t ) q .<label>(1)</label></formula><p>Here, q enumerates all integral spatial locations on the feature map f (F t ), and G(?, ?) is the bilinear interpolation kernel function as in <ref type="bibr" target="#b2">[3]</ref>. After obtaining the value of f (F t ) pn , we use similarity function Sim to measure the distance between the vector f (F t ) pn and the vector g(F t+k ) p0 . Suppose that both f (F t ) pn and f (g t+k ) p0 are c dimensional vectors, then we have the similarity value s(p n ):</p><formula xml:id="formula_2">s(p n ) = Sim(f (F t ) pn , g(F t+k ) p0 ).<label>(2)</label></formula><p>A very common function Sim can be dot-product. After getting all similarity value s(p n ) on the location p n , then the normalized similarity weights can be calculated by:</p><formula xml:id="formula_3">S(p n ) = s(p n ) N n=1 s(p n )</formula><p>. Aggregation. After obtaining each of the calculated correspondence similarity weights S(p n ) on location p n , then the estimated value on location p 0 for F t+k can be calculated as:</p><formula xml:id="formula_4">F t+k (p 0 ) = N n=1 S(p n ) ? F t (p n ).<label>(4)</label></formula><p>Updating. In order to learn the ground truth distribution of correspondences, the sampled locations are also updated by the back-propagation during training. We use the dot-product for function Sim for simplicity. Then we have:</p><formula xml:id="formula_5">s(p n ) = q G(p n , q) ? f (F t ) q ? g(F t+k ) p0 .<label>(5)</label></formula><p>Thus, the gradients for location p n can be calculated by:</p><formula xml:id="formula_6">?s(p n ) ?p n = q ?G(p n , q) ?p n ? f (F t ) q ? g(F t+k ) p0 .<label>(6)</label></formula><p>?G(pn,q) ?p l can be easily calculated due to the function G(., .) is bilinear interpolation kernel. According to the above gradient calculation in Eq. 6, the sampled location p n will be iteratively updated according to final detection loss, which allows the learned sampling locations progressively match the ground truth distribution of correspondences. After training, the final learned sampling locations could be applied to propagate and align features across frames during the inference process. And LSTA is the core of SRFU and DFA, which would be introduced next in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sparsely Recursive Feature Updating</head><p>SRFU module allows to leverage the inherent temporal cues insides videos to propagate and aggregate high-level features of sparse keyframes over the whole video. Specifically, SRFU module maintains and recursively updates a temporal feature F memory over the whole video. As shown in <ref type="figure" target="#fig_3">Fig. 4(a)</ref>, during this procedure, directly updating the memory feature by the new keyframes feature F t 1 is sub-optimal due to the motion misalignment during keyframes t 0 and t 1 . Thus, our LSTS module could estimate the motion and generate the aligned feature F align t 1</p><p>. After that, an aggregation unit is proposed to generate the updated memory feature F memory t 1</p><p>. Specially, the concatenation of F t 1 and F align t 1 would be fed into a several layers of convolutions with a softmax operation to produce the corresponding aggregation weights W t 1 and W align</p><formula xml:id="formula_7">t 1 , where W t 1 +W align t 1 = 1. F memory t 1 = W t 1 F t 1 + W align t 1 F align t 1 .<label>(7)</label></formula><p>Then the memory feature on the keyframes t 1 can be updated by Eq. 7, where is the Hadamard product (i.e. element-wise multiplication) after broadcasting the weight maps. And the memory feature F memory t 1 together with F t 1 would be aggregated to generate the task feature for the keyframes. To validate the effectiveness of proposed SRFU, we divide SRFU into Sparse Deep Feature Extraction, Keyframe Memory Update and Quality-Aware Memory Update. Each component of SRFU module will be explained and discussed in detail in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dense Feature Aggregation</head><p>Considering the computational complexity, lightweight feature extractor networks are utilized for the non-keyframes, which would extract the low-level features. Thus, DFA module allows to reuse the sparse high-level features of keyframe to improve the quality of that of the non-keyframes. Specifically, as shown in <ref type="figure" target="#fig_3">Fig. 4(b)</ref>, the non-keyframes feature F low into Non-Keyframe Transform, Non-Keyframe Aggregation and Quality-Aware Non-Keyframe Aggregation. Each component of DFA module will be explained and discussed in detail in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>We evaluate our method on the ImageNet VID dataset, which is the benchmark for video object detection <ref type="bibr" target="#b27">[28]</ref>. And the ImageNet VID dataset is composed of 3862 training videos and 555 validation videos containing 30 object categories. All videos are fully annotated with bounding boxes and tracking IDs. And mean average precision (mAP) is used as the evaluation metric following the previous methods <ref type="bibr" target="#b41">[42]</ref>.</p><p>The ImageNet VID dataset has extreme redundancy among video frames, which prevents efficient training. At the same time, video frames of the ImageNet VID dataset have poorer quality than images in the ImageNet DET <ref type="bibr" target="#b27">[28]</ref> dataset. So, we follow the previous method <ref type="bibr" target="#b39">[40]</ref> to use both ImageNet VID and DET dataset for training. For the ImageNet DET set, only the same 30 class categories of ImageNet VID are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Detail</head><p>For the training process, each mini-batch contains three images. In both the training and testing stage, the shorter side of the images will be resized to 600 pixels <ref type="bibr" target="#b26">[27]</ref>. Feature before conv4 3 will be treated as Low-Level Feature Extractor. The whole ResNet will be used for High-Level Feature Extractor. Following the setting of most previous methods, the R-FCN detector <ref type="bibr" target="#b1">[2]</ref> pretrained on Im-ageNet <ref type="bibr" target="#b4">[5]</ref> with ResNet-101 <ref type="bibr" target="#b15">[16]</ref> serves as the single-frame detector. During the training stage, we adopt OHEM strategy <ref type="bibr" target="#b29">[30]</ref> and horizontal flip data augmentation. In our experiment, each GPU will hold one sample, namely three images sampled from one video or repetition of the static image. We train our network on an 8-GPUs machine for 4 epochs with SGD optimization, starting with a learning rate of 2.5e-4 and reducing it by a factor of 10 at every 2.33 epochs. The keyframe interval is 10 frames in default, as in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. Aggregation Unit. The aggregation weights of the features are generated by a quality estimation network. It has three randomly initialized layers: a 3 ? 3 ? 256 convolution, a 1 ? 1 ? 16 convolution and a 1 ? 1 ? 1 convolutions. The output is position-wise raw score map which will be applied on each channel of corresponding features. The normalized weights and the features are fused to obtain the aggregated result. Transform. To reduce the computational complexity, we only extract low-level features for the non-keyframes, which is a lack of high-level semantic information. A lightweight neural convolution unit containing 3 randomly initialized layers: a 3 ? 3 ? 256 convolution, a 3 ? 3 ? 512 convolution and a 3 ? 3 ? 1024 convolutions has been utilized to compensate the semantic information. <ref type="table">Table 1</ref>. Performance comparison with the state-of-the-arts on ImageNet VID validation set. In terms of both accuracy and speed, Our method outperforms the most of them and has fewer parameters than the existing optical flow-based models. V means that the speed is tested on TITAN V GPU</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Online mAP (%) Runtime(FPS) #Params(M) Backbone TCN <ref type="bibr" target="#b20">[21]</ref> 47.5 --GoogLeNet TPN+LSTM <ref type="bibr" target="#b19">[20]</ref> 68.4 2.1 -GoogLeNet R-FCN <ref type="bibr" target="#b1">[2]</ref> 73.9 4.05 60.0 ResNet-101 DFF <ref type="bibr" target="#b41">[42]</ref> 73.1 20.25 97.8 ResNet-101 D (&amp;T loss) <ref type="bibr" target="#b7">[8]</ref> 75.8 7.8 -ResNet-101 LWDN <ref type="bibr" target="#b17">[18]</ref> 76.3 20 77.5</p><p>ResNet-101 FGFA <ref type="bibr" target="#b40">[41]</ref> 76.3 1.4 100.5</p><p>ResNet-101 ST-lattice <ref type="bibr" target="#b0">[1]</ref> 79.5 20 -ResNet-101 MANet <ref type="bibr" target="#b34">[35]</ref> 78.1 5.0 -ResNet-101 OGEMNet <ref type="bibr" target="#b3">[4]</ref> 76.8 14.9 -ResNet-101 Towards <ref type="bibr" target="#b39">[40]</ref> 78.6 13.0 -ResNet-101 + DCN RDN <ref type="bibr" target="#b5">[6]</ref> 81.8 10.6(V) -ResNet-101 SELSA <ref type="bibr" target="#b36">[37]</ref> 80.3 --ResNet-101 LRTR <ref type="bibr" target="#b30">[31]</ref> 80. <ref type="bibr" target="#b5">6</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We compare our method with existing state-of-the-art image and video object detectors. The results are shown in <ref type="table">Table 1</ref>. From the table, we can make the following conclusion. First of all, our method outperforms most previous approaches considering the speed and accuracy trade-off. Secondly, our proposed approach has fewer parameter comparing with flow-warping based method. Without external optical flow network, our approach can significantly simplify the overall detection framework. Lastly, the results indicate that our LSTS module can learn feature correspondences between consecutive video frames more precise than optical flow-warping based methods. To conclude, our detector surpasses the static image-based R-FCN detector with a large margin (+3.3%) while maintaining high speed (23.0FPS). Furthermore, the parameter count (64.5M) is fewer than other video object detectors using an optical flow network (e.g., around 100M), which also indicates that our method is more friendly for mobile devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>We conduct ablation studies on ImageNet VID dataset to demonstrate the effectiveness of proposed LSTS module and the proposed framework. We first introduce the configuration of each element in our proposed framework for ablation studies. Then we compare our LSTS with both optical flow and existing non-optical flow alternatives. Finally, we conduct ablation studies of different modules in our framework. Effectiveness of the proposed framework. We first describe each component about proposed SRFU and DFA. Then we compare our method with optical flowwarping based method under different configurations to validate the effectiveness of our proposed LSTS module. Each component of SRFU and DFA is listed following:</p><p>-Sparse Deep Feature Extraction. The entire backbone network is used to extract feature only on keyframes. -Keyframe Memory Update. The keyframe feature aggregates with the last keyframe memory to generate the task feature and updated memory feature (see <ref type="figure" target="#fig_3">Fig. 4(a)</ref>). The weights are naively fixed to 0.5. -Quality-Aware Memory Update. The keyframe feature aggregates with the last keyframe memory to generate the task feature and updated memory feature using a quality-aware aggregation unit. -Non-Keyframe Transform. We apply a transform unit on the low-level feature to generate a high-level semantic feature on the non-keyframe. -Non-Keyframe Aggregation. The task feature for the non-keyframe is naively aggregated with an aligned feature from keyframes, and the current low-level feature is obtained by a part of the backbone network. -Quality-Aware Non-Keyframe Aggregation. The task feature for the nonkeyframe is aggregated with an aligned feature from the keyframe using a quality-aware aggregation unit, and the current high-level feature is obtained through a transform unit. Our frame-by-frame baseline achieves 74.1% mAP and runs at 10.1FPS. After using the sparse deep feature, we have 73.5% mAP and runs at 23.8FPS. When applying the quality-aware keyframe memory propagation, we have 75.9% mAP and runs at 23.5FPS with 64.0 M parameters. Last, non-keyframe qualityaware aggregation can also improve performance which achieves 76.4% mAP at 23.3FPS. By adding quality-aware memory aggregation, non-keyframe transformer unit, and quality-aware non-keyframe aggregation, our approach can achieve 77.2% mAP and run 23.0FPS with 64.5M parameters. Comparison with Optical Flow-Based Method. Optical flow can predict motion field between consecutive frames. DFF <ref type="bibr" target="#b41">[42]</ref> proposed to propagate feature across frames by using flow-warping operation. To validate the effectiveness of LSTS on estimating spatial correspondences, we make a detailed comparison with optical flow. The results can be seen as in <ref type="table">Table.</ref> 2. Our proposed LSTS can outperform optical flow on all settings with fewer model parameters. Comparison with Non-Optical Flow-Based Method. The results of using different feature propagation methods are listed in <ref type="table">Table.</ref> 3. By attending on the local region, our method outperforms the Non-Local by a large margin. The reason is that the motion distribution is limited to the near center, as shown in 5. Our method can surpass both the MatchTrans and Non-Local a lot, which show the effectiveness of LSTS. Learnable Sampled Locations. To demonstrate the effectiveness of learning sampled locations, we perform ablation study on two different initialization methods, Uniform Initialization and Gaussian Initialization.</p><p>The first one is just like MatchTrans <ref type="bibr" target="#b37">[38]</ref> module with the neighbourhoods are set to 4. While the second is two-dimensional Gaussian Distribution with zero means and one variance The results of different initialization settings can be seen in <ref type="table">Table 4</ref>. We can figure out, no matter what the initialization meth-   For the dataset distribution, we random sample 100 videos from the training dataset, then calculate motion across frames by FlowNet <ref type="bibr" target="#b6">[7]</ref>. To verify the effectiveness of learnable spatial-temporal sampling, we also compare the learned offset distribution with the initialized Gaussian distribution.</p><p>ods are, there is a consistent trend that the performance can be significantly boosted by learning sampled locations. To be more specific, Gaussian initialization can achieve 77.2% mAP. Comparing with the fixed Gaussian initialization 75.5%, learnable sampling locations could obtain 1.7% mAP improvement. And Uniform initialization can achieve 76.8% mAP. Comparing with the fixed Uniform initialization 75.5%. learnable sampling locations could obtain 1.3% mAP improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have proposed a novel module, Learnable Spatio-Temporal Sampling (LSTS), which could estimate spatial correspondences across frames accurately. Based on this module, Sparsely Recursive Feature Updating (SRFU) and Dense Feature Aggregation (DFA) are proposed to model the temporal relation and enhance the features on the non-keyframes, respectively. Elaborate ablative studies have shown the advancement of our LSTS module and architecture design. Without any whistle and bell, the proposed framework has achieved state-of-the-art performance (82.1% mAP) on ImageNet VID dataset. We hope the proposed differential paradigm could extend to more tasks, such as sampling locations for general convolution operation, sampling locations of aggregating features for semantic segmentation, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>This research was supported by the Major Project for New Generation of AI under Grant No. 2018AAA0100400, the National Natural Science Foundation of China under Grants 91646207, 61976208 and 61620106003. We also would like to thank Lin Song for the discussions and suggestions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Comparison between flow-warping with LSTS for feature propagation. Ft and F t+k denote features extracted from two adjacent frames It and I t+k , respectively. Previous work directly applies optical flow to represent the feature-level shift while our LSTS could learn more accurate correspondences from data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FFig. 3 .</head><label>3</label><figDesc>t+k ). The sampled correspondence locations (blue square) on f (F t ) are denoted as p n , where n = 1, ..., N , and N is the number of the sampled locations. Let f (F t ) pn and Illustration of LSTS module. LSTS basically consists of 4 steps: 1) some locations on the feature are randomly sampled. 2) The affinity weight is calculated by similarity comparison. 3) Next, the features Ft together with weights will be aggregated to obtain features F t+k . 4) the locations would be updated iteratively by back-propagation during training. After training, the final learned sampling locations would be used for inference</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Architecture of SRFU and DFA. is the Aggregation Unit. Transform unit only consists of several convolutions, which is used to improve low-level features on the non-keyframes. For SRFU, LSTS module is utilized to aggregate last keyframe F memory t 0 to current keyframe t 1 . While for DFA, LSTS module aims to propagate the keyframe memory feature F memory t 1 to non-keyframe t 1 + k to boost the feature quality to obtain better detection results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>The comparison of offset distribution on the horizontal and vertical between ours and the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies on accuracy, runtime and complexity between ours and flow-warping methods. ? belong to SRFU and ? belong to DFA</figDesc><table><row><cell>Architecture Component</cell><cell>(a) (b) (c) (d) (e) (f) (g)</cell></row><row><cell>Sparse Deep Feature Extraction  ?</cell><cell></cell></row><row><cell>Keyframe Memory Update  ?</cell><cell></cell></row><row><cell>Quality-Aware Memory Update  ?</cell><cell></cell></row><row><cell>Non-Keyframe Aggregation  ?</cell><cell></cell></row><row><cell>Non-Keyframe Transformer  ?</cell><cell></cell></row><row><cell>Quality-Aware Non-Keyframe Aggregation  ?</cell><cell></cell></row><row><cell>Optical flow</cell><cell></cell></row><row><cell>mAP(%)</cell><cell>73.0 75.2 75.4 75.5 75.7 75.9 76.1</cell></row><row><cell>Runtime(FPS)</cell><cell>29.4 29.4 29.4 19.2 18.9 19.2 18.9</cell></row><row><cell>#Params(M)</cell><cell>96.7 96.7 97.0 100.3 100.4 100.4 100.5</cell></row><row><cell>Ours</cell><cell></cell></row><row><cell>mAP(%)</cell><cell>73.5 75.8 75.9 76.4 76.5 76.8 77.2</cell></row><row><cell>Runtime(FPS)</cell><cell>23.8 23.5 23.5 23.3 23.0 23.3 23.0</cell></row><row><cell>#Params(M)</cell><cell>63.8 63.7 64.0 64.3 64.4 64.4 64.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparisons with MatchTrans<ref type="bibr" target="#b37">[38]</ref> and Non-Local<ref type="bibr" target="#b35">[36]</ref> </figDesc><table><row><cell>Method</cell><cell cols="3">mAP (%) Runtime(FPS) #Params(M)</cell></row><row><cell>Non-Local</cell><cell>74.2</cell><cell>25.0</cell><cell>64.5</cell></row><row><cell>MatchTrans</cell><cell>75.5</cell><cell>24.1</cell><cell>64.5</cell></row><row><cell>Ours</cell><cell>77.2</cell><cell>23.0</cell><cell>64.5</cell></row><row><cell cols="4">Table 4. Comparisons of LSTS with different initialization methods</cell></row><row><cell cols="4">Method Learning mAP(%) Runtime(FPS) #Params(M)</cell></row><row><cell>Uniform</cell><cell>75.5 76.8</cell><cell>21.7 21.7</cell><cell>64.5 64.5</cell></row><row><cell>Gaussian</cell><cell>75.5 77.2</cell><cell>23.0 23.0</cell><cell>64.5 64.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t 1 +k would be fed into a Transform unit which only brings few computation cost to predict the semantic-level feature F high t 1 +k . Due to the motion misalignment between the time step of t 1 and t 1 + k, the proposed LSTS module is applied on the keyframe memory feature F memory t 1 to generate the aligned feature F align t 1 +k . After obtaining F align t 1 +k , an aggregation unit is utilized to predict the aggregation weights W align t 1 +k and W high t 1 +k , where W align t 1 +k + W high t 1 +k = 1.F task t 1 +k = W align t 1 +k F align t 1 +k + W high t 1 +k F high t 1 +k .(8)Finally, the task feature F task t 1 +k on the non-keyframe t 1 + k can be calculated in Eq. 8, where is the Hadamard product (i.e. element-wise multiplication) after broadcasting the weight maps. Comparing with low-level feature F low t 1 +k , F task t 1 +k contains more semantic-level information and allows to obtain good detection results. To validate the effectiveness of proposed DFA, we divide DFA</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optimizing video object detection via a scale-time lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7814" to="7823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurPIS. pp</title>
		<imprint>
			<biblScope unit="page" from="379" to="387" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Object guided external memory network for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6678" to="6687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Relation distillation networks for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7023" to="7032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3038" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dynamic fusion with intra-and inter-modality attention flow for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="6639" to="6648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Question-guided hybrid convolution for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="469" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.08465</idno>
		<title level="m">Seq-nms for video object detection</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Impression network for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hetang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Video object detection with locally-weighted deformable neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning motion priors for efficient video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05253</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object detection in videos with tubelet proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="727" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object detection from video tubelets with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="817" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Differentiable kernel evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1834" to="1843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurPIS. pp</title>
		<imprint>
			<biblScope unit="page" from="2204" to="2212" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurPIS. pp</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04119</idno>
		<title level="m">Action recognition using visual attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Leveraging long-range temporal relationships between proposals for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9756" to="9764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurPIS. pp</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Fully motion-aware network for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="542" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<title level="m">Non-local neural networks. In: CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Sequence level semantics aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9217" to="9225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Video object detection with an aligned spatial-temporal memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="485" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards high performance video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7210" to="7218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="408" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2349" to="2358" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
